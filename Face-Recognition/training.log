Training: 2023-05-04 13:56:23,311-rank_id: 0
Training: 2023-05-04 13:59:43,608-: loss                     arcface
Training: 2023-05-04 13:59:43,609-: network                  r100
Training: 2023-05-04 13:59:43,609-: resume                   True
Training: 2023-05-04 13:59:43,609-: output                   work_dirs/ms1mv3_r100_lr02
Training: 2023-05-04 13:59:43,609-: embedding_size           512
Training: 2023-05-04 13:59:43,609-: sample_rate              1.0
Training: 2023-05-04 13:59:43,609-: fp16                     True
Training: 2023-05-04 13:59:43,609-: momentum                 0.9
Training: 2023-05-04 13:59:43,609-: weight_decay             0.0005
Training: 2023-05-04 13:59:43,609-: batch_size               256
Training: 2023-05-04 13:59:43,609-: lr                       0.02
Training: 2023-05-04 13:59:43,609-: dali                     False
Training: 2023-05-04 13:59:43,609-: verbose                  10000
Training: 2023-05-04 13:59:43,609-: frequent                 10
Training: 2023-05-04 13:59:43,609-: score                    None
Training: 2023-05-04 13:59:43,609-: rec                      ../../../../InsightFace_Pytorch/data/faces_emore/
Training: 2023-05-04 13:59:43,609-: num_classes              85742
Training: 2023-05-04 13:59:43,609-: num_image                5822653
Training: 2023-05-04 13:59:43,609-: num_epoch                25
Training: 2023-05-04 13:59:43,609-: warmup_epoch             0
Training: 2023-05-04 13:59:43,609-: val_targets              ['lfw', 'cfp_fp', 'agedb_30', 'calfw', 'cplfw']
Training: 2023-05-04 13:59:43,609-: warmup_step              0
Training: 2023-05-04 13:59:43,609-: total_step               142150
Training: 2023-05-04 14:02:08,086-Reducer buckets have been rebuilt in this iteration.
Training: 2023-05-04 14:02:18,322-Speed 1317.27 samples/sec  Loss 126.6422  LearningRate 0.1999  ProxyLR: 9.9972  Epoch: 0  Global Step: 20   Fp16 Grad Scale: 16384  Required: 46 hours
Training: 2023-05-04 14:02:27,276-Speed 1143.69 samples/sec  Loss 122.6271  LearningRate 0.1999  ProxyLR: 9.9958  Epoch: 0  Global Step: 30   Fp16 Grad Scale: 16384  Required: 42 hours
Training: 2023-05-04 14:02:35,352-Speed 1268.03 samples/sec  Loss 123.1818  LearningRate 0.1999  ProxyLR: 9.9944  Epoch: 0  Global Step: 40   Fp16 Grad Scale: 8192  Required: 40 hours
Training: 2023-05-04 14:02:43,454-Speed 1264.03 samples/sec  Loss 121.2799  LearningRate 0.1999  ProxyLR: 9.9930  Epoch: 0  Global Step: 50   Fp16 Grad Scale: 8192  Required: 38 hours
Training: 2023-05-04 14:02:51,666-Speed 1247.10 samples/sec  Loss 120.0369  LearningRate 0.1998  ProxyLR: 9.9916  Epoch: 0  Global Step: 60   Fp16 Grad Scale: 8192  Required: 37 hours
Training: 2023-05-04 14:03:13,524-Speed 468.47 samples/sec  Loss 118.9506  LearningRate 0.1998  ProxyLR: 9.9902  Epoch: 0  Global Step: 70   Fp16 Grad Scale: 8192  Required: 44 hours
Training: 2023-05-04 14:03:34,564-Speed 486.73 samples/sec  Loss 119.0516  LearningRate 0.1998  ProxyLR: 9.9887  Epoch: 0  Global Step: 80   Fp16 Grad Scale: 8192  Required: 49 hours
Training: 2023-05-04 14:03:54,493-Speed 513.84 samples/sec  Loss 117.8895  LearningRate 0.1997  ProxyLR: 9.9873  Epoch: 0  Global Step: 90   Fp16 Grad Scale: 8192  Required: 52 hours
Training: 2023-05-04 14:04:11,102-Speed 616.54 samples/sec  Loss 117.6045  LearningRate 0.1997  ProxyLR: 9.9859  Epoch: 0  Global Step: 100   Fp16 Grad Scale: 8192  Required: 54 hours
Training: 2023-05-04 14:04:27,571-Speed 621.82 samples/sec  Loss 117.1603  LearningRate 0.1997  ProxyLR: 9.9845  Epoch: 0  Global Step: 110   Fp16 Grad Scale: 8192  Required: 55 hours
Training: 2023-05-04 14:04:41,069-Speed 758.62 samples/sec  Loss 116.3039  LearningRate 0.1997  ProxyLR: 9.9831  Epoch: 0  Global Step: 120   Fp16 Grad Scale: 8192  Required: 54 hours
Training: 2023-05-04 14:04:51,833-Speed 951.43 samples/sec  Loss 115.4128  LearningRate 0.1996  ProxyLR: 9.9817  Epoch: 0  Global Step: 130   Fp16 Grad Scale: 8192  Required: 54 hours
Training: 2023-05-04 14:05:00,027-Speed 1249.71 samples/sec  Loss 113.9625  LearningRate 0.1996  ProxyLR: 9.9803  Epoch: 0  Global Step: 140   Fp16 Grad Scale: 16384  Required: 52 hours
Training: 2023-05-04 14:05:07,663-Speed 1341.12 samples/sec  Loss 112.5030  LearningRate 0.1996  ProxyLR: 9.9789  Epoch: 0  Global Step: 150   Fp16 Grad Scale: 16384  Required: 51 hours
Training: 2023-05-04 14:05:15,055-Speed 1385.41 samples/sec  Loss 111.8444  LearningRate 0.1996  ProxyLR: 9.9775  Epoch: 0  Global Step: 160   Fp16 Grad Scale: 16384  Required: 49 hours
Training: 2023-05-04 14:05:22,193-Speed 1434.66 samples/sec  Loss 110.6542  LearningRate 0.1995  ProxyLR: 9.9761  Epoch: 0  Global Step: 170   Fp16 Grad Scale: 16384  Required: 48 hours
Training: 2023-05-04 14:05:28,681-Speed 1578.45 samples/sec  Loss 110.4047  LearningRate 0.1995  ProxyLR: 9.9747  Epoch: 0  Global Step: 180   Fp16 Grad Scale: 16384  Required: 47 hours
Training: 2023-05-04 14:05:34,858-Speed 1658.05 samples/sec  Loss 108.5218  LearningRate 0.1995  ProxyLR: 9.9733  Epoch: 0  Global Step: 190   Fp16 Grad Scale: 16384  Required: 46 hours
Training: 2023-05-04 14:05:40,636-Speed 1772.55 samples/sec  Loss 107.7956  LearningRate 0.1994  ProxyLR: 9.9719  Epoch: 0  Global Step: 200   Fp16 Grad Scale: 16384  Required: 44 hours
Training: 2023-05-04 14:05:46,044-Speed 1893.45 samples/sec  Loss 107.1367  LearningRate 0.1994  ProxyLR: 9.9705  Epoch: 0  Global Step: 210   Fp16 Grad Scale: 16384  Required: 43 hours
Training: 2023-05-04 14:05:51,741-Speed 1797.79 samples/sec  Loss 106.2695  LearningRate 0.1994  ProxyLR: 9.9691  Epoch: 0  Global Step: 220   Fp16 Grad Scale: 16384  Required: 42 hours
Training: 2023-05-04 14:05:57,296-Speed 1843.52 samples/sec  Loss 105.4606  LearningRate 0.1994  ProxyLR: 9.9677  Epoch: 0  Global Step: 230   Fp16 Grad Scale: 16384  Required: 42 hours
Training: 2023-05-04 14:06:02,967-Speed 1805.89 samples/sec  Loss 104.1416  LearningRate 0.1993  ProxyLR: 9.9663  Epoch: 0  Global Step: 240   Fp16 Grad Scale: 32768  Required: 41 hours
Training: 2023-05-04 14:06:09,438-Speed 1582.66 samples/sec  Loss 103.5161  LearningRate 0.1993  ProxyLR: 9.9649  Epoch: 0  Global Step: 250   Fp16 Grad Scale: 32768  Required: 40 hours
Training: 2023-05-04 14:06:14,280-Speed 2114.85 samples/sec  Loss 103.5792  LearningRate 0.1993  ProxyLR: 9.9635  Epoch: 0  Global Step: 260   Fp16 Grad Scale: 32768  Required: 39 hours
Training: 2023-05-04 14:06:19,600-Speed 1925.13 samples/sec  Loss 102.3509  LearningRate 0.1992  ProxyLR: 9.9620  Epoch: 0  Global Step: 270   Fp16 Grad Scale: 32768  Required: 39 hours
Training: 2023-05-04 14:06:24,817-Speed 1963.25 samples/sec  Loss 101.6954  LearningRate 0.1992  ProxyLR: 9.9606  Epoch: 0  Global Step: 280   Fp16 Grad Scale: 32768  Required: 38 hours
Training: 2023-05-04 14:06:29,874-Speed 2025.14 samples/sec  Loss 100.1343  LearningRate 0.1992  ProxyLR: 9.9592  Epoch: 0  Global Step: 290   Fp16 Grad Scale: 32768  Required: 37 hours
Training: 2023-05-04 14:06:35,106-Speed 1957.48 samples/sec  Loss 98.6161  LearningRate 0.1992  ProxyLR: 9.9578  Epoch: 0  Global Step: 300   Fp16 Grad Scale: 32768  Required: 37 hours
Training: 2023-05-04 14:06:40,481-Speed 1905.10 samples/sec  Loss 97.6632  LearningRate 0.1991  ProxyLR: 9.9564  Epoch: 0  Global Step: 310   Fp16 Grad Scale: 32768  Required: 36 hours
Training: 2023-05-04 14:06:46,285-Speed 1764.69 samples/sec  Loss 97.8592  LearningRate 0.1991  ProxyLR: 9.9550  Epoch: 0  Global Step: 320   Fp16 Grad Scale: 32768  Required: 36 hours
Training: 2023-05-04 14:06:51,429-Speed 1990.81 samples/sec  Loss 97.9171  LearningRate 0.1991  ProxyLR: 9.9536  Epoch: 0  Global Step: 330   Fp16 Grad Scale: 16384  Required: 35 hours
Training: 2023-05-04 14:06:56,269-Speed 2115.85 samples/sec  Loss 97.8506  LearningRate 0.1990  ProxyLR: 9.9522  Epoch: 0  Global Step: 340   Fp16 Grad Scale: 4096  Required: 35 hours
Training: 2023-05-04 14:07:01,034-Speed 2149.54 samples/sec  Loss 97.1422  LearningRate 0.1990  ProxyLR: 9.9508  Epoch: 0  Global Step: 350   Fp16 Grad Scale: 4096  Required: 34 hours
Training: 2023-05-04 14:07:05,160-Speed 2482.03 samples/sec  Loss 94.5269  LearningRate 0.1990  ProxyLR: 9.9494  Epoch: 0  Global Step: 360   Fp16 Grad Scale: 4096  Required: 34 hours
Training: 2023-05-04 14:07:09,609-Speed 2302.18 samples/sec  Loss 94.9382  LearningRate 0.1990  ProxyLR: 9.9480  Epoch: 0  Global Step: 370   Fp16 Grad Scale: 4096  Required: 34 hours
Training: 2023-05-04 14:07:13,843-Speed 2418.53 samples/sec  Loss 94.7380  LearningRate 0.1989  ProxyLR: 9.9466  Epoch: 0  Global Step: 380   Fp16 Grad Scale: 4096  Required: 33 hours
Training: 2023-05-04 14:07:18,724-Speed 2098.45 samples/sec  Loss 93.3714  LearningRate 0.1989  ProxyLR: 9.9452  Epoch: 0  Global Step: 390   Fp16 Grad Scale: 4096  Required: 33 hours
Training: 2023-05-04 14:07:23,749-Speed 2038.15 samples/sec  Loss 92.3023  LearningRate 0.1989  ProxyLR: 9.9438  Epoch: 0  Global Step: 400   Fp16 Grad Scale: 4096  Required: 32 hours
Training: 2023-05-04 14:07:27,967-Speed 2428.08 samples/sec  Loss 91.2454  LearningRate 0.1988  ProxyLR: 9.9424  Epoch: 0  Global Step: 410   Fp16 Grad Scale: 4096  Required: 32 hours
Training: 2023-05-04 14:07:32,000-Speed 2539.07 samples/sec  Loss 90.5556  LearningRate 0.1988  ProxyLR: 9.9410  Epoch: 0  Global Step: 420   Fp16 Grad Scale: 4096  Required: 32 hours
Training: 2023-05-04 14:07:35,959-Speed 2586.94 samples/sec  Loss 88.9856  LearningRate 0.1988  ProxyLR: 9.9396  Epoch: 0  Global Step: 430   Fp16 Grad Scale: 4096  Required: 31 hours
Training: 2023-05-04 14:07:40,520-Speed 2245.69 samples/sec  Loss 88.0774  LearningRate 0.1988  ProxyLR: 9.9382  Epoch: 0  Global Step: 440   Fp16 Grad Scale: 8192  Required: 31 hours
Training: 2023-05-04 14:07:44,469-Speed 2592.91 samples/sec  Loss 89.3417  LearningRate 0.1987  ProxyLR: 9.9368  Epoch: 0  Global Step: 450   Fp16 Grad Scale: 8192  Required: 31 hours
Training: 2023-05-04 14:07:49,120-Speed 2202.36 samples/sec  Loss 87.4185  LearningRate 0.1987  ProxyLR: 9.9354  Epoch: 0  Global Step: 460   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:07:55,246-Speed 1671.82 samples/sec  Loss 85.6462  LearningRate 0.1987  ProxyLR: 9.9340  Epoch: 0  Global Step: 470   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:05,956-Speed 956.13 samples/sec  Loss 84.3875  LearningRate 0.1987  ProxyLR: 9.9326  Epoch: 0  Global Step: 480   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:12,217-Speed 1635.83 samples/sec  Loss 83.0555  LearningRate 0.1986  ProxyLR: 9.9312  Epoch: 0  Global Step: 490   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:18,694-Speed 1581.30 samples/sec  Loss 82.9236  LearningRate 0.1986  ProxyLR: 9.9298  Epoch: 0  Global Step: 500   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:23,638-Speed 2071.39 samples/sec  Loss 82.0308  LearningRate 0.1986  ProxyLR: 9.9284  Epoch: 0  Global Step: 510   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:28,457-Speed 2125.21 samples/sec  Loss 81.1895  LearningRate 0.1985  ProxyLR: 9.9270  Epoch: 0  Global Step: 520   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:33,917-Speed 1875.57 samples/sec  Loss 80.2355  LearningRate 0.1985  ProxyLR: 9.9256  Epoch: 0  Global Step: 530   Fp16 Grad Scale: 8192  Required: 30 hours
Training: 2023-05-04 14:08:39,922-Speed 1705.61 samples/sec  Loss 78.8155  LearningRate 0.1985  ProxyLR: 9.9242  Epoch: 0  Global Step: 540   Fp16 Grad Scale: 16384  Required: 30 hours
Training: 2023-05-04 14:08:46,651-Speed 1521.91 samples/sec  Loss 77.5447  LearningRate 0.1985  ProxyLR: 9.9228  Epoch: 0  Global Step: 550   Fp16 Grad Scale: 16384  Required: 29 hours
Training: 2023-05-04 14:08:50,893-Speed 2414.44 samples/sec  Loss 76.8630  LearningRate 0.1984  ProxyLR: 9.9214  Epoch: 0  Global Step: 560   Fp16 Grad Scale: 16384  Required: 29 hours
Training: 2023-05-04 14:08:55,229-Speed 2361.86 samples/sec  Loss 76.5904  LearningRate 0.1984  ProxyLR: 9.9200  Epoch: 0  Global Step: 570   Fp16 Grad Scale: 4096  Required: 29 hours
Training: 2023-05-04 14:08:59,388-Speed 2463.04 samples/sec  Loss 75.9820  LearningRate 0.1984  ProxyLR: 9.9186  Epoch: 0  Global Step: 580   Fp16 Grad Scale: 4096  Required: 29 hours
Training: 2023-05-04 14:09:03,653-Speed 2401.38 samples/sec  Loss 74.3448  LearningRate 0.1983  ProxyLR: 9.9172  Epoch: 0  Global Step: 590   Fp16 Grad Scale: 4096  Required: 29 hours
Training: 2023-05-04 14:09:09,016-Speed 1909.49 samples/sec  Loss 73.8149  LearningRate 0.1983  ProxyLR: 9.9158  Epoch: 0  Global Step: 600   Fp16 Grad Scale: 4096  Required: 28 hours
Training: 2023-05-04 14:09:13,270-Speed 2407.41 samples/sec  Loss 73.5432  LearningRate 0.1983  ProxyLR: 9.9144  Epoch: 0  Global Step: 610   Fp16 Grad Scale: 4096  Required: 28 hours
Training: 2023-05-04 14:09:17,113-Speed 2665.23 samples/sec  Loss 71.8821  LearningRate 0.1983  ProxyLR: 9.9130  Epoch: 0  Global Step: 620   Fp16 Grad Scale: 4096  Required: 28 hours
Training: 2023-05-04 14:09:21,072-Speed 2587.07 samples/sec  Loss 70.6154  LearningRate 0.1982  ProxyLR: 9.9116  Epoch: 0  Global Step: 630   Fp16 Grad Scale: 4096  Required: 28 hours
Training: 2023-05-04 14:09:25,874-Speed 2132.42 samples/sec  Loss 69.8844  LearningRate 0.1982  ProxyLR: 9.9102  Epoch: 0  Global Step: 640   Fp16 Grad Scale: 4096  Required: 28 hours
Training: 2023-05-04 14:09:30,198-Speed 2368.85 samples/sec  Loss 69.8445  LearningRate 0.1982  ProxyLR: 9.9088  Epoch: 0  Global Step: 650   Fp16 Grad Scale: 4096  Required: 28 hours
Training: 2023-05-04 14:09:34,020-Speed 2679.30 samples/sec  Loss 68.4974  LearningRate 0.1981  ProxyLR: 9.9074  Epoch: 0  Global Step: 660   Fp16 Grad Scale: 4096  Required: 27 hours
Training: 2023-05-04 14:09:37,197-Speed 3223.49 samples/sec  Loss 67.8431  LearningRate 0.1981  ProxyLR: 9.9060  Epoch: 0  Global Step: 670   Fp16 Grad Scale: 8192  Required: 27 hours
Training: 2023-05-04 14:09:41,043-Speed 2663.57 samples/sec  Loss 66.6305  LearningRate 0.1981  ProxyLR: 9.9046  Epoch: 0  Global Step: 680   Fp16 Grad Scale: 8192  Required: 27 hours
Training: 2023-05-04 14:09:44,720-Speed 2785.52 samples/sec  Loss 66.0873  LearningRate 0.1981  ProxyLR: 9.9032  Epoch: 0  Global Step: 690   Fp16 Grad Scale: 8192  Required: 27 hours
Training: 2023-05-04 14:09:48,174-Speed 2964.46 samples/sec  Loss 65.8977  LearningRate 0.1980  ProxyLR: 9.9018  Epoch: 0  Global Step: 700   Fp16 Grad Scale: 8192  Required: 27 hours
Training: 2023-05-04 14:09:51,471-Speed 3106.50 samples/sec  Loss 65.0170  LearningRate 0.1980  ProxyLR: 9.9004  Epoch: 0  Global Step: 710   Fp16 Grad Scale: 8192  Required: 26 hours
Training: 2023-05-04 14:10:28,512-rank_id: 0
Training: 2023-05-04 14:10:30,709-Note: NumExpr detected 64 cores but "NUMEXPR_MAX_THREADS" not set, so enforcing safe limit of 8.
Training: 2023-05-04 14:10:30,709-NumExpr defaulting to 8 threads.
Training: 2023-05-04 14:10:56,044-rank_id: 0
Training: 2023-05-04 14:11:06,977-: loss                     arcface
Training: 2023-05-04 14:11:06,978-: network                  r100
Training: 2023-05-04 14:11:06,978-: resume                   True
Training: 2023-05-04 14:11:06,978-: output                   work_dirs/ms1mv3_r100_lr02
Training: 2023-05-04 14:11:06,978-: embedding_size           512
Training: 2023-05-04 14:11:06,978-: sample_rate              1.0
Training: 2023-05-04 14:11:06,978-: fp16                     True
Training: 2023-05-04 14:11:06,978-: momentum                 0.9
Training: 2023-05-04 14:11:06,978-: weight_decay             0.0005
Training: 2023-05-04 14:11:06,978-: batch_size               256
Training: 2023-05-04 14:11:06,978-: lr                       0.02
Training: 2023-05-04 14:11:06,978-: dali                     False
Training: 2023-05-04 14:11:06,978-: verbose                  10000
Training: 2023-05-04 14:11:06,978-: frequent                 10
Training: 2023-05-04 14:11:06,978-: score                    None
Training: 2023-05-04 14:11:06,978-: rec                      ../../../../InsightFace_Pytorch/data/faces_emore/
Training: 2023-05-04 14:11:06,978-: num_classes              85742
Training: 2023-05-04 14:11:06,978-: num_image                5822653
Training: 2023-05-04 14:11:06,978-: num_epoch                25
Training: 2023-05-04 14:11:06,978-: warmup_epoch             0
Training: 2023-05-04 14:11:06,978-: val_targets              ['lfw', 'cfp_fp', 'agedb_30', 'calfw', 'cplfw']
Training: 2023-05-04 14:11:06,978-: warmup_step              0
Training: 2023-05-04 14:11:06,978-: total_step               142150
Training: 2023-05-04 14:12:47,451-Reducer buckets have been rebuilt in this iteration.
Training: 2023-05-04 14:12:52,570-Speed 3808.81 samples/sec  Loss 126.3922  LearningRate 0.1999  ProxyLR: 9.9972  Epoch: 0  Global Step: 20   Fp16 Grad Scale: 16384  Required: 26 hours
Training: 2023-05-04 14:12:55,226-Speed 3855.68 samples/sec  Loss 123.3905  LearningRate 0.1999  ProxyLR: 9.9958  Epoch: 0  Global Step: 30   Fp16 Grad Scale: 16384  Required: 21 hours
Training: 2023-05-04 14:12:57,863-Speed 3885.28 samples/sec  Loss 121.6490  LearningRate 0.1999  ProxyLR: 9.9944  Epoch: 0  Global Step: 40   Fp16 Grad Scale: 16384  Required: 19 hours
Training: 2023-05-04 14:13:00,498-Speed 3886.63 samples/sec  Loss 121.2751  LearningRate 0.1999  ProxyLR: 9.9930  Epoch: 0  Global Step: 50   Fp16 Grad Scale: 16384  Required: 17 hours
Training: 2023-05-04 14:13:03,137-Speed 3881.56 samples/sec  Loss 119.3948  LearningRate 0.1998  ProxyLR: 9.9916  Epoch: 0  Global Step: 60   Fp16 Grad Scale: 16384  Required: 16 hours
Training: 2023-05-04 14:13:05,769-Speed 3890.28 samples/sec  Loss 119.3675  LearningRate 0.1998  ProxyLR: 9.9902  Epoch: 0  Global Step: 70   Fp16 Grad Scale: 16384  Required: 15 hours
Training: 2023-05-04 14:13:08,420-Speed 3864.68 samples/sec  Loss 118.1735  LearningRate 0.1998  ProxyLR: 9.9887  Epoch: 0  Global Step: 80   Fp16 Grad Scale: 16384  Required: 15 hours
Training: 2023-05-04 14:13:11,060-Speed 3879.84 samples/sec  Loss 116.0225  LearningRate 0.1997  ProxyLR: 9.9873  Epoch: 0  Global Step: 90   Fp16 Grad Scale: 16384  Required: 14 hours
Training: 2023-05-04 14:13:13,703-Speed 3875.02 samples/sec  Loss 115.3046  LearningRate 0.1997  ProxyLR: 9.9859  Epoch: 0  Global Step: 100   Fp16 Grad Scale: 16384  Required: 14 hours
Training: 2023-05-04 14:13:16,355-Speed 3862.28 samples/sec  Loss 114.9118  LearningRate 0.1997  ProxyLR: 9.9845  Epoch: 0  Global Step: 110   Fp16 Grad Scale: 32768  Required: 13 hours
Training: 2023-05-04 14:13:19,015-Speed 3851.42 samples/sec  Loss 113.2278  LearningRate 0.1997  ProxyLR: 9.9831  Epoch: 0  Global Step: 120   Fp16 Grad Scale: 32768  Required: 13 hours
Training: 2023-05-04 14:13:21,651-Speed 3885.18 samples/sec  Loss 113.6125  LearningRate 0.1996  ProxyLR: 9.9817  Epoch: 0  Global Step: 130   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:13:24,302-Speed 3863.40 samples/sec  Loss 111.5843  LearningRate 0.1996  ProxyLR: 9.9803  Epoch: 0  Global Step: 140   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:13:26,943-Speed 3877.98 samples/sec  Loss 111.1289  LearningRate 0.1996  ProxyLR: 9.9789  Epoch: 0  Global Step: 150   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:13:29,581-Speed 3882.18 samples/sec  Loss 109.8310  LearningRate 0.1996  ProxyLR: 9.9775  Epoch: 0  Global Step: 160   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:32,214-Speed 3890.12 samples/sec  Loss 108.6585  LearningRate 0.1995  ProxyLR: 9.9761  Epoch: 0  Global Step: 170   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:34,853-Speed 3881.46 samples/sec  Loss 107.8317  LearningRate 0.1995  ProxyLR: 9.9747  Epoch: 0  Global Step: 180   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:37,486-Speed 3889.88 samples/sec  Loss 106.4365  LearningRate 0.1995  ProxyLR: 9.9733  Epoch: 0  Global Step: 190   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:40,140-Speed 3859.67 samples/sec  Loss 105.1907  LearningRate 0.1994  ProxyLR: 9.9719  Epoch: 0  Global Step: 200   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:42,798-Speed 3852.55 samples/sec  Loss 104.6423  LearningRate 0.1994  ProxyLR: 9.9705  Epoch: 0  Global Step: 210   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:45,449-Speed 3864.45 samples/sec  Loss 103.1676  LearningRate 0.1994  ProxyLR: 9.9691  Epoch: 0  Global Step: 220   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:13:48,113-Speed 3844.37 samples/sec  Loss 102.3641  LearningRate 0.1994  ProxyLR: 9.9677  Epoch: 0  Global Step: 230   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:13:50,779-Speed 3842.50 samples/sec  Loss 101.2265  LearningRate 0.1993  ProxyLR: 9.9663  Epoch: 0  Global Step: 240   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:13:53,442-Speed 3845.21 samples/sec  Loss 99.3056  LearningRate 0.1993  ProxyLR: 9.9649  Epoch: 0  Global Step: 250   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:13:56,083-Speed 3879.27 samples/sec  Loss 99.0793  LearningRate 0.1993  ProxyLR: 9.9635  Epoch: 0  Global Step: 260   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:13:58,747-Speed 3845.14 samples/sec  Loss 98.1583  LearningRate 0.1992  ProxyLR: 9.9620  Epoch: 0  Global Step: 270   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:14:01,397-Speed 3865.19 samples/sec  Loss 96.5450  LearningRate 0.1992  ProxyLR: 9.9606  Epoch: 0  Global Step: 280   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:14:04,047-Speed 3865.19 samples/sec  Loss 95.6442  LearningRate 0.1992  ProxyLR: 9.9592  Epoch: 0  Global Step: 290   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:14:06,694-Speed 3868.53 samples/sec  Loss 95.3196  LearningRate 0.1992  ProxyLR: 9.9578  Epoch: 0  Global Step: 300   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:14:09,340-Speed 3871.98 samples/sec  Loss 93.5259  LearningRate 0.1991  ProxyLR: 9.9564  Epoch: 0  Global Step: 310   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:14:11,981-Speed 3878.07 samples/sec  Loss 92.2048  LearningRate 0.1991  ProxyLR: 9.9550  Epoch: 0  Global Step: 320   Fp16 Grad Scale: 32768  Required: 11 hours
Training: 2023-05-04 14:14:14,625-Speed 3873.77 samples/sec  Loss 91.7369  LearningRate 0.1991  ProxyLR: 9.9536  Epoch: 0  Global Step: 330   Fp16 Grad Scale: 32768  Required: 11 hours
Training: 2023-05-04 14:14:17,259-Speed 3887.59 samples/sec  Loss 91.0726  LearningRate 0.1990  ProxyLR: 9.9522  Epoch: 0  Global Step: 340   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:14:19,903-Speed 3874.29 samples/sec  Loss 90.2218  LearningRate 0.1990  ProxyLR: 9.9508  Epoch: 0  Global Step: 350   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:14:22,550-Speed 3869.77 samples/sec  Loss 89.9314  LearningRate 0.1990  ProxyLR: 9.9494  Epoch: 0  Global Step: 360   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:14:25,200-Speed 3864.02 samples/sec  Loss 89.0371  LearningRate 0.1990  ProxyLR: 9.9480  Epoch: 0  Global Step: 370   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:14:27,841-Speed 3879.12 samples/sec  Loss 87.1311  LearningRate 0.1989  ProxyLR: 9.9466  Epoch: 0  Global Step: 380   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:30,496-Speed 3857.64 samples/sec  Loss 86.4649  LearningRate 0.1989  ProxyLR: 9.9452  Epoch: 0  Global Step: 390   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:33,135-Speed 3881.74 samples/sec  Loss 85.2255  LearningRate 0.1989  ProxyLR: 9.9438  Epoch: 0  Global Step: 400   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:35,788-Speed 3861.31 samples/sec  Loss 84.9791  LearningRate 0.1988  ProxyLR: 9.9424  Epoch: 0  Global Step: 410   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:38,426-Speed 3881.29 samples/sec  Loss 83.3895  LearningRate 0.1988  ProxyLR: 9.9410  Epoch: 0  Global Step: 420   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:41,065-Speed 3881.74 samples/sec  Loss 82.0255  LearningRate 0.1988  ProxyLR: 9.9396  Epoch: 0  Global Step: 430   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:43,702-Speed 3883.76 samples/sec  Loss 81.7464  LearningRate 0.1988  ProxyLR: 9.9382  Epoch: 0  Global Step: 440   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:46,338-Speed 3886.72 samples/sec  Loss 79.9582  LearningRate 0.1987  ProxyLR: 9.9368  Epoch: 0  Global Step: 450   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:48,967-Speed 3894.62 samples/sec  Loss 79.7320  LearningRate 0.1987  ProxyLR: 9.9354  Epoch: 0  Global Step: 460   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:51,597-Speed 3894.98 samples/sec  Loss 78.3501  LearningRate 0.1987  ProxyLR: 9.9340  Epoch: 0  Global Step: 470   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:14:54,230-Speed 3890.58 samples/sec  Loss 77.5622  LearningRate 0.1987  ProxyLR: 9.9326  Epoch: 0  Global Step: 480   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:14:56,862-Speed 3891.13 samples/sec  Loss 76.5769  LearningRate 0.1986  ProxyLR: 9.9312  Epoch: 0  Global Step: 490   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:14:59,494-Speed 3890.81 samples/sec  Loss 75.3859  LearningRate 0.1986  ProxyLR: 9.9298  Epoch: 0  Global Step: 500   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:02,128-Speed 3889.08 samples/sec  Loss 75.4064  LearningRate 0.1986  ProxyLR: 9.9284  Epoch: 0  Global Step: 510   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:04,769-Speed 3878.47 samples/sec  Loss 74.1706  LearningRate 0.1985  ProxyLR: 9.9270  Epoch: 0  Global Step: 520   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:07,399-Speed 3894.10 samples/sec  Loss 72.9104  LearningRate 0.1985  ProxyLR: 9.9256  Epoch: 0  Global Step: 530   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:10,029-Speed 3894.84 samples/sec  Loss 71.4614  LearningRate 0.1985  ProxyLR: 9.9242  Epoch: 0  Global Step: 540   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:12,658-Speed 3896.02 samples/sec  Loss 72.0364  LearningRate 0.1985  ProxyLR: 9.9228  Epoch: 0  Global Step: 550   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:15,288-Speed 3894.15 samples/sec  Loss 71.9375  LearningRate 0.1984  ProxyLR: 9.9214  Epoch: 0  Global Step: 560   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:17,916-Speed 3896.88 samples/sec  Loss 69.8151  LearningRate 0.1984  ProxyLR: 9.9200  Epoch: 0  Global Step: 570   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:20,546-Speed 3895.09 samples/sec  Loss 68.7633  LearningRate 0.1984  ProxyLR: 9.9186  Epoch: 0  Global Step: 580   Fp16 Grad Scale: 32768  Required: 11 hours
Training: 2023-05-04 14:15:23,164-Speed 3911.61 samples/sec  Loss 68.5447  LearningRate 0.1983  ProxyLR: 9.9172  Epoch: 0  Global Step: 590   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:25,797-Speed 3890.23 samples/sec  Loss 67.7440  LearningRate 0.1983  ProxyLR: 9.9158  Epoch: 0  Global Step: 600   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:28,429-Speed 3891.70 samples/sec  Loss 66.8214  LearningRate 0.1983  ProxyLR: 9.9144  Epoch: 0  Global Step: 610   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:31,059-Speed 3895.14 samples/sec  Loss 65.7205  LearningRate 0.1983  ProxyLR: 9.9130  Epoch: 0  Global Step: 620   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:33,688-Speed 3896.06 samples/sec  Loss 65.4097  LearningRate 0.1982  ProxyLR: 9.9116  Epoch: 0  Global Step: 630   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:36,318-Speed 3894.47 samples/sec  Loss 64.5180  LearningRate 0.1982  ProxyLR: 9.9102  Epoch: 0  Global Step: 640   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:38,948-Speed 3894.17 samples/sec  Loss 64.6062  LearningRate 0.1982  ProxyLR: 9.9088  Epoch: 0  Global Step: 650   Fp16 Grad Scale: 16384  Required: 11 hours
Training: 2023-05-04 14:15:41,555-Speed 3928.74 samples/sec  Loss 64.5582  LearningRate 0.1981  ProxyLR: 9.9074  Epoch: 0  Global Step: 660   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:15:44,186-Speed 3893.05 samples/sec  Loss 62.1050  LearningRate 0.1981  ProxyLR: 9.9060  Epoch: 0  Global Step: 670   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:15:46,815-Speed 3896.81 samples/sec  Loss 62.3132  LearningRate 0.1981  ProxyLR: 9.9046  Epoch: 0  Global Step: 680   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:15:49,444-Speed 3895.90 samples/sec  Loss 60.0930  LearningRate 0.1981  ProxyLR: 9.9032  Epoch: 0  Global Step: 690   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:15:52,075-Speed 3893.26 samples/sec  Loss 60.2296  LearningRate 0.1980  ProxyLR: 9.9018  Epoch: 0  Global Step: 700   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:15:54,710-Speed 3886.08 samples/sec  Loss 59.2676  LearningRate 0.1980  ProxyLR: 9.9004  Epoch: 0  Global Step: 710   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:15:57,356-Speed 3870.92 samples/sec  Loss 59.1643  LearningRate 0.1980  ProxyLR: 9.8990  Epoch: 0  Global Step: 720   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:16:03,410-Speed 1691.60 samples/sec  Loss 57.8110  LearningRate 0.1980  ProxyLR: 9.8976  Epoch: 0  Global Step: 730   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:16:08,870-Speed 1875.77 samples/sec  Loss 58.3187  LearningRate 0.1979  ProxyLR: 9.8962  Epoch: 0  Global Step: 740   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:16:14,652-Speed 1771.24 samples/sec  Loss 57.9187  LearningRate 0.1979  ProxyLR: 9.8948  Epoch: 0  Global Step: 750   Fp16 Grad Scale: 4096  Required: 11 hours
Training: 2023-05-04 14:16:20,191-Speed 1848.96 samples/sec  Loss 56.6600  LearningRate 0.1979  ProxyLR: 9.8934  Epoch: 0  Global Step: 760   Fp16 Grad Scale: 8192  Required: 11 hours
Training: 2023-05-04 14:16:26,277-Speed 1682.79 samples/sec  Loss 56.4443  LearningRate 0.1978  ProxyLR: 9.8920  Epoch: 0  Global Step: 770   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:16:31,859-Speed 1834.49 samples/sec  Loss 55.6225  LearningRate 0.1978  ProxyLR: 9.8906  Epoch: 0  Global Step: 780   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:16:36,538-Speed 2189.16 samples/sec  Loss 55.0709  LearningRate 0.1978  ProxyLR: 9.8892  Epoch: 0  Global Step: 790   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:16:42,024-Speed 1866.49 samples/sec  Loss 53.8319  LearningRate 0.1978  ProxyLR: 9.8878  Epoch: 0  Global Step: 800   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:16:47,193-Speed 1981.49 samples/sec  Loss 57.9424  LearningRate 0.1977  ProxyLR: 9.8864  Epoch: 0  Global Step: 810   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:16:52,150-Speed 2065.97 samples/sec  Loss 56.8781  LearningRate 0.1977  ProxyLR: 9.8850  Epoch: 0  Global Step: 820   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:16:56,874-Speed 2168.02 samples/sec  Loss 55.3617  LearningRate 0.1977  ProxyLR: 9.8836  Epoch: 0  Global Step: 830   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:17:02,411-Speed 1849.48 samples/sec  Loss 53.8174  LearningRate 0.1976  ProxyLR: 9.8822  Epoch: 0  Global Step: 840   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:17:07,606-Speed 1971.56 samples/sec  Loss 53.4787  LearningRate 0.1976  ProxyLR: 9.8808  Epoch: 0  Global Step: 850   Fp16 Grad Scale: 8192  Required: 12 hours
Training: 2023-05-04 14:17:12,009-Speed 2326.09 samples/sec  Loss 51.9416  LearningRate 0.1976  ProxyLR: 9.8794  Epoch: 0  Global Step: 860   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:17:15,711-Speed 2766.51 samples/sec  Loss 51.9466  LearningRate 0.1976  ProxyLR: 9.8780  Epoch: 0  Global Step: 870   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:17:19,179-Speed 2953.24 samples/sec  Loss 50.9343  LearningRate 0.1975  ProxyLR: 9.8766  Epoch: 0  Global Step: 880   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:17:22,771-Speed 2851.68 samples/sec  Loss 50.5190  LearningRate 0.1975  ProxyLR: 9.8752  Epoch: 0  Global Step: 890   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:17:25,922-Speed 3249.91 samples/sec  Loss 49.4801  LearningRate 0.1975  ProxyLR: 9.8738  Epoch: 0  Global Step: 900   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:17:29,119-Speed 3203.55 samples/sec  Loss 48.8031  LearningRate 0.1974  ProxyLR: 9.8724  Epoch: 0  Global Step: 910   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:17:32,319-Speed 3200.59 samples/sec  Loss 48.3031  LearningRate 0.1974  ProxyLR: 9.8710  Epoch: 0  Global Step: 920   Fp16 Grad Scale: 16384  Required: 13 hours
Training: 2023-05-04 14:17:35,098-Speed 3686.70 samples/sec  Loss 47.9899  LearningRate 0.1974  ProxyLR: 9.8696  Epoch: 0  Global Step: 930   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:17:37,877-Speed 3685.10 samples/sec  Loss 46.9705  LearningRate 0.1974  ProxyLR: 9.8682  Epoch: 0  Global Step: 940   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:17:40,834-Speed 3463.37 samples/sec  Loss 46.4595  LearningRate 0.1973  ProxyLR: 9.8668  Epoch: 0  Global Step: 950   Fp16 Grad Scale: 16384  Required: 12 hours
Training: 2023-05-04 14:17:43,849-Speed 3397.12 samples/sec  Loss 45.8336  LearningRate 0.1973  ProxyLR: 9.8654  Epoch: 0  Global Step: 960   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:17:46,560-Speed 3778.78 samples/sec  Loss 44.8788  LearningRate 0.1973  ProxyLR: 9.8640  Epoch: 0  Global Step: 970   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:17:49,438-Speed 3558.56 samples/sec  Loss 44.0688  LearningRate 0.1973  ProxyLR: 9.8626  Epoch: 0  Global Step: 980   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:17:52,108-Speed 3836.66 samples/sec  Loss 43.8401  LearningRate 0.1972  ProxyLR: 9.8612  Epoch: 0  Global Step: 990   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:17:54,795-Speed 3811.53 samples/sec  Loss 43.0853  LearningRate 0.1972  ProxyLR: 9.8598  Epoch: 0  Global Step: 1000   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:17:57,492-Speed 3798.29 samples/sec  Loss 42.5131  LearningRate 0.1972  ProxyLR: 9.8584  Epoch: 0  Global Step: 1010   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:18:00,170-Speed 3823.88 samples/sec  Loss 41.7245  LearningRate 0.1971  ProxyLR: 9.8570  Epoch: 0  Global Step: 1020   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:18:02,841-Speed 3835.19 samples/sec  Loss 41.5105  LearningRate 0.1971  ProxyLR: 9.8556  Epoch: 0  Global Step: 1030   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:18:05,523-Speed 3819.42 samples/sec  Loss 41.0053  LearningRate 0.1971  ProxyLR: 9.8542  Epoch: 0  Global Step: 1040   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:18:08,190-Speed 3840.69 samples/sec  Loss 40.2124  LearningRate 0.1971  ProxyLR: 9.8528  Epoch: 0  Global Step: 1050   Fp16 Grad Scale: 32768  Required: 12 hours
Training: 2023-05-04 14:18:10,852-Speed 3847.41 samples/sec  Loss 38.9895  LearningRate 0.1970  ProxyLR: 9.8514  Epoch: 0  Global Step: 1060   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:13,526-Speed 3830.39 samples/sec  Loss 38.7743  LearningRate 0.1970  ProxyLR: 9.8500  Epoch: 0  Global Step: 1070   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:16,194-Speed 3838.05 samples/sec  Loss 38.4159  LearningRate 0.1970  ProxyLR: 9.8486  Epoch: 0  Global Step: 1080   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:18,867-Speed 3832.27 samples/sec  Loss 37.5799  LearningRate 0.1969  ProxyLR: 9.8472  Epoch: 0  Global Step: 1090   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:21,530-Speed 3845.65 samples/sec  Loss 38.2286  LearningRate 0.1969  ProxyLR: 9.8458  Epoch: 0  Global Step: 1100   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:24,189-Speed 3852.16 samples/sec  Loss 37.2709  LearningRate 0.1969  ProxyLR: 9.8444  Epoch: 0  Global Step: 1110   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:26,850-Speed 3848.96 samples/sec  Loss 36.4290  LearningRate 0.1969  ProxyLR: 9.8430  Epoch: 0  Global Step: 1120   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:29,500-Speed 3866.48 samples/sec  Loss 35.8763  LearningRate 0.1968  ProxyLR: 9.8416  Epoch: 0  Global Step: 1130   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:32,154-Speed 3858.27 samples/sec  Loss 35.7138  LearningRate 0.1968  ProxyLR: 9.8402  Epoch: 0  Global Step: 1140   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:34,815-Speed 3849.32 samples/sec  Loss 34.8400  LearningRate 0.1968  ProxyLR: 9.8389  Epoch: 0  Global Step: 1150   Fp16 Grad Scale: 65536  Required: 12 hours
Training: 2023-05-04 14:18:37,460-Speed 3872.63 samples/sec  Loss 34.8571  LearningRate 0.1967  ProxyLR: 9.8375  Epoch: 0  Global Step: 1160   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:40,111-Speed 3864.03 samples/sec  Loss 34.5772  LearningRate 0.1967  ProxyLR: 9.8361  Epoch: 0  Global Step: 1170   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:42,770-Speed 3852.08 samples/sec  Loss 34.0875  LearningRate 0.1967  ProxyLR: 9.8347  Epoch: 0  Global Step: 1180   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:45,423-Speed 3860.74 samples/sec  Loss 32.8485  LearningRate 0.1967  ProxyLR: 9.8333  Epoch: 0  Global Step: 1190   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:48,093-Speed 3837.00 samples/sec  Loss 33.3348  LearningRate 0.1966  ProxyLR: 9.8319  Epoch: 0  Global Step: 1200   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:50,745-Speed 3860.98 samples/sec  Loss 32.5517  LearningRate 0.1966  ProxyLR: 9.8305  Epoch: 0  Global Step: 1210   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:53,424-Speed 3824.16 samples/sec  Loss 32.7945  LearningRate 0.1966  ProxyLR: 9.8291  Epoch: 0  Global Step: 1220   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:56,099-Speed 3827.79 samples/sec  Loss 32.1451  LearningRate 0.1966  ProxyLR: 9.8277  Epoch: 0  Global Step: 1230   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:18:58,766-Speed 3840.83 samples/sec  Loss 31.8529  LearningRate 0.1965  ProxyLR: 9.8263  Epoch: 0  Global Step: 1240   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:01,418-Speed 3862.71 samples/sec  Loss 30.7363  LearningRate 0.1965  ProxyLR: 9.8249  Epoch: 0  Global Step: 1250   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:04,083-Speed 3843.15 samples/sec  Loss 30.2239  LearningRate 0.1965  ProxyLR: 9.8235  Epoch: 0  Global Step: 1260   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:06,755-Speed 3832.13 samples/sec  Loss 30.1024  LearningRate 0.1964  ProxyLR: 9.8221  Epoch: 0  Global Step: 1270   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:09,415-Speed 3850.30 samples/sec  Loss 29.3652  LearningRate 0.1964  ProxyLR: 9.8207  Epoch: 0  Global Step: 1280   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:12,098-Speed 3818.21 samples/sec  Loss 29.6317  LearningRate 0.1964  ProxyLR: 9.8193  Epoch: 0  Global Step: 1290   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:14,774-Speed 3828.06 samples/sec  Loss 29.4844  LearningRate 0.1964  ProxyLR: 9.8179  Epoch: 0  Global Step: 1300   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:17,459-Speed 3814.00 samples/sec  Loss 28.3041  LearningRate 0.1963  ProxyLR: 9.8165  Epoch: 0  Global Step: 1310   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:20,133-Speed 3830.65 samples/sec  Loss 27.9005  LearningRate 0.1963  ProxyLR: 9.8151  Epoch: 0  Global Step: 1320   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:22,816-Speed 3817.72 samples/sec  Loss 27.8172  LearningRate 0.1963  ProxyLR: 9.8137  Epoch: 0  Global Step: 1330   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:25,496-Speed 3820.78 samples/sec  Loss 28.1348  LearningRate 0.1962  ProxyLR: 9.8124  Epoch: 0  Global Step: 1340   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:28,192-Speed 3799.71 samples/sec  Loss 27.2782  LearningRate 0.1962  ProxyLR: 9.8110  Epoch: 0  Global Step: 1350   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:30,886-Speed 3802.50 samples/sec  Loss 27.2596  LearningRate 0.1962  ProxyLR: 9.8096  Epoch: 0  Global Step: 1360   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:33,576-Speed 3807.02 samples/sec  Loss 26.5113  LearningRate 0.1962  ProxyLR: 9.8082  Epoch: 0  Global Step: 1370   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:36,250-Speed 3830.85 samples/sec  Loss 26.4515  LearningRate 0.1961  ProxyLR: 9.8068  Epoch: 0  Global Step: 1380   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:38,916-Speed 3840.91 samples/sec  Loss 26.6999  LearningRate 0.1961  ProxyLR: 9.8054  Epoch: 0  Global Step: 1390   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:41,576-Speed 3850.19 samples/sec  Loss 25.8090  LearningRate 0.1961  ProxyLR: 9.8040  Epoch: 0  Global Step: 1400   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:44,257-Speed 3821.24 samples/sec  Loss 25.6111  LearningRate 0.1961  ProxyLR: 9.8026  Epoch: 0  Global Step: 1410   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:46,932-Speed 3828.00 samples/sec  Loss 25.2013  LearningRate 0.1960  ProxyLR: 9.8012  Epoch: 0  Global Step: 1420   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:19:49,588-Speed 3857.20 samples/sec  Loss 25.4004  LearningRate 0.1960  ProxyLR: 9.7998  Epoch: 0  Global Step: 1430   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:52,264-Speed 3826.93 samples/sec  Loss 24.5740  LearningRate 0.1960  ProxyLR: 9.7984  Epoch: 0  Global Step: 1440   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:54,960-Speed 3799.23 samples/sec  Loss 24.5377  LearningRate 0.1959  ProxyLR: 9.7970  Epoch: 0  Global Step: 1450   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:19:57,656-Speed 3799.86 samples/sec  Loss 24.0452  LearningRate 0.1959  ProxyLR: 9.7956  Epoch: 0  Global Step: 1460   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:00,329-Speed 3831.46 samples/sec  Loss 23.9491  LearningRate 0.1959  ProxyLR: 9.7942  Epoch: 0  Global Step: 1470   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:03,011-Speed 3818.22 samples/sec  Loss 23.2390  LearningRate 0.1959  ProxyLR: 9.7929  Epoch: 0  Global Step: 1480   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:05,684-Speed 3831.48 samples/sec  Loss 23.8026  LearningRate 0.1958  ProxyLR: 9.7915  Epoch: 0  Global Step: 1490   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:08,348-Speed 3845.57 samples/sec  Loss 23.1655  LearningRate 0.1958  ProxyLR: 9.7901  Epoch: 0  Global Step: 1500   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:11,019-Speed 3835.34 samples/sec  Loss 22.6512  LearningRate 0.1958  ProxyLR: 9.7887  Epoch: 0  Global Step: 1510   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:13,695-Speed 3827.13 samples/sec  Loss 22.8237  LearningRate 0.1957  ProxyLR: 9.7873  Epoch: 0  Global Step: 1520   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:16,350-Speed 3856.73 samples/sec  Loss 22.5221  LearningRate 0.1957  ProxyLR: 9.7859  Epoch: 0  Global Step: 1530   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:19,018-Speed 3840.35 samples/sec  Loss 21.8932  LearningRate 0.1957  ProxyLR: 9.7845  Epoch: 0  Global Step: 1540   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:21,690-Speed 3832.19 samples/sec  Loss 21.6582  LearningRate 0.1957  ProxyLR: 9.7831  Epoch: 0  Global Step: 1550   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:24,355-Speed 3844.04 samples/sec  Loss 21.5469  LearningRate 0.1956  ProxyLR: 9.7817  Epoch: 0  Global Step: 1560   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:27,025-Speed 3843.05 samples/sec  Loss 21.1484  LearningRate 0.1956  ProxyLR: 9.7803  Epoch: 0  Global Step: 1570   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:29,682-Speed 3854.53 samples/sec  Loss 21.6702  LearningRate 0.1956  ProxyLR: 9.7789  Epoch: 0  Global Step: 1580   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:32,357-Speed 3828.54 samples/sec  Loss 20.4426  LearningRate 0.1956  ProxyLR: 9.7775  Epoch: 0  Global Step: 1590   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:35,026-Speed 3837.65 samples/sec  Loss 21.0875  LearningRate 0.1955  ProxyLR: 9.7762  Epoch: 0  Global Step: 1600   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:37,684-Speed 3854.13 samples/sec  Loss 20.5547  LearningRate 0.1955  ProxyLR: 9.7748  Epoch: 0  Global Step: 1610   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:40,344-Speed 3849.98 samples/sec  Loss 20.4665  LearningRate 0.1955  ProxyLR: 9.7734  Epoch: 0  Global Step: 1620   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:42,994-Speed 3865.26 samples/sec  Loss 20.6173  LearningRate 0.1954  ProxyLR: 9.7720  Epoch: 0  Global Step: 1630   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:20:45,631-Speed 3884.00 samples/sec  Loss 20.2237  LearningRate 0.1954  ProxyLR: 9.7706  Epoch: 0  Global Step: 1640   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:48,275-Speed 3874.22 samples/sec  Loss 20.4036  LearningRate 0.1954  ProxyLR: 9.7692  Epoch: 0  Global Step: 1650   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:50,923-Speed 3868.60 samples/sec  Loss 19.7093  LearningRate 0.1954  ProxyLR: 9.7678  Epoch: 0  Global Step: 1660   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:53,577-Speed 3858.77 samples/sec  Loss 19.7472  LearningRate 0.1953  ProxyLR: 9.7664  Epoch: 0  Global Step: 1670   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:56,232-Speed 3857.96 samples/sec  Loss 19.3115  LearningRate 0.1953  ProxyLR: 9.7650  Epoch: 0  Global Step: 1680   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:20:58,880-Speed 3868.42 samples/sec  Loss 19.3193  LearningRate 0.1953  ProxyLR: 9.7636  Epoch: 0  Global Step: 1690   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:21:01,520-Speed 3880.09 samples/sec  Loss 18.9893  LearningRate 0.1952  ProxyLR: 9.7622  Epoch: 0  Global Step: 1700   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:21:04,166-Speed 3870.21 samples/sec  Loss 19.1869  LearningRate 0.1952  ProxyLR: 9.7609  Epoch: 0  Global Step: 1710   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:21:06,808-Speed 3876.86 samples/sec  Loss 18.8494  LearningRate 0.1952  ProxyLR: 9.7595  Epoch: 0  Global Step: 1720   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:21:09,454-Speed 3870.83 samples/sec  Loss 18.4661  LearningRate 0.1952  ProxyLR: 9.7581  Epoch: 0  Global Step: 1730   Fp16 Grad Scale: 131072  Required: 12 hours
Training: 2023-05-04 14:21:12,099-Speed 3872.21 samples/sec  Loss 18.0838  LearningRate 0.1951  ProxyLR: 9.7567  Epoch: 0  Global Step: 1740   Fp16 Grad Scale: 262144  Required: 12 hours
Training: 2023-05-04 14:21:14,725-Speed 3900.92 samples/sec  Loss 18.4563  LearningRate 0.1951  ProxyLR: 9.7553  Epoch: 0  Global Step: 1750   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:17,355-Speed 3894.15 samples/sec  Loss 18.1006  LearningRate 0.1951  ProxyLR: 9.7539  Epoch: 0  Global Step: 1760   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:19,999-Speed 3874.54 samples/sec  Loss 17.7847  LearningRate 0.1951  ProxyLR: 9.7525  Epoch: 0  Global Step: 1770   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:22,642-Speed 3874.78 samples/sec  Loss 17.8993  LearningRate 0.1950  ProxyLR: 9.7511  Epoch: 0  Global Step: 1780   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:25,276-Speed 3888.71 samples/sec  Loss 18.0503  LearningRate 0.1950  ProxyLR: 9.7497  Epoch: 0  Global Step: 1790   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:27,916-Speed 3879.89 samples/sec  Loss 17.6844  LearningRate 0.1950  ProxyLR: 9.7483  Epoch: 0  Global Step: 1800   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:30,566-Speed 3865.79 samples/sec  Loss 17.1996  LearningRate 0.1949  ProxyLR: 9.7470  Epoch: 0  Global Step: 1810   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:21:33,184-Speed 3911.83 samples/sec  Loss 17.4005  LearningRate 0.1949  ProxyLR: 9.7456  Epoch: 0  Global Step: 1820   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:35,849-Speed 3843.08 samples/sec  Loss 17.2860  LearningRate 0.1949  ProxyLR: 9.7442  Epoch: 0  Global Step: 1830   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:38,494-Speed 3872.07 samples/sec  Loss 16.7580  LearningRate 0.1949  ProxyLR: 9.7428  Epoch: 0  Global Step: 1840   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:41,134-Speed 3881.12 samples/sec  Loss 16.9722  LearningRate 0.1948  ProxyLR: 9.7414  Epoch: 0  Global Step: 1850   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:43,764-Speed 3893.45 samples/sec  Loss 16.6266  LearningRate 0.1948  ProxyLR: 9.7400  Epoch: 0  Global Step: 1860   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:46,400-Speed 3885.40 samples/sec  Loss 17.1509  LearningRate 0.1948  ProxyLR: 9.7386  Epoch: 0  Global Step: 1870   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:49,040-Speed 3880.24 samples/sec  Loss 15.8638  LearningRate 0.1947  ProxyLR: 9.7372  Epoch: 0  Global Step: 1880   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:51,679-Speed 3881.54 samples/sec  Loss 15.7537  LearningRate 0.1947  ProxyLR: 9.7359  Epoch: 0  Global Step: 1890   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:54,316-Speed 3883.67 samples/sec  Loss 16.2776  LearningRate 0.1947  ProxyLR: 9.7345  Epoch: 0  Global Step: 1900   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:56,960-Speed 3874.86 samples/sec  Loss 15.5264  LearningRate 0.1947  ProxyLR: 9.7331  Epoch: 0  Global Step: 1910   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:21:59,597-Speed 3884.56 samples/sec  Loss 15.6758  LearningRate 0.1946  ProxyLR: 9.7317  Epoch: 0  Global Step: 1920   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:02,234-Speed 3884.02 samples/sec  Loss 16.1546  LearningRate 0.1946  ProxyLR: 9.7303  Epoch: 0  Global Step: 1930   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:04,869-Speed 3886.24 samples/sec  Loss 15.3999  LearningRate 0.1946  ProxyLR: 9.7289  Epoch: 0  Global Step: 1940   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:07,498-Speed 3896.73 samples/sec  Loss 15.1622  LearningRate 0.1946  ProxyLR: 9.7275  Epoch: 0  Global Step: 1950   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:10,135-Speed 3883.70 samples/sec  Loss 15.5680  LearningRate 0.1945  ProxyLR: 9.7261  Epoch: 0  Global Step: 1960   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:12,761-Speed 3900.04 samples/sec  Loss 14.8852  LearningRate 0.1945  ProxyLR: 9.7247  Epoch: 0  Global Step: 1970   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:15,392-Speed 3893.69 samples/sec  Loss 15.2513  LearningRate 0.1945  ProxyLR: 9.7234  Epoch: 0  Global Step: 1980   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:18,026-Speed 3887.75 samples/sec  Loss 15.1302  LearningRate 0.1944  ProxyLR: 9.7220  Epoch: 0  Global Step: 1990   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:20,667-Speed 3878.54 samples/sec  Loss 15.1749  LearningRate 0.1944  ProxyLR: 9.7206  Epoch: 0  Global Step: 2000   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:23,297-Speed 3895.09 samples/sec  Loss 14.4098  LearningRate 0.1944  ProxyLR: 9.7192  Epoch: 0  Global Step: 2010   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:25,914-Speed 3913.68 samples/sec  Loss 14.2075  LearningRate 0.1944  ProxyLR: 9.7178  Epoch: 0  Global Step: 2020   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:28,548-Speed 3889.22 samples/sec  Loss 14.1782  LearningRate 0.1943  ProxyLR: 9.7164  Epoch: 0  Global Step: 2030   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:22:31,177-Speed 3895.93 samples/sec  Loss 14.4179  LearningRate 0.1943  ProxyLR: 9.7150  Epoch: 0  Global Step: 2040   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:33,808-Speed 3892.87 samples/sec  Loss 13.9075  LearningRate 0.1943  ProxyLR: 9.7137  Epoch: 0  Global Step: 2050   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:36,443-Speed 3887.62 samples/sec  Loss 14.5866  LearningRate 0.1942  ProxyLR: 9.7123  Epoch: 0  Global Step: 2060   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:39,070-Speed 3897.97 samples/sec  Loss 14.6378  LearningRate 0.1942  ProxyLR: 9.7109  Epoch: 0  Global Step: 2070   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:41,708-Speed 3882.63 samples/sec  Loss 14.0156  LearningRate 0.1942  ProxyLR: 9.7095  Epoch: 0  Global Step: 2080   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:44,351-Speed 3875.93 samples/sec  Loss 14.1732  LearningRate 0.1942  ProxyLR: 9.7081  Epoch: 0  Global Step: 2090   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:46,995-Speed 3873.41 samples/sec  Loss 14.3212  LearningRate 0.1941  ProxyLR: 9.7067  Epoch: 0  Global Step: 2100   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:49,635-Speed 3880.52 samples/sec  Loss 13.6480  LearningRate 0.1941  ProxyLR: 9.7053  Epoch: 0  Global Step: 2110   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:52,269-Speed 3887.75 samples/sec  Loss 13.3009  LearningRate 0.1941  ProxyLR: 9.7039  Epoch: 0  Global Step: 2120   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:54,911-Speed 3877.61 samples/sec  Loss 13.1338  LearningRate 0.1941  ProxyLR: 9.7026  Epoch: 0  Global Step: 2130   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:22:57,548-Speed 3882.74 samples/sec  Loss 13.2340  LearningRate 0.1940  ProxyLR: 9.7012  Epoch: 0  Global Step: 2140   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:00,182-Speed 3889.15 samples/sec  Loss 12.7733  LearningRate 0.1940  ProxyLR: 9.6998  Epoch: 0  Global Step: 2150   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:02,813-Speed 3893.45 samples/sec  Loss 13.3447  LearningRate 0.1940  ProxyLR: 9.6984  Epoch: 0  Global Step: 2160   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:05,451-Speed 3882.97 samples/sec  Loss 13.0224  LearningRate 0.1939  ProxyLR: 9.6970  Epoch: 0  Global Step: 2170   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:08,086-Speed 3886.40 samples/sec  Loss 13.1440  LearningRate 0.1939  ProxyLR: 9.6956  Epoch: 0  Global Step: 2180   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:10,723-Speed 3884.61 samples/sec  Loss 13.0251  LearningRate 0.1939  ProxyLR: 9.6942  Epoch: 0  Global Step: 2190   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:13,356-Speed 3889.63 samples/sec  Loss 12.5671  LearningRate 0.1939  ProxyLR: 9.6929  Epoch: 0  Global Step: 2200   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:15,987-Speed 3893.59 samples/sec  Loss 12.8806  LearningRate 0.1938  ProxyLR: 9.6915  Epoch: 0  Global Step: 2210   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:18,612-Speed 3902.44 samples/sec  Loss 12.2502  LearningRate 0.1938  ProxyLR: 9.6901  Epoch: 0  Global Step: 2220   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:21,249-Speed 3883.14 samples/sec  Loss 12.1464  LearningRate 0.1938  ProxyLR: 9.6887  Epoch: 0  Global Step: 2230   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:23,870-Speed 3908.05 samples/sec  Loss 12.1700  LearningRate 0.1937  ProxyLR: 9.6873  Epoch: 0  Global Step: 2240   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:26,505-Speed 3887.10 samples/sec  Loss 12.2473  LearningRate 0.1937  ProxyLR: 9.6859  Epoch: 0  Global Step: 2250   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:29,132-Speed 3899.93 samples/sec  Loss 12.5674  LearningRate 0.1937  ProxyLR: 9.6846  Epoch: 0  Global Step: 2260   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:31,762-Speed 3893.75 samples/sec  Loss 12.2820  LearningRate 0.1937  ProxyLR: 9.6832  Epoch: 0  Global Step: 2270   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:34,389-Speed 3898.88 samples/sec  Loss 11.7677  LearningRate 0.1936  ProxyLR: 9.6818  Epoch: 0  Global Step: 2280   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:37,027-Speed 3882.42 samples/sec  Loss 11.9794  LearningRate 0.1936  ProxyLR: 9.6804  Epoch: 0  Global Step: 2290   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:39,665-Speed 3883.36 samples/sec  Loss 11.7454  LearningRate 0.1936  ProxyLR: 9.6790  Epoch: 0  Global Step: 2300   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:42,291-Speed 3901.07 samples/sec  Loss 11.8542  LearningRate 0.1936  ProxyLR: 9.6776  Epoch: 0  Global Step: 2310   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:44,923-Speed 3890.88 samples/sec  Loss 11.5516  LearningRate 0.1935  ProxyLR: 9.6762  Epoch: 0  Global Step: 2320   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:47,547-Speed 3902.71 samples/sec  Loss 11.7846  LearningRate 0.1935  ProxyLR: 9.6749  Epoch: 0  Global Step: 2330   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:50,179-Speed 3892.64 samples/sec  Loss 11.4127  LearningRate 0.1935  ProxyLR: 9.6735  Epoch: 0  Global Step: 2340   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:23:52,809-Speed 3893.69 samples/sec  Loss 11.4067  LearningRate 0.1934  ProxyLR: 9.6721  Epoch: 0  Global Step: 2350   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:23:55,423-Speed 3918.81 samples/sec  Loss 11.5068  LearningRate 0.1934  ProxyLR: 9.6707  Epoch: 0  Global Step: 2360   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:23:58,050-Speed 3899.49 samples/sec  Loss 11.6152  LearningRate 0.1934  ProxyLR: 9.6693  Epoch: 0  Global Step: 2370   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:00,681-Speed 3892.30 samples/sec  Loss 11.8310  LearningRate 0.1934  ProxyLR: 9.6679  Epoch: 0  Global Step: 2380   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:03,308-Speed 3899.56 samples/sec  Loss 11.2430  LearningRate 0.1933  ProxyLR: 9.6666  Epoch: 0  Global Step: 2390   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:05,942-Speed 3887.97 samples/sec  Loss 10.9478  LearningRate 0.1933  ProxyLR: 9.6652  Epoch: 0  Global Step: 2400   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:08,571-Speed 3896.63 samples/sec  Loss 11.2289  LearningRate 0.1933  ProxyLR: 9.6638  Epoch: 0  Global Step: 2410   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:11,200-Speed 3895.98 samples/sec  Loss 11.2393  LearningRate 0.1932  ProxyLR: 9.6624  Epoch: 0  Global Step: 2420   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:13,827-Speed 3897.77 samples/sec  Loss 11.2631  LearningRate 0.1932  ProxyLR: 9.6610  Epoch: 0  Global Step: 2430   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:16,459-Speed 3891.74 samples/sec  Loss 11.0156  LearningRate 0.1932  ProxyLR: 9.6596  Epoch: 0  Global Step: 2440   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:19,087-Speed 3897.15 samples/sec  Loss 11.2941  LearningRate 0.1932  ProxyLR: 9.6583  Epoch: 0  Global Step: 2450   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:21,703-Speed 3916.45 samples/sec  Loss 10.7540  LearningRate 0.1931  ProxyLR: 9.6569  Epoch: 0  Global Step: 2460   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:24,333-Speed 3895.30 samples/sec  Loss 10.5588  LearningRate 0.1931  ProxyLR: 9.6555  Epoch: 0  Global Step: 2470   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:26,967-Speed 3887.40 samples/sec  Loss 10.6453  LearningRate 0.1931  ProxyLR: 9.6541  Epoch: 0  Global Step: 2480   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:29,594-Speed 3899.28 samples/sec  Loss 10.3872  LearningRate 0.1931  ProxyLR: 9.6527  Epoch: 0  Global Step: 2490   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:32,224-Speed 3894.50 samples/sec  Loss 10.5638  LearningRate 0.1930  ProxyLR: 9.6514  Epoch: 0  Global Step: 2500   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:34,851-Speed 3899.51 samples/sec  Loss 10.6837  LearningRate 0.1930  ProxyLR: 9.6500  Epoch: 0  Global Step: 2510   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:37,475-Speed 3903.05 samples/sec  Loss 10.5366  LearningRate 0.1930  ProxyLR: 9.6486  Epoch: 0  Global Step: 2520   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:40,098-Speed 3904.75 samples/sec  Loss 10.1949  LearningRate 0.1929  ProxyLR: 9.6472  Epoch: 0  Global Step: 2530   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:42,727-Speed 3895.33 samples/sec  Loss 10.5068  LearningRate 0.1929  ProxyLR: 9.6458  Epoch: 0  Global Step: 2540   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:45,356-Speed 3896.59 samples/sec  Loss 10.2933  LearningRate 0.1929  ProxyLR: 9.6444  Epoch: 0  Global Step: 2550   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:47,979-Speed 3904.63 samples/sec  Loss 10.0768  LearningRate 0.1929  ProxyLR: 9.6431  Epoch: 0  Global Step: 2560   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:50,619-Speed 3879.73 samples/sec  Loss 10.2308  LearningRate 0.1928  ProxyLR: 9.6417  Epoch: 0  Global Step: 2570   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:53,253-Speed 3888.23 samples/sec  Loss 9.9903  LearningRate 0.1928  ProxyLR: 9.6403  Epoch: 0  Global Step: 2580   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:55,879-Speed 3901.36 samples/sec  Loss 9.5116  LearningRate 0.1928  ProxyLR: 9.6389  Epoch: 0  Global Step: 2590   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:24:58,507-Speed 3898.05 samples/sec  Loss 9.9789  LearningRate 0.1928  ProxyLR: 9.6375  Epoch: 0  Global Step: 2600   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:01,134-Speed 3898.19 samples/sec  Loss 9.9900  LearningRate 0.1927  ProxyLR: 9.6362  Epoch: 0  Global Step: 2610   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:03,762-Speed 3897.04 samples/sec  Loss 10.0317  LearningRate 0.1927  ProxyLR: 9.6348  Epoch: 0  Global Step: 2620   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:06,390-Speed 3898.14 samples/sec  Loss 9.8847  LearningRate 0.1927  ProxyLR: 9.6334  Epoch: 0  Global Step: 2630   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:09,019-Speed 3895.68 samples/sec  Loss 9.4727  LearningRate 0.1926  ProxyLR: 9.6320  Epoch: 0  Global Step: 2640   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:11,645-Speed 3900.09 samples/sec  Loss 9.5767  LearningRate 0.1926  ProxyLR: 9.6306  Epoch: 0  Global Step: 2650   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:14,273-Speed 3898.32 samples/sec  Loss 9.6297  LearningRate 0.1926  ProxyLR: 9.6292  Epoch: 0  Global Step: 2660   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:25:16,903-Speed 3894.69 samples/sec  Loss 9.6407  LearningRate 0.1926  ProxyLR: 9.6279  Epoch: 0  Global Step: 2670   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:25:19,519-Speed 3915.20 samples/sec  Loss 9.5634  LearningRate 0.1925  ProxyLR: 9.6265  Epoch: 0  Global Step: 2680   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:22,148-Speed 3896.12 samples/sec  Loss 9.5029  LearningRate 0.1925  ProxyLR: 9.6251  Epoch: 0  Global Step: 2690   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:24,777-Speed 3895.92 samples/sec  Loss 9.7855  LearningRate 0.1925  ProxyLR: 9.6237  Epoch: 0  Global Step: 2700   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:27,410-Speed 3890.29 samples/sec  Loss 9.3020  LearningRate 0.1924  ProxyLR: 9.6223  Epoch: 0  Global Step: 2710   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:30,043-Speed 3889.30 samples/sec  Loss 9.0118  LearningRate 0.1924  ProxyLR: 9.6210  Epoch: 0  Global Step: 2720   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:32,676-Speed 3891.04 samples/sec  Loss 9.2924  LearningRate 0.1924  ProxyLR: 9.6196  Epoch: 0  Global Step: 2730   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:35,311-Speed 3886.78 samples/sec  Loss 9.1471  LearningRate 0.1924  ProxyLR: 9.6182  Epoch: 0  Global Step: 2740   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:37,945-Speed 3888.69 samples/sec  Loss 9.3589  LearningRate 0.1923  ProxyLR: 9.6168  Epoch: 0  Global Step: 2750   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:40,579-Speed 3887.94 samples/sec  Loss 9.2911  LearningRate 0.1923  ProxyLR: 9.6154  Epoch: 0  Global Step: 2760   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:43,215-Speed 3886.47 samples/sec  Loss 9.2523  LearningRate 0.1923  ProxyLR: 9.6141  Epoch: 0  Global Step: 2770   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:45,848-Speed 3889.85 samples/sec  Loss 8.8363  LearningRate 0.1923  ProxyLR: 9.6127  Epoch: 0  Global Step: 2780   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:25:48,468-Speed 3908.58 samples/sec  Loss 9.2305  LearningRate 0.1922  ProxyLR: 9.6113  Epoch: 0  Global Step: 2790   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:51,106-Speed 3883.23 samples/sec  Loss 9.2206  LearningRate 0.1922  ProxyLR: 9.6099  Epoch: 0  Global Step: 2800   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:53,740-Speed 3887.68 samples/sec  Loss 8.9571  LearningRate 0.1922  ProxyLR: 9.6086  Epoch: 0  Global Step: 2810   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:56,374-Speed 3888.80 samples/sec  Loss 9.0430  LearningRate 0.1921  ProxyLR: 9.6072  Epoch: 0  Global Step: 2820   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:25:58,996-Speed 3907.52 samples/sec  Loss 8.9326  LearningRate 0.1921  ProxyLR: 9.6058  Epoch: 0  Global Step: 2830   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:01,632-Speed 3884.60 samples/sec  Loss 9.0118  LearningRate 0.1921  ProxyLR: 9.6044  Epoch: 0  Global Step: 2840   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:04,272-Speed 3880.35 samples/sec  Loss 9.0588  LearningRate 0.1921  ProxyLR: 9.6030  Epoch: 0  Global Step: 2850   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:06,911-Speed 3880.98 samples/sec  Loss 8.5184  LearningRate 0.1920  ProxyLR: 9.6017  Epoch: 0  Global Step: 2860   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:09,546-Speed 3886.86 samples/sec  Loss 8.6203  LearningRate 0.1920  ProxyLR: 9.6003  Epoch: 0  Global Step: 2870   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:12,181-Speed 3888.11 samples/sec  Loss 8.7320  LearningRate 0.1920  ProxyLR: 9.5989  Epoch: 0  Global Step: 2880   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:14,816-Speed 3887.38 samples/sec  Loss 8.0742  LearningRate 0.1920  ProxyLR: 9.5975  Epoch: 0  Global Step: 2890   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:17,448-Speed 3890.72 samples/sec  Loss 8.4913  LearningRate 0.1919  ProxyLR: 9.5961  Epoch: 0  Global Step: 2900   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:20,095-Speed 3869.61 samples/sec  Loss 8.9583  LearningRate 0.1919  ProxyLR: 9.5948  Epoch: 0  Global Step: 2910   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:22,725-Speed 3894.56 samples/sec  Loss 8.1958  LearningRate 0.1919  ProxyLR: 9.5934  Epoch: 0  Global Step: 2920   Fp16 Grad Scale: 65536  Required: 11 hours
Training: 2023-05-04 14:26:25,358-Speed 3890.53 samples/sec  Loss 8.4682  LearningRate 0.1918  ProxyLR: 9.5920  Epoch: 0  Global Step: 2930   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:27,989-Speed 3892.64 samples/sec  Loss 8.5389  LearningRate 0.1918  ProxyLR: 9.5906  Epoch: 0  Global Step: 2940   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:30,620-Speed 3892.36 samples/sec  Loss 8.4939  LearningRate 0.1918  ProxyLR: 9.5893  Epoch: 0  Global Step: 2950   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:33,253-Speed 3891.34 samples/sec  Loss 8.0541  LearningRate 0.1918  ProxyLR: 9.5879  Epoch: 0  Global Step: 2960   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:35,892-Speed 3881.29 samples/sec  Loss 8.1120  LearningRate 0.1917  ProxyLR: 9.5865  Epoch: 0  Global Step: 2970   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:38,520-Speed 3896.68 samples/sec  Loss 8.5624  LearningRate 0.1917  ProxyLR: 9.5851  Epoch: 0  Global Step: 2980   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:41,151-Speed 3892.45 samples/sec  Loss 8.0791  LearningRate 0.1917  ProxyLR: 9.5837  Epoch: 0  Global Step: 2990   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:43,781-Speed 3895.74 samples/sec  Loss 8.1499  LearningRate 0.1916  ProxyLR: 9.5824  Epoch: 0  Global Step: 3000   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:46,410-Speed 3895.89 samples/sec  Loss 7.7375  LearningRate 0.1916  ProxyLR: 9.5810  Epoch: 0  Global Step: 3010   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:49,039-Speed 3896.12 samples/sec  Loss 8.1634  LearningRate 0.1916  ProxyLR: 9.5796  Epoch: 0  Global Step: 3020   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:51,660-Speed 3907.12 samples/sec  Loss 7.8867  LearningRate 0.1916  ProxyLR: 9.5782  Epoch: 0  Global Step: 3030   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:54,292-Speed 3891.90 samples/sec  Loss 8.5246  LearningRate 0.1915  ProxyLR: 9.5769  Epoch: 0  Global Step: 3040   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:56,920-Speed 3896.53 samples/sec  Loss 8.0688  LearningRate 0.1915  ProxyLR: 9.5755  Epoch: 0  Global Step: 3050   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:26:59,554-Speed 3889.69 samples/sec  Loss 8.0073  LearningRate 0.1915  ProxyLR: 9.5741  Epoch: 0  Global Step: 3060   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:02,185-Speed 3892.74 samples/sec  Loss 7.9786  LearningRate 0.1915  ProxyLR: 9.5727  Epoch: 0  Global Step: 3070   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:04,818-Speed 3891.25 samples/sec  Loss 7.6762  LearningRate 0.1914  ProxyLR: 9.5713  Epoch: 0  Global Step: 3080   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:07,447-Speed 3895.38 samples/sec  Loss 7.5464  LearningRate 0.1914  ProxyLR: 9.5700  Epoch: 0  Global Step: 3090   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:10,079-Speed 3892.32 samples/sec  Loss 7.7012  LearningRate 0.1914  ProxyLR: 9.5686  Epoch: 0  Global Step: 3100   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:12,714-Speed 3885.86 samples/sec  Loss 7.8119  LearningRate 0.1913  ProxyLR: 9.5672  Epoch: 0  Global Step: 3110   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:15,346-Speed 3892.34 samples/sec  Loss 7.7556  LearningRate 0.1913  ProxyLR: 9.5658  Epoch: 0  Global Step: 3120   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:17,980-Speed 3888.95 samples/sec  Loss 7.6885  LearningRate 0.1913  ProxyLR: 9.5645  Epoch: 0  Global Step: 3130   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:27:20,614-Speed 3887.80 samples/sec  Loss 7.6360  LearningRate 0.1913  ProxyLR: 9.5631  Epoch: 0  Global Step: 3140   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:27:23,234-Speed 3910.32 samples/sec  Loss 7.5072  LearningRate 0.1912  ProxyLR: 9.5617  Epoch: 0  Global Step: 3150   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:25,865-Speed 3891.87 samples/sec  Loss 7.6039  LearningRate 0.1912  ProxyLR: 9.5603  Epoch: 0  Global Step: 3160   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:28,500-Speed 3888.60 samples/sec  Loss 7.3445  LearningRate 0.1912  ProxyLR: 9.5590  Epoch: 0  Global Step: 3170   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:31,132-Speed 3890.60 samples/sec  Loss 7.4151  LearningRate 0.1912  ProxyLR: 9.5576  Epoch: 0  Global Step: 3180   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:33,765-Speed 3890.20 samples/sec  Loss 7.5039  LearningRate 0.1911  ProxyLR: 9.5562  Epoch: 0  Global Step: 3190   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:36,397-Speed 3891.95 samples/sec  Loss 7.6680  LearningRate 0.1911  ProxyLR: 9.5548  Epoch: 0  Global Step: 3200   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:39,031-Speed 3889.57 samples/sec  Loss 7.4985  LearningRate 0.1911  ProxyLR: 9.5535  Epoch: 0  Global Step: 3210   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:41,663-Speed 3890.90 samples/sec  Loss 7.3323  LearningRate 0.1910  ProxyLR: 9.5521  Epoch: 0  Global Step: 3220   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:44,296-Speed 3890.14 samples/sec  Loss 7.0245  LearningRate 0.1910  ProxyLR: 9.5507  Epoch: 0  Global Step: 3230   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:46,928-Speed 3891.80 samples/sec  Loss 7.1446  LearningRate 0.1910  ProxyLR: 9.5493  Epoch: 0  Global Step: 3240   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:49,546-Speed 3912.36 samples/sec  Loss 7.3520  LearningRate 0.1910  ProxyLR: 9.5480  Epoch: 0  Global Step: 3250   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:52,175-Speed 3896.14 samples/sec  Loss 6.9995  LearningRate 0.1909  ProxyLR: 9.5466  Epoch: 0  Global Step: 3260   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:54,800-Speed 3901.26 samples/sec  Loss 6.9487  LearningRate 0.1909  ProxyLR: 9.5452  Epoch: 0  Global Step: 3270   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:27:57,427-Speed 3898.99 samples/sec  Loss 7.2547  LearningRate 0.1909  ProxyLR: 9.5438  Epoch: 0  Global Step: 3280   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:00,052-Speed 3901.77 samples/sec  Loss 7.5235  LearningRate 0.1908  ProxyLR: 9.5425  Epoch: 0  Global Step: 3290   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:02,676-Speed 3903.25 samples/sec  Loss 7.3264  LearningRate 0.1908  ProxyLR: 9.5411  Epoch: 0  Global Step: 3300   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:05,308-Speed 3892.75 samples/sec  Loss 7.1313  LearningRate 0.1908  ProxyLR: 9.5397  Epoch: 0  Global Step: 3310   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:07,932-Speed 3902.66 samples/sec  Loss 7.1864  LearningRate 0.1908  ProxyLR: 9.5383  Epoch: 0  Global Step: 3320   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:10,558-Speed 3901.11 samples/sec  Loss 7.1453  LearningRate 0.1907  ProxyLR: 9.5370  Epoch: 0  Global Step: 3330   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:13,183-Speed 3902.07 samples/sec  Loss 7.0360  LearningRate 0.1907  ProxyLR: 9.5356  Epoch: 0  Global Step: 3340   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:15,796-Speed 3919.14 samples/sec  Loss 6.7941  LearningRate 0.1907  ProxyLR: 9.5342  Epoch: 0  Global Step: 3350   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:18,424-Speed 3897.69 samples/sec  Loss 6.7104  LearningRate 0.1907  ProxyLR: 9.5328  Epoch: 0  Global Step: 3360   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:21,054-Speed 3894.28 samples/sec  Loss 7.1618  LearningRate 0.1906  ProxyLR: 9.5315  Epoch: 0  Global Step: 3370   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:23,684-Speed 3895.11 samples/sec  Loss 7.0053  LearningRate 0.1906  ProxyLR: 9.5301  Epoch: 0  Global Step: 3380   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:26,314-Speed 3894.69 samples/sec  Loss 7.0516  LearningRate 0.1906  ProxyLR: 9.5287  Epoch: 0  Global Step: 3390   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:28,943-Speed 3895.13 samples/sec  Loss 6.6415  LearningRate 0.1905  ProxyLR: 9.5274  Epoch: 0  Global Step: 3400   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:31,574-Speed 3894.17 samples/sec  Loss 7.0666  LearningRate 0.1905  ProxyLR: 9.5260  Epoch: 0  Global Step: 3410   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:34,202-Speed 3897.23 samples/sec  Loss 6.8266  LearningRate 0.1905  ProxyLR: 9.5246  Epoch: 0  Global Step: 3420   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:36,828-Speed 3900.60 samples/sec  Loss 6.5988  LearningRate 0.1905  ProxyLR: 9.5232  Epoch: 0  Global Step: 3430   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:39,453-Speed 3901.15 samples/sec  Loss 6.9027  LearningRate 0.1904  ProxyLR: 9.5219  Epoch: 0  Global Step: 3440   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:42,080-Speed 3899.43 samples/sec  Loss 6.8159  LearningRate 0.1904  ProxyLR: 9.5205  Epoch: 0  Global Step: 3450   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:28:44,692-Speed 3920.97 samples/sec  Loss 6.4341  LearningRate 0.1904  ProxyLR: 9.5191  Epoch: 0  Global Step: 3460   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:47,321-Speed 3896.38 samples/sec  Loss 6.8529  LearningRate 0.1904  ProxyLR: 9.5177  Epoch: 0  Global Step: 3470   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:49,950-Speed 3897.13 samples/sec  Loss 6.7157  LearningRate 0.1903  ProxyLR: 9.5164  Epoch: 0  Global Step: 3480   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:52,580-Speed 3893.92 samples/sec  Loss 6.6008  LearningRate 0.1903  ProxyLR: 9.5150  Epoch: 0  Global Step: 3490   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:55,208-Speed 3897.02 samples/sec  Loss 6.8943  LearningRate 0.1903  ProxyLR: 9.5136  Epoch: 0  Global Step: 3500   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:28:57,836-Speed 3898.37 samples/sec  Loss 6.7842  LearningRate 0.1902  ProxyLR: 9.5123  Epoch: 0  Global Step: 3510   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:00,461-Speed 3901.67 samples/sec  Loss 6.3906  LearningRate 0.1902  ProxyLR: 9.5109  Epoch: 0  Global Step: 3520   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:03,086-Speed 3902.49 samples/sec  Loss 6.4607  LearningRate 0.1902  ProxyLR: 9.5095  Epoch: 0  Global Step: 3530   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:05,712-Speed 3900.34 samples/sec  Loss 6.7820  LearningRate 0.1902  ProxyLR: 9.5081  Epoch: 0  Global Step: 3540   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:08,337-Speed 3901.65 samples/sec  Loss 6.4068  LearningRate 0.1901  ProxyLR: 9.5068  Epoch: 0  Global Step: 3550   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:10,963-Speed 3899.66 samples/sec  Loss 6.3532  LearningRate 0.1901  ProxyLR: 9.5054  Epoch: 0  Global Step: 3560   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:29:13,574-Speed 3922.68 samples/sec  Loss 6.6408  LearningRate 0.1901  ProxyLR: 9.5040  Epoch: 0  Global Step: 3570   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:16,200-Speed 3901.33 samples/sec  Loss 6.2888  LearningRate 0.1901  ProxyLR: 9.5026  Epoch: 0  Global Step: 3580   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:18,827-Speed 3898.14 samples/sec  Loss 6.2093  LearningRate 0.1900  ProxyLR: 9.5013  Epoch: 0  Global Step: 3590   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:21,450-Speed 3905.30 samples/sec  Loss 5.8378  LearningRate 0.1900  ProxyLR: 9.4999  Epoch: 0  Global Step: 3600   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:24,075-Speed 3901.90 samples/sec  Loss 6.3429  LearningRate 0.1900  ProxyLR: 9.4985  Epoch: 0  Global Step: 3610   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:26,698-Speed 3905.32 samples/sec  Loss 6.4525  LearningRate 0.1899  ProxyLR: 9.4972  Epoch: 0  Global Step: 3620   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:29,322-Speed 3904.12 samples/sec  Loss 6.4290  LearningRate 0.1899  ProxyLR: 9.4958  Epoch: 0  Global Step: 3630   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:31,947-Speed 3901.93 samples/sec  Loss 6.3076  LearningRate 0.1899  ProxyLR: 9.4944  Epoch: 0  Global Step: 3640   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:34,569-Speed 3906.11 samples/sec  Loss 6.5663  LearningRate 0.1899  ProxyLR: 9.4931  Epoch: 0  Global Step: 3650   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:37,194-Speed 3901.76 samples/sec  Loss 6.2904  LearningRate 0.1898  ProxyLR: 9.4917  Epoch: 0  Global Step: 3660   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:39,805-Speed 3922.13 samples/sec  Loss 6.3544  LearningRate 0.1898  ProxyLR: 9.4903  Epoch: 0  Global Step: 3670   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:42,429-Speed 3903.69 samples/sec  Loss 6.4731  LearningRate 0.1898  ProxyLR: 9.4889  Epoch: 0  Global Step: 3680   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:45,052-Speed 3904.38 samples/sec  Loss 6.4539  LearningRate 0.1898  ProxyLR: 9.4876  Epoch: 0  Global Step: 3690   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:47,677-Speed 3903.19 samples/sec  Loss 6.2012  LearningRate 0.1897  ProxyLR: 9.4862  Epoch: 0  Global Step: 3700   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:50,302-Speed 3901.56 samples/sec  Loss 6.2126  LearningRate 0.1897  ProxyLR: 9.4848  Epoch: 0  Global Step: 3710   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:52,928-Speed 3900.70 samples/sec  Loss 6.1180  LearningRate 0.1897  ProxyLR: 9.4835  Epoch: 0  Global Step: 3720   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:55,553-Speed 3901.67 samples/sec  Loss 5.8288  LearningRate 0.1896  ProxyLR: 9.4821  Epoch: 0  Global Step: 3730   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:29:58,179-Speed 3901.02 samples/sec  Loss 6.2634  LearningRate 0.1896  ProxyLR: 9.4807  Epoch: 0  Global Step: 3740   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:00,805-Speed 3899.98 samples/sec  Loss 5.7780  LearningRate 0.1896  ProxyLR: 9.4793  Epoch: 0  Global Step: 3750   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:03,435-Speed 3894.71 samples/sec  Loss 6.4071  LearningRate 0.1896  ProxyLR: 9.4780  Epoch: 0  Global Step: 3760   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:06,062-Speed 3899.03 samples/sec  Loss 6.3456  LearningRate 0.1895  ProxyLR: 9.4766  Epoch: 0  Global Step: 3770   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:30:08,689-Speed 3898.22 samples/sec  Loss 6.2676  LearningRate 0.1895  ProxyLR: 9.4752  Epoch: 0  Global Step: 3780   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:30:11,318-Speed 3895.97 samples/sec  Loss 5.6207  LearningRate 0.1895  ProxyLR: 9.4739  Epoch: 0  Global Step: 3790   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:30:13,934-Speed 3915.89 samples/sec  Loss 6.1417  LearningRate 0.1894  ProxyLR: 9.4725  Epoch: 0  Global Step: 3800   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:16,562-Speed 3897.59 samples/sec  Loss 5.6709  LearningRate 0.1894  ProxyLR: 9.4711  Epoch: 0  Global Step: 3810   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:19,186-Speed 3902.73 samples/sec  Loss 5.9977  LearningRate 0.1894  ProxyLR: 9.4698  Epoch: 0  Global Step: 3820   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:21,811-Speed 3901.72 samples/sec  Loss 5.8957  LearningRate 0.1894  ProxyLR: 9.4684  Epoch: 0  Global Step: 3830   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:24,436-Speed 3902.77 samples/sec  Loss 6.2009  LearningRate 0.1893  ProxyLR: 9.4670  Epoch: 0  Global Step: 3840   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:27,060-Speed 3903.70 samples/sec  Loss 5.9299  LearningRate 0.1893  ProxyLR: 9.4657  Epoch: 0  Global Step: 3850   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:29,686-Speed 3899.52 samples/sec  Loss 5.8793  LearningRate 0.1893  ProxyLR: 9.4643  Epoch: 0  Global Step: 3860   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:32,315-Speed 3895.86 samples/sec  Loss 6.0232  LearningRate 0.1893  ProxyLR: 9.4629  Epoch: 0  Global Step: 3870   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:34,946-Speed 3892.63 samples/sec  Loss 6.1963  LearningRate 0.1892  ProxyLR: 9.4615  Epoch: 0  Global Step: 3880   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:37,574-Speed 3898.86 samples/sec  Loss 5.7642  LearningRate 0.1892  ProxyLR: 9.4602  Epoch: 0  Global Step: 3890   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:40,200-Speed 3899.32 samples/sec  Loss 5.7688  LearningRate 0.1892  ProxyLR: 9.4588  Epoch: 0  Global Step: 3900   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:30:42,813-Speed 3919.75 samples/sec  Loss 5.8448  LearningRate 0.1891  ProxyLR: 9.4574  Epoch: 0  Global Step: 3910   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:45,440-Speed 3900.06 samples/sec  Loss 5.5903  LearningRate 0.1891  ProxyLR: 9.4561  Epoch: 0  Global Step: 3920   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:48,064-Speed 3903.05 samples/sec  Loss 5.7366  LearningRate 0.1891  ProxyLR: 9.4547  Epoch: 0  Global Step: 3930   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:50,689-Speed 3902.09 samples/sec  Loss 5.7099  LearningRate 0.1891  ProxyLR: 9.4533  Epoch: 0  Global Step: 3940   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:53,312-Speed 3903.69 samples/sec  Loss 5.7969  LearningRate 0.1890  ProxyLR: 9.4520  Epoch: 0  Global Step: 3950   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:55,936-Speed 3903.91 samples/sec  Loss 6.0057  LearningRate 0.1890  ProxyLR: 9.4506  Epoch: 0  Global Step: 3960   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:30:58,561-Speed 3901.98 samples/sec  Loss 5.6210  LearningRate 0.1890  ProxyLR: 9.4492  Epoch: 0  Global Step: 3970   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:01,186-Speed 3902.19 samples/sec  Loss 5.5029  LearningRate 0.1890  ProxyLR: 9.4479  Epoch: 0  Global Step: 3980   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:03,810-Speed 3903.81 samples/sec  Loss 5.7160  LearningRate 0.1889  ProxyLR: 9.4465  Epoch: 0  Global Step: 3990   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:06,435-Speed 3902.64 samples/sec  Loss 5.6189  LearningRate 0.1889  ProxyLR: 9.4451  Epoch: 0  Global Step: 4000   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:09,057-Speed 3906.41 samples/sec  Loss 5.4935  LearningRate 0.1889  ProxyLR: 9.4438  Epoch: 0  Global Step: 4010   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:11,678-Speed 3907.30 samples/sec  Loss 5.6286  LearningRate 0.1888  ProxyLR: 9.4424  Epoch: 0  Global Step: 4020   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:14,303-Speed 3902.34 samples/sec  Loss 5.3226  LearningRate 0.1888  ProxyLR: 9.4410  Epoch: 0  Global Step: 4030   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:16,927-Speed 3902.11 samples/sec  Loss 5.2817  LearningRate 0.1888  ProxyLR: 9.4397  Epoch: 0  Global Step: 4040   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:19,551-Speed 3903.51 samples/sec  Loss 5.7227  LearningRate 0.1888  ProxyLR: 9.4383  Epoch: 0  Global Step: 4050   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:22,178-Speed 3899.88 samples/sec  Loss 5.2958  LearningRate 0.1887  ProxyLR: 9.4369  Epoch: 0  Global Step: 4060   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:24,805-Speed 3899.03 samples/sec  Loss 5.3330  LearningRate 0.1887  ProxyLR: 9.4356  Epoch: 0  Global Step: 4070   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:27,432-Speed 3899.07 samples/sec  Loss 5.4151  LearningRate 0.1887  ProxyLR: 9.4342  Epoch: 0  Global Step: 4080   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:30,041-Speed 3925.23 samples/sec  Loss 5.4722  LearningRate 0.1887  ProxyLR: 9.4328  Epoch: 0  Global Step: 4090   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:32,665-Speed 3902.91 samples/sec  Loss 5.4248  LearningRate 0.1886  ProxyLR: 9.4315  Epoch: 0  Global Step: 4100   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:35,289-Speed 3903.52 samples/sec  Loss 5.2960  LearningRate 0.1886  ProxyLR: 9.4301  Epoch: 0  Global Step: 4110   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:37,917-Speed 3898.01 samples/sec  Loss 5.4637  LearningRate 0.1886  ProxyLR: 9.4287  Epoch: 0  Global Step: 4120   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:40,543-Speed 3900.38 samples/sec  Loss 5.3207  LearningRate 0.1885  ProxyLR: 9.4274  Epoch: 0  Global Step: 4130   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:43,170-Speed 3898.62 samples/sec  Loss 5.5821  LearningRate 0.1885  ProxyLR: 9.4260  Epoch: 0  Global Step: 4140   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:45,799-Speed 3895.73 samples/sec  Loss 5.3423  LearningRate 0.1885  ProxyLR: 9.4246  Epoch: 0  Global Step: 4150   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:48,427-Speed 3898.48 samples/sec  Loss 5.2740  LearningRate 0.1885  ProxyLR: 9.4233  Epoch: 0  Global Step: 4160   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:51,053-Speed 3899.84 samples/sec  Loss 5.6141  LearningRate 0.1884  ProxyLR: 9.4219  Epoch: 0  Global Step: 4170   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:53,679-Speed 3901.34 samples/sec  Loss 5.1643  LearningRate 0.1884  ProxyLR: 9.4205  Epoch: 0  Global Step: 4180   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:31:56,306-Speed 3898.11 samples/sec  Loss 5.3841  LearningRate 0.1884  ProxyLR: 9.4192  Epoch: 0  Global Step: 4190   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:31:58,933-Speed 3899.00 samples/sec  Loss 5.3132  LearningRate 0.1884  ProxyLR: 9.4178  Epoch: 0  Global Step: 4200   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:01,560-Speed 3899.02 samples/sec  Loss 5.3441  LearningRate 0.1883  ProxyLR: 9.4164  Epoch: 0  Global Step: 4210   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:04,187-Speed 3899.72 samples/sec  Loss 4.8256  LearningRate 0.1883  ProxyLR: 9.4151  Epoch: 0  Global Step: 4220   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:06,812-Speed 3901.37 samples/sec  Loss 5.3700  LearningRate 0.1883  ProxyLR: 9.4137  Epoch: 0  Global Step: 4230   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:09,436-Speed 3903.96 samples/sec  Loss 5.1763  LearningRate 0.1882  ProxyLR: 9.4123  Epoch: 0  Global Step: 4240   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:12,060-Speed 3903.14 samples/sec  Loss 5.1855  LearningRate 0.1882  ProxyLR: 9.4110  Epoch: 0  Global Step: 4250   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:14,682-Speed 3905.82 samples/sec  Loss 5.2471  LearningRate 0.1882  ProxyLR: 9.4096  Epoch: 0  Global Step: 4260   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:17,307-Speed 3902.49 samples/sec  Loss 5.2293  LearningRate 0.1882  ProxyLR: 9.4082  Epoch: 0  Global Step: 4270   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:19,932-Speed 3901.59 samples/sec  Loss 5.3188  LearningRate 0.1881  ProxyLR: 9.4069  Epoch: 0  Global Step: 4280   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:22,541-Speed 3925.50 samples/sec  Loss 5.4041  LearningRate 0.1881  ProxyLR: 9.4055  Epoch: 0  Global Step: 4290   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:25,164-Speed 3904.82 samples/sec  Loss 5.1834  LearningRate 0.1881  ProxyLR: 9.4042  Epoch: 0  Global Step: 4300   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:27,788-Speed 3903.32 samples/sec  Loss 4.8845  LearningRate 0.1881  ProxyLR: 9.4028  Epoch: 0  Global Step: 4310   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:30,412-Speed 3904.31 samples/sec  Loss 5.1695  LearningRate 0.1880  ProxyLR: 9.4014  Epoch: 0  Global Step: 4320   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:33,035-Speed 3904.64 samples/sec  Loss 5.1704  LearningRate 0.1880  ProxyLR: 9.4001  Epoch: 0  Global Step: 4330   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:35,659-Speed 3903.68 samples/sec  Loss 5.1454  LearningRate 0.1880  ProxyLR: 9.3987  Epoch: 0  Global Step: 4340   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:38,283-Speed 3902.68 samples/sec  Loss 5.4853  LearningRate 0.1879  ProxyLR: 9.3973  Epoch: 0  Global Step: 4350   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:40,908-Speed 3901.79 samples/sec  Loss 5.2580  LearningRate 0.1879  ProxyLR: 9.3960  Epoch: 0  Global Step: 4360   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:43,531-Speed 3904.84 samples/sec  Loss 4.9507  LearningRate 0.1879  ProxyLR: 9.3946  Epoch: 0  Global Step: 4370   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:46,155-Speed 3903.89 samples/sec  Loss 5.0401  LearningRate 0.1879  ProxyLR: 9.3932  Epoch: 0  Global Step: 4380   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:48,780-Speed 3901.76 samples/sec  Loss 5.0222  LearningRate 0.1878  ProxyLR: 9.3919  Epoch: 0  Global Step: 4390   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:51,408-Speed 3896.71 samples/sec  Loss 4.7722  LearningRate 0.1878  ProxyLR: 9.3905  Epoch: 0  Global Step: 4400   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:32:54,025-Speed 3915.01 samples/sec  Loss 4.8159  LearningRate 0.1878  ProxyLR: 9.3892  Epoch: 0  Global Step: 4410   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:56,653-Speed 3897.01 samples/sec  Loss 5.1522  LearningRate 0.1878  ProxyLR: 9.3878  Epoch: 0  Global Step: 4420   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:32:59,283-Speed 3894.76 samples/sec  Loss 4.9807  LearningRate 0.1877  ProxyLR: 9.3864  Epoch: 0  Global Step: 4430   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:01,911-Speed 3897.94 samples/sec  Loss 5.1846  LearningRate 0.1877  ProxyLR: 9.3851  Epoch: 0  Global Step: 4440   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:04,540-Speed 3895.66 samples/sec  Loss 4.9890  LearningRate 0.1877  ProxyLR: 9.3837  Epoch: 0  Global Step: 4450   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:07,169-Speed 3895.21 samples/sec  Loss 4.7143  LearningRate 0.1876  ProxyLR: 9.3823  Epoch: 0  Global Step: 4460   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:09,800-Speed 3893.82 samples/sec  Loss 4.6206  LearningRate 0.1876  ProxyLR: 9.3810  Epoch: 0  Global Step: 4470   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:12,428-Speed 3896.62 samples/sec  Loss 4.8415  LearningRate 0.1876  ProxyLR: 9.3796  Epoch: 0  Global Step: 4480   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:15,057-Speed 3896.24 samples/sec  Loss 5.1249  LearningRate 0.1876  ProxyLR: 9.3782  Epoch: 0  Global Step: 4490   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:17,685-Speed 3897.25 samples/sec  Loss 4.8098  LearningRate 0.1875  ProxyLR: 9.3769  Epoch: 0  Global Step: 4500   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:20,320-Speed 3887.48 samples/sec  Loss 4.7432  LearningRate 0.1875  ProxyLR: 9.3755  Epoch: 0  Global Step: 4510   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:33:22,941-Speed 3908.38 samples/sec  Loss 4.9725  LearningRate 0.1875  ProxyLR: 9.3742  Epoch: 0  Global Step: 4520   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:25,580-Speed 3881.54 samples/sec  Loss 5.0098  LearningRate 0.1875  ProxyLR: 9.3728  Epoch: 0  Global Step: 4530   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:28,214-Speed 3888.20 samples/sec  Loss 4.7028  LearningRate 0.1874  ProxyLR: 9.3714  Epoch: 0  Global Step: 4540   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:30,847-Speed 3889.85 samples/sec  Loss 4.8189  LearningRate 0.1874  ProxyLR: 9.3701  Epoch: 0  Global Step: 4550   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:33,482-Speed 3887.45 samples/sec  Loss 4.8985  LearningRate 0.1874  ProxyLR: 9.3687  Epoch: 0  Global Step: 4560   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:36,117-Speed 3887.72 samples/sec  Loss 4.6686  LearningRate 0.1873  ProxyLR: 9.3674  Epoch: 0  Global Step: 4570   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:38,751-Speed 3887.72 samples/sec  Loss 4.9321  LearningRate 0.1873  ProxyLR: 9.3660  Epoch: 0  Global Step: 4580   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:41,385-Speed 3888.13 samples/sec  Loss 4.7979  LearningRate 0.1873  ProxyLR: 9.3646  Epoch: 0  Global Step: 4590   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:44,018-Speed 3890.66 samples/sec  Loss 4.6260  LearningRate 0.1873  ProxyLR: 9.3633  Epoch: 0  Global Step: 4600   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:46,650-Speed 3890.94 samples/sec  Loss 4.9789  LearningRate 0.1872  ProxyLR: 9.3619  Epoch: 0  Global Step: 4610   Fp16 Grad Scale: 131072  Required: 11 hours
Training: 2023-05-04 14:33:49,285-Speed 3887.72 samples/sec  Loss 4.6450  LearningRate 0.1872  ProxyLR: 9.3605  Epoch: 0  Global Step: 4620   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:33:51,918-Speed 3889.70 samples/sec  Loss 4.5273  LearningRate 0.1872  ProxyLR: 9.3592  Epoch: 0  Global Step: 4630   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:33:54,555-Speed 3884.73 samples/sec  Loss 4.6347  LearningRate 0.1872  ProxyLR: 9.3578  Epoch: 0  Global Step: 4640   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 14:33:57,174-Speed 3910.30 samples/sec  Loss 4.6907  LearningRate 0.1871  ProxyLR: 9.3565  Epoch: 0  Global Step: 4650   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:33:59,808-Speed 3888.43 samples/sec  Loss 4.8534  LearningRate 0.1871  ProxyLR: 9.3551  Epoch: 0  Global Step: 4660   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:02,443-Speed 3888.20 samples/sec  Loss 4.6969  LearningRate 0.1871  ProxyLR: 9.3537  Epoch: 0  Global Step: 4670   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:05,078-Speed 3886.75 samples/sec  Loss 4.6899  LearningRate 0.1870  ProxyLR: 9.3524  Epoch: 0  Global Step: 4680   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:07,708-Speed 3894.94 samples/sec  Loss 4.6658  LearningRate 0.1870  ProxyLR: 9.3510  Epoch: 0  Global Step: 4690   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:10,339-Speed 3893.29 samples/sec  Loss 4.8119  LearningRate 0.1870  ProxyLR: 9.3497  Epoch: 0  Global Step: 4700   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:12,970-Speed 3891.92 samples/sec  Loss 4.5678  LearningRate 0.1870  ProxyLR: 9.3483  Epoch: 0  Global Step: 4710   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:15,604-Speed 3889.40 samples/sec  Loss 4.5662  LearningRate 0.1869  ProxyLR: 9.3469  Epoch: 0  Global Step: 4720   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:18,233-Speed 3895.10 samples/sec  Loss 4.8855  LearningRate 0.1869  ProxyLR: 9.3456  Epoch: 0  Global Step: 4730   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:20,862-Speed 3896.72 samples/sec  Loss 4.7768  LearningRate 0.1869  ProxyLR: 9.3442  Epoch: 0  Global Step: 4740   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:23,485-Speed 3904.44 samples/sec  Loss 4.6012  LearningRate 0.1869  ProxyLR: 9.3429  Epoch: 0  Global Step: 4750   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:26,119-Speed 3889.28 samples/sec  Loss 4.5800  LearningRate 0.1868  ProxyLR: 9.3415  Epoch: 0  Global Step: 4760   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:28,751-Speed 3891.35 samples/sec  Loss 4.4607  LearningRate 0.1868  ProxyLR: 9.3401  Epoch: 0  Global Step: 4770   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:31,383-Speed 3892.00 samples/sec  Loss 4.6900  LearningRate 0.1868  ProxyLR: 9.3388  Epoch: 0  Global Step: 4780   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:34,015-Speed 3891.44 samples/sec  Loss 4.8401  LearningRate 0.1867  ProxyLR: 9.3374  Epoch: 0  Global Step: 4790   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:36,646-Speed 3892.81 samples/sec  Loss 4.6932  LearningRate 0.1867  ProxyLR: 9.3361  Epoch: 0  Global Step: 4800   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:39,278-Speed 3891.39 samples/sec  Loss 4.3616  LearningRate 0.1867  ProxyLR: 9.3347  Epoch: 0  Global Step: 4810   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:41,910-Speed 3890.46 samples/sec  Loss 4.7725  LearningRate 0.1867  ProxyLR: 9.3333  Epoch: 0  Global Step: 4820   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:44,542-Speed 3892.28 samples/sec  Loss 4.4942  LearningRate 0.1866  ProxyLR: 9.3320  Epoch: 0  Global Step: 4830   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:47,174-Speed 3892.24 samples/sec  Loss 4.3733  LearningRate 0.1866  ProxyLR: 9.3306  Epoch: 0  Global Step: 4840   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:34:49,806-Speed 3891.50 samples/sec  Loss 4.6041  LearningRate 0.1866  ProxyLR: 9.3293  Epoch: 0  Global Step: 4850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:34:52,438-Speed 3891.04 samples/sec  Loss 4.6123  LearningRate 0.1866  ProxyLR: 9.3279  Epoch: 0  Global Step: 4860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:34:55,069-Speed 3892.68 samples/sec  Loss 4.6225  LearningRate 0.1865  ProxyLR: 9.3265  Epoch: 0  Global Step: 4870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:34:57,702-Speed 3890.65 samples/sec  Loss 4.3365  LearningRate 0.1865  ProxyLR: 9.3252  Epoch: 0  Global Step: 4880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:00,334-Speed 3891.31 samples/sec  Loss 4.5688  LearningRate 0.1865  ProxyLR: 9.3238  Epoch: 0  Global Step: 4890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:02,966-Speed 3890.73 samples/sec  Loss 4.4246  LearningRate 0.1864  ProxyLR: 9.3225  Epoch: 0  Global Step: 4900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:05,600-Speed 3889.84 samples/sec  Loss 4.6417  LearningRate 0.1864  ProxyLR: 9.3211  Epoch: 0  Global Step: 4910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:08,235-Speed 3887.11 samples/sec  Loss 4.4935  LearningRate 0.1864  ProxyLR: 9.3198  Epoch: 0  Global Step: 4920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:10,868-Speed 3889.07 samples/sec  Loss 4.3663  LearningRate 0.1864  ProxyLR: 9.3184  Epoch: 0  Global Step: 4930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:13,497-Speed 3895.91 samples/sec  Loss 4.2626  LearningRate 0.1863  ProxyLR: 9.3170  Epoch: 0  Global Step: 4940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:16,119-Speed 3907.13 samples/sec  Loss 4.3636  LearningRate 0.1863  ProxyLR: 9.3157  Epoch: 0  Global Step: 4950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:18,753-Speed 3888.12 samples/sec  Loss 4.5673  LearningRate 0.1863  ProxyLR: 9.3143  Epoch: 0  Global Step: 4960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:21,386-Speed 3889.95 samples/sec  Loss 4.6809  LearningRate 0.1863  ProxyLR: 9.3130  Epoch: 0  Global Step: 4970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:24,022-Speed 3886.71 samples/sec  Loss 4.4464  LearningRate 0.1862  ProxyLR: 9.3116  Epoch: 0  Global Step: 4980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:26,652-Speed 3894.05 samples/sec  Loss 4.4430  LearningRate 0.1862  ProxyLR: 9.3102  Epoch: 0  Global Step: 4990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:29,274-Speed 3906.27 samples/sec  Loss 4.1105  LearningRate 0.1862  ProxyLR: 9.3089  Epoch: 0  Global Step: 5000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:31,901-Speed 3899.16 samples/sec  Loss 4.3914  LearningRate 0.1862  ProxyLR: 9.3075  Epoch: 0  Global Step: 5010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:34,527-Speed 3900.91 samples/sec  Loss 4.4100  LearningRate 0.1861  ProxyLR: 9.3062  Epoch: 0  Global Step: 5020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:37,152-Speed 3901.11 samples/sec  Loss 4.4788  LearningRate 0.1861  ProxyLR: 9.3048  Epoch: 0  Global Step: 5030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:39,779-Speed 3899.41 samples/sec  Loss 4.3517  LearningRate 0.1861  ProxyLR: 9.3035  Epoch: 0  Global Step: 5040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:35:42,393-Speed 3918.03 samples/sec  Loss 4.3021  LearningRate 0.1860  ProxyLR: 9.3021  Epoch: 0  Global Step: 5050   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:35:45,020-Speed 3899.75 samples/sec  Loss 4.0986  LearningRate 0.1860  ProxyLR: 9.3007  Epoch: 0  Global Step: 5060   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:35:47,647-Speed 3899.20 samples/sec  Loss 4.4663  LearningRate 0.1860  ProxyLR: 9.2994  Epoch: 0  Global Step: 5070   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:35:50,274-Speed 3898.81 samples/sec  Loss 4.1313  LearningRate 0.1860  ProxyLR: 9.2980  Epoch: 0  Global Step: 5080   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:35:52,900-Speed 3899.33 samples/sec  Loss 4.1608  LearningRate 0.1859  ProxyLR: 9.2967  Epoch: 0  Global Step: 5090   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:35:55,527-Speed 3899.11 samples/sec  Loss 4.3323  LearningRate 0.1859  ProxyLR: 9.2953  Epoch: 0  Global Step: 5100   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:35:58,158-Speed 3893.57 samples/sec  Loss 4.2226  LearningRate 0.1859  ProxyLR: 9.2940  Epoch: 0  Global Step: 5110   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:36:00,788-Speed 3894.86 samples/sec  Loss 4.5192  LearningRate 0.1859  ProxyLR: 9.2926  Epoch: 0  Global Step: 5120   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:36:03,415-Speed 3898.20 samples/sec  Loss 4.3285  LearningRate 0.1858  ProxyLR: 9.2913  Epoch: 0  Global Step: 5130   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:36:06,043-Speed 3898.38 samples/sec  Loss 4.4780  LearningRate 0.1858  ProxyLR: 9.2899  Epoch: 0  Global Step: 5140   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:36:08,672-Speed 3895.42 samples/sec  Loss 4.3570  LearningRate 0.1858  ProxyLR: 9.2885  Epoch: 0  Global Step: 5150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:11,303-Speed 3893.89 samples/sec  Loss 4.3989  LearningRate 0.1857  ProxyLR: 9.2872  Epoch: 0  Global Step: 5160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:13,935-Speed 3890.54 samples/sec  Loss 4.3183  LearningRate 0.1857  ProxyLR: 9.2858  Epoch: 0  Global Step: 5170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:16,567-Speed 3891.97 samples/sec  Loss 4.1198  LearningRate 0.1857  ProxyLR: 9.2845  Epoch: 0  Global Step: 5180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:19,197-Speed 3893.89 samples/sec  Loss 4.2618  LearningRate 0.1857  ProxyLR: 9.2831  Epoch: 0  Global Step: 5190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:21,827-Speed 3894.41 samples/sec  Loss 4.1169  LearningRate 0.1856  ProxyLR: 9.2818  Epoch: 0  Global Step: 5200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:24,456-Speed 3896.71 samples/sec  Loss 4.6080  LearningRate 0.1856  ProxyLR: 9.2804  Epoch: 0  Global Step: 5210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:27,085-Speed 3896.56 samples/sec  Loss 4.4351  LearningRate 0.1856  ProxyLR: 9.2790  Epoch: 0  Global Step: 5220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:29,714-Speed 3895.20 samples/sec  Loss 4.1037  LearningRate 0.1856  ProxyLR: 9.2777  Epoch: 0  Global Step: 5230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:32,345-Speed 3893.18 samples/sec  Loss 4.2562  LearningRate 0.1855  ProxyLR: 9.2763  Epoch: 0  Global Step: 5240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:34,961-Speed 3914.75 samples/sec  Loss 4.0635  LearningRate 0.1855  ProxyLR: 9.2750  Epoch: 0  Global Step: 5250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:37,592-Speed 3893.08 samples/sec  Loss 4.2006  LearningRate 0.1855  ProxyLR: 9.2736  Epoch: 0  Global Step: 5260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:40,220-Speed 3898.64 samples/sec  Loss 4.3998  LearningRate 0.1854  ProxyLR: 9.2723  Epoch: 0  Global Step: 5270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:42,853-Speed 3890.27 samples/sec  Loss 4.2222  LearningRate 0.1854  ProxyLR: 9.2709  Epoch: 0  Global Step: 5280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:45,487-Speed 3888.07 samples/sec  Loss 4.3074  LearningRate 0.1854  ProxyLR: 9.2696  Epoch: 0  Global Step: 5290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:48,121-Speed 3888.54 samples/sec  Loss 4.1190  LearningRate 0.1854  ProxyLR: 9.2682  Epoch: 0  Global Step: 5300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:50,754-Speed 3889.47 samples/sec  Loss 4.1624  LearningRate 0.1853  ProxyLR: 9.2669  Epoch: 0  Global Step: 5310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:53,385-Speed 3893.28 samples/sec  Loss 4.3070  LearningRate 0.1853  ProxyLR: 9.2655  Epoch: 0  Global Step: 5320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:56,016-Speed 3893.49 samples/sec  Loss 4.2886  LearningRate 0.1853  ProxyLR: 9.2641  Epoch: 0  Global Step: 5330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:36:58,649-Speed 3889.12 samples/sec  Loss 4.4378  LearningRate 0.1853  ProxyLR: 9.2628  Epoch: 0  Global Step: 5340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:01,281-Speed 3892.52 samples/sec  Loss 4.3387  LearningRate 0.1852  ProxyLR: 9.2614  Epoch: 0  Global Step: 5350   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:37:03,901-Speed 3909.60 samples/sec  Loss 4.2583  LearningRate 0.1852  ProxyLR: 9.2601  Epoch: 0  Global Step: 5360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:06,531-Speed 3894.24 samples/sec  Loss 4.1322  LearningRate 0.1852  ProxyLR: 9.2587  Epoch: 0  Global Step: 5370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:09,162-Speed 3891.90 samples/sec  Loss 4.1472  LearningRate 0.1851  ProxyLR: 9.2574  Epoch: 0  Global Step: 5380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:11,793-Speed 3893.28 samples/sec  Loss 4.0727  LearningRate 0.1851  ProxyLR: 9.2560  Epoch: 0  Global Step: 5390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:14,426-Speed 3890.48 samples/sec  Loss 4.2357  LearningRate 0.1851  ProxyLR: 9.2547  Epoch: 0  Global Step: 5400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:17,065-Speed 3881.38 samples/sec  Loss 4.0818  LearningRate 0.1851  ProxyLR: 9.2533  Epoch: 0  Global Step: 5410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:19,700-Speed 3886.57 samples/sec  Loss 4.0856  LearningRate 0.1850  ProxyLR: 9.2520  Epoch: 0  Global Step: 5420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:22,324-Speed 3904.61 samples/sec  Loss 4.1268  LearningRate 0.1850  ProxyLR: 9.2506  Epoch: 0  Global Step: 5430   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:24,960-Speed 3884.92 samples/sec  Loss 4.1527  LearningRate 0.1850  ProxyLR: 9.2493  Epoch: 0  Global Step: 5440   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:27,597-Speed 3884.93 samples/sec  Loss 4.1077  LearningRate 0.1850  ProxyLR: 9.2479  Epoch: 0  Global Step: 5450   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:30,229-Speed 3890.92 samples/sec  Loss 4.1103  LearningRate 0.1849  ProxyLR: 9.2466  Epoch: 0  Global Step: 5460   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:32,863-Speed 3889.00 samples/sec  Loss 4.0223  LearningRate 0.1849  ProxyLR: 9.2452  Epoch: 0  Global Step: 5470   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:35,496-Speed 3889.37 samples/sec  Loss 3.8256  LearningRate 0.1849  ProxyLR: 9.2438  Epoch: 0  Global Step: 5480   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:38,129-Speed 3889.70 samples/sec  Loss 4.0177  LearningRate 0.1848  ProxyLR: 9.2425  Epoch: 0  Global Step: 5490   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:40,760-Speed 3892.73 samples/sec  Loss 4.0347  LearningRate 0.1848  ProxyLR: 9.2411  Epoch: 0  Global Step: 5500   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:43,391-Speed 3893.86 samples/sec  Loss 4.0000  LearningRate 0.1848  ProxyLR: 9.2398  Epoch: 0  Global Step: 5510   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:46,022-Speed 3892.05 samples/sec  Loss 4.0001  LearningRate 0.1848  ProxyLR: 9.2384  Epoch: 0  Global Step: 5520   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:48,654-Speed 3892.14 samples/sec  Loss 4.2541  LearningRate 0.1847  ProxyLR: 9.2371  Epoch: 0  Global Step: 5530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:37:51,272-Speed 3912.03 samples/sec  Loss 4.1474  LearningRate 0.1847  ProxyLR: 9.2357  Epoch: 0  Global Step: 5540   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:53,906-Speed 3889.03 samples/sec  Loss 4.0162  LearningRate 0.1847  ProxyLR: 9.2344  Epoch: 0  Global Step: 5550   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:56,536-Speed 3894.81 samples/sec  Loss 4.1626  LearningRate 0.1847  ProxyLR: 9.2330  Epoch: 0  Global Step: 5560   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:37:59,164-Speed 3897.44 samples/sec  Loss 4.0869  LearningRate 0.1846  ProxyLR: 9.2317  Epoch: 0  Global Step: 5570   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:01,790-Speed 3899.67 samples/sec  Loss 4.1074  LearningRate 0.1846  ProxyLR: 9.2303  Epoch: 0  Global Step: 5580   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:04,415-Speed 3902.30 samples/sec  Loss 3.9545  LearningRate 0.1846  ProxyLR: 9.2290  Epoch: 0  Global Step: 5590   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:07,042-Speed 3899.12 samples/sec  Loss 3.8732  LearningRate 0.1846  ProxyLR: 9.2276  Epoch: 0  Global Step: 5600   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:09,672-Speed 3895.16 samples/sec  Loss 3.9896  LearningRate 0.1845  ProxyLR: 9.2263  Epoch: 0  Global Step: 5610   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:12,304-Speed 3891.50 samples/sec  Loss 3.7645  LearningRate 0.1845  ProxyLR: 9.2249  Epoch: 0  Global Step: 5620   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:14,935-Speed 3893.05 samples/sec  Loss 3.8843  LearningRate 0.1845  ProxyLR: 9.2236  Epoch: 0  Global Step: 5630   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:38:17,562-Speed 3897.55 samples/sec  Loss 3.8927  LearningRate 0.1844  ProxyLR: 9.2222  Epoch: 0  Global Step: 5640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:20,191-Speed 3896.82 samples/sec  Loss 3.9856  LearningRate 0.1844  ProxyLR: 9.2209  Epoch: 0  Global Step: 5650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:22,819-Speed 3896.94 samples/sec  Loss 3.8556  LearningRate 0.1844  ProxyLR: 9.2195  Epoch: 0  Global Step: 5660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:25,445-Speed 3900.36 samples/sec  Loss 3.9983  LearningRate 0.1844  ProxyLR: 9.2182  Epoch: 0  Global Step: 5670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:28,132-Speed 3812.52 samples/sec  Loss 4.0176  LearningRate 0.1843  ProxyLR: 9.2168  Epoch: 0  Global Step: 5680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:37,876-Speed 1050.96 samples/sec  Loss 3.1649  LearningRate 0.1843  ProxyLR: 9.2155  Epoch: 1  Global Step: 5690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:40,530-Speed 3859.95 samples/sec  Loss 2.5279  LearningRate 0.1843  ProxyLR: 9.2141  Epoch: 1  Global Step: 5700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:43,160-Speed 3894.49 samples/sec  Loss 2.4910  LearningRate 0.1843  ProxyLR: 9.2128  Epoch: 1  Global Step: 5710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:45,852-Speed 3804.16 samples/sec  Loss 2.5288  LearningRate 0.1842  ProxyLR: 9.2114  Epoch: 1  Global Step: 5720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:48,484-Speed 3892.68 samples/sec  Loss 2.3022  LearningRate 0.1842  ProxyLR: 9.2101  Epoch: 1  Global Step: 5730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:51,116-Speed 3890.65 samples/sec  Loss 2.4613  LearningRate 0.1842  ProxyLR: 9.2087  Epoch: 1  Global Step: 5740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:38:53,749-Speed 3890.84 samples/sec  Loss 2.3016  LearningRate 0.1841  ProxyLR: 9.2074  Epoch: 1  Global Step: 5750   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:38:56,367-Speed 3912.55 samples/sec  Loss 2.3664  LearningRate 0.1841  ProxyLR: 9.2060  Epoch: 1  Global Step: 5760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:38:59,000-Speed 3890.11 samples/sec  Loss 2.4386  LearningRate 0.1841  ProxyLR: 9.2047  Epoch: 1  Global Step: 5770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:01,630-Speed 3894.13 samples/sec  Loss 2.3940  LearningRate 0.1841  ProxyLR: 9.2033  Epoch: 1  Global Step: 5780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:04,261-Speed 3893.44 samples/sec  Loss 2.3678  LearningRate 0.1840  ProxyLR: 9.2020  Epoch: 1  Global Step: 5790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:06,892-Speed 3892.85 samples/sec  Loss 2.4744  LearningRate 0.1840  ProxyLR: 9.2006  Epoch: 1  Global Step: 5800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:09,525-Speed 3890.48 samples/sec  Loss 2.3998  LearningRate 0.1840  ProxyLR: 9.1993  Epoch: 1  Global Step: 5810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:12,153-Speed 3896.48 samples/sec  Loss 2.3812  LearningRate 0.1840  ProxyLR: 9.1979  Epoch: 1  Global Step: 5820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:14,780-Speed 3898.88 samples/sec  Loss 2.5443  LearningRate 0.1839  ProxyLR: 9.1966  Epoch: 1  Global Step: 5830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:17,408-Speed 3897.41 samples/sec  Loss 2.4642  LearningRate 0.1839  ProxyLR: 9.1952  Epoch: 1  Global Step: 5840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:20,036-Speed 3897.84 samples/sec  Loss 2.4929  LearningRate 0.1839  ProxyLR: 9.1939  Epoch: 1  Global Step: 5850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:22,649-Speed 3919.66 samples/sec  Loss 2.2368  LearningRate 0.1839  ProxyLR: 9.1925  Epoch: 1  Global Step: 5860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:25,277-Speed 3897.46 samples/sec  Loss 2.2786  LearningRate 0.1838  ProxyLR: 9.1912  Epoch: 1  Global Step: 5870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:27,904-Speed 3898.94 samples/sec  Loss 2.3407  LearningRate 0.1838  ProxyLR: 9.1898  Epoch: 1  Global Step: 5880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:30,530-Speed 3901.29 samples/sec  Loss 2.3382  LearningRate 0.1838  ProxyLR: 9.1885  Epoch: 1  Global Step: 5890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:33,208-Speed 3824.16 samples/sec  Loss 2.3755  LearningRate 0.1837  ProxyLR: 9.1871  Epoch: 1  Global Step: 5900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:35,833-Speed 3901.65 samples/sec  Loss 2.3126  LearningRate 0.1837  ProxyLR: 9.1858  Epoch: 1  Global Step: 5910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:38,462-Speed 3897.13 samples/sec  Loss 2.3396  LearningRate 0.1837  ProxyLR: 9.1844  Epoch: 1  Global Step: 5920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:41,414-Speed 3468.68 samples/sec  Loss 2.5551  LearningRate 0.1837  ProxyLR: 9.1831  Epoch: 1  Global Step: 5930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:44,040-Speed 3901.96 samples/sec  Loss 2.4698  LearningRate 0.1836  ProxyLR: 9.1817  Epoch: 1  Global Step: 5940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:46,714-Speed 3829.95 samples/sec  Loss 2.4131  LearningRate 0.1836  ProxyLR: 9.1804  Epoch: 1  Global Step: 5950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:49,339-Speed 3901.83 samples/sec  Loss 2.6065  LearningRate 0.1836  ProxyLR: 9.1790  Epoch: 1  Global Step: 5960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:39:51,964-Speed 3901.63 samples/sec  Loss 2.2707  LearningRate 0.1836  ProxyLR: 9.1777  Epoch: 1  Global Step: 5970   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:39:54,577-Speed 3920.30 samples/sec  Loss 2.5228  LearningRate 0.1835  ProxyLR: 9.1763  Epoch: 1  Global Step: 5980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:57,202-Speed 3902.14 samples/sec  Loss 2.3188  LearningRate 0.1835  ProxyLR: 9.1750  Epoch: 1  Global Step: 5990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:39:59,827-Speed 3901.67 samples/sec  Loss 2.3929  LearningRate 0.1835  ProxyLR: 9.1736  Epoch: 1  Global Step: 6000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:02,453-Speed 3901.13 samples/sec  Loss 2.2704  LearningRate 0.1834  ProxyLR: 9.1723  Epoch: 1  Global Step: 6010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:05,078-Speed 3900.79 samples/sec  Loss 2.3507  LearningRate 0.1834  ProxyLR: 9.1709  Epoch: 1  Global Step: 6020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:07,706-Speed 3897.44 samples/sec  Loss 2.4021  LearningRate 0.1834  ProxyLR: 9.1696  Epoch: 1  Global Step: 6030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:10,332-Speed 3901.14 samples/sec  Loss 2.4226  LearningRate 0.1834  ProxyLR: 9.1682  Epoch: 1  Global Step: 6040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:12,958-Speed 3901.04 samples/sec  Loss 2.3092  LearningRate 0.1833  ProxyLR: 9.1669  Epoch: 1  Global Step: 6050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:15,584-Speed 3899.29 samples/sec  Loss 2.3265  LearningRate 0.1833  ProxyLR: 9.1656  Epoch: 1  Global Step: 6060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:18,209-Speed 3902.07 samples/sec  Loss 2.2851  LearningRate 0.1833  ProxyLR: 9.1642  Epoch: 1  Global Step: 6070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:20,835-Speed 3900.64 samples/sec  Loss 2.5094  LearningRate 0.1833  ProxyLR: 9.1629  Epoch: 1  Global Step: 6080   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:40:23,462-Speed 3898.98 samples/sec  Loss 2.5152  LearningRate 0.1832  ProxyLR: 9.1615  Epoch: 1  Global Step: 6090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:40:26,075-Speed 3919.68 samples/sec  Loss 2.2126  LearningRate 0.1832  ProxyLR: 9.1602  Epoch: 1  Global Step: 6100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:28,699-Speed 3903.27 samples/sec  Loss 2.4271  LearningRate 0.1832  ProxyLR: 9.1588  Epoch: 1  Global Step: 6110   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:31,326-Speed 3898.85 samples/sec  Loss 2.3712  LearningRate 0.1831  ProxyLR: 9.1575  Epoch: 1  Global Step: 6120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:33,952-Speed 3900.38 samples/sec  Loss 2.3335  LearningRate 0.1831  ProxyLR: 9.1561  Epoch: 1  Global Step: 6130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:36,587-Speed 3887.34 samples/sec  Loss 2.3469  LearningRate 0.1831  ProxyLR: 9.1548  Epoch: 1  Global Step: 6140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:39,220-Speed 3891.06 samples/sec  Loss 2.4966  LearningRate 0.1831  ProxyLR: 9.1534  Epoch: 1  Global Step: 6150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:41,849-Speed 3896.41 samples/sec  Loss 2.4141  LearningRate 0.1830  ProxyLR: 9.1521  Epoch: 1  Global Step: 6160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:44,480-Speed 3893.22 samples/sec  Loss 2.3677  LearningRate 0.1830  ProxyLR: 9.1507  Epoch: 1  Global Step: 6170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:47,108-Speed 3896.77 samples/sec  Loss 2.3381  LearningRate 0.1830  ProxyLR: 9.1494  Epoch: 1  Global Step: 6180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:49,739-Speed 3893.17 samples/sec  Loss 2.3896  LearningRate 0.1830  ProxyLR: 9.1481  Epoch: 1  Global Step: 6190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:40:52,371-Speed 3892.12 samples/sec  Loss 2.3574  LearningRate 0.1829  ProxyLR: 9.1467  Epoch: 1  Global Step: 6200   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:40:55,001-Speed 3894.22 samples/sec  Loss 2.2445  LearningRate 0.1829  ProxyLR: 9.1454  Epoch: 1  Global Step: 6210   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:40:57,619-Speed 3911.68 samples/sec  Loss 2.4036  LearningRate 0.1829  ProxyLR: 9.1440  Epoch: 1  Global Step: 6220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:00,249-Speed 3894.95 samples/sec  Loss 2.5468  LearningRate 0.1829  ProxyLR: 9.1427  Epoch: 1  Global Step: 6230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:02,880-Speed 3892.65 samples/sec  Loss 2.3082  LearningRate 0.1828  ProxyLR: 9.1413  Epoch: 1  Global Step: 6240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:05,511-Speed 3893.68 samples/sec  Loss 2.6149  LearningRate 0.1828  ProxyLR: 9.1400  Epoch: 1  Global Step: 6250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:08,142-Speed 3892.48 samples/sec  Loss 2.3024  LearningRate 0.1828  ProxyLR: 9.1386  Epoch: 1  Global Step: 6260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:10,773-Speed 3893.29 samples/sec  Loss 2.5113  LearningRate 0.1827  ProxyLR: 9.1373  Epoch: 1  Global Step: 6270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:13,403-Speed 3894.36 samples/sec  Loss 2.4223  LearningRate 0.1827  ProxyLR: 9.1359  Epoch: 1  Global Step: 6280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:16,034-Speed 3893.78 samples/sec  Loss 2.3184  LearningRate 0.1827  ProxyLR: 9.1346  Epoch: 1  Global Step: 6290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:18,666-Speed 3891.75 samples/sec  Loss 2.5681  LearningRate 0.1827  ProxyLR: 9.1333  Epoch: 1  Global Step: 6300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:21,297-Speed 3892.69 samples/sec  Loss 2.4302  LearningRate 0.1826  ProxyLR: 9.1319  Epoch: 1  Global Step: 6310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:23,916-Speed 3911.36 samples/sec  Loss 2.4362  LearningRate 0.1826  ProxyLR: 9.1306  Epoch: 1  Global Step: 6320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:26,532-Speed 3914.43 samples/sec  Loss 2.5747  LearningRate 0.1826  ProxyLR: 9.1292  Epoch: 1  Global Step: 6330   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:29,164-Speed 3891.51 samples/sec  Loss 2.4059  LearningRate 0.1826  ProxyLR: 9.1279  Epoch: 1  Global Step: 6340   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:31,796-Speed 3892.74 samples/sec  Loss 2.4975  LearningRate 0.1825  ProxyLR: 9.1265  Epoch: 1  Global Step: 6350   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:34,425-Speed 3895.46 samples/sec  Loss 2.5011  LearningRate 0.1825  ProxyLR: 9.1252  Epoch: 1  Global Step: 6360   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:37,053-Speed 3897.67 samples/sec  Loss 2.4892  LearningRate 0.1825  ProxyLR: 9.1238  Epoch: 1  Global Step: 6370   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:39,682-Speed 3895.50 samples/sec  Loss 2.2207  LearningRate 0.1825  ProxyLR: 9.1225  Epoch: 1  Global Step: 6380   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:42,312-Speed 3894.53 samples/sec  Loss 2.5111  LearningRate 0.1824  ProxyLR: 9.1212  Epoch: 1  Global Step: 6390   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:44,942-Speed 3894.06 samples/sec  Loss 2.3126  LearningRate 0.1824  ProxyLR: 9.1198  Epoch: 1  Global Step: 6400   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:47,571-Speed 3896.53 samples/sec  Loss 2.4284  LearningRate 0.1824  ProxyLR: 9.1185  Epoch: 1  Global Step: 6410   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:50,200-Speed 3896.76 samples/sec  Loss 2.3822  LearningRate 0.1823  ProxyLR: 9.1171  Epoch: 1  Global Step: 6420   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:41:52,829-Speed 3895.77 samples/sec  Loss 2.3507  LearningRate 0.1823  ProxyLR: 9.1158  Epoch: 1  Global Step: 6430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:55,457-Speed 3896.05 samples/sec  Loss 2.4399  LearningRate 0.1823  ProxyLR: 9.1144  Epoch: 1  Global Step: 6440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:41:58,085-Speed 3897.92 samples/sec  Loss 2.6754  LearningRate 0.1823  ProxyLR: 9.1131  Epoch: 1  Global Step: 6450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:00,714-Speed 3895.35 samples/sec  Loss 2.6088  LearningRate 0.1822  ProxyLR: 9.1118  Epoch: 1  Global Step: 6460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:03,343-Speed 3895.99 samples/sec  Loss 2.4918  LearningRate 0.1822  ProxyLR: 9.1104  Epoch: 1  Global Step: 6470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:05,970-Speed 3899.13 samples/sec  Loss 2.4277  LearningRate 0.1822  ProxyLR: 9.1091  Epoch: 1  Global Step: 6480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:08,599-Speed 3896.79 samples/sec  Loss 2.5030  LearningRate 0.1822  ProxyLR: 9.1077  Epoch: 1  Global Step: 6490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:11,225-Speed 3900.86 samples/sec  Loss 2.6436  LearningRate 0.1821  ProxyLR: 9.1064  Epoch: 1  Global Step: 6500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:13,859-Speed 3888.38 samples/sec  Loss 2.3840  LearningRate 0.1821  ProxyLR: 9.1050  Epoch: 1  Global Step: 6510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:16,494-Speed 3887.55 samples/sec  Loss 2.5814  LearningRate 0.1821  ProxyLR: 9.1037  Epoch: 1  Global Step: 6520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:19,115-Speed 3907.81 samples/sec  Loss 2.5385  LearningRate 0.1820  ProxyLR: 9.1024  Epoch: 1  Global Step: 6530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:21,750-Speed 3886.06 samples/sec  Loss 2.4737  LearningRate 0.1820  ProxyLR: 9.1010  Epoch: 1  Global Step: 6540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:24,382-Speed 3892.10 samples/sec  Loss 2.6301  LearningRate 0.1820  ProxyLR: 9.0997  Epoch: 1  Global Step: 6550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:27,014-Speed 3891.50 samples/sec  Loss 2.5219  LearningRate 0.1820  ProxyLR: 9.0983  Epoch: 1  Global Step: 6560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:29,647-Speed 3889.46 samples/sec  Loss 2.4967  LearningRate 0.1819  ProxyLR: 9.0970  Epoch: 1  Global Step: 6570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:32,281-Speed 3888.87 samples/sec  Loss 2.5208  LearningRate 0.1819  ProxyLR: 9.0956  Epoch: 1  Global Step: 6580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:34,908-Speed 3898.84 samples/sec  Loss 2.6286  LearningRate 0.1819  ProxyLR: 9.0943  Epoch: 1  Global Step: 6590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:37,538-Speed 3895.13 samples/sec  Loss 2.6984  LearningRate 0.1819  ProxyLR: 9.0930  Epoch: 1  Global Step: 6600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:40,166-Speed 3895.96 samples/sec  Loss 2.5425  LearningRate 0.1818  ProxyLR: 9.0916  Epoch: 1  Global Step: 6610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:42,795-Speed 3897.18 samples/sec  Loss 2.5617  LearningRate 0.1818  ProxyLR: 9.0903  Epoch: 1  Global Step: 6620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:45,424-Speed 3895.78 samples/sec  Loss 2.6903  LearningRate 0.1818  ProxyLR: 9.0889  Epoch: 1  Global Step: 6630   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:42:48,039-Speed 3915.79 samples/sec  Loss 2.4068  LearningRate 0.1818  ProxyLR: 9.0876  Epoch: 1  Global Step: 6640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:50,667-Speed 3897.88 samples/sec  Loss 2.4669  LearningRate 0.1817  ProxyLR: 9.0863  Epoch: 1  Global Step: 6650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:53,296-Speed 3896.54 samples/sec  Loss 2.4052  LearningRate 0.1817  ProxyLR: 9.0849  Epoch: 1  Global Step: 6660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:55,922-Speed 3899.93 samples/sec  Loss 2.4702  LearningRate 0.1817  ProxyLR: 9.0836  Epoch: 1  Global Step: 6670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:42:58,550-Speed 3897.05 samples/sec  Loss 2.5335  LearningRate 0.1816  ProxyLR: 9.0822  Epoch: 1  Global Step: 6680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:01,179-Speed 3895.83 samples/sec  Loss 2.5581  LearningRate 0.1816  ProxyLR: 9.0809  Epoch: 1  Global Step: 6690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:03,806-Speed 3899.57 samples/sec  Loss 2.4986  LearningRate 0.1816  ProxyLR: 9.0795  Epoch: 1  Global Step: 6700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:06,434-Speed 3896.90 samples/sec  Loss 2.6269  LearningRate 0.1816  ProxyLR: 9.0782  Epoch: 1  Global Step: 6710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:09,064-Speed 3895.53 samples/sec  Loss 2.4254  LearningRate 0.1815  ProxyLR: 9.0769  Epoch: 1  Global Step: 6720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:11,697-Speed 3889.85 samples/sec  Loss 2.5911  LearningRate 0.1815  ProxyLR: 9.0755  Epoch: 1  Global Step: 6730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:14,331-Speed 3887.73 samples/sec  Loss 2.5176  LearningRate 0.1815  ProxyLR: 9.0742  Epoch: 1  Global Step: 6740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:43:16,947-Speed 3915.56 samples/sec  Loss 2.3299  LearningRate 0.1815  ProxyLR: 9.0728  Epoch: 1  Global Step: 6750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:19,575-Speed 3897.90 samples/sec  Loss 2.7374  LearningRate 0.1814  ProxyLR: 9.0715  Epoch: 1  Global Step: 6760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:22,204-Speed 3895.19 samples/sec  Loss 2.6136  LearningRate 0.1814  ProxyLR: 9.0702  Epoch: 1  Global Step: 6770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:24,834-Speed 3894.35 samples/sec  Loss 2.5692  LearningRate 0.1814  ProxyLR: 9.0688  Epoch: 1  Global Step: 6780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:27,463-Speed 3896.22 samples/sec  Loss 2.5094  LearningRate 0.1813  ProxyLR: 9.0675  Epoch: 1  Global Step: 6790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:30,091-Speed 3897.10 samples/sec  Loss 2.5145  LearningRate 0.1813  ProxyLR: 9.0661  Epoch: 1  Global Step: 6800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:32,717-Speed 3900.15 samples/sec  Loss 2.6685  LearningRate 0.1813  ProxyLR: 9.0648  Epoch: 1  Global Step: 6810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:35,344-Speed 3899.01 samples/sec  Loss 2.5366  LearningRate 0.1813  ProxyLR: 9.0635  Epoch: 1  Global Step: 6820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:37,974-Speed 3895.30 samples/sec  Loss 2.7034  LearningRate 0.1812  ProxyLR: 9.0621  Epoch: 1  Global Step: 6830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:40,601-Speed 3899.26 samples/sec  Loss 2.4583  LearningRate 0.1812  ProxyLR: 9.0608  Epoch: 1  Global Step: 6840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:43,213-Speed 3919.96 samples/sec  Loss 2.6277  LearningRate 0.1812  ProxyLR: 9.0595  Epoch: 1  Global Step: 6850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:45,844-Speed 3893.85 samples/sec  Loss 2.4089  LearningRate 0.1812  ProxyLR: 9.0581  Epoch: 1  Global Step: 6860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:48,472-Speed 3897.27 samples/sec  Loss 2.5854  LearningRate 0.1811  ProxyLR: 9.0568  Epoch: 1  Global Step: 6870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:51,102-Speed 3894.82 samples/sec  Loss 2.6194  LearningRate 0.1811  ProxyLR: 9.0554  Epoch: 1  Global Step: 6880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:53,729-Speed 3897.82 samples/sec  Loss 2.6630  LearningRate 0.1811  ProxyLR: 9.0541  Epoch: 1  Global Step: 6890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:56,358-Speed 3896.97 samples/sec  Loss 2.6490  LearningRate 0.1811  ProxyLR: 9.0528  Epoch: 1  Global Step: 6900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:43:58,985-Speed 3898.67 samples/sec  Loss 2.6644  LearningRate 0.1810  ProxyLR: 9.0514  Epoch: 1  Global Step: 6910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:01,612-Speed 3899.40 samples/sec  Loss 2.7102  LearningRate 0.1810  ProxyLR: 9.0501  Epoch: 1  Global Step: 6920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:04,237-Speed 3901.96 samples/sec  Loss 2.5129  LearningRate 0.1810  ProxyLR: 9.0487  Epoch: 1  Global Step: 6930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:06,862-Speed 3901.62 samples/sec  Loss 2.4728  LearningRate 0.1809  ProxyLR: 9.0474  Epoch: 1  Global Step: 6940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:09,475-Speed 3919.27 samples/sec  Loss 2.6223  LearningRate 0.1809  ProxyLR: 9.0461  Epoch: 1  Global Step: 6950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:12,102-Speed 3900.00 samples/sec  Loss 2.5336  LearningRate 0.1809  ProxyLR: 9.0447  Epoch: 1  Global Step: 6960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:14,728-Speed 3899.29 samples/sec  Loss 2.5213  LearningRate 0.1809  ProxyLR: 9.0434  Epoch: 1  Global Step: 6970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:17,356-Speed 3897.92 samples/sec  Loss 2.5635  LearningRate 0.1808  ProxyLR: 9.0420  Epoch: 1  Global Step: 6980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:19,981-Speed 3901.89 samples/sec  Loss 2.5268  LearningRate 0.1808  ProxyLR: 9.0407  Epoch: 1  Global Step: 6990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:22,606-Speed 3901.62 samples/sec  Loss 2.5113  LearningRate 0.1808  ProxyLR: 9.0394  Epoch: 1  Global Step: 7000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:25,233-Speed 3899.22 samples/sec  Loss 2.7526  LearningRate 0.1808  ProxyLR: 9.0380  Epoch: 1  Global Step: 7010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:27,858-Speed 3901.57 samples/sec  Loss 2.6445  LearningRate 0.1807  ProxyLR: 9.0367  Epoch: 1  Global Step: 7020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:30,482-Speed 3902.75 samples/sec  Loss 2.5709  LearningRate 0.1807  ProxyLR: 9.0354  Epoch: 1  Global Step: 7030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:33,108-Speed 3900.39 samples/sec  Loss 2.6613  LearningRate 0.1807  ProxyLR: 9.0340  Epoch: 1  Global Step: 7040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:35,722-Speed 3919.55 samples/sec  Loss 2.5835  LearningRate 0.1807  ProxyLR: 9.0327  Epoch: 1  Global Step: 7050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:38,347-Speed 3901.32 samples/sec  Loss 2.5299  LearningRate 0.1806  ProxyLR: 9.0314  Epoch: 1  Global Step: 7060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:40,972-Speed 3902.39 samples/sec  Loss 2.6330  LearningRate 0.1806  ProxyLR: 9.0300  Epoch: 1  Global Step: 7070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:43,597-Speed 3901.42 samples/sec  Loss 2.6877  LearningRate 0.1806  ProxyLR: 9.0287  Epoch: 1  Global Step: 7080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:46,222-Speed 3901.40 samples/sec  Loss 2.6586  LearningRate 0.1805  ProxyLR: 9.0273  Epoch: 1  Global Step: 7090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:48,848-Speed 3901.18 samples/sec  Loss 2.4508  LearningRate 0.1805  ProxyLR: 9.0260  Epoch: 1  Global Step: 7100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:51,473-Speed 3901.53 samples/sec  Loss 2.4194  LearningRate 0.1805  ProxyLR: 9.0247  Epoch: 1  Global Step: 7110   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:54,098-Speed 3902.90 samples/sec  Loss 2.5412  LearningRate 0.1805  ProxyLR: 9.0233  Epoch: 1  Global Step: 7120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:56,723-Speed 3901.45 samples/sec  Loss 2.5379  LearningRate 0.1804  ProxyLR: 9.0220  Epoch: 1  Global Step: 7130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:44:59,347-Speed 3902.99 samples/sec  Loss 2.7008  LearningRate 0.1804  ProxyLR: 9.0207  Epoch: 1  Global Step: 7140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:01,972-Speed 3901.54 samples/sec  Loss 2.4040  LearningRate 0.1804  ProxyLR: 9.0193  Epoch: 1  Global Step: 7150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:45:04,583-Speed 3922.62 samples/sec  Loss 2.5421  LearningRate 0.1804  ProxyLR: 9.0180  Epoch: 1  Global Step: 7160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:07,208-Speed 3902.38 samples/sec  Loss 2.8300  LearningRate 0.1803  ProxyLR: 9.0166  Epoch: 1  Global Step: 7170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:09,833-Speed 3901.95 samples/sec  Loss 2.7290  LearningRate 0.1803  ProxyLR: 9.0153  Epoch: 1  Global Step: 7180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:12,457-Speed 3903.04 samples/sec  Loss 2.6933  LearningRate 0.1803  ProxyLR: 9.0140  Epoch: 1  Global Step: 7190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:15,084-Speed 3899.61 samples/sec  Loss 2.7314  LearningRate 0.1803  ProxyLR: 9.0126  Epoch: 1  Global Step: 7200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:17,696-Speed 3921.11 samples/sec  Loss 2.5543  LearningRate 0.1802  ProxyLR: 9.0113  Epoch: 1  Global Step: 7210   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:20,322-Speed 3901.01 samples/sec  Loss 2.6086  LearningRate 0.1802  ProxyLR: 9.0100  Epoch: 1  Global Step: 7220   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:22,947-Speed 3900.52 samples/sec  Loss 2.5375  LearningRate 0.1802  ProxyLR: 9.0086  Epoch: 1  Global Step: 7230   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:25,573-Speed 3900.22 samples/sec  Loss 2.6522  LearningRate 0.1801  ProxyLR: 9.0073  Epoch: 1  Global Step: 7240   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:28,199-Speed 3900.94 samples/sec  Loss 2.7138  LearningRate 0.1801  ProxyLR: 9.0060  Epoch: 1  Global Step: 7250   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:30,824-Speed 3901.52 samples/sec  Loss 2.6596  LearningRate 0.1801  ProxyLR: 9.0046  Epoch: 1  Global Step: 7260   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:33,450-Speed 3901.56 samples/sec  Loss 2.6521  LearningRate 0.1801  ProxyLR: 9.0033  Epoch: 1  Global Step: 7270   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:36,075-Speed 3901.29 samples/sec  Loss 2.5974  LearningRate 0.1800  ProxyLR: 9.0020  Epoch: 1  Global Step: 7280   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:38,699-Speed 3903.07 samples/sec  Loss 2.7168  LearningRate 0.1800  ProxyLR: 9.0006  Epoch: 1  Global Step: 7290   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:41,326-Speed 3898.69 samples/sec  Loss 2.5633  LearningRate 0.1800  ProxyLR: 8.9993  Epoch: 1  Global Step: 7300   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:45:43,952-Speed 3900.95 samples/sec  Loss 2.5620  LearningRate 0.1800  ProxyLR: 8.9980  Epoch: 1  Global Step: 7310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:46,578-Speed 3900.26 samples/sec  Loss 2.6891  LearningRate 0.1799  ProxyLR: 8.9966  Epoch: 1  Global Step: 7320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:49,205-Speed 3899.33 samples/sec  Loss 2.5373  LearningRate 0.1799  ProxyLR: 8.9953  Epoch: 1  Global Step: 7330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:51,830-Speed 3902.13 samples/sec  Loss 2.4132  LearningRate 0.1799  ProxyLR: 8.9940  Epoch: 1  Global Step: 7340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:54,455-Speed 3901.61 samples/sec  Loss 2.7846  LearningRate 0.1799  ProxyLR: 8.9926  Epoch: 1  Global Step: 7350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:57,080-Speed 3901.44 samples/sec  Loss 2.5502  LearningRate 0.1798  ProxyLR: 8.9913  Epoch: 1  Global Step: 7360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:45:59,705-Speed 3902.33 samples/sec  Loss 2.7031  LearningRate 0.1798  ProxyLR: 8.9899  Epoch: 1  Global Step: 7370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:02,330-Speed 3901.22 samples/sec  Loss 2.5050  LearningRate 0.1798  ProxyLR: 8.9886  Epoch: 1  Global Step: 7380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:04,954-Speed 3903.91 samples/sec  Loss 2.6080  LearningRate 0.1797  ProxyLR: 8.9873  Epoch: 1  Global Step: 7390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:07,579-Speed 3902.42 samples/sec  Loss 2.4941  LearningRate 0.1797  ProxyLR: 8.9859  Epoch: 1  Global Step: 7400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:10,203-Speed 3903.03 samples/sec  Loss 2.3738  LearningRate 0.1797  ProxyLR: 8.9846  Epoch: 1  Global Step: 7410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:46:12,815-Speed 3921.07 samples/sec  Loss 2.5334  LearningRate 0.1797  ProxyLR: 8.9833  Epoch: 1  Global Step: 7420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:15,441-Speed 3900.34 samples/sec  Loss 2.6735  LearningRate 0.1796  ProxyLR: 8.9819  Epoch: 1  Global Step: 7430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:18,067-Speed 3900.38 samples/sec  Loss 2.5724  LearningRate 0.1796  ProxyLR: 8.9806  Epoch: 1  Global Step: 7440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:20,692-Speed 3901.21 samples/sec  Loss 2.7058  LearningRate 0.1796  ProxyLR: 8.9793  Epoch: 1  Global Step: 7450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:23,316-Speed 3903.30 samples/sec  Loss 2.5432  LearningRate 0.1796  ProxyLR: 8.9779  Epoch: 1  Global Step: 7460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:25,941-Speed 3902.72 samples/sec  Loss 2.5289  LearningRate 0.1795  ProxyLR: 8.9766  Epoch: 1  Global Step: 7470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:28,565-Speed 3902.73 samples/sec  Loss 2.8303  LearningRate 0.1795  ProxyLR: 8.9753  Epoch: 1  Global Step: 7480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:31,191-Speed 3900.86 samples/sec  Loss 2.7305  LearningRate 0.1795  ProxyLR: 8.9739  Epoch: 1  Global Step: 7490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:33,816-Speed 3901.78 samples/sec  Loss 2.6795  LearningRate 0.1795  ProxyLR: 8.9726  Epoch: 1  Global Step: 7500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:36,447-Speed 3893.62 samples/sec  Loss 2.4855  LearningRate 0.1794  ProxyLR: 8.9713  Epoch: 1  Global Step: 7510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:39,059-Speed 3920.61 samples/sec  Loss 2.4767  LearningRate 0.1794  ProxyLR: 8.9699  Epoch: 1  Global Step: 7520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:41,692-Speed 3891.12 samples/sec  Loss 2.7127  LearningRate 0.1794  ProxyLR: 8.9686  Epoch: 1  Global Step: 7530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:44,323-Speed 3892.01 samples/sec  Loss 2.8004  LearningRate 0.1793  ProxyLR: 8.9673  Epoch: 1  Global Step: 7540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:46,956-Speed 3890.70 samples/sec  Loss 2.5355  LearningRate 0.1793  ProxyLR: 8.9660  Epoch: 1  Global Step: 7550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:49,589-Speed 3890.08 samples/sec  Loss 2.6146  LearningRate 0.1793  ProxyLR: 8.9646  Epoch: 1  Global Step: 7560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:52,220-Speed 3893.18 samples/sec  Loss 2.8078  LearningRate 0.1793  ProxyLR: 8.9633  Epoch: 1  Global Step: 7570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:54,851-Speed 3892.91 samples/sec  Loss 2.6587  LearningRate 0.1792  ProxyLR: 8.9620  Epoch: 1  Global Step: 7580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:46:57,483-Speed 3891.45 samples/sec  Loss 2.6784  LearningRate 0.1792  ProxyLR: 8.9606  Epoch: 1  Global Step: 7590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:00,114-Speed 3892.00 samples/sec  Loss 2.7418  LearningRate 0.1792  ProxyLR: 8.9593  Epoch: 1  Global Step: 7600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:02,746-Speed 3892.22 samples/sec  Loss 2.7928  LearningRate 0.1792  ProxyLR: 8.9580  Epoch: 1  Global Step: 7610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:05,364-Speed 3911.80 samples/sec  Loss 2.5847  LearningRate 0.1791  ProxyLR: 8.9566  Epoch: 1  Global Step: 7620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:07,995-Speed 3892.68 samples/sec  Loss 2.6788  LearningRate 0.1791  ProxyLR: 8.9553  Epoch: 1  Global Step: 7630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:10,628-Speed 3890.54 samples/sec  Loss 2.5538  LearningRate 0.1791  ProxyLR: 8.9540  Epoch: 1  Global Step: 7640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:13,262-Speed 3888.36 samples/sec  Loss 2.6224  LearningRate 0.1791  ProxyLR: 8.9526  Epoch: 1  Global Step: 7650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:15,894-Speed 3891.68 samples/sec  Loss 2.8419  LearningRate 0.1790  ProxyLR: 8.9513  Epoch: 1  Global Step: 7660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:18,526-Speed 3891.60 samples/sec  Loss 2.5432  LearningRate 0.1790  ProxyLR: 8.9500  Epoch: 1  Global Step: 7670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:21,157-Speed 3892.13 samples/sec  Loss 2.6051  LearningRate 0.1790  ProxyLR: 8.9486  Epoch: 1  Global Step: 7680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:23,790-Speed 3891.67 samples/sec  Loss 2.6647  LearningRate 0.1789  ProxyLR: 8.9473  Epoch: 1  Global Step: 7690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:26,420-Speed 3893.60 samples/sec  Loss 2.7036  LearningRate 0.1789  ProxyLR: 8.9460  Epoch: 1  Global Step: 7700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:29,049-Speed 3896.58 samples/sec  Loss 2.6693  LearningRate 0.1789  ProxyLR: 8.9446  Epoch: 1  Global Step: 7710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:31,677-Speed 3897.84 samples/sec  Loss 2.6806  LearningRate 0.1789  ProxyLR: 8.9433  Epoch: 1  Global Step: 7720   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:47:34,305-Speed 3896.25 samples/sec  Loss 2.5811  LearningRate 0.1788  ProxyLR: 8.9420  Epoch: 1  Global Step: 7730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:47:36,922-Speed 3914.88 samples/sec  Loss 2.5516  LearningRate 0.1788  ProxyLR: 8.9407  Epoch: 1  Global Step: 7740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:39,549-Speed 3898.59 samples/sec  Loss 2.4353  LearningRate 0.1788  ProxyLR: 8.9393  Epoch: 1  Global Step: 7750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:42,177-Speed 3897.32 samples/sec  Loss 2.8789  LearningRate 0.1788  ProxyLR: 8.9380  Epoch: 1  Global Step: 7760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:44,804-Speed 3898.17 samples/sec  Loss 2.7662  LearningRate 0.1787  ProxyLR: 8.9367  Epoch: 1  Global Step: 7770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:47,432-Speed 3898.09 samples/sec  Loss 2.4498  LearningRate 0.1787  ProxyLR: 8.9353  Epoch: 1  Global Step: 7780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:50,060-Speed 3897.96 samples/sec  Loss 2.6345  LearningRate 0.1787  ProxyLR: 8.9340  Epoch: 1  Global Step: 7790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:52,686-Speed 3900.07 samples/sec  Loss 2.8304  LearningRate 0.1787  ProxyLR: 8.9327  Epoch: 1  Global Step: 7800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:55,315-Speed 3895.80 samples/sec  Loss 2.5603  LearningRate 0.1786  ProxyLR: 8.9313  Epoch: 1  Global Step: 7810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:47:57,942-Speed 3898.15 samples/sec  Loss 2.7255  LearningRate 0.1786  ProxyLR: 8.9300  Epoch: 1  Global Step: 7820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:00,569-Speed 3898.97 samples/sec  Loss 2.6234  LearningRate 0.1786  ProxyLR: 8.9287  Epoch: 1  Global Step: 7830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:03,198-Speed 3896.44 samples/sec  Loss 2.7211  LearningRate 0.1785  ProxyLR: 8.9274  Epoch: 1  Global Step: 7840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:48:05,812-Speed 3918.72 samples/sec  Loss 2.5472  LearningRate 0.1785  ProxyLR: 8.9260  Epoch: 1  Global Step: 7850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:08,439-Speed 3898.61 samples/sec  Loss 2.5704  LearningRate 0.1785  ProxyLR: 8.9247  Epoch: 1  Global Step: 7860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:11,064-Speed 3901.55 samples/sec  Loss 2.5739  LearningRate 0.1785  ProxyLR: 8.9234  Epoch: 1  Global Step: 7870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:13,690-Speed 3899.89 samples/sec  Loss 2.9765  LearningRate 0.1784  ProxyLR: 8.9220  Epoch: 1  Global Step: 7880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:16,317-Speed 3900.10 samples/sec  Loss 2.6843  LearningRate 0.1784  ProxyLR: 8.9207  Epoch: 1  Global Step: 7890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:18,945-Speed 3897.04 samples/sec  Loss 2.6050  LearningRate 0.1784  ProxyLR: 8.9194  Epoch: 1  Global Step: 7900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:21,571-Speed 3901.15 samples/sec  Loss 2.6769  LearningRate 0.1784  ProxyLR: 8.9181  Epoch: 1  Global Step: 7910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:24,196-Speed 3901.52 samples/sec  Loss 2.7697  LearningRate 0.1783  ProxyLR: 8.9167  Epoch: 1  Global Step: 7920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:26,820-Speed 3903.68 samples/sec  Loss 2.7904  LearningRate 0.1783  ProxyLR: 8.9154  Epoch: 1  Global Step: 7930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:29,447-Speed 3898.42 samples/sec  Loss 2.6068  LearningRate 0.1783  ProxyLR: 8.9141  Epoch: 1  Global Step: 7940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:32,074-Speed 3898.25 samples/sec  Loss 2.6374  LearningRate 0.1783  ProxyLR: 8.9127  Epoch: 1  Global Step: 7950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:48:34,688-Speed 3918.41 samples/sec  Loss 2.6060  LearningRate 0.1782  ProxyLR: 8.9114  Epoch: 1  Global Step: 7960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:37,316-Speed 3897.53 samples/sec  Loss 2.7074  LearningRate 0.1782  ProxyLR: 8.9101  Epoch: 1  Global Step: 7970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:39,946-Speed 3894.50 samples/sec  Loss 2.6741  LearningRate 0.1782  ProxyLR: 8.9088  Epoch: 1  Global Step: 7980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:42,576-Speed 3894.56 samples/sec  Loss 2.6149  LearningRate 0.1781  ProxyLR: 8.9074  Epoch: 1  Global Step: 7990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:45,205-Speed 3896.20 samples/sec  Loss 2.4489  LearningRate 0.1781  ProxyLR: 8.9061  Epoch: 1  Global Step: 8000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:47,833-Speed 3896.73 samples/sec  Loss 2.6922  LearningRate 0.1781  ProxyLR: 8.9048  Epoch: 1  Global Step: 8010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:50,462-Speed 3896.91 samples/sec  Loss 2.4823  LearningRate 0.1781  ProxyLR: 8.9034  Epoch: 1  Global Step: 8020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:53,090-Speed 3897.09 samples/sec  Loss 2.5969  LearningRate 0.1780  ProxyLR: 8.9021  Epoch: 1  Global Step: 8030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:55,721-Speed 3892.36 samples/sec  Loss 2.8095  LearningRate 0.1780  ProxyLR: 8.9008  Epoch: 1  Global Step: 8040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:48:58,349-Speed 3897.97 samples/sec  Loss 2.5716  LearningRate 0.1780  ProxyLR: 8.8995  Epoch: 1  Global Step: 8050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:00,963-Speed 3918.06 samples/sec  Loss 2.8827  LearningRate 0.1780  ProxyLR: 8.8981  Epoch: 1  Global Step: 8060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:03,592-Speed 3895.79 samples/sec  Loss 2.6183  LearningRate 0.1779  ProxyLR: 8.8968  Epoch: 1  Global Step: 8070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:06,222-Speed 3894.48 samples/sec  Loss 2.8155  LearningRate 0.1779  ProxyLR: 8.8955  Epoch: 1  Global Step: 8080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:08,857-Speed 3887.33 samples/sec  Loss 2.5390  LearningRate 0.1779  ProxyLR: 8.8942  Epoch: 1  Global Step: 8090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:11,487-Speed 3894.14 samples/sec  Loss 2.6951  LearningRate 0.1779  ProxyLR: 8.8928  Epoch: 1  Global Step: 8100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:14,116-Speed 3896.50 samples/sec  Loss 2.7320  LearningRate 0.1778  ProxyLR: 8.8915  Epoch: 1  Global Step: 8110   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:16,747-Speed 3893.23 samples/sec  Loss 2.6888  LearningRate 0.1778  ProxyLR: 8.8902  Epoch: 1  Global Step: 8120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:19,380-Speed 3890.65 samples/sec  Loss 2.6354  LearningRate 0.1778  ProxyLR: 8.8888  Epoch: 1  Global Step: 8130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:22,012-Speed 3890.16 samples/sec  Loss 2.7355  LearningRate 0.1778  ProxyLR: 8.8875  Epoch: 1  Global Step: 8140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:24,647-Speed 3888.09 samples/sec  Loss 2.5985  LearningRate 0.1777  ProxyLR: 8.8862  Epoch: 1  Global Step: 8150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:27,280-Speed 3889.26 samples/sec  Loss 2.6860  LearningRate 0.1777  ProxyLR: 8.8849  Epoch: 1  Global Step: 8160   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:49:29,902-Speed 3906.83 samples/sec  Loss 2.6922  LearningRate 0.1777  ProxyLR: 8.8835  Epoch: 1  Global Step: 8170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:32,535-Speed 3889.06 samples/sec  Loss 2.6117  LearningRate 0.1776  ProxyLR: 8.8822  Epoch: 1  Global Step: 8180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:35,167-Speed 3891.55 samples/sec  Loss 2.6718  LearningRate 0.1776  ProxyLR: 8.8809  Epoch: 1  Global Step: 8190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:37,801-Speed 3889.49 samples/sec  Loss 2.7927  LearningRate 0.1776  ProxyLR: 8.8796  Epoch: 1  Global Step: 8200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:40,435-Speed 3888.65 samples/sec  Loss 2.7107  LearningRate 0.1776  ProxyLR: 8.8782  Epoch: 1  Global Step: 8210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:43,069-Speed 3887.53 samples/sec  Loss 2.8097  LearningRate 0.1775  ProxyLR: 8.8769  Epoch: 1  Global Step: 8220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:45,701-Speed 3892.59 samples/sec  Loss 2.7023  LearningRate 0.1775  ProxyLR: 8.8756  Epoch: 1  Global Step: 8230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:48,333-Speed 3891.15 samples/sec  Loss 2.8024  LearningRate 0.1775  ProxyLR: 8.8743  Epoch: 1  Global Step: 8240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:50,966-Speed 3890.52 samples/sec  Loss 2.6818  LearningRate 0.1775  ProxyLR: 8.8729  Epoch: 1  Global Step: 8250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:53,600-Speed 3888.55 samples/sec  Loss 2.8625  LearningRate 0.1774  ProxyLR: 8.8716  Epoch: 1  Global Step: 8260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:49:56,232-Speed 3890.32 samples/sec  Loss 2.6317  LearningRate 0.1774  ProxyLR: 8.8703  Epoch: 1  Global Step: 8270   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:49:58,852-Speed 3910.55 samples/sec  Loss 2.7067  LearningRate 0.1774  ProxyLR: 8.8690  Epoch: 1  Global Step: 8280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:50:01,484-Speed 3891.79 samples/sec  Loss 2.7237  LearningRate 0.1774  ProxyLR: 8.8676  Epoch: 1  Global Step: 8290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:50:04,117-Speed 3889.83 samples/sec  Loss 2.6489  LearningRate 0.1773  ProxyLR: 8.8663  Epoch: 1  Global Step: 8300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:50:06,739-Speed 3906.46 samples/sec  Loss 2.5897  LearningRate 0.1773  ProxyLR: 8.8650  Epoch: 1  Global Step: 8310   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:09,374-Speed 3886.58 samples/sec  Loss 2.6829  LearningRate 0.1773  ProxyLR: 8.8637  Epoch: 1  Global Step: 8320   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:12,010-Speed 3885.90 samples/sec  Loss 2.6303  LearningRate 0.1772  ProxyLR: 8.8623  Epoch: 1  Global Step: 8330   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:14,647-Speed 3884.72 samples/sec  Loss 2.7083  LearningRate 0.1772  ProxyLR: 8.8610  Epoch: 1  Global Step: 8340   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:17,282-Speed 3886.37 samples/sec  Loss 2.8679  LearningRate 0.1772  ProxyLR: 8.8597  Epoch: 1  Global Step: 8350   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:19,918-Speed 3886.53 samples/sec  Loss 2.7461  LearningRate 0.1772  ProxyLR: 8.8584  Epoch: 1  Global Step: 8360   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:22,554-Speed 3885.17 samples/sec  Loss 2.8216  LearningRate 0.1771  ProxyLR: 8.8570  Epoch: 1  Global Step: 8370   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:25,190-Speed 3886.52 samples/sec  Loss 2.8303  LearningRate 0.1771  ProxyLR: 8.8557  Epoch: 1  Global Step: 8380   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:27,826-Speed 3885.03 samples/sec  Loss 2.6598  LearningRate 0.1771  ProxyLR: 8.8544  Epoch: 1  Global Step: 8390   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:30,460-Speed 3888.10 samples/sec  Loss 2.6801  LearningRate 0.1771  ProxyLR: 8.8531  Epoch: 1  Global Step: 8400   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:33,094-Speed 3889.68 samples/sec  Loss 2.7338  LearningRate 0.1770  ProxyLR: 8.8517  Epoch: 1  Global Step: 8410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:50:35,712-Speed 3911.17 samples/sec  Loss 2.7189  LearningRate 0.1770  ProxyLR: 8.8504  Epoch: 1  Global Step: 8420   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:38,346-Speed 3888.56 samples/sec  Loss 2.7921  LearningRate 0.1770  ProxyLR: 8.8491  Epoch: 1  Global Step: 8430   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:40,982-Speed 3885.74 samples/sec  Loss 2.4536  LearningRate 0.1770  ProxyLR: 8.8478  Epoch: 1  Global Step: 8440   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:43,619-Speed 3885.39 samples/sec  Loss 2.7193  LearningRate 0.1769  ProxyLR: 8.8465  Epoch: 1  Global Step: 8450   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:46,254-Speed 3886.84 samples/sec  Loss 2.8794  LearningRate 0.1769  ProxyLR: 8.8451  Epoch: 1  Global Step: 8460   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:48,889-Speed 3887.46 samples/sec  Loss 2.9468  LearningRate 0.1769  ProxyLR: 8.8438  Epoch: 1  Global Step: 8470   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:51,519-Speed 3893.17 samples/sec  Loss 2.7929  LearningRate 0.1768  ProxyLR: 8.8425  Epoch: 1  Global Step: 8480   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:54,152-Speed 3890.68 samples/sec  Loss 2.7368  LearningRate 0.1768  ProxyLR: 8.8412  Epoch: 1  Global Step: 8490   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:56,784-Speed 3892.49 samples/sec  Loss 2.7688  LearningRate 0.1768  ProxyLR: 8.8398  Epoch: 1  Global Step: 8500   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:50:59,417-Speed 3889.36 samples/sec  Loss 2.8011  LearningRate 0.1768  ProxyLR: 8.8385  Epoch: 1  Global Step: 8510   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:51:02,049-Speed 3892.38 samples/sec  Loss 2.6752  LearningRate 0.1767  ProxyLR: 8.8372  Epoch: 1  Global Step: 8520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:04,678-Speed 3896.01 samples/sec  Loss 2.7192  LearningRate 0.1767  ProxyLR: 8.8359  Epoch: 1  Global Step: 8530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:07,306-Speed 3897.56 samples/sec  Loss 2.8315  LearningRate 0.1767  ProxyLR: 8.8345  Epoch: 1  Global Step: 8540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:09,933-Speed 3898.52 samples/sec  Loss 2.4566  LearningRate 0.1767  ProxyLR: 8.8332  Epoch: 1  Global Step: 8550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:12,558-Speed 3901.13 samples/sec  Loss 2.5781  LearningRate 0.1766  ProxyLR: 8.8319  Epoch: 1  Global Step: 8560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:15,186-Speed 3897.58 samples/sec  Loss 2.6207  LearningRate 0.1766  ProxyLR: 8.8306  Epoch: 1  Global Step: 8570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:17,813-Speed 3898.95 samples/sec  Loss 2.7793  LearningRate 0.1766  ProxyLR: 8.8293  Epoch: 1  Global Step: 8580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:20,440-Speed 3898.99 samples/sec  Loss 2.5363  LearningRate 0.1766  ProxyLR: 8.8279  Epoch: 1  Global Step: 8590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:23,068-Speed 3898.07 samples/sec  Loss 2.6615  LearningRate 0.1765  ProxyLR: 8.8266  Epoch: 1  Global Step: 8600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:25,696-Speed 3897.81 samples/sec  Loss 2.9198  LearningRate 0.1765  ProxyLR: 8.8253  Epoch: 1  Global Step: 8610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:28,325-Speed 3896.24 samples/sec  Loss 2.5736  LearningRate 0.1765  ProxyLR: 8.8240  Epoch: 1  Global Step: 8620   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:51:30,938-Speed 3919.28 samples/sec  Loss 2.5402  LearningRate 0.1765  ProxyLR: 8.8226  Epoch: 1  Global Step: 8630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:33,568-Speed 3895.03 samples/sec  Loss 2.9051  LearningRate 0.1764  ProxyLR: 8.8213  Epoch: 1  Global Step: 8640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:36,195-Speed 3899.09 samples/sec  Loss 2.6773  LearningRate 0.1764  ProxyLR: 8.8200  Epoch: 1  Global Step: 8650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:38,822-Speed 3898.68 samples/sec  Loss 2.5632  LearningRate 0.1764  ProxyLR: 8.8187  Epoch: 1  Global Step: 8660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:41,449-Speed 3899.04 samples/sec  Loss 2.7738  LearningRate 0.1763  ProxyLR: 8.8174  Epoch: 1  Global Step: 8670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:44,076-Speed 3898.82 samples/sec  Loss 2.7867  LearningRate 0.1763  ProxyLR: 8.8160  Epoch: 1  Global Step: 8680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:46,704-Speed 3898.02 samples/sec  Loss 2.4677  LearningRate 0.1763  ProxyLR: 8.8147  Epoch: 1  Global Step: 8690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:49,331-Speed 3899.60 samples/sec  Loss 2.6188  LearningRate 0.1763  ProxyLR: 8.8134  Epoch: 1  Global Step: 8700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:51,958-Speed 3898.37 samples/sec  Loss 2.6861  LearningRate 0.1762  ProxyLR: 8.8121  Epoch: 1  Global Step: 8710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:54,586-Speed 3897.24 samples/sec  Loss 2.6875  LearningRate 0.1762  ProxyLR: 8.8108  Epoch: 1  Global Step: 8720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:57,200-Speed 3919.06 samples/sec  Loss 2.5826  LearningRate 0.1762  ProxyLR: 8.8094  Epoch: 1  Global Step: 8730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:51:59,826-Speed 3899.20 samples/sec  Loss 2.7053  LearningRate 0.1762  ProxyLR: 8.8081  Epoch: 1  Global Step: 8740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:02,441-Speed 3918.22 samples/sec  Loss 2.3859  LearningRate 0.1761  ProxyLR: 8.8068  Epoch: 1  Global Step: 8750   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:05,070-Speed 3894.73 samples/sec  Loss 2.6660  LearningRate 0.1761  ProxyLR: 8.8055  Epoch: 1  Global Step: 8760   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:07,699-Speed 3897.30 samples/sec  Loss 2.6898  LearningRate 0.1761  ProxyLR: 8.8042  Epoch: 1  Global Step: 8770   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:10,327-Speed 3896.71 samples/sec  Loss 2.8038  LearningRate 0.1761  ProxyLR: 8.8028  Epoch: 1  Global Step: 8780   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:12,957-Speed 3894.80 samples/sec  Loss 2.6565  LearningRate 0.1760  ProxyLR: 8.8015  Epoch: 1  Global Step: 8790   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:15,584-Speed 3898.32 samples/sec  Loss 2.7510  LearningRate 0.1760  ProxyLR: 8.8002  Epoch: 1  Global Step: 8800   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:18,212-Speed 3897.78 samples/sec  Loss 2.6985  LearningRate 0.1760  ProxyLR: 8.7989  Epoch: 1  Global Step: 8810   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:20,840-Speed 3898.33 samples/sec  Loss 2.7890  LearningRate 0.1760  ProxyLR: 8.7976  Epoch: 1  Global Step: 8820   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:23,466-Speed 3899.82 samples/sec  Loss 2.8017  LearningRate 0.1759  ProxyLR: 8.7962  Epoch: 1  Global Step: 8830   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:26,094-Speed 3897.36 samples/sec  Loss 2.4939  LearningRate 0.1759  ProxyLR: 8.7949  Epoch: 1  Global Step: 8840   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:52:28,722-Speed 3897.84 samples/sec  Loss 2.5883  LearningRate 0.1759  ProxyLR: 8.7936  Epoch: 1  Global Step: 8850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:31,350-Speed 3897.95 samples/sec  Loss 2.5819  LearningRate 0.1758  ProxyLR: 8.7923  Epoch: 1  Global Step: 8860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:33,976-Speed 3899.69 samples/sec  Loss 2.5387  LearningRate 0.1758  ProxyLR: 8.7910  Epoch: 1  Global Step: 8870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:36,605-Speed 3896.63 samples/sec  Loss 2.7725  LearningRate 0.1758  ProxyLR: 8.7896  Epoch: 1  Global Step: 8880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:39,234-Speed 3896.19 samples/sec  Loss 2.7415  LearningRate 0.1758  ProxyLR: 8.7883  Epoch: 1  Global Step: 8890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:41,862-Speed 3896.51 samples/sec  Loss 2.5765  LearningRate 0.1757  ProxyLR: 8.7870  Epoch: 1  Global Step: 8900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:44,489-Speed 3899.08 samples/sec  Loss 2.6584  LearningRate 0.1757  ProxyLR: 8.7857  Epoch: 1  Global Step: 8910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:47,120-Speed 3892.99 samples/sec  Loss 2.5305  LearningRate 0.1757  ProxyLR: 8.7844  Epoch: 1  Global Step: 8920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:49,751-Speed 3893.70 samples/sec  Loss 2.5455  LearningRate 0.1757  ProxyLR: 8.7830  Epoch: 1  Global Step: 8930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:52,380-Speed 3896.09 samples/sec  Loss 2.6183  LearningRate 0.1756  ProxyLR: 8.7817  Epoch: 1  Global Step: 8940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:52:55,007-Speed 3898.41 samples/sec  Loss 2.5762  LearningRate 0.1756  ProxyLR: 8.7804  Epoch: 1  Global Step: 8950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:52:57,621-Speed 3918.77 samples/sec  Loss 2.4380  LearningRate 0.1756  ProxyLR: 8.7791  Epoch: 1  Global Step: 8960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:00,248-Speed 3898.93 samples/sec  Loss 2.8245  LearningRate 0.1756  ProxyLR: 8.7778  Epoch: 1  Global Step: 8970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:02,875-Speed 3898.34 samples/sec  Loss 2.7101  LearningRate 0.1755  ProxyLR: 8.7765  Epoch: 1  Global Step: 8980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:05,502-Speed 3900.14 samples/sec  Loss 2.6216  LearningRate 0.1755  ProxyLR: 8.7751  Epoch: 1  Global Step: 8990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:08,130-Speed 3896.42 samples/sec  Loss 2.7004  LearningRate 0.1755  ProxyLR: 8.7738  Epoch: 1  Global Step: 9000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:10,757-Speed 3899.80 samples/sec  Loss 2.7000  LearningRate 0.1755  ProxyLR: 8.7725  Epoch: 1  Global Step: 9010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:13,384-Speed 3898.07 samples/sec  Loss 2.7748  LearningRate 0.1754  ProxyLR: 8.7712  Epoch: 1  Global Step: 9020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:16,011-Speed 3899.06 samples/sec  Loss 2.7791  LearningRate 0.1754  ProxyLR: 8.7699  Epoch: 1  Global Step: 9030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:18,638-Speed 3899.00 samples/sec  Loss 2.8535  LearningRate 0.1754  ProxyLR: 8.7685  Epoch: 1  Global Step: 9040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:21,264-Speed 3900.42 samples/sec  Loss 2.8071  LearningRate 0.1753  ProxyLR: 8.7672  Epoch: 1  Global Step: 9050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:23,892-Speed 3897.65 samples/sec  Loss 2.6670  LearningRate 0.1753  ProxyLR: 8.7659  Epoch: 1  Global Step: 9060   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:53:26,519-Speed 3899.22 samples/sec  Loss 2.8276  LearningRate 0.1753  ProxyLR: 8.7646  Epoch: 1  Global Step: 9070   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:53:29,133-Speed 3917.94 samples/sec  Loss 2.9007  LearningRate 0.1753  ProxyLR: 8.7633  Epoch: 1  Global Step: 9080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:31,761-Speed 3898.06 samples/sec  Loss 2.7710  LearningRate 0.1752  ProxyLR: 8.7620  Epoch: 1  Global Step: 9090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:34,391-Speed 3895.62 samples/sec  Loss 2.6436  LearningRate 0.1752  ProxyLR: 8.7606  Epoch: 1  Global Step: 9100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:53:37,008-Speed 3913.53 samples/sec  Loss 2.8082  LearningRate 0.1752  ProxyLR: 8.7593  Epoch: 1  Global Step: 9110   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:39,643-Speed 3886.05 samples/sec  Loss 2.6823  LearningRate 0.1752  ProxyLR: 8.7580  Epoch: 1  Global Step: 9120   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:42,277-Speed 3889.18 samples/sec  Loss 2.8683  LearningRate 0.1751  ProxyLR: 8.7567  Epoch: 1  Global Step: 9130   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:44,911-Speed 3889.37 samples/sec  Loss 2.7365  LearningRate 0.1751  ProxyLR: 8.7554  Epoch: 1  Global Step: 9140   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:47,543-Speed 3891.58 samples/sec  Loss 2.6352  LearningRate 0.1751  ProxyLR: 8.7541  Epoch: 1  Global Step: 9150   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:50,176-Speed 3889.55 samples/sec  Loss 2.5979  LearningRate 0.1751  ProxyLR: 8.7527  Epoch: 1  Global Step: 9160   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:52,806-Speed 3894.33 samples/sec  Loss 2.7375  LearningRate 0.1750  ProxyLR: 8.7514  Epoch: 1  Global Step: 9170   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:55,438-Speed 3892.15 samples/sec  Loss 2.6208  LearningRate 0.1750  ProxyLR: 8.7501  Epoch: 1  Global Step: 9180   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:53:58,068-Speed 3893.28 samples/sec  Loss 2.7513  LearningRate 0.1750  ProxyLR: 8.7488  Epoch: 1  Global Step: 9190   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:54:00,700-Speed 3891.59 samples/sec  Loss 2.7509  LearningRate 0.1749  ProxyLR: 8.7475  Epoch: 1  Global Step: 9200   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:54:03,338-Speed 3883.71 samples/sec  Loss 2.6926  LearningRate 0.1749  ProxyLR: 8.7462  Epoch: 1  Global Step: 9210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:05,972-Speed 3888.73 samples/sec  Loss 2.6124  LearningRate 0.1749  ProxyLR: 8.7448  Epoch: 1  Global Step: 9220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:08,606-Speed 3888.72 samples/sec  Loss 2.6910  LearningRate 0.1749  ProxyLR: 8.7435  Epoch: 1  Global Step: 9230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:11,241-Speed 3887.69 samples/sec  Loss 2.4991  LearningRate 0.1748  ProxyLR: 8.7422  Epoch: 1  Global Step: 9240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:13,876-Speed 3887.38 samples/sec  Loss 2.7482  LearningRate 0.1748  ProxyLR: 8.7409  Epoch: 1  Global Step: 9250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:16,509-Speed 3889.52 samples/sec  Loss 2.5643  LearningRate 0.1748  ProxyLR: 8.7396  Epoch: 1  Global Step: 9260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:19,141-Speed 3891.62 samples/sec  Loss 2.7160  LearningRate 0.1748  ProxyLR: 8.7383  Epoch: 1  Global Step: 9270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:21,773-Speed 3892.36 samples/sec  Loss 2.7435  LearningRate 0.1747  ProxyLR: 8.7370  Epoch: 1  Global Step: 9280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:24,403-Speed 3894.24 samples/sec  Loss 2.7181  LearningRate 0.1747  ProxyLR: 8.7356  Epoch: 1  Global Step: 9290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:27,037-Speed 3888.16 samples/sec  Loss 2.6040  LearningRate 0.1747  ProxyLR: 8.7343  Epoch: 1  Global Step: 9300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:29,658-Speed 3907.78 samples/sec  Loss 2.7244  LearningRate 0.1747  ProxyLR: 8.7330  Epoch: 1  Global Step: 9310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:32,291-Speed 3890.62 samples/sec  Loss 2.7662  LearningRate 0.1746  ProxyLR: 8.7317  Epoch: 1  Global Step: 9320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:34,924-Speed 3890.46 samples/sec  Loss 2.8007  LearningRate 0.1746  ProxyLR: 8.7304  Epoch: 1  Global Step: 9330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:37,555-Speed 3892.24 samples/sec  Loss 2.7536  LearningRate 0.1746  ProxyLR: 8.7291  Epoch: 1  Global Step: 9340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:40,185-Speed 3894.61 samples/sec  Loss 2.5343  LearningRate 0.1746  ProxyLR: 8.7278  Epoch: 1  Global Step: 9350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:42,816-Speed 3893.38 samples/sec  Loss 2.7267  LearningRate 0.1745  ProxyLR: 8.7264  Epoch: 1  Global Step: 9360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:45,450-Speed 3889.50 samples/sec  Loss 2.7709  LearningRate 0.1745  ProxyLR: 8.7251  Epoch: 1  Global Step: 9370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:48,084-Speed 3887.17 samples/sec  Loss 2.7849  LearningRate 0.1745  ProxyLR: 8.7238  Epoch: 1  Global Step: 9380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:50,719-Speed 3887.46 samples/sec  Loss 2.6991  LearningRate 0.1744  ProxyLR: 8.7225  Epoch: 1  Global Step: 9390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:53,351-Speed 3891.40 samples/sec  Loss 2.7555  LearningRate 0.1744  ProxyLR: 8.7212  Epoch: 1  Global Step: 9400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:54:55,981-Speed 3895.60 samples/sec  Loss 2.7799  LearningRate 0.1744  ProxyLR: 8.7199  Epoch: 1  Global Step: 9410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:54:58,610-Speed 3895.14 samples/sec  Loss 2.6938  LearningRate 0.1744  ProxyLR: 8.7186  Epoch: 1  Global Step: 9420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:55:01,228-Speed 3913.47 samples/sec  Loss 2.6355  LearningRate 0.1743  ProxyLR: 8.7172  Epoch: 1  Global Step: 9430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:03,859-Speed 3892.81 samples/sec  Loss 2.7480  LearningRate 0.1743  ProxyLR: 8.7159  Epoch: 1  Global Step: 9440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:06,491-Speed 3891.62 samples/sec  Loss 2.6606  LearningRate 0.1743  ProxyLR: 8.7146  Epoch: 1  Global Step: 9450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:09,122-Speed 3892.85 samples/sec  Loss 2.6321  LearningRate 0.1743  ProxyLR: 8.7133  Epoch: 1  Global Step: 9460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:11,753-Speed 3892.95 samples/sec  Loss 2.6299  LearningRate 0.1742  ProxyLR: 8.7120  Epoch: 1  Global Step: 9470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:14,383-Speed 3893.77 samples/sec  Loss 2.6433  LearningRate 0.1742  ProxyLR: 8.7107  Epoch: 1  Global Step: 9480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:17,015-Speed 3892.90 samples/sec  Loss 2.8176  LearningRate 0.1742  ProxyLR: 8.7094  Epoch: 1  Global Step: 9490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:19,647-Speed 3891.21 samples/sec  Loss 2.7458  LearningRate 0.1742  ProxyLR: 8.7080  Epoch: 1  Global Step: 9500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:22,277-Speed 3894.90 samples/sec  Loss 2.7565  LearningRate 0.1741  ProxyLR: 8.7067  Epoch: 1  Global Step: 9510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:24,909-Speed 3891.32 samples/sec  Loss 2.7886  LearningRate 0.1741  ProxyLR: 8.7054  Epoch: 1  Global Step: 9520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:27,526-Speed 3912.90 samples/sec  Loss 2.6532  LearningRate 0.1741  ProxyLR: 8.7041  Epoch: 1  Global Step: 9530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:30,158-Speed 3892.80 samples/sec  Loss 2.6880  LearningRate 0.1741  ProxyLR: 8.7028  Epoch: 1  Global Step: 9540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:32,789-Speed 3892.27 samples/sec  Loss 2.6944  LearningRate 0.1740  ProxyLR: 8.7015  Epoch: 1  Global Step: 9550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:35,420-Speed 3893.75 samples/sec  Loss 2.5717  LearningRate 0.1740  ProxyLR: 8.7002  Epoch: 1  Global Step: 9560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:38,050-Speed 3893.67 samples/sec  Loss 2.5550  LearningRate 0.1740  ProxyLR: 8.6989  Epoch: 1  Global Step: 9570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:40,681-Speed 3892.68 samples/sec  Loss 2.8685  LearningRate 0.1740  ProxyLR: 8.6975  Epoch: 1  Global Step: 9580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:43,316-Speed 3887.12 samples/sec  Loss 2.5383  LearningRate 0.1739  ProxyLR: 8.6962  Epoch: 1  Global Step: 9590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:45,947-Speed 3893.72 samples/sec  Loss 2.6472  LearningRate 0.1739  ProxyLR: 8.6949  Epoch: 1  Global Step: 9600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:48,578-Speed 3893.23 samples/sec  Loss 2.5791  LearningRate 0.1739  ProxyLR: 8.6936  Epoch: 1  Global Step: 9610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:51,209-Speed 3893.25 samples/sec  Loss 2.5061  LearningRate 0.1738  ProxyLR: 8.6923  Epoch: 1  Global Step: 9620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:53,839-Speed 3893.95 samples/sec  Loss 2.6289  LearningRate 0.1738  ProxyLR: 8.6910  Epoch: 1  Global Step: 9630   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:55:56,456-Speed 3913.38 samples/sec  Loss 2.5975  LearningRate 0.1738  ProxyLR: 8.6897  Epoch: 1  Global Step: 9640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:55:59,086-Speed 3894.48 samples/sec  Loss 2.6143  LearningRate 0.1738  ProxyLR: 8.6884  Epoch: 1  Global Step: 9650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:01,716-Speed 3894.37 samples/sec  Loss 2.6577  LearningRate 0.1737  ProxyLR: 8.6871  Epoch: 1  Global Step: 9660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:04,347-Speed 3893.67 samples/sec  Loss 2.7643  LearningRate 0.1737  ProxyLR: 8.6857  Epoch: 1  Global Step: 9670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:06,978-Speed 3892.45 samples/sec  Loss 2.5938  LearningRate 0.1737  ProxyLR: 8.6844  Epoch: 1  Global Step: 9680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:09,609-Speed 3894.25 samples/sec  Loss 2.8004  LearningRate 0.1737  ProxyLR: 8.6831  Epoch: 1  Global Step: 9690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:12,240-Speed 3893.57 samples/sec  Loss 2.6709  LearningRate 0.1736  ProxyLR: 8.6818  Epoch: 1  Global Step: 9700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:14,872-Speed 3890.21 samples/sec  Loss 2.5148  LearningRate 0.1736  ProxyLR: 8.6805  Epoch: 1  Global Step: 9710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:17,505-Speed 3891.31 samples/sec  Loss 2.8425  LearningRate 0.1736  ProxyLR: 8.6792  Epoch: 1  Global Step: 9720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:20,135-Speed 3893.73 samples/sec  Loss 2.6886  LearningRate 0.1736  ProxyLR: 8.6779  Epoch: 1  Global Step: 9730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:22,766-Speed 3893.60 samples/sec  Loss 2.6083  LearningRate 0.1735  ProxyLR: 8.6766  Epoch: 1  Global Step: 9740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 14:56:25,382-Speed 3914.18 samples/sec  Loss 2.8775  LearningRate 0.1735  ProxyLR: 8.6753  Epoch: 1  Global Step: 9750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:28,014-Speed 3891.58 samples/sec  Loss 2.7159  LearningRate 0.1735  ProxyLR: 8.6739  Epoch: 1  Global Step: 9760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:30,645-Speed 3893.37 samples/sec  Loss 2.7065  LearningRate 0.1735  ProxyLR: 8.6726  Epoch: 1  Global Step: 9770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:33,277-Speed 3891.26 samples/sec  Loss 2.7793  LearningRate 0.1734  ProxyLR: 8.6713  Epoch: 1  Global Step: 9780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:35,906-Speed 3896.07 samples/sec  Loss 2.4523  LearningRate 0.1734  ProxyLR: 8.6700  Epoch: 1  Global Step: 9790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:38,537-Speed 3893.38 samples/sec  Loss 2.6362  LearningRate 0.1734  ProxyLR: 8.6687  Epoch: 1  Global Step: 9800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:41,167-Speed 3894.95 samples/sec  Loss 2.3868  LearningRate 0.1733  ProxyLR: 8.6674  Epoch: 1  Global Step: 9810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:43,797-Speed 3893.97 samples/sec  Loss 2.6019  LearningRate 0.1733  ProxyLR: 8.6661  Epoch: 1  Global Step: 9820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:46,429-Speed 3892.31 samples/sec  Loss 2.7024  LearningRate 0.1733  ProxyLR: 8.6648  Epoch: 1  Global Step: 9830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:49,059-Speed 3893.80 samples/sec  Loss 2.7540  LearningRate 0.1733  ProxyLR: 8.6635  Epoch: 1  Global Step: 9840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:56:51,663-Speed 3932.65 samples/sec  Loss 2.6602  LearningRate 0.1732  ProxyLR: 8.6622  Epoch: 1  Global Step: 9850   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:56:54,292-Speed 3896.44 samples/sec  Loss 2.8161  LearningRate 0.1732  ProxyLR: 8.6608  Epoch: 1  Global Step: 9860   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:56:56,922-Speed 3894.37 samples/sec  Loss 2.4941  LearningRate 0.1732  ProxyLR: 8.6595  Epoch: 1  Global Step: 9870   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:56:59,553-Speed 3892.67 samples/sec  Loss 2.5652  LearningRate 0.1732  ProxyLR: 8.6582  Epoch: 1  Global Step: 9880   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:02,185-Speed 3891.70 samples/sec  Loss 2.6313  LearningRate 0.1731  ProxyLR: 8.6569  Epoch: 1  Global Step: 9890   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:04,817-Speed 3893.12 samples/sec  Loss 2.7858  LearningRate 0.1731  ProxyLR: 8.6556  Epoch: 1  Global Step: 9900   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:07,446-Speed 3894.73 samples/sec  Loss 2.6745  LearningRate 0.1731  ProxyLR: 8.6543  Epoch: 1  Global Step: 9910   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:10,077-Speed 3893.16 samples/sec  Loss 2.6491  LearningRate 0.1731  ProxyLR: 8.6530  Epoch: 1  Global Step: 9920   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:12,708-Speed 3893.75 samples/sec  Loss 2.6041  LearningRate 0.1730  ProxyLR: 8.6517  Epoch: 1  Global Step: 9930   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:15,339-Speed 3893.42 samples/sec  Loss 2.4724  LearningRate 0.1730  ProxyLR: 8.6504  Epoch: 1  Global Step: 9940   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 14:57:17,968-Speed 3895.02 samples/sec  Loss 2.6503  LearningRate 0.1730  ProxyLR: 8.6491  Epoch: 1  Global Step: 9950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:57:20,597-Speed 3896.92 samples/sec  Loss 2.6384  LearningRate 0.1730  ProxyLR: 8.6478  Epoch: 1  Global Step: 9960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:57:23,230-Speed 3889.90 samples/sec  Loss 2.6978  LearningRate 0.1729  ProxyLR: 8.6464  Epoch: 1  Global Step: 9970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:57:25,867-Speed 3883.23 samples/sec  Loss 2.6580  LearningRate 0.1729  ProxyLR: 8.6451  Epoch: 1  Global Step: 9980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:57:28,496-Speed 3896.01 samples/sec  Loss 2.7202  LearningRate 0.1729  ProxyLR: 8.6438  Epoch: 1  Global Step: 9990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:57:31,125-Speed 3896.28 samples/sec  Loss 2.6553  LearningRate 0.1729  ProxyLR: 8.6425  Epoch: 1  Global Step: 10000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 14:58:21,743-[lfw][10000]XNorm: 21.549797
Training: 2023-05-04 14:58:21,743-[lfw][10000]Accuracy-Flip: 0.99033+-0.00386
Training: 2023-05-04 14:58:21,744-[lfw][10000]Accuracy-Highest: 0.99033
Training: 2023-05-04 14:58:21,744-[lfw][10000]TPR@1stNon-Zero-FPR of 0.00033: 0.91467
Training: 2023-05-04 14:58:21,744-[lfw][10000]Highest TPR@FPR: 0.91467
Training: 2023-05-04 14:59:18,594-[cfp_fp][10000]XNorm: 20.610168
Training: 2023-05-04 14:59:18,594-[cfp_fp][10000]Accuracy-Flip: 0.85743+-0.01440
Training: 2023-05-04 14:59:18,595-[cfp_fp][10000]Accuracy-Highest: 0.85743
Training: 2023-05-04 14:59:18,595-[cfp_fp][10000]TPR@1stNon-Zero-FPR of 0.00029: 0.17257
Training: 2023-05-04 14:59:18,595-[cfp_fp][10000]Highest TPR@FPR: 0.17257
Training: 2023-05-04 15:00:08,102-[agedb_30][10000]XNorm: 21.589996
Training: 2023-05-04 15:00:08,102-[agedb_30][10000]Accuracy-Flip: 0.87683+-0.01696
Training: 2023-05-04 15:00:08,102-[agedb_30][10000]Accuracy-Highest: 0.87683
Training: 2023-05-04 15:00:08,103-[agedb_30][10000]TPR@1stNon-Zero-FPR of 0.00033: 0.06633
Training: 2023-05-04 15:00:08,103-[agedb_30][10000]Highest TPR@FPR: 0.06633
Training: 2023-05-04 15:00:59,023-[calfw][10000]XNorm: 22.016582
Training: 2023-05-04 15:00:59,024-[calfw][10000]Accuracy-Flip: 0.91783+-0.01145
Training: 2023-05-04 15:00:59,024-[calfw][10000]Accuracy-Highest: 0.91783
Training: 2023-05-04 15:00:59,024-[calfw][10000]TPR@1stNon-Zero-FPR of 0.00033: 0.51900
Training: 2023-05-04 15:00:59,024-[calfw][10000]Highest TPR@FPR: 0.51900
Training: 2023-05-04 15:01:49,989-[cplfw][10000]XNorm: 19.912959
Training: 2023-05-04 15:01:49,990-[cplfw][10000]Accuracy-Flip: 0.82800+-0.02350
Training: 2023-05-04 15:01:49,990-[cplfw][10000]Accuracy-Highest: 0.82800
Training: 2023-05-04 15:01:49,990-[cplfw][10000]TPR@1stNon-Zero-FPR of 0.00033: 0.00167
Training: 2023-05-04 15:01:49,990-[cplfw][10000]Highest TPR@FPR: 0.00167
Training: 2023-05-04 15:01:52,646-Speed 39.16 samples/sec  Loss 2.6050  LearningRate 0.1728  ProxyLR: 8.6412  Epoch: 1  Global Step: 10010   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:01:55,267-Speed 3907.18 samples/sec  Loss 2.6986  LearningRate 0.1728  ProxyLR: 8.6399  Epoch: 1  Global Step: 10020   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:01:57,889-Speed 3906.62 samples/sec  Loss 2.7431  LearningRate 0.1728  ProxyLR: 8.6386  Epoch: 1  Global Step: 10030   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:00,513-Speed 3903.39 samples/sec  Loss 2.6433  LearningRate 0.1727  ProxyLR: 8.6373  Epoch: 1  Global Step: 10040   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:03,138-Speed 3903.04 samples/sec  Loss 2.6771  LearningRate 0.1727  ProxyLR: 8.6360  Epoch: 1  Global Step: 10050   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:02:05,748-Speed 3923.84 samples/sec  Loss 2.7626  LearningRate 0.1727  ProxyLR: 8.6347  Epoch: 1  Global Step: 10060   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:08,374-Speed 3901.07 samples/sec  Loss 2.7320  LearningRate 0.1727  ProxyLR: 8.6334  Epoch: 1  Global Step: 10070   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:10,998-Speed 3903.70 samples/sec  Loss 2.6371  LearningRate 0.1726  ProxyLR: 8.6321  Epoch: 1  Global Step: 10080   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:13,624-Speed 3900.54 samples/sec  Loss 2.8240  LearningRate 0.1726  ProxyLR: 8.6308  Epoch: 1  Global Step: 10090   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:16,250-Speed 3900.52 samples/sec  Loss 2.8076  LearningRate 0.1726  ProxyLR: 8.6294  Epoch: 1  Global Step: 10100   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:18,876-Speed 3900.73 samples/sec  Loss 2.7541  LearningRate 0.1726  ProxyLR: 8.6281  Epoch: 1  Global Step: 10110   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:21,503-Speed 3898.73 samples/sec  Loss 2.5661  LearningRate 0.1725  ProxyLR: 8.6268  Epoch: 1  Global Step: 10120   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:24,129-Speed 3899.47 samples/sec  Loss 2.5982  LearningRate 0.1725  ProxyLR: 8.6255  Epoch: 1  Global Step: 10130   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:26,759-Speed 3895.06 samples/sec  Loss 2.5308  LearningRate 0.1725  ProxyLR: 8.6242  Epoch: 1  Global Step: 10140   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:29,391-Speed 3891.36 samples/sec  Loss 2.6694  LearningRate 0.1725  ProxyLR: 8.6229  Epoch: 1  Global Step: 10150   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:32,008-Speed 3915.05 samples/sec  Loss 2.8720  LearningRate 0.1724  ProxyLR: 8.6216  Epoch: 1  Global Step: 10160   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:34,637-Speed 3896.41 samples/sec  Loss 2.5303  LearningRate 0.1724  ProxyLR: 8.6203  Epoch: 1  Global Step: 10170   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:37,266-Speed 3895.12 samples/sec  Loss 2.5719  LearningRate 0.1724  ProxyLR: 8.6190  Epoch: 1  Global Step: 10180   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:39,896-Speed 3894.32 samples/sec  Loss 2.5939  LearningRate 0.1724  ProxyLR: 8.6177  Epoch: 1  Global Step: 10190   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:42,525-Speed 3896.12 samples/sec  Loss 2.7130  LearningRate 0.1723  ProxyLR: 8.6164  Epoch: 1  Global Step: 10200   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:45,156-Speed 3893.29 samples/sec  Loss 2.4969  LearningRate 0.1723  ProxyLR: 8.6151  Epoch: 1  Global Step: 10210   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:47,787-Speed 3893.58 samples/sec  Loss 2.5255  LearningRate 0.1723  ProxyLR: 8.6138  Epoch: 1  Global Step: 10220   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:50,417-Speed 3894.60 samples/sec  Loss 2.8198  LearningRate 0.1722  ProxyLR: 8.6125  Epoch: 1  Global Step: 10230   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:53,051-Speed 3888.36 samples/sec  Loss 2.5543  LearningRate 0.1722  ProxyLR: 8.6112  Epoch: 1  Global Step: 10240   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:55,682-Speed 3893.50 samples/sec  Loss 2.5183  LearningRate 0.1722  ProxyLR: 8.6099  Epoch: 1  Global Step: 10250   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:02:58,312-Speed 3894.91 samples/sec  Loss 2.6672  LearningRate 0.1722  ProxyLR: 8.6086  Epoch: 1  Global Step: 10260   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:03:00,930-Speed 3912.71 samples/sec  Loss 2.5553  LearningRate 0.1721  ProxyLR: 8.6072  Epoch: 1  Global Step: 10270   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:03,562-Speed 3890.91 samples/sec  Loss 2.5616  LearningRate 0.1721  ProxyLR: 8.6059  Epoch: 1  Global Step: 10280   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:06,194-Speed 3891.88 samples/sec  Loss 2.4298  LearningRate 0.1721  ProxyLR: 8.6046  Epoch: 1  Global Step: 10290   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:08,826-Speed 3892.51 samples/sec  Loss 2.6972  LearningRate 0.1721  ProxyLR: 8.6033  Epoch: 1  Global Step: 10300   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:11,458-Speed 3890.70 samples/sec  Loss 2.7400  LearningRate 0.1720  ProxyLR: 8.6020  Epoch: 1  Global Step: 10310   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:14,090-Speed 3891.98 samples/sec  Loss 2.6238  LearningRate 0.1720  ProxyLR: 8.6007  Epoch: 1  Global Step: 10320   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:16,722-Speed 3891.89 samples/sec  Loss 2.5329  LearningRate 0.1720  ProxyLR: 8.5994  Epoch: 1  Global Step: 10330   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:19,354-Speed 3891.16 samples/sec  Loss 2.4860  LearningRate 0.1720  ProxyLR: 8.5981  Epoch: 1  Global Step: 10340   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:21,985-Speed 3893.51 samples/sec  Loss 2.5323  LearningRate 0.1719  ProxyLR: 8.5968  Epoch: 1  Global Step: 10350   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:24,617-Speed 3891.10 samples/sec  Loss 2.7245  LearningRate 0.1719  ProxyLR: 8.5955  Epoch: 1  Global Step: 10360   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:27,249-Speed 3891.08 samples/sec  Loss 2.7451  LearningRate 0.1719  ProxyLR: 8.5942  Epoch: 1  Global Step: 10370   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:03:29,881-Speed 3892.59 samples/sec  Loss 2.5839  LearningRate 0.1719  ProxyLR: 8.5929  Epoch: 1  Global Step: 10380   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:03:32,512-Speed 3893.19 samples/sec  Loss 2.5767  LearningRate 0.1718  ProxyLR: 8.5916  Epoch: 1  Global Step: 10390   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:03:35,145-Speed 3889.38 samples/sec  Loss 2.6674  LearningRate 0.1718  ProxyLR: 8.5903  Epoch: 1  Global Step: 10400   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:03:37,764-Speed 3912.10 samples/sec  Loss 2.5115  LearningRate 0.1718  ProxyLR: 8.5890  Epoch: 1  Global Step: 10410   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:40,394-Speed 3893.57 samples/sec  Loss 2.6082  LearningRate 0.1718  ProxyLR: 8.5877  Epoch: 1  Global Step: 10420   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:43,025-Speed 3893.03 samples/sec  Loss 2.7187  LearningRate 0.1717  ProxyLR: 8.5864  Epoch: 1  Global Step: 10430   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:45,655-Speed 3894.48 samples/sec  Loss 2.6364  LearningRate 0.1717  ProxyLR: 8.5851  Epoch: 1  Global Step: 10440   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:48,284-Speed 3896.13 samples/sec  Loss 2.7139  LearningRate 0.1717  ProxyLR: 8.5838  Epoch: 1  Global Step: 10450   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:50,913-Speed 3896.60 samples/sec  Loss 2.7833  LearningRate 0.1716  ProxyLR: 8.5825  Epoch: 1  Global Step: 10460   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:53,541-Speed 3897.57 samples/sec  Loss 2.7689  LearningRate 0.1716  ProxyLR: 8.5812  Epoch: 1  Global Step: 10470   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:56,172-Speed 3893.26 samples/sec  Loss 2.7267  LearningRate 0.1716  ProxyLR: 8.5799  Epoch: 1  Global Step: 10480   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:03:58,801-Speed 3895.24 samples/sec  Loss 2.6864  LearningRate 0.1716  ProxyLR: 8.5786  Epoch: 1  Global Step: 10490   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:01,430-Speed 3896.30 samples/sec  Loss 2.6991  LearningRate 0.1715  ProxyLR: 8.5772  Epoch: 1  Global Step: 10500   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:04,047-Speed 3914.55 samples/sec  Loss 2.5076  LearningRate 0.1715  ProxyLR: 8.5759  Epoch: 1  Global Step: 10510   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:06,676-Speed 3896.80 samples/sec  Loss 2.5770  LearningRate 0.1715  ProxyLR: 8.5746  Epoch: 1  Global Step: 10520   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:09,303-Speed 3898.30 samples/sec  Loss 2.5143  LearningRate 0.1715  ProxyLR: 8.5733  Epoch: 1  Global Step: 10530   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:11,933-Speed 3894.36 samples/sec  Loss 2.5808  LearningRate 0.1714  ProxyLR: 8.5720  Epoch: 1  Global Step: 10540   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:14,563-Speed 3894.78 samples/sec  Loss 2.7487  LearningRate 0.1714  ProxyLR: 8.5707  Epoch: 1  Global Step: 10550   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:17,195-Speed 3892.52 samples/sec  Loss 2.3857  LearningRate 0.1714  ProxyLR: 8.5694  Epoch: 1  Global Step: 10560   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:19,824-Speed 3894.86 samples/sec  Loss 2.5663  LearningRate 0.1714  ProxyLR: 8.5681  Epoch: 1  Global Step: 10570   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:22,456-Speed 3893.02 samples/sec  Loss 2.5356  LearningRate 0.1713  ProxyLR: 8.5668  Epoch: 1  Global Step: 10580   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:25,086-Speed 3893.63 samples/sec  Loss 2.6215  LearningRate 0.1713  ProxyLR: 8.5655  Epoch: 1  Global Step: 10590   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:27,720-Speed 3889.55 samples/sec  Loss 2.5489  LearningRate 0.1713  ProxyLR: 8.5642  Epoch: 1  Global Step: 10600   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:30,342-Speed 3906.49 samples/sec  Loss 2.6245  LearningRate 0.1713  ProxyLR: 8.5629  Epoch: 1  Global Step: 10610   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:32,980-Speed 3882.42 samples/sec  Loss 2.5036  LearningRate 0.1712  ProxyLR: 8.5616  Epoch: 1  Global Step: 10620   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:35,608-Speed 3896.58 samples/sec  Loss 2.4731  LearningRate 0.1712  ProxyLR: 8.5603  Epoch: 1  Global Step: 10630   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:38,239-Speed 3894.12 samples/sec  Loss 2.5129  LearningRate 0.1712  ProxyLR: 8.5590  Epoch: 1  Global Step: 10640   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:40,871-Speed 3892.05 samples/sec  Loss 2.7117  LearningRate 0.1712  ProxyLR: 8.5577  Epoch: 1  Global Step: 10650   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:43,503-Speed 3890.77 samples/sec  Loss 2.7293  LearningRate 0.1711  ProxyLR: 8.5564  Epoch: 1  Global Step: 10660   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:46,136-Speed 3890.74 samples/sec  Loss 2.7121  LearningRate 0.1711  ProxyLR: 8.5551  Epoch: 1  Global Step: 10670   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:48,767-Speed 3892.82 samples/sec  Loss 2.6774  LearningRate 0.1711  ProxyLR: 8.5538  Epoch: 1  Global Step: 10680   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:51,400-Speed 3890.58 samples/sec  Loss 2.6059  LearningRate 0.1711  ProxyLR: 8.5525  Epoch: 1  Global Step: 10690   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:54,032-Speed 3891.06 samples/sec  Loss 2.6003  LearningRate 0.1710  ProxyLR: 8.5512  Epoch: 1  Global Step: 10700   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:04:56,666-Speed 3888.69 samples/sec  Loss 2.4761  LearningRate 0.1710  ProxyLR: 8.5499  Epoch: 1  Global Step: 10710   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:04:59,301-Speed 3888.26 samples/sec  Loss 2.5756  LearningRate 0.1710  ProxyLR: 8.5486  Epoch: 1  Global Step: 10720   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:05:01,920-Speed 3910.22 samples/sec  Loss 2.6164  LearningRate 0.1709  ProxyLR: 8.5473  Epoch: 1  Global Step: 10730   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:04,553-Speed 3890.30 samples/sec  Loss 2.6521  LearningRate 0.1709  ProxyLR: 8.5460  Epoch: 1  Global Step: 10740   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:07,186-Speed 3889.26 samples/sec  Loss 2.5543  LearningRate 0.1709  ProxyLR: 8.5447  Epoch: 1  Global Step: 10750   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:09,821-Speed 3888.12 samples/sec  Loss 2.6537  LearningRate 0.1709  ProxyLR: 8.5434  Epoch: 1  Global Step: 10760   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:12,453-Speed 3892.19 samples/sec  Loss 2.6193  LearningRate 0.1708  ProxyLR: 8.5421  Epoch: 1  Global Step: 10770   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:15,083-Speed 3893.51 samples/sec  Loss 2.6359  LearningRate 0.1708  ProxyLR: 8.5408  Epoch: 1  Global Step: 10780   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:17,714-Speed 3893.94 samples/sec  Loss 2.5246  LearningRate 0.1708  ProxyLR: 8.5395  Epoch: 1  Global Step: 10790   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:20,345-Speed 3892.37 samples/sec  Loss 2.4833  LearningRate 0.1708  ProxyLR: 8.5382  Epoch: 1  Global Step: 10800   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:22,973-Speed 3897.30 samples/sec  Loss 2.5097  LearningRate 0.1707  ProxyLR: 8.5369  Epoch: 1  Global Step: 10810   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:25,610-Speed 3884.74 samples/sec  Loss 2.5410  LearningRate 0.1707  ProxyLR: 8.5356  Epoch: 1  Global Step: 10820   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:28,243-Speed 3890.70 samples/sec  Loss 2.4210  LearningRate 0.1707  ProxyLR: 8.5343  Epoch: 1  Global Step: 10830   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:05:30,873-Speed 3893.35 samples/sec  Loss 2.4829  LearningRate 0.1707  ProxyLR: 8.5330  Epoch: 1  Global Step: 10840   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:05:33,490-Speed 3915.02 samples/sec  Loss 2.6167  LearningRate 0.1706  ProxyLR: 8.5317  Epoch: 1  Global Step: 10850   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:36,122-Speed 3891.33 samples/sec  Loss 2.6998  LearningRate 0.1706  ProxyLR: 8.5304  Epoch: 1  Global Step: 10860   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:38,754-Speed 3891.71 samples/sec  Loss 2.7293  LearningRate 0.1706  ProxyLR: 8.5291  Epoch: 1  Global Step: 10870   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:41,384-Speed 3894.12 samples/sec  Loss 2.5940  LearningRate 0.1706  ProxyLR: 8.5278  Epoch: 1  Global Step: 10880   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:44,014-Speed 3894.73 samples/sec  Loss 2.6960  LearningRate 0.1705  ProxyLR: 8.5265  Epoch: 1  Global Step: 10890   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:46,648-Speed 3889.69 samples/sec  Loss 2.5182  LearningRate 0.1705  ProxyLR: 8.5252  Epoch: 1  Global Step: 10900   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:49,279-Speed 3892.50 samples/sec  Loss 2.4893  LearningRate 0.1705  ProxyLR: 8.5239  Epoch: 1  Global Step: 10910   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:51,919-Speed 3879.42 samples/sec  Loss 2.5420  LearningRate 0.1705  ProxyLR: 8.5226  Epoch: 1  Global Step: 10920   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:54,556-Speed 3885.23 samples/sec  Loss 2.5264  LearningRate 0.1704  ProxyLR: 8.5213  Epoch: 1  Global Step: 10930   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:57,190-Speed 3888.01 samples/sec  Loss 2.5717  LearningRate 0.1704  ProxyLR: 8.5200  Epoch: 1  Global Step: 10940   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:05:59,829-Speed 3881.42 samples/sec  Loss 2.7281  LearningRate 0.1704  ProxyLR: 8.5187  Epoch: 1  Global Step: 10950   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:06:02,452-Speed 3905.50 samples/sec  Loss 2.5570  LearningRate 0.1703  ProxyLR: 8.5174  Epoch: 1  Global Step: 10960   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:05,088-Speed 3885.63 samples/sec  Loss 2.6104  LearningRate 0.1703  ProxyLR: 8.5161  Epoch: 1  Global Step: 10970   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:07,721-Speed 3889.73 samples/sec  Loss 2.5952  LearningRate 0.1703  ProxyLR: 8.5148  Epoch: 1  Global Step: 10980   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:10,352-Speed 3892.20 samples/sec  Loss 2.6541  LearningRate 0.1703  ProxyLR: 8.5135  Epoch: 1  Global Step: 10990   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:12,985-Speed 3890.29 samples/sec  Loss 2.7136  LearningRate 0.1702  ProxyLR: 8.5122  Epoch: 1  Global Step: 11000   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:15,619-Speed 3889.23 samples/sec  Loss 2.6803  LearningRate 0.1702  ProxyLR: 8.5109  Epoch: 1  Global Step: 11010   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:18,253-Speed 3889.06 samples/sec  Loss 2.5432  LearningRate 0.1702  ProxyLR: 8.5096  Epoch: 1  Global Step: 11020   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:20,886-Speed 3889.66 samples/sec  Loss 2.3757  LearningRate 0.1702  ProxyLR: 8.5083  Epoch: 1  Global Step: 11030   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:23,520-Speed 3889.50 samples/sec  Loss 2.6699  LearningRate 0.1701  ProxyLR: 8.5070  Epoch: 1  Global Step: 11040   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:26,153-Speed 3889.92 samples/sec  Loss 2.6571  LearningRate 0.1701  ProxyLR: 8.5057  Epoch: 1  Global Step: 11050   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:28,786-Speed 3889.63 samples/sec  Loss 2.6747  LearningRate 0.1701  ProxyLR: 8.5044  Epoch: 1  Global Step: 11060   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:06:31,407-Speed 3908.88 samples/sec  Loss 2.5812  LearningRate 0.1701  ProxyLR: 8.5031  Epoch: 1  Global Step: 11070   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:34,042-Speed 3887.25 samples/sec  Loss 2.4867  LearningRate 0.1700  ProxyLR: 8.5018  Epoch: 1  Global Step: 11080   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:36,677-Speed 3887.10 samples/sec  Loss 2.5095  LearningRate 0.1700  ProxyLR: 8.5005  Epoch: 1  Global Step: 11090   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:39,312-Speed 3887.32 samples/sec  Loss 2.5418  LearningRate 0.1700  ProxyLR: 8.4992  Epoch: 1  Global Step: 11100   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:41,947-Speed 3886.18 samples/sec  Loss 2.3760  LearningRate 0.1700  ProxyLR: 8.4979  Epoch: 1  Global Step: 11110   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:44,585-Speed 3883.56 samples/sec  Loss 2.4536  LearningRate 0.1699  ProxyLR: 8.4967  Epoch: 1  Global Step: 11120   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:47,222-Speed 3883.56 samples/sec  Loss 2.6185  LearningRate 0.1699  ProxyLR: 8.4954  Epoch: 1  Global Step: 11130   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:49,859-Speed 3884.26 samples/sec  Loss 2.5252  LearningRate 0.1699  ProxyLR: 8.4941  Epoch: 1  Global Step: 11140   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:52,495-Speed 3886.01 samples/sec  Loss 2.6506  LearningRate 0.1699  ProxyLR: 8.4928  Epoch: 1  Global Step: 11150   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:55,131-Speed 3885.51 samples/sec  Loss 2.4902  LearningRate 0.1698  ProxyLR: 8.4915  Epoch: 1  Global Step: 11160   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:06:57,754-Speed 3905.19 samples/sec  Loss 2.5938  LearningRate 0.1698  ProxyLR: 8.4902  Epoch: 1  Global Step: 11170   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:00,391-Speed 3884.46 samples/sec  Loss 2.7135  LearningRate 0.1698  ProxyLR: 8.4889  Epoch: 1  Global Step: 11180   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:03,029-Speed 3882.84 samples/sec  Loss 2.4659  LearningRate 0.1698  ProxyLR: 8.4876  Epoch: 1  Global Step: 11190   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:05,664-Speed 3886.48 samples/sec  Loss 2.6679  LearningRate 0.1697  ProxyLR: 8.4863  Epoch: 1  Global Step: 11200   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:08,300-Speed 3886.04 samples/sec  Loss 2.5481  LearningRate 0.1697  ProxyLR: 8.4850  Epoch: 1  Global Step: 11210   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:10,939-Speed 3882.20 samples/sec  Loss 2.5686  LearningRate 0.1697  ProxyLR: 8.4837  Epoch: 1  Global Step: 11220   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:13,576-Speed 3884.10 samples/sec  Loss 2.6445  LearningRate 0.1696  ProxyLR: 8.4824  Epoch: 1  Global Step: 11230   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:16,215-Speed 3880.94 samples/sec  Loss 2.4954  LearningRate 0.1696  ProxyLR: 8.4811  Epoch: 1  Global Step: 11240   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:18,855-Speed 3879.72 samples/sec  Loss 2.5438  LearningRate 0.1696  ProxyLR: 8.4798  Epoch: 1  Global Step: 11250   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:21,493-Speed 3882.33 samples/sec  Loss 2.4794  LearningRate 0.1696  ProxyLR: 8.4785  Epoch: 1  Global Step: 11260   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:24,131-Speed 3883.15 samples/sec  Loss 2.6251  LearningRate 0.1695  ProxyLR: 8.4772  Epoch: 1  Global Step: 11270   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:07:26,756-Speed 3902.13 samples/sec  Loss 2.6979  LearningRate 0.1695  ProxyLR: 8.4759  Epoch: 1  Global Step: 11280   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:29,394-Speed 3883.40 samples/sec  Loss 2.3413  LearningRate 0.1695  ProxyLR: 8.4746  Epoch: 1  Global Step: 11290   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:32,033-Speed 3880.56 samples/sec  Loss 2.5790  LearningRate 0.1695  ProxyLR: 8.4733  Epoch: 1  Global Step: 11300   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:34,672-Speed 3880.75 samples/sec  Loss 2.4055  LearningRate 0.1694  ProxyLR: 8.4720  Epoch: 1  Global Step: 11310   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:37,312-Speed 3880.34 samples/sec  Loss 2.5448  LearningRate 0.1694  ProxyLR: 8.4707  Epoch: 1  Global Step: 11320   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:39,951-Speed 3881.69 samples/sec  Loss 2.6877  LearningRate 0.1694  ProxyLR: 8.4694  Epoch: 1  Global Step: 11330   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:42,590-Speed 3881.06 samples/sec  Loss 2.4920  LearningRate 0.1694  ProxyLR: 8.4681  Epoch: 1  Global Step: 11340   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:45,231-Speed 3878.77 samples/sec  Loss 2.5618  LearningRate 0.1693  ProxyLR: 8.4668  Epoch: 1  Global Step: 11350   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:47,865-Speed 3887.63 samples/sec  Loss 2.6756  LearningRate 0.1693  ProxyLR: 8.4656  Epoch: 1  Global Step: 11360   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:07:50,549-Speed 3816.71 samples/sec  Loss 2.6144  LearningRate 0.1693  ProxyLR: 8.4643  Epoch: 1  Global Step: 11370   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:00,207-Speed 1060.38 samples/sec  Loss 1.8066  LearningRate 0.1693  ProxyLR: 8.4630  Epoch: 2  Global Step: 11380   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:08:02,860-Speed 3860.93 samples/sec  Loss 1.6131  LearningRate 0.1692  ProxyLR: 8.4617  Epoch: 2  Global Step: 11390   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:08:05,513-Speed 3860.34 samples/sec  Loss 1.6100  LearningRate 0.1692  ProxyLR: 8.4604  Epoch: 2  Global Step: 11400   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:08:08,156-Speed 3875.97 samples/sec  Loss 1.4961  LearningRate 0.1692  ProxyLR: 8.4591  Epoch: 2  Global Step: 11410   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:10,783-Speed 3899.12 samples/sec  Loss 1.5648  LearningRate 0.1692  ProxyLR: 8.4578  Epoch: 2  Global Step: 11420   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:13,411-Speed 3896.99 samples/sec  Loss 1.4631  LearningRate 0.1691  ProxyLR: 8.4565  Epoch: 2  Global Step: 11430   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:16,038-Speed 3899.08 samples/sec  Loss 1.5274  LearningRate 0.1691  ProxyLR: 8.4552  Epoch: 2  Global Step: 11440   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:18,664-Speed 3900.36 samples/sec  Loss 1.5361  LearningRate 0.1691  ProxyLR: 8.4539  Epoch: 2  Global Step: 11450   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:21,293-Speed 3896.53 samples/sec  Loss 1.4631  LearningRate 0.1691  ProxyLR: 8.4526  Epoch: 2  Global Step: 11460   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:23,923-Speed 3894.18 samples/sec  Loss 1.5031  LearningRate 0.1690  ProxyLR: 8.4513  Epoch: 2  Global Step: 11470   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:26,554-Speed 3893.34 samples/sec  Loss 1.3771  LearningRate 0.1690  ProxyLR: 8.4500  Epoch: 2  Global Step: 11480   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:29,183-Speed 3896.13 samples/sec  Loss 1.5958  LearningRate 0.1690  ProxyLR: 8.4487  Epoch: 2  Global Step: 11490   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:31,810-Speed 3898.60 samples/sec  Loss 1.3746  LearningRate 0.1689  ProxyLR: 8.4474  Epoch: 2  Global Step: 11500   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:34,423-Speed 3919.30 samples/sec  Loss 1.4143  LearningRate 0.1689  ProxyLR: 8.4461  Epoch: 2  Global Step: 11510   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:37,051-Speed 3898.05 samples/sec  Loss 1.5221  LearningRate 0.1689  ProxyLR: 8.4449  Epoch: 2  Global Step: 11520   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:39,704-Speed 3860.35 samples/sec  Loss 1.4525  LearningRate 0.1689  ProxyLR: 8.4436  Epoch: 2  Global Step: 11530   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:42,334-Speed 3895.40 samples/sec  Loss 1.5977  LearningRate 0.1688  ProxyLR: 8.4423  Epoch: 2  Global Step: 11540   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:44,961-Speed 3898.50 samples/sec  Loss 1.4457  LearningRate 0.1688  ProxyLR: 8.4410  Epoch: 2  Global Step: 11550   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:47,613-Speed 3862.14 samples/sec  Loss 1.4538  LearningRate 0.1688  ProxyLR: 8.4397  Epoch: 2  Global Step: 11560   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:50,240-Speed 3899.33 samples/sec  Loss 1.4123  LearningRate 0.1688  ProxyLR: 8.4384  Epoch: 2  Global Step: 11570   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:52,867-Speed 3898.33 samples/sec  Loss 1.4677  LearningRate 0.1687  ProxyLR: 8.4371  Epoch: 2  Global Step: 11580   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:55,494-Speed 3898.94 samples/sec  Loss 1.6179  LearningRate 0.1687  ProxyLR: 8.4358  Epoch: 2  Global Step: 11590   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:08:58,121-Speed 3899.38 samples/sec  Loss 1.4517  LearningRate 0.1687  ProxyLR: 8.4345  Epoch: 2  Global Step: 11600   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:00,747-Speed 3899.40 samples/sec  Loss 1.4507  LearningRate 0.1687  ProxyLR: 8.4332  Epoch: 2  Global Step: 11610   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:03,374-Speed 3899.40 samples/sec  Loss 1.4352  LearningRate 0.1686  ProxyLR: 8.4319  Epoch: 2  Global Step: 11620   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:06,000-Speed 3900.59 samples/sec  Loss 1.5969  LearningRate 0.1686  ProxyLR: 8.4306  Epoch: 2  Global Step: 11630   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:08,628-Speed 3896.95 samples/sec  Loss 1.3106  LearningRate 0.1686  ProxyLR: 8.4293  Epoch: 2  Global Step: 11640   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:11,255-Speed 3899.89 samples/sec  Loss 1.4361  LearningRate 0.1686  ProxyLR: 8.4281  Epoch: 2  Global Step: 11650   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:13,880-Speed 3901.30 samples/sec  Loss 1.4114  LearningRate 0.1685  ProxyLR: 8.4268  Epoch: 2  Global Step: 11660   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:16,506-Speed 3900.88 samples/sec  Loss 1.4459  LearningRate 0.1685  ProxyLR: 8.4255  Epoch: 2  Global Step: 11670   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:19,133-Speed 3898.90 samples/sec  Loss 1.5632  LearningRate 0.1685  ProxyLR: 8.4242  Epoch: 2  Global Step: 11680   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:21,761-Speed 3897.73 samples/sec  Loss 1.4474  LearningRate 0.1685  ProxyLR: 8.4229  Epoch: 2  Global Step: 11690   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:24,389-Speed 3896.42 samples/sec  Loss 1.3737  LearningRate 0.1684  ProxyLR: 8.4216  Epoch: 2  Global Step: 11700   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:27,005-Speed 3916.47 samples/sec  Loss 1.4203  LearningRate 0.1684  ProxyLR: 8.4203  Epoch: 2  Global Step: 11710   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:29,631-Speed 3900.17 samples/sec  Loss 1.5047  LearningRate 0.1684  ProxyLR: 8.4190  Epoch: 2  Global Step: 11720   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:32,257-Speed 3900.36 samples/sec  Loss 1.4486  LearningRate 0.1684  ProxyLR: 8.4177  Epoch: 2  Global Step: 11730   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:34,882-Speed 3900.85 samples/sec  Loss 1.6140  LearningRate 0.1683  ProxyLR: 8.4164  Epoch: 2  Global Step: 11740   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:09:37,496-Speed 3919.42 samples/sec  Loss 1.5434  LearningRate 0.1683  ProxyLR: 8.4151  Epoch: 2  Global Step: 11750   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:40,121-Speed 3900.98 samples/sec  Loss 1.7193  LearningRate 0.1683  ProxyLR: 8.4139  Epoch: 2  Global Step: 11760   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:42,748-Speed 3899.25 samples/sec  Loss 1.5514  LearningRate 0.1683  ProxyLR: 8.4126  Epoch: 2  Global Step: 11770   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:45,376-Speed 3897.17 samples/sec  Loss 1.6102  LearningRate 0.1682  ProxyLR: 8.4113  Epoch: 2  Global Step: 11780   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:48,001-Speed 3901.77 samples/sec  Loss 1.4514  LearningRate 0.1682  ProxyLR: 8.4100  Epoch: 2  Global Step: 11790   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:50,627-Speed 3901.06 samples/sec  Loss 1.4972  LearningRate 0.1682  ProxyLR: 8.4087  Epoch: 2  Global Step: 11800   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:53,254-Speed 3898.86 samples/sec  Loss 1.3340  LearningRate 0.1681  ProxyLR: 8.4074  Epoch: 2  Global Step: 11810   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:55,880-Speed 3900.58 samples/sec  Loss 1.5399  LearningRate 0.1681  ProxyLR: 8.4061  Epoch: 2  Global Step: 11820   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:09:58,506-Speed 3899.96 samples/sec  Loss 1.4692  LearningRate 0.1681  ProxyLR: 8.4048  Epoch: 2  Global Step: 11830   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:01,132-Speed 3900.06 samples/sec  Loss 1.5153  LearningRate 0.1681  ProxyLR: 8.4035  Epoch: 2  Global Step: 11840   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:03,758-Speed 3900.77 samples/sec  Loss 1.6285  LearningRate 0.1680  ProxyLR: 8.4022  Epoch: 2  Global Step: 11850   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:10:06,371-Speed 3919.81 samples/sec  Loss 1.5497  LearningRate 0.1680  ProxyLR: 8.4010  Epoch: 2  Global Step: 11860   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:08,997-Speed 3900.14 samples/sec  Loss 1.4985  LearningRate 0.1680  ProxyLR: 8.3997  Epoch: 2  Global Step: 11870   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:11,623-Speed 3900.46 samples/sec  Loss 1.4562  LearningRate 0.1680  ProxyLR: 8.3984  Epoch: 2  Global Step: 11880   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:14,249-Speed 3899.66 samples/sec  Loss 1.4119  LearningRate 0.1679  ProxyLR: 8.3971  Epoch: 2  Global Step: 11890   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:16,876-Speed 3899.43 samples/sec  Loss 1.5307  LearningRate 0.1679  ProxyLR: 8.3958  Epoch: 2  Global Step: 11900   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:19,502-Speed 3901.09 samples/sec  Loss 1.5537  LearningRate 0.1679  ProxyLR: 8.3945  Epoch: 2  Global Step: 11910   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:22,128-Speed 3900.16 samples/sec  Loss 1.6210  LearningRate 0.1679  ProxyLR: 8.3932  Epoch: 2  Global Step: 11920   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:24,757-Speed 3896.71 samples/sec  Loss 1.5297  LearningRate 0.1678  ProxyLR: 8.3919  Epoch: 2  Global Step: 11930   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:27,383-Speed 3899.10 samples/sec  Loss 1.6654  LearningRate 0.1678  ProxyLR: 8.3906  Epoch: 2  Global Step: 11940   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:30,011-Speed 3898.70 samples/sec  Loss 1.5158  LearningRate 0.1678  ProxyLR: 8.3893  Epoch: 2  Global Step: 11950   Fp16 Grad Scale: 262144  Required: 11 hours
Training: 2023-05-04 15:10:32,638-Speed 3898.28 samples/sec  Loss 1.5747  LearningRate 0.1678  ProxyLR: 8.3881  Epoch: 2  Global Step: 11960   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:10:35,266-Speed 3897.06 samples/sec  Loss 1.4980  LearningRate 0.1677  ProxyLR: 8.3868  Epoch: 2  Global Step: 11970   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:10:37,893-Speed 3899.24 samples/sec  Loss 1.5363  LearningRate 0.1677  ProxyLR: 8.3855  Epoch: 2  Global Step: 11980   Fp16 Grad Scale: 524288  Required: 11 hours
Training: 2023-05-04 15:10:40,508-Speed 3916.72 samples/sec  Loss 1.5071  LearningRate 0.1677  ProxyLR: 8.3842  Epoch: 2  Global Step: 11990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:43,136-Speed 3897.18 samples/sec  Loss 1.5763  LearningRate 0.1677  ProxyLR: 8.3829  Epoch: 2  Global Step: 12000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:45,764-Speed 3897.55 samples/sec  Loss 1.6225  LearningRate 0.1676  ProxyLR: 8.3816  Epoch: 2  Global Step: 12010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:48,393-Speed 3895.90 samples/sec  Loss 1.6379  LearningRate 0.1676  ProxyLR: 8.3803  Epoch: 2  Global Step: 12020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:51,023-Speed 3895.09 samples/sec  Loss 1.5414  LearningRate 0.1676  ProxyLR: 8.3790  Epoch: 2  Global Step: 12030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:53,649-Speed 3901.13 samples/sec  Loss 1.5011  LearningRate 0.1676  ProxyLR: 8.3778  Epoch: 2  Global Step: 12040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:56,274-Speed 3901.54 samples/sec  Loss 1.5349  LearningRate 0.1675  ProxyLR: 8.3765  Epoch: 2  Global Step: 12050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:10:58,899-Speed 3901.49 samples/sec  Loss 1.5912  LearningRate 0.1675  ProxyLR: 8.3752  Epoch: 2  Global Step: 12060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:01,526-Speed 3899.13 samples/sec  Loss 1.5894  LearningRate 0.1675  ProxyLR: 8.3739  Epoch: 2  Global Step: 12070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:04,150-Speed 3902.96 samples/sec  Loss 1.5746  LearningRate 0.1675  ProxyLR: 8.3726  Epoch: 2  Global Step: 12080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:06,774-Speed 3903.77 samples/sec  Loss 1.7373  LearningRate 0.1674  ProxyLR: 8.3713  Epoch: 2  Global Step: 12090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:11:09,397-Speed 3904.61 samples/sec  Loss 1.4353  LearningRate 0.1674  ProxyLR: 8.3700  Epoch: 2  Global Step: 12100   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:11:12,021-Speed 3903.25 samples/sec  Loss 1.6889  LearningRate 0.1674  ProxyLR: 8.3687  Epoch: 2  Global Step: 12110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:11:14,632-Speed 3923.27 samples/sec  Loss 1.7018  LearningRate 0.1673  ProxyLR: 8.3675  Epoch: 2  Global Step: 12120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:17,257-Speed 3901.84 samples/sec  Loss 1.6703  LearningRate 0.1673  ProxyLR: 8.3662  Epoch: 2  Global Step: 12130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:19,880-Speed 3904.21 samples/sec  Loss 1.5957  LearningRate 0.1673  ProxyLR: 8.3649  Epoch: 2  Global Step: 12140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:22,505-Speed 3902.19 samples/sec  Loss 1.5681  LearningRate 0.1673  ProxyLR: 8.3636  Epoch: 2  Global Step: 12150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:25,130-Speed 3902.52 samples/sec  Loss 1.6209  LearningRate 0.1672  ProxyLR: 8.3623  Epoch: 2  Global Step: 12160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:27,755-Speed 3901.47 samples/sec  Loss 1.5080  LearningRate 0.1672  ProxyLR: 8.3610  Epoch: 2  Global Step: 12170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:30,380-Speed 3902.52 samples/sec  Loss 1.5386  LearningRate 0.1672  ProxyLR: 8.3597  Epoch: 2  Global Step: 12180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:33,006-Speed 3900.69 samples/sec  Loss 1.5435  LearningRate 0.1672  ProxyLR: 8.3584  Epoch: 2  Global Step: 12190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:35,630-Speed 3902.63 samples/sec  Loss 1.5539  LearningRate 0.1671  ProxyLR: 8.3572  Epoch: 2  Global Step: 12200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:38,254-Speed 3903.21 samples/sec  Loss 1.6403  LearningRate 0.1671  ProxyLR: 8.3559  Epoch: 2  Global Step: 12210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:40,878-Speed 3904.49 samples/sec  Loss 1.5845  LearningRate 0.1671  ProxyLR: 8.3546  Epoch: 2  Global Step: 12220   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:11:43,488-Speed 3923.80 samples/sec  Loss 1.6603  LearningRate 0.1671  ProxyLR: 8.3533  Epoch: 2  Global Step: 12230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:46,111-Speed 3905.18 samples/sec  Loss 1.7446  LearningRate 0.1670  ProxyLR: 8.3520  Epoch: 2  Global Step: 12240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:48,733-Speed 3905.90 samples/sec  Loss 1.8192  LearningRate 0.1670  ProxyLR: 8.3507  Epoch: 2  Global Step: 12250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:51,356-Speed 3904.66 samples/sec  Loss 1.6391  LearningRate 0.1670  ProxyLR: 8.3494  Epoch: 2  Global Step: 12260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:53,978-Speed 3906.66 samples/sec  Loss 1.6167  LearningRate 0.1670  ProxyLR: 8.3482  Epoch: 2  Global Step: 12270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:56,601-Speed 3906.07 samples/sec  Loss 1.5920  LearningRate 0.1669  ProxyLR: 8.3469  Epoch: 2  Global Step: 12280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:11:59,225-Speed 3903.41 samples/sec  Loss 1.7029  LearningRate 0.1669  ProxyLR: 8.3456  Epoch: 2  Global Step: 12290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:01,848-Speed 3903.62 samples/sec  Loss 1.5968  LearningRate 0.1669  ProxyLR: 8.3443  Epoch: 2  Global Step: 12300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:04,472-Speed 3904.27 samples/sec  Loss 1.5380  LearningRate 0.1669  ProxyLR: 8.3430  Epoch: 2  Global Step: 12310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:07,094-Speed 3905.42 samples/sec  Loss 1.6377  LearningRate 0.1668  ProxyLR: 8.3417  Epoch: 2  Global Step: 12320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:09,720-Speed 3901.40 samples/sec  Loss 1.6438  LearningRate 0.1668  ProxyLR: 8.3404  Epoch: 2  Global Step: 12330   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:12:12,343-Speed 3903.93 samples/sec  Loss 1.7379  LearningRate 0.1668  ProxyLR: 8.3392  Epoch: 2  Global Step: 12340   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:12:14,969-Speed 3901.10 samples/sec  Loss 1.6678  LearningRate 0.1668  ProxyLR: 8.3379  Epoch: 2  Global Step: 12350   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:12:17,596-Speed 3899.30 samples/sec  Loss 1.7623  LearningRate 0.1667  ProxyLR: 8.3366  Epoch: 2  Global Step: 12360   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:12:20,211-Speed 3917.09 samples/sec  Loss 1.6556  LearningRate 0.1667  ProxyLR: 8.3353  Epoch: 2  Global Step: 12370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:22,839-Speed 3897.50 samples/sec  Loss 1.6985  LearningRate 0.1667  ProxyLR: 8.3340  Epoch: 2  Global Step: 12380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:25,469-Speed 3893.72 samples/sec  Loss 1.6207  LearningRate 0.1667  ProxyLR: 8.3327  Epoch: 2  Global Step: 12390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:28,101-Speed 3892.11 samples/sec  Loss 1.5780  LearningRate 0.1666  ProxyLR: 8.3315  Epoch: 2  Global Step: 12400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:30,732-Speed 3892.85 samples/sec  Loss 1.6575  LearningRate 0.1666  ProxyLR: 8.3302  Epoch: 2  Global Step: 12410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:33,367-Speed 3886.57 samples/sec  Loss 1.6471  LearningRate 0.1666  ProxyLR: 8.3289  Epoch: 2  Global Step: 12420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:36,001-Speed 3889.11 samples/sec  Loss 1.6636  LearningRate 0.1666  ProxyLR: 8.3276  Epoch: 2  Global Step: 12430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:38,633-Speed 3891.12 samples/sec  Loss 1.5493  LearningRate 0.1665  ProxyLR: 8.3263  Epoch: 2  Global Step: 12440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:41,267-Speed 3888.22 samples/sec  Loss 1.7416  LearningRate 0.1665  ProxyLR: 8.3250  Epoch: 2  Global Step: 12450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:43,901-Speed 3889.51 samples/sec  Loss 1.6131  LearningRate 0.1665  ProxyLR: 8.3238  Epoch: 2  Global Step: 12460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:46,532-Speed 3892.10 samples/sec  Loss 1.7259  LearningRate 0.1664  ProxyLR: 8.3225  Epoch: 2  Global Step: 12470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:12:49,152-Speed 3909.22 samples/sec  Loss 1.6925  LearningRate 0.1664  ProxyLR: 8.3212  Epoch: 2  Global Step: 12480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:51,786-Speed 3889.86 samples/sec  Loss 1.5274  LearningRate 0.1664  ProxyLR: 8.3199  Epoch: 2  Global Step: 12490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:54,420-Speed 3888.12 samples/sec  Loss 1.7094  LearningRate 0.1664  ProxyLR: 8.3186  Epoch: 2  Global Step: 12500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:57,055-Speed 3888.03 samples/sec  Loss 1.5845  LearningRate 0.1663  ProxyLR: 8.3173  Epoch: 2  Global Step: 12510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:12:59,688-Speed 3889.82 samples/sec  Loss 1.6095  LearningRate 0.1663  ProxyLR: 8.3161  Epoch: 2  Global Step: 12520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:02,319-Speed 3892.07 samples/sec  Loss 1.6738  LearningRate 0.1663  ProxyLR: 8.3148  Epoch: 2  Global Step: 12530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:04,954-Speed 3888.21 samples/sec  Loss 1.6644  LearningRate 0.1663  ProxyLR: 8.3135  Epoch: 2  Global Step: 12540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:07,588-Speed 3888.30 samples/sec  Loss 1.6675  LearningRate 0.1662  ProxyLR: 8.3122  Epoch: 2  Global Step: 12550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:10,223-Speed 3887.24 samples/sec  Loss 1.7166  LearningRate 0.1662  ProxyLR: 8.3109  Epoch: 2  Global Step: 12560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:12,857-Speed 3888.44 samples/sec  Loss 1.7823  LearningRate 0.1662  ProxyLR: 8.3096  Epoch: 2  Global Step: 12570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:15,492-Speed 3887.07 samples/sec  Loss 1.6931  LearningRate 0.1662  ProxyLR: 8.3084  Epoch: 2  Global Step: 12580   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:13:18,125-Speed 3889.71 samples/sec  Loss 1.8852  LearningRate 0.1661  ProxyLR: 8.3071  Epoch: 2  Global Step: 12590   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:13:20,762-Speed 3884.30 samples/sec  Loss 1.6236  LearningRate 0.1661  ProxyLR: 8.3058  Epoch: 2  Global Step: 12600   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:13:23,397-Speed 3886.87 samples/sec  Loss 1.7798  LearningRate 0.1661  ProxyLR: 8.3045  Epoch: 2  Global Step: 12610   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:13:26,032-Speed 3886.78 samples/sec  Loss 1.7618  LearningRate 0.1661  ProxyLR: 8.3032  Epoch: 2  Global Step: 12620   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:13:28,648-Speed 3916.14 samples/sec  Loss 1.7842  LearningRate 0.1660  ProxyLR: 8.3019  Epoch: 2  Global Step: 12630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:31,282-Speed 3888.71 samples/sec  Loss 1.8786  LearningRate 0.1660  ProxyLR: 8.3007  Epoch: 2  Global Step: 12640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:33,914-Speed 3891.12 samples/sec  Loss 1.7203  LearningRate 0.1660  ProxyLR: 8.2994  Epoch: 2  Global Step: 12650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:36,546-Speed 3892.47 samples/sec  Loss 1.9024  LearningRate 0.1660  ProxyLR: 8.2981  Epoch: 2  Global Step: 12660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:39,177-Speed 3893.04 samples/sec  Loss 1.6864  LearningRate 0.1659  ProxyLR: 8.2968  Epoch: 2  Global Step: 12670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:41,808-Speed 3892.62 samples/sec  Loss 1.7259  LearningRate 0.1659  ProxyLR: 8.2955  Epoch: 2  Global Step: 12680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:44,439-Speed 3892.22 samples/sec  Loss 1.6092  LearningRate 0.1659  ProxyLR: 8.2943  Epoch: 2  Global Step: 12690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:47,070-Speed 3893.98 samples/sec  Loss 1.6861  LearningRate 0.1659  ProxyLR: 8.2930  Epoch: 2  Global Step: 12700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:49,702-Speed 3890.90 samples/sec  Loss 1.6119  LearningRate 0.1658  ProxyLR: 8.2917  Epoch: 2  Global Step: 12710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:52,336-Speed 3888.98 samples/sec  Loss 1.6682  LearningRate 0.1658  ProxyLR: 8.2904  Epoch: 2  Global Step: 12720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:13:54,967-Speed 3893.54 samples/sec  Loss 1.6778  LearningRate 0.1658  ProxyLR: 8.2891  Epoch: 2  Global Step: 12730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:13:57,599-Speed 3891.03 samples/sec  Loss 1.7257  LearningRate 0.1658  ProxyLR: 8.2879  Epoch: 2  Global Step: 12740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:00,231-Speed 3891.70 samples/sec  Loss 1.6125  LearningRate 0.1657  ProxyLR: 8.2866  Epoch: 2  Global Step: 12750   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:02,861-Speed 3893.49 samples/sec  Loss 1.6598  LearningRate 0.1657  ProxyLR: 8.2853  Epoch: 2  Global Step: 12760   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:05,493-Speed 3892.41 samples/sec  Loss 1.7985  LearningRate 0.1657  ProxyLR: 8.2840  Epoch: 2  Global Step: 12770   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:08,125-Speed 3891.40 samples/sec  Loss 1.6835  LearningRate 0.1657  ProxyLR: 8.2827  Epoch: 2  Global Step: 12780   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:10,757-Speed 3890.57 samples/sec  Loss 1.9074  LearningRate 0.1656  ProxyLR: 8.2814  Epoch: 2  Global Step: 12790   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:13,390-Speed 3891.33 samples/sec  Loss 1.8296  LearningRate 0.1656  ProxyLR: 8.2802  Epoch: 2  Global Step: 12800   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:16,007-Speed 3913.73 samples/sec  Loss 1.6798  LearningRate 0.1656  ProxyLR: 8.2789  Epoch: 2  Global Step: 12810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:18,636-Speed 3895.33 samples/sec  Loss 1.8115  LearningRate 0.1656  ProxyLR: 8.2776  Epoch: 2  Global Step: 12820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:21,267-Speed 3893.18 samples/sec  Loss 1.7994  LearningRate 0.1655  ProxyLR: 8.2763  Epoch: 2  Global Step: 12830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:23,898-Speed 3894.12 samples/sec  Loss 1.8095  LearningRate 0.1655  ProxyLR: 8.2750  Epoch: 2  Global Step: 12840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:26,529-Speed 3892.02 samples/sec  Loss 1.7979  LearningRate 0.1655  ProxyLR: 8.2738  Epoch: 2  Global Step: 12850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:29,160-Speed 3893.48 samples/sec  Loss 1.6961  LearningRate 0.1654  ProxyLR: 8.2725  Epoch: 2  Global Step: 12860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:31,792-Speed 3891.37 samples/sec  Loss 1.8372  LearningRate 0.1654  ProxyLR: 8.2712  Epoch: 2  Global Step: 12870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:34,424-Speed 3891.56 samples/sec  Loss 1.7087  LearningRate 0.1654  ProxyLR: 8.2699  Epoch: 2  Global Step: 12880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:37,058-Speed 3889.20 samples/sec  Loss 1.7437  LearningRate 0.1654  ProxyLR: 8.2686  Epoch: 2  Global Step: 12890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:39,688-Speed 3894.50 samples/sec  Loss 1.9283  LearningRate 0.1653  ProxyLR: 8.2674  Epoch: 2  Global Step: 12900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:42,319-Speed 3891.92 samples/sec  Loss 1.7616  LearningRate 0.1653  ProxyLR: 8.2661  Epoch: 2  Global Step: 12910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:44,953-Speed 3888.39 samples/sec  Loss 1.8061  LearningRate 0.1653  ProxyLR: 8.2648  Epoch: 2  Global Step: 12920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:47,587-Speed 3889.69 samples/sec  Loss 1.8044  LearningRate 0.1653  ProxyLR: 8.2635  Epoch: 2  Global Step: 12930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:14:50,208-Speed 3908.00 samples/sec  Loss 1.9666  LearningRate 0.1652  ProxyLR: 8.2623  Epoch: 2  Global Step: 12940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:52,841-Speed 3889.50 samples/sec  Loss 1.8853  LearningRate 0.1652  ProxyLR: 8.2610  Epoch: 2  Global Step: 12950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:55,475-Speed 3888.73 samples/sec  Loss 1.7487  LearningRate 0.1652  ProxyLR: 8.2597  Epoch: 2  Global Step: 12960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:14:58,109-Speed 3889.03 samples/sec  Loss 1.7572  LearningRate 0.1652  ProxyLR: 8.2584  Epoch: 2  Global Step: 12970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:00,740-Speed 3892.82 samples/sec  Loss 1.7450  LearningRate 0.1651  ProxyLR: 8.2571  Epoch: 2  Global Step: 12980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:03,367-Speed 3899.30 samples/sec  Loss 1.8149  LearningRate 0.1651  ProxyLR: 8.2559  Epoch: 2  Global Step: 12990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:05,994-Speed 3898.31 samples/sec  Loss 1.7114  LearningRate 0.1651  ProxyLR: 8.2546  Epoch: 2  Global Step: 13000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:08,624-Speed 3895.16 samples/sec  Loss 1.7997  LearningRate 0.1651  ProxyLR: 8.2533  Epoch: 2  Global Step: 13010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:11,248-Speed 3902.51 samples/sec  Loss 1.8031  LearningRate 0.1650  ProxyLR: 8.2520  Epoch: 2  Global Step: 13020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:13,876-Speed 3898.12 samples/sec  Loss 1.9251  LearningRate 0.1650  ProxyLR: 8.2507  Epoch: 2  Global Step: 13030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:16,502-Speed 3899.84 samples/sec  Loss 1.6779  LearningRate 0.1650  ProxyLR: 8.2495  Epoch: 2  Global Step: 13040   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:15:19,112-Speed 3924.34 samples/sec  Loss 1.9832  LearningRate 0.1650  ProxyLR: 8.2482  Epoch: 2  Global Step: 13050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:21,737-Speed 3902.17 samples/sec  Loss 1.8387  LearningRate 0.1649  ProxyLR: 8.2469  Epoch: 2  Global Step: 13060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:24,362-Speed 3901.81 samples/sec  Loss 1.8137  LearningRate 0.1649  ProxyLR: 8.2456  Epoch: 2  Global Step: 13070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:26,986-Speed 3903.29 samples/sec  Loss 1.8371  LearningRate 0.1649  ProxyLR: 8.2444  Epoch: 2  Global Step: 13080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:29,612-Speed 3899.97 samples/sec  Loss 1.7855  LearningRate 0.1649  ProxyLR: 8.2431  Epoch: 2  Global Step: 13090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:32,236-Speed 3903.43 samples/sec  Loss 1.9212  LearningRate 0.1648  ProxyLR: 8.2418  Epoch: 2  Global Step: 13100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:34,865-Speed 3896.18 samples/sec  Loss 1.7810  LearningRate 0.1648  ProxyLR: 8.2405  Epoch: 2  Global Step: 13110   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:37,497-Speed 3891.31 samples/sec  Loss 1.8033  LearningRate 0.1648  ProxyLR: 8.2392  Epoch: 2  Global Step: 13120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:40,128-Speed 3893.46 samples/sec  Loss 1.7412  LearningRate 0.1648  ProxyLR: 8.2380  Epoch: 2  Global Step: 13130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:42,759-Speed 3893.05 samples/sec  Loss 1.8376  LearningRate 0.1647  ProxyLR: 8.2367  Epoch: 2  Global Step: 13140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:45,372-Speed 3920.11 samples/sec  Loss 1.8527  LearningRate 0.1647  ProxyLR: 8.2354  Epoch: 2  Global Step: 13150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:47,999-Speed 3899.36 samples/sec  Loss 1.8657  LearningRate 0.1647  ProxyLR: 8.2341  Epoch: 2  Global Step: 13160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:50,629-Speed 3894.74 samples/sec  Loss 1.8628  LearningRate 0.1647  ProxyLR: 8.2329  Epoch: 2  Global Step: 13170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:53,256-Speed 3898.08 samples/sec  Loss 1.9440  LearningRate 0.1646  ProxyLR: 8.2316  Epoch: 2  Global Step: 13180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:55,883-Speed 3899.78 samples/sec  Loss 1.8874  LearningRate 0.1646  ProxyLR: 8.2303  Epoch: 2  Global Step: 13190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:15:58,510-Speed 3899.10 samples/sec  Loss 1.8113  LearningRate 0.1646  ProxyLR: 8.2290  Epoch: 2  Global Step: 13200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:01,138-Speed 3897.20 samples/sec  Loss 1.8295  LearningRate 0.1646  ProxyLR: 8.2278  Epoch: 2  Global Step: 13210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:03,765-Speed 3898.68 samples/sec  Loss 1.8665  LearningRate 0.1645  ProxyLR: 8.2265  Epoch: 2  Global Step: 13220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:06,392-Speed 3899.25 samples/sec  Loss 1.9849  LearningRate 0.1645  ProxyLR: 8.2252  Epoch: 2  Global Step: 13230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:09,021-Speed 3896.19 samples/sec  Loss 1.7225  LearningRate 0.1645  ProxyLR: 8.2239  Epoch: 2  Global Step: 13240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:11,647-Speed 3900.08 samples/sec  Loss 1.8321  LearningRate 0.1645  ProxyLR: 8.2227  Epoch: 2  Global Step: 13250   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:16:14,275-Speed 3898.14 samples/sec  Loss 1.8334  LearningRate 0.1644  ProxyLR: 8.2214  Epoch: 2  Global Step: 13260   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:16:16,901-Speed 3899.58 samples/sec  Loss 2.0142  LearningRate 0.1644  ProxyLR: 8.2201  Epoch: 2  Global Step: 13270   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:16:19,515-Speed 3919.07 samples/sec  Loss 1.8528  LearningRate 0.1644  ProxyLR: 8.2188  Epoch: 2  Global Step: 13280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:22,139-Speed 3902.61 samples/sec  Loss 1.8738  LearningRate 0.1644  ProxyLR: 8.2176  Epoch: 2  Global Step: 13290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:24,763-Speed 3903.81 samples/sec  Loss 1.8435  LearningRate 0.1643  ProxyLR: 8.2163  Epoch: 2  Global Step: 13300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:27,389-Speed 3899.76 samples/sec  Loss 1.9578  LearningRate 0.1643  ProxyLR: 8.2150  Epoch: 2  Global Step: 13310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:30,018-Speed 3895.75 samples/sec  Loss 1.9288  LearningRate 0.1643  ProxyLR: 8.2137  Epoch: 2  Global Step: 13320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:32,652-Speed 3888.63 samples/sec  Loss 1.9318  LearningRate 0.1642  ProxyLR: 8.2125  Epoch: 2  Global Step: 13330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:35,287-Speed 3887.81 samples/sec  Loss 1.8420  LearningRate 0.1642  ProxyLR: 8.2112  Epoch: 2  Global Step: 13340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:37,923-Speed 3885.85 samples/sec  Loss 1.7542  LearningRate 0.1642  ProxyLR: 8.2099  Epoch: 2  Global Step: 13350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:40,555-Speed 3891.64 samples/sec  Loss 1.9658  LearningRate 0.1642  ProxyLR: 8.2086  Epoch: 2  Global Step: 13360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:43,191-Speed 3886.38 samples/sec  Loss 2.0825  LearningRate 0.1641  ProxyLR: 8.2074  Epoch: 2  Global Step: 13370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:45,811-Speed 3908.65 samples/sec  Loss 1.9868  LearningRate 0.1641  ProxyLR: 8.2061  Epoch: 2  Global Step: 13380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:48,684-Speed 3565.18 samples/sec  Loss 1.9082  LearningRate 0.1641  ProxyLR: 8.2048  Epoch: 2  Global Step: 13390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:51,318-Speed 3888.29 samples/sec  Loss 1.7908  LearningRate 0.1641  ProxyLR: 8.2035  Epoch: 2  Global Step: 13400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:53,952-Speed 3888.96 samples/sec  Loss 1.8611  LearningRate 0.1640  ProxyLR: 8.2023  Epoch: 2  Global Step: 13410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:56,586-Speed 3888.57 samples/sec  Loss 1.7739  LearningRate 0.1640  ProxyLR: 8.2010  Epoch: 2  Global Step: 13420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:16:59,221-Speed 3887.53 samples/sec  Loss 1.7432  LearningRate 0.1640  ProxyLR: 8.1997  Epoch: 2  Global Step: 13430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:01,854-Speed 3889.96 samples/sec  Loss 1.8146  LearningRate 0.1640  ProxyLR: 8.1984  Epoch: 2  Global Step: 13440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:04,483-Speed 3894.66 samples/sec  Loss 1.9431  LearningRate 0.1639  ProxyLR: 8.1972  Epoch: 2  Global Step: 13450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:07,117-Speed 3889.53 samples/sec  Loss 1.8394  LearningRate 0.1639  ProxyLR: 8.1959  Epoch: 2  Global Step: 13460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:09,753-Speed 3884.97 samples/sec  Loss 1.9656  LearningRate 0.1639  ProxyLR: 8.1946  Epoch: 2  Global Step: 13470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:12,385-Speed 3892.97 samples/sec  Loss 1.9027  LearningRate 0.1639  ProxyLR: 8.1933  Epoch: 2  Global Step: 13480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:17:15,001-Speed 3914.41 samples/sec  Loss 1.7923  LearningRate 0.1638  ProxyLR: 8.1921  Epoch: 2  Global Step: 13490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:17,634-Speed 3890.12 samples/sec  Loss 1.8719  LearningRate 0.1638  ProxyLR: 8.1908  Epoch: 2  Global Step: 13500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:20,261-Speed 3898.60 samples/sec  Loss 1.8678  LearningRate 0.1638  ProxyLR: 8.1895  Epoch: 2  Global Step: 13510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:22,890-Speed 3896.15 samples/sec  Loss 1.9173  LearningRate 0.1638  ProxyLR: 8.1882  Epoch: 2  Global Step: 13520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:25,522-Speed 3891.43 samples/sec  Loss 1.8612  LearningRate 0.1637  ProxyLR: 8.1870  Epoch: 2  Global Step: 13530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:28,155-Speed 3890.48 samples/sec  Loss 2.0454  LearningRate 0.1637  ProxyLR: 8.1857  Epoch: 2  Global Step: 13540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:30,787-Speed 3891.79 samples/sec  Loss 1.8675  LearningRate 0.1637  ProxyLR: 8.1844  Epoch: 2  Global Step: 13550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:33,416-Speed 3896.09 samples/sec  Loss 1.9363  LearningRate 0.1637  ProxyLR: 8.1832  Epoch: 2  Global Step: 13560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:36,046-Speed 3893.89 samples/sec  Loss 1.9664  LearningRate 0.1636  ProxyLR: 8.1819  Epoch: 2  Global Step: 13570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:38,677-Speed 3893.28 samples/sec  Loss 1.9350  LearningRate 0.1636  ProxyLR: 8.1806  Epoch: 2  Global Step: 13580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:41,294-Speed 3913.35 samples/sec  Loss 2.0544  LearningRate 0.1636  ProxyLR: 8.1793  Epoch: 2  Global Step: 13590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:43,926-Speed 3891.88 samples/sec  Loss 1.8976  LearningRate 0.1636  ProxyLR: 8.1781  Epoch: 2  Global Step: 13600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:46,560-Speed 3889.60 samples/sec  Loss 1.9623  LearningRate 0.1635  ProxyLR: 8.1768  Epoch: 2  Global Step: 13610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:49,194-Speed 3888.06 samples/sec  Loss 2.0780  LearningRate 0.1635  ProxyLR: 8.1755  Epoch: 2  Global Step: 13620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:51,824-Speed 3893.91 samples/sec  Loss 1.9735  LearningRate 0.1635  ProxyLR: 8.1742  Epoch: 2  Global Step: 13630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:54,459-Speed 3886.89 samples/sec  Loss 1.9769  LearningRate 0.1635  ProxyLR: 8.1730  Epoch: 2  Global Step: 13640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:57,094-Speed 3887.77 samples/sec  Loss 1.8816  LearningRate 0.1634  ProxyLR: 8.1717  Epoch: 2  Global Step: 13650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:17:59,731-Speed 3884.22 samples/sec  Loss 1.9898  LearningRate 0.1634  ProxyLR: 8.1704  Epoch: 2  Global Step: 13660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:02,367-Speed 3886.00 samples/sec  Loss 1.9872  LearningRate 0.1634  ProxyLR: 8.1692  Epoch: 2  Global Step: 13670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:05,003-Speed 3884.93 samples/sec  Loss 1.8985  LearningRate 0.1634  ProxyLR: 8.1679  Epoch: 2  Global Step: 13680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:07,637-Speed 3888.25 samples/sec  Loss 1.8908  LearningRate 0.1633  ProxyLR: 8.1666  Epoch: 2  Global Step: 13690   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:18:10,272-Speed 3887.94 samples/sec  Loss 1.9696  LearningRate 0.1633  ProxyLR: 8.1653  Epoch: 2  Global Step: 13700   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:18:13,133-Speed 3579.65 samples/sec  Loss 1.8934  LearningRate 0.1633  ProxyLR: 8.1641  Epoch: 2  Global Step: 13710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:15,765-Speed 3891.38 samples/sec  Loss 1.9866  LearningRate 0.1633  ProxyLR: 8.1628  Epoch: 2  Global Step: 13720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:18,401-Speed 3886.60 samples/sec  Loss 1.9046  LearningRate 0.1632  ProxyLR: 8.1615  Epoch: 2  Global Step: 13730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:21,036-Speed 3886.45 samples/sec  Loss 2.0333  LearningRate 0.1632  ProxyLR: 8.1603  Epoch: 2  Global Step: 13740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:23,671-Speed 3888.01 samples/sec  Loss 2.0910  LearningRate 0.1632  ProxyLR: 8.1590  Epoch: 2  Global Step: 13750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:26,305-Speed 3887.44 samples/sec  Loss 1.9245  LearningRate 0.1632  ProxyLR: 8.1577  Epoch: 2  Global Step: 13760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:28,940-Speed 3888.04 samples/sec  Loss 1.8232  LearningRate 0.1631  ProxyLR: 8.1564  Epoch: 2  Global Step: 13770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:31,577-Speed 3883.77 samples/sec  Loss 1.9291  LearningRate 0.1631  ProxyLR: 8.1552  Epoch: 2  Global Step: 13780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:34,210-Speed 3890.38 samples/sec  Loss 1.9953  LearningRate 0.1631  ProxyLR: 8.1539  Epoch: 2  Global Step: 13790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:37,078-Speed 3571.13 samples/sec  Loss 1.9892  LearningRate 0.1631  ProxyLR: 8.1526  Epoch: 2  Global Step: 13800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:39,710-Speed 3891.14 samples/sec  Loss 2.1421  LearningRate 0.1630  ProxyLR: 8.1514  Epoch: 2  Global Step: 13810   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:18:42,331-Speed 3908.78 samples/sec  Loss 2.1309  LearningRate 0.1630  ProxyLR: 8.1501  Epoch: 2  Global Step: 13820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:44,963-Speed 3892.01 samples/sec  Loss 1.9818  LearningRate 0.1630  ProxyLR: 8.1488  Epoch: 2  Global Step: 13830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:47,592-Speed 3894.99 samples/sec  Loss 1.8594  LearningRate 0.1630  ProxyLR: 8.1476  Epoch: 2  Global Step: 13840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:50,223-Speed 3893.30 samples/sec  Loss 1.9171  LearningRate 0.1629  ProxyLR: 8.1463  Epoch: 2  Global Step: 13850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:52,854-Speed 3893.84 samples/sec  Loss 2.0192  LearningRate 0.1629  ProxyLR: 8.1450  Epoch: 2  Global Step: 13860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:55,488-Speed 3888.24 samples/sec  Loss 1.8151  LearningRate 0.1629  ProxyLR: 8.1437  Epoch: 2  Global Step: 13870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:18:58,121-Speed 3889.43 samples/sec  Loss 1.9098  LearningRate 0.1628  ProxyLR: 8.1425  Epoch: 2  Global Step: 13880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:00,754-Speed 3890.08 samples/sec  Loss 1.8523  LearningRate 0.1628  ProxyLR: 8.1412  Epoch: 2  Global Step: 13890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:03,389-Speed 3887.02 samples/sec  Loss 1.9538  LearningRate 0.1628  ProxyLR: 8.1399  Epoch: 2  Global Step: 13900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:06,023-Speed 3888.99 samples/sec  Loss 1.9997  LearningRate 0.1628  ProxyLR: 8.1387  Epoch: 2  Global Step: 13910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:08,656-Speed 3889.67 samples/sec  Loss 1.9476  LearningRate 0.1627  ProxyLR: 8.1374  Epoch: 2  Global Step: 13920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:19:11,288-Speed 3892.20 samples/sec  Loss 2.0461  LearningRate 0.1627  ProxyLR: 8.1361  Epoch: 2  Global Step: 13930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:19:13,919-Speed 3893.10 samples/sec  Loss 2.0407  LearningRate 0.1627  ProxyLR: 8.1349  Epoch: 2  Global Step: 13940   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:19:16,551-Speed 3891.09 samples/sec  Loss 1.8873  LearningRate 0.1627  ProxyLR: 8.1336  Epoch: 2  Global Step: 13950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:19:19,181-Speed 3894.40 samples/sec  Loss 1.9531  LearningRate 0.1626  ProxyLR: 8.1323  Epoch: 2  Global Step: 13960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:19:21,800-Speed 3911.77 samples/sec  Loss 1.7972  LearningRate 0.1626  ProxyLR: 8.1311  Epoch: 2  Global Step: 13970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:24,433-Speed 3890.10 samples/sec  Loss 1.8794  LearningRate 0.1626  ProxyLR: 8.1298  Epoch: 2  Global Step: 13980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:27,065-Speed 3891.73 samples/sec  Loss 1.8932  LearningRate 0.1626  ProxyLR: 8.1285  Epoch: 2  Global Step: 13990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:29,697-Speed 3890.55 samples/sec  Loss 2.0427  LearningRate 0.1625  ProxyLR: 8.1272  Epoch: 2  Global Step: 14000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:32,327-Speed 3895.64 samples/sec  Loss 1.8927  LearningRate 0.1625  ProxyLR: 8.1260  Epoch: 2  Global Step: 14010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:34,958-Speed 3892.18 samples/sec  Loss 2.0207  LearningRate 0.1625  ProxyLR: 8.1247  Epoch: 2  Global Step: 14020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:37,589-Speed 3893.29 samples/sec  Loss 1.9507  LearningRate 0.1625  ProxyLR: 8.1234  Epoch: 2  Global Step: 14030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:40,221-Speed 3890.65 samples/sec  Loss 1.9344  LearningRate 0.1624  ProxyLR: 8.1222  Epoch: 2  Global Step: 14040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:42,853-Speed 3892.88 samples/sec  Loss 1.9605  LearningRate 0.1624  ProxyLR: 8.1209  Epoch: 2  Global Step: 14050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:45,484-Speed 3892.31 samples/sec  Loss 2.0337  LearningRate 0.1624  ProxyLR: 8.1196  Epoch: 2  Global Step: 14060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:48,102-Speed 3911.82 samples/sec  Loss 2.0887  LearningRate 0.1624  ProxyLR: 8.1184  Epoch: 2  Global Step: 14070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:50,734-Speed 3891.72 samples/sec  Loss 2.0448  LearningRate 0.1623  ProxyLR: 8.1171  Epoch: 2  Global Step: 14080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:53,371-Speed 3885.25 samples/sec  Loss 1.7876  LearningRate 0.1623  ProxyLR: 8.1158  Epoch: 2  Global Step: 14090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:56,004-Speed 3889.71 samples/sec  Loss 1.9776  LearningRate 0.1623  ProxyLR: 8.1146  Epoch: 2  Global Step: 14100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:19:58,639-Speed 3887.87 samples/sec  Loss 1.9269  LearningRate 0.1623  ProxyLR: 8.1133  Epoch: 2  Global Step: 14110   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:01,273-Speed 3888.52 samples/sec  Loss 1.9207  LearningRate 0.1622  ProxyLR: 8.1120  Epoch: 2  Global Step: 14120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:03,905-Speed 3890.90 samples/sec  Loss 2.0838  LearningRate 0.1622  ProxyLR: 8.1108  Epoch: 2  Global Step: 14130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:06,538-Speed 3889.48 samples/sec  Loss 2.1112  LearningRate 0.1622  ProxyLR: 8.1095  Epoch: 2  Global Step: 14140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:09,175-Speed 3884.30 samples/sec  Loss 2.0938  LearningRate 0.1622  ProxyLR: 8.1082  Epoch: 2  Global Step: 14150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:11,811-Speed 3885.96 samples/sec  Loss 2.0280  LearningRate 0.1621  ProxyLR: 8.1070  Epoch: 2  Global Step: 14160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:14,447-Speed 3885.95 samples/sec  Loss 1.8752  LearningRate 0.1621  ProxyLR: 8.1057  Epoch: 2  Global Step: 14170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:17,071-Speed 3903.03 samples/sec  Loss 2.0301  LearningRate 0.1621  ProxyLR: 8.1044  Epoch: 2  Global Step: 14180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:19,705-Speed 3889.52 samples/sec  Loss 2.0038  LearningRate 0.1621  ProxyLR: 8.1032  Epoch: 2  Global Step: 14190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:22,339-Speed 3887.36 samples/sec  Loss 1.9678  LearningRate 0.1620  ProxyLR: 8.1019  Epoch: 2  Global Step: 14200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:24,975-Speed 3886.97 samples/sec  Loss 1.9325  LearningRate 0.1620  ProxyLR: 8.1006  Epoch: 2  Global Step: 14210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:27,611-Speed 3885.63 samples/sec  Loss 2.0632  LearningRate 0.1620  ProxyLR: 8.0994  Epoch: 2  Global Step: 14220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:30,245-Speed 3888.29 samples/sec  Loss 2.0655  LearningRate 0.1620  ProxyLR: 8.0981  Epoch: 2  Global Step: 14230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:32,878-Speed 3890.73 samples/sec  Loss 2.0398  LearningRate 0.1619  ProxyLR: 8.0968  Epoch: 2  Global Step: 14240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:35,511-Speed 3889.63 samples/sec  Loss 2.0462  LearningRate 0.1619  ProxyLR: 8.0956  Epoch: 2  Global Step: 14250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:38,147-Speed 3886.33 samples/sec  Loss 1.9521  LearningRate 0.1619  ProxyLR: 8.0943  Epoch: 2  Global Step: 14260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:40,780-Speed 3889.38 samples/sec  Loss 2.0585  LearningRate 0.1619  ProxyLR: 8.0930  Epoch: 2  Global Step: 14270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:20:43,414-Speed 3889.15 samples/sec  Loss 1.9905  LearningRate 0.1618  ProxyLR: 8.0918  Epoch: 2  Global Step: 14280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:46,046-Speed 3890.53 samples/sec  Loss 1.8753  LearningRate 0.1618  ProxyLR: 8.0905  Epoch: 2  Global Step: 14290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:48,679-Speed 3890.86 samples/sec  Loss 1.9513  LearningRate 0.1618  ProxyLR: 8.0892  Epoch: 2  Global Step: 14300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:51,310-Speed 3892.00 samples/sec  Loss 1.9533  LearningRate 0.1618  ProxyLR: 8.0880  Epoch: 2  Global Step: 14310   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:53,943-Speed 3890.63 samples/sec  Loss 1.9366  LearningRate 0.1617  ProxyLR: 8.0867  Epoch: 2  Global Step: 14320   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:56,578-Speed 3886.89 samples/sec  Loss 2.0050  LearningRate 0.1617  ProxyLR: 8.0854  Epoch: 2  Global Step: 14330   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:20:59,215-Speed 3884.32 samples/sec  Loss 1.8997  LearningRate 0.1617  ProxyLR: 8.0842  Epoch: 2  Global Step: 14340   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:01,849-Speed 3888.33 samples/sec  Loss 2.0047  LearningRate 0.1617  ProxyLR: 8.0829  Epoch: 2  Global Step: 14350   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:04,484-Speed 3887.04 samples/sec  Loss 2.0608  LearningRate 0.1616  ProxyLR: 8.0816  Epoch: 2  Global Step: 14360   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:07,119-Speed 3888.00 samples/sec  Loss 2.0859  LearningRate 0.1616  ProxyLR: 8.0804  Epoch: 2  Global Step: 14370   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:09,740-Speed 3907.59 samples/sec  Loss 2.0182  LearningRate 0.1616  ProxyLR: 8.0791  Epoch: 2  Global Step: 14380   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:12,375-Speed 3887.04 samples/sec  Loss 1.9153  LearningRate 0.1616  ProxyLR: 8.0779  Epoch: 2  Global Step: 14390   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:15,011-Speed 3885.57 samples/sec  Loss 2.0131  LearningRate 0.1615  ProxyLR: 8.0766  Epoch: 2  Global Step: 14400   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:17,632-Speed 3908.52 samples/sec  Loss 2.0705  LearningRate 0.1615  ProxyLR: 8.0753  Epoch: 2  Global Step: 14410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:20,265-Speed 3889.35 samples/sec  Loss 2.0739  LearningRate 0.1615  ProxyLR: 8.0741  Epoch: 2  Global Step: 14420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:22,898-Speed 3889.47 samples/sec  Loss 1.9546  LearningRate 0.1615  ProxyLR: 8.0728  Epoch: 2  Global Step: 14430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:25,532-Speed 3888.64 samples/sec  Loss 2.0198  LearningRate 0.1614  ProxyLR: 8.0715  Epoch: 2  Global Step: 14440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:28,164-Speed 3892.17 samples/sec  Loss 2.1392  LearningRate 0.1614  ProxyLR: 8.0703  Epoch: 2  Global Step: 14450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:30,792-Speed 3897.34 samples/sec  Loss 2.1003  LearningRate 0.1614  ProxyLR: 8.0690  Epoch: 2  Global Step: 14460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:33,423-Speed 3893.21 samples/sec  Loss 1.9328  LearningRate 0.1614  ProxyLR: 8.0677  Epoch: 2  Global Step: 14470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:36,054-Speed 3892.47 samples/sec  Loss 1.9878  LearningRate 0.1613  ProxyLR: 8.0665  Epoch: 2  Global Step: 14480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:38,686-Speed 3892.07 samples/sec  Loss 1.8897  LearningRate 0.1613  ProxyLR: 8.0652  Epoch: 2  Global Step: 14490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:41,319-Speed 3889.96 samples/sec  Loss 2.0584  LearningRate 0.1613  ProxyLR: 8.0640  Epoch: 2  Global Step: 14500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:21:43,951-Speed 3892.17 samples/sec  Loss 1.9047  LearningRate 0.1613  ProxyLR: 8.0627  Epoch: 2  Global Step: 14510   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:46,580-Speed 3895.45 samples/sec  Loss 2.1644  LearningRate 0.1612  ProxyLR: 8.0614  Epoch: 2  Global Step: 14520   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:49,212-Speed 3891.60 samples/sec  Loss 2.0043  LearningRate 0.1612  ProxyLR: 8.0602  Epoch: 2  Global Step: 14530   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:51,847-Speed 3887.82 samples/sec  Loss 2.0253  LearningRate 0.1612  ProxyLR: 8.0589  Epoch: 2  Global Step: 14540   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:54,480-Speed 3890.20 samples/sec  Loss 1.8507  LearningRate 0.1612  ProxyLR: 8.0576  Epoch: 2  Global Step: 14550   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:57,113-Speed 3889.65 samples/sec  Loss 2.0447  LearningRate 0.1611  ProxyLR: 8.0564  Epoch: 2  Global Step: 14560   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:21:59,746-Speed 3889.24 samples/sec  Loss 2.1025  LearningRate 0.1611  ProxyLR: 8.0551  Epoch: 2  Global Step: 14570   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:22:02,364-Speed 3912.54 samples/sec  Loss 1.9574  LearningRate 0.1611  ProxyLR: 8.0538  Epoch: 2  Global Step: 14580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:04,996-Speed 3891.36 samples/sec  Loss 1.9845  LearningRate 0.1611  ProxyLR: 8.0526  Epoch: 2  Global Step: 14590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:07,630-Speed 3889.42 samples/sec  Loss 2.0021  LearningRate 0.1610  ProxyLR: 8.0513  Epoch: 2  Global Step: 14600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:10,261-Speed 3893.35 samples/sec  Loss 1.9560  LearningRate 0.1610  ProxyLR: 8.0501  Epoch: 2  Global Step: 14610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:12,892-Speed 3893.19 samples/sec  Loss 2.2216  LearningRate 0.1610  ProxyLR: 8.0488  Epoch: 2  Global Step: 14620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:15,524-Speed 3890.80 samples/sec  Loss 2.0408  LearningRate 0.1610  ProxyLR: 8.0475  Epoch: 2  Global Step: 14630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:18,155-Speed 3892.42 samples/sec  Loss 2.0143  LearningRate 0.1609  ProxyLR: 8.0463  Epoch: 2  Global Step: 14640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:20,789-Speed 3889.62 samples/sec  Loss 2.0708  LearningRate 0.1609  ProxyLR: 8.0450  Epoch: 2  Global Step: 14650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:23,424-Speed 3887.04 samples/sec  Loss 2.0723  LearningRate 0.1609  ProxyLR: 8.0437  Epoch: 2  Global Step: 14660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:26,055-Speed 3893.84 samples/sec  Loss 2.1562  LearningRate 0.1608  ProxyLR: 8.0425  Epoch: 2  Global Step: 14670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:28,686-Speed 3892.30 samples/sec  Loss 1.9762  LearningRate 0.1608  ProxyLR: 8.0412  Epoch: 2  Global Step: 14680   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:22:31,305-Speed 3911.82 samples/sec  Loss 1.9708  LearningRate 0.1608  ProxyLR: 8.0400  Epoch: 2  Global Step: 14690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:33,935-Speed 3894.52 samples/sec  Loss 2.0562  LearningRate 0.1608  ProxyLR: 8.0387  Epoch: 2  Global Step: 14700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:36,567-Speed 3890.89 samples/sec  Loss 2.0636  LearningRate 0.1607  ProxyLR: 8.0374  Epoch: 2  Global Step: 14710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:39,201-Speed 3888.60 samples/sec  Loss 2.0257  LearningRate 0.1607  ProxyLR: 8.0362  Epoch: 2  Global Step: 14720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:41,833-Speed 3891.51 samples/sec  Loss 2.0260  LearningRate 0.1607  ProxyLR: 8.0349  Epoch: 2  Global Step: 14730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:44,469-Speed 3886.10 samples/sec  Loss 2.0845  LearningRate 0.1607  ProxyLR: 8.0337  Epoch: 2  Global Step: 14740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:47,101-Speed 3890.68 samples/sec  Loss 2.1347  LearningRate 0.1606  ProxyLR: 8.0324  Epoch: 2  Global Step: 14750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:49,734-Speed 3890.39 samples/sec  Loss 1.9191  LearningRate 0.1606  ProxyLR: 8.0311  Epoch: 2  Global Step: 14760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:52,369-Speed 3886.99 samples/sec  Loss 2.0665  LearningRate 0.1606  ProxyLR: 8.0299  Epoch: 2  Global Step: 14770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:55,002-Speed 3889.33 samples/sec  Loss 2.1943  LearningRate 0.1606  ProxyLR: 8.0286  Epoch: 2  Global Step: 14780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:22:57,622-Speed 3910.43 samples/sec  Loss 1.9207  LearningRate 0.1605  ProxyLR: 8.0274  Epoch: 2  Global Step: 14790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:00,254-Speed 3890.51 samples/sec  Loss 2.0792  LearningRate 0.1605  ProxyLR: 8.0261  Epoch: 2  Global Step: 14800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:02,887-Speed 3890.88 samples/sec  Loss 2.1437  LearningRate 0.1605  ProxyLR: 8.0248  Epoch: 2  Global Step: 14810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:05,522-Speed 3887.54 samples/sec  Loss 2.1185  LearningRate 0.1605  ProxyLR: 8.0236  Epoch: 2  Global Step: 14820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:08,156-Speed 3888.92 samples/sec  Loss 2.0339  LearningRate 0.1604  ProxyLR: 8.0223  Epoch: 2  Global Step: 14830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:10,789-Speed 3889.12 samples/sec  Loss 2.0409  LearningRate 0.1604  ProxyLR: 8.0211  Epoch: 2  Global Step: 14840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:13,426-Speed 3884.13 samples/sec  Loss 2.0250  LearningRate 0.1604  ProxyLR: 8.0198  Epoch: 2  Global Step: 14850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:16,060-Speed 3888.81 samples/sec  Loss 2.0225  LearningRate 0.1604  ProxyLR: 8.0185  Epoch: 2  Global Step: 14860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:18,693-Speed 3890.37 samples/sec  Loss 2.0978  LearningRate 0.1603  ProxyLR: 8.0173  Epoch: 2  Global Step: 14870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:21,328-Speed 3887.49 samples/sec  Loss 2.1144  LearningRate 0.1603  ProxyLR: 8.0160  Epoch: 2  Global Step: 14880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:23,961-Speed 3888.77 samples/sec  Loss 1.9897  LearningRate 0.1603  ProxyLR: 8.0148  Epoch: 2  Global Step: 14890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:23:26,596-Speed 3888.29 samples/sec  Loss 1.9746  LearningRate 0.1603  ProxyLR: 8.0135  Epoch: 2  Global Step: 14900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:23:29,214-Speed 3911.12 samples/sec  Loss 2.0263  LearningRate 0.1602  ProxyLR: 8.0122  Epoch: 2  Global Step: 14910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:31,848-Speed 3889.28 samples/sec  Loss 1.9870  LearningRate 0.1602  ProxyLR: 8.0110  Epoch: 2  Global Step: 14920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:34,480-Speed 3890.89 samples/sec  Loss 2.1663  LearningRate 0.1602  ProxyLR: 8.0097  Epoch: 2  Global Step: 14930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:37,114-Speed 3888.99 samples/sec  Loss 2.1191  LearningRate 0.1602  ProxyLR: 8.0085  Epoch: 2  Global Step: 14940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:39,747-Speed 3890.92 samples/sec  Loss 2.1170  LearningRate 0.1601  ProxyLR: 8.0072  Epoch: 2  Global Step: 14950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:42,380-Speed 3888.90 samples/sec  Loss 2.0083  LearningRate 0.1601  ProxyLR: 8.0059  Epoch: 2  Global Step: 14960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:45,012-Speed 3891.84 samples/sec  Loss 2.0144  LearningRate 0.1601  ProxyLR: 8.0047  Epoch: 2  Global Step: 14970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:47,641-Speed 3896.09 samples/sec  Loss 2.0403  LearningRate 0.1601  ProxyLR: 8.0034  Epoch: 2  Global Step: 14980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:50,270-Speed 3895.84 samples/sec  Loss 1.9581  LearningRate 0.1600  ProxyLR: 8.0022  Epoch: 2  Global Step: 14990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:52,900-Speed 3895.12 samples/sec  Loss 2.0249  LearningRate 0.1600  ProxyLR: 8.0009  Epoch: 2  Global Step: 15000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:55,519-Speed 3911.43 samples/sec  Loss 2.1487  LearningRate 0.1600  ProxyLR: 7.9996  Epoch: 2  Global Step: 15010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:23:58,149-Speed 3894.16 samples/sec  Loss 2.1571  LearningRate 0.1600  ProxyLR: 7.9984  Epoch: 2  Global Step: 15020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:00,779-Speed 3894.53 samples/sec  Loss 2.1809  LearningRate 0.1599  ProxyLR: 7.9971  Epoch: 2  Global Step: 15030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:03,408-Speed 3896.50 samples/sec  Loss 2.0353  LearningRate 0.1599  ProxyLR: 7.9959  Epoch: 2  Global Step: 15040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:06,036-Speed 3896.52 samples/sec  Loss 2.1667  LearningRate 0.1599  ProxyLR: 7.9946  Epoch: 2  Global Step: 15050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:08,668-Speed 3892.57 samples/sec  Loss 2.1098  LearningRate 0.1599  ProxyLR: 7.9934  Epoch: 2  Global Step: 15060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:11,298-Speed 3893.86 samples/sec  Loss 1.9068  LearningRate 0.1598  ProxyLR: 7.9921  Epoch: 2  Global Step: 15070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:13,930-Speed 3891.45 samples/sec  Loss 1.9360  LearningRate 0.1598  ProxyLR: 7.9908  Epoch: 2  Global Step: 15080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:16,560-Speed 3894.51 samples/sec  Loss 1.9961  LearningRate 0.1598  ProxyLR: 7.9896  Epoch: 2  Global Step: 15090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:19,191-Speed 3892.74 samples/sec  Loss 1.9010  LearningRate 0.1598  ProxyLR: 7.9883  Epoch: 2  Global Step: 15100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:21,821-Speed 3895.34 samples/sec  Loss 1.9595  LearningRate 0.1597  ProxyLR: 7.9871  Epoch: 2  Global Step: 15110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:24:24,437-Speed 3914.26 samples/sec  Loss 1.9960  LearningRate 0.1597  ProxyLR: 7.9858  Epoch: 2  Global Step: 15120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:27,066-Speed 3895.81 samples/sec  Loss 2.0191  LearningRate 0.1597  ProxyLR: 7.9846  Epoch: 2  Global Step: 15130   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:29,697-Speed 3893.22 samples/sec  Loss 1.9120  LearningRate 0.1597  ProxyLR: 7.9833  Epoch: 2  Global Step: 15140   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:32,328-Speed 3893.71 samples/sec  Loss 2.2238  LearningRate 0.1596  ProxyLR: 7.9820  Epoch: 2  Global Step: 15150   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:34,958-Speed 3894.79 samples/sec  Loss 2.0757  LearningRate 0.1596  ProxyLR: 7.9808  Epoch: 2  Global Step: 15160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:37,589-Speed 3892.56 samples/sec  Loss 1.9972  LearningRate 0.1596  ProxyLR: 7.9795  Epoch: 2  Global Step: 15170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:40,219-Speed 3894.71 samples/sec  Loss 1.8763  LearningRate 0.1596  ProxyLR: 7.9783  Epoch: 2  Global Step: 15180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:42,849-Speed 3894.84 samples/sec  Loss 2.2100  LearningRate 0.1595  ProxyLR: 7.9770  Epoch: 2  Global Step: 15190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:45,479-Speed 3893.64 samples/sec  Loss 2.2947  LearningRate 0.1595  ProxyLR: 7.9758  Epoch: 2  Global Step: 15200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:48,109-Speed 3895.32 samples/sec  Loss 2.0524  LearningRate 0.1595  ProxyLR: 7.9745  Epoch: 2  Global Step: 15210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:24:50,738-Speed 3896.05 samples/sec  Loss 1.9194  LearningRate 0.1595  ProxyLR: 7.9732  Epoch: 2  Global Step: 15220   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:24:53,367-Speed 3895.46 samples/sec  Loss 1.9643  LearningRate 0.1594  ProxyLR: 7.9720  Epoch: 2  Global Step: 15230   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:24:55,997-Speed 3894.61 samples/sec  Loss 2.1264  LearningRate 0.1594  ProxyLR: 7.9707  Epoch: 2  Global Step: 15240   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:24:58,627-Speed 3893.53 samples/sec  Loss 2.0250  LearningRate 0.1594  ProxyLR: 7.9695  Epoch: 2  Global Step: 15250   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:01,256-Speed 3895.90 samples/sec  Loss 1.8826  LearningRate 0.1594  ProxyLR: 7.9682  Epoch: 2  Global Step: 15260   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:03,889-Speed 3891.11 samples/sec  Loss 1.9241  LearningRate 0.1593  ProxyLR: 7.9670  Epoch: 2  Global Step: 15270   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:06,507-Speed 3911.96 samples/sec  Loss 2.0909  LearningRate 0.1593  ProxyLR: 7.9657  Epoch: 2  Global Step: 15280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:09,139-Speed 3890.95 samples/sec  Loss 2.0491  LearningRate 0.1593  ProxyLR: 7.9644  Epoch: 2  Global Step: 15290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:11,772-Speed 3890.06 samples/sec  Loss 2.0169  LearningRate 0.1593  ProxyLR: 7.9632  Epoch: 2  Global Step: 15300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:14,407-Speed 3887.48 samples/sec  Loss 2.0811  LearningRate 0.1592  ProxyLR: 7.9619  Epoch: 2  Global Step: 15310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:17,041-Speed 3888.15 samples/sec  Loss 2.0801  LearningRate 0.1592  ProxyLR: 7.9607  Epoch: 2  Global Step: 15320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:19,675-Speed 3889.65 samples/sec  Loss 2.0106  LearningRate 0.1592  ProxyLR: 7.9594  Epoch: 2  Global Step: 15330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:22,307-Speed 3890.77 samples/sec  Loss 2.0700  LearningRate 0.1592  ProxyLR: 7.9582  Epoch: 2  Global Step: 15340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:24,940-Speed 3890.24 samples/sec  Loss 2.0357  LearningRate 0.1591  ProxyLR: 7.9569  Epoch: 2  Global Step: 15350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:27,575-Speed 3888.02 samples/sec  Loss 2.0396  LearningRate 0.1591  ProxyLR: 7.9557  Epoch: 2  Global Step: 15360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:30,207-Speed 3890.27 samples/sec  Loss 2.2449  LearningRate 0.1591  ProxyLR: 7.9544  Epoch: 2  Global Step: 15370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:25:32,840-Speed 3891.01 samples/sec  Loss 2.0556  LearningRate 0.1591  ProxyLR: 7.9532  Epoch: 2  Global Step: 15380   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:35,471-Speed 3892.11 samples/sec  Loss 1.9899  LearningRate 0.1590  ProxyLR: 7.9519  Epoch: 2  Global Step: 15390   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:38,105-Speed 3888.91 samples/sec  Loss 2.0421  LearningRate 0.1590  ProxyLR: 7.9506  Epoch: 2  Global Step: 15400   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:40,740-Speed 3887.61 samples/sec  Loss 2.2513  LearningRate 0.1590  ProxyLR: 7.9494  Epoch: 2  Global Step: 15410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:43,374-Speed 3888.41 samples/sec  Loss 2.1266  LearningRate 0.1590  ProxyLR: 7.9481  Epoch: 2  Global Step: 15420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:46,008-Speed 3887.84 samples/sec  Loss 2.0123  LearningRate 0.1589  ProxyLR: 7.9469  Epoch: 2  Global Step: 15430   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:48,641-Speed 3890.85 samples/sec  Loss 2.0379  LearningRate 0.1589  ProxyLR: 7.9456  Epoch: 2  Global Step: 15440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:51,275-Speed 3887.62 samples/sec  Loss 1.9985  LearningRate 0.1589  ProxyLR: 7.9444  Epoch: 2  Global Step: 15450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:53,909-Speed 3889.78 samples/sec  Loss 2.0355  LearningRate 0.1589  ProxyLR: 7.9431  Epoch: 2  Global Step: 15460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:56,541-Speed 3890.64 samples/sec  Loss 2.0259  LearningRate 0.1588  ProxyLR: 7.9419  Epoch: 2  Global Step: 15470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:25:59,160-Speed 3911.36 samples/sec  Loss 2.0024  LearningRate 0.1588  ProxyLR: 7.9406  Epoch: 2  Global Step: 15480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:01,792-Speed 3890.99 samples/sec  Loss 2.0553  LearningRate 0.1588  ProxyLR: 7.9394  Epoch: 2  Global Step: 15490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:04,424-Speed 3891.35 samples/sec  Loss 2.0472  LearningRate 0.1588  ProxyLR: 7.9381  Epoch: 2  Global Step: 15500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:07,043-Speed 3911.70 samples/sec  Loss 2.0968  LearningRate 0.1587  ProxyLR: 7.9368  Epoch: 2  Global Step: 15510   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:09,675-Speed 3891.98 samples/sec  Loss 2.2256  LearningRate 0.1587  ProxyLR: 7.9356  Epoch: 2  Global Step: 15520   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:12,309-Speed 3888.65 samples/sec  Loss 2.0905  LearningRate 0.1587  ProxyLR: 7.9343  Epoch: 2  Global Step: 15530   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:14,940-Speed 3892.20 samples/sec  Loss 2.0424  LearningRate 0.1587  ProxyLR: 7.9331  Epoch: 2  Global Step: 15540   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:17,575-Speed 3887.06 samples/sec  Loss 2.0354  LearningRate 0.1586  ProxyLR: 7.9318  Epoch: 2  Global Step: 15550   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:20,211-Speed 3886.41 samples/sec  Loss 1.9692  LearningRate 0.1586  ProxyLR: 7.9306  Epoch: 2  Global Step: 15560   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:22,845-Speed 3888.59 samples/sec  Loss 1.8702  LearningRate 0.1586  ProxyLR: 7.9293  Epoch: 2  Global Step: 15570   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:25,482-Speed 3884.55 samples/sec  Loss 2.0275  LearningRate 0.1586  ProxyLR: 7.9281  Epoch: 2  Global Step: 15580   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:28,118-Speed 3885.82 samples/sec  Loss 2.0070  LearningRate 0.1585  ProxyLR: 7.9268  Epoch: 2  Global Step: 15590   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:30,753-Speed 3886.02 samples/sec  Loss 2.1027  LearningRate 0.1585  ProxyLR: 7.9256  Epoch: 2  Global Step: 15600   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:26:33,393-Speed 3880.76 samples/sec  Loss 2.0679  LearningRate 0.1585  ProxyLR: 7.9243  Epoch: 2  Global Step: 15610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:36,030-Speed 3883.13 samples/sec  Loss 1.9694  LearningRate 0.1585  ProxyLR: 7.9231  Epoch: 2  Global Step: 15620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:38,668-Speed 3882.25 samples/sec  Loss 1.9753  LearningRate 0.1584  ProxyLR: 7.9218  Epoch: 2  Global Step: 15630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:41,308-Speed 3880.97 samples/sec  Loss 1.9999  LearningRate 0.1584  ProxyLR: 7.9206  Epoch: 2  Global Step: 15640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:43,945-Speed 3882.80 samples/sec  Loss 1.9074  LearningRate 0.1584  ProxyLR: 7.9193  Epoch: 2  Global Step: 15650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:46,579-Speed 3889.39 samples/sec  Loss 1.9306  LearningRate 0.1584  ProxyLR: 7.9181  Epoch: 2  Global Step: 15660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:49,212-Speed 3890.69 samples/sec  Loss 1.9300  LearningRate 0.1583  ProxyLR: 7.9168  Epoch: 2  Global Step: 15670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:51,845-Speed 3889.61 samples/sec  Loss 2.1061  LearningRate 0.1583  ProxyLR: 7.9156  Epoch: 2  Global Step: 15680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:54,477-Speed 3891.54 samples/sec  Loss 2.2259  LearningRate 0.1583  ProxyLR: 7.9143  Epoch: 2  Global Step: 15690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:57,112-Speed 3887.27 samples/sec  Loss 2.1788  LearningRate 0.1583  ProxyLR: 7.9131  Epoch: 2  Global Step: 15700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:26:59,745-Speed 3888.82 samples/sec  Loss 2.0953  LearningRate 0.1582  ProxyLR: 7.9118  Epoch: 2  Global Step: 15710   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:27:02,367-Speed 3908.02 samples/sec  Loss 2.0233  LearningRate 0.1582  ProxyLR: 7.9105  Epoch: 2  Global Step: 15720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:04,999-Speed 3891.75 samples/sec  Loss 2.0533  LearningRate 0.1582  ProxyLR: 7.9093  Epoch: 2  Global Step: 15730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:07,633-Speed 3888.56 samples/sec  Loss 1.8330  LearningRate 0.1582  ProxyLR: 7.9080  Epoch: 2  Global Step: 15740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:10,269-Speed 3885.05 samples/sec  Loss 2.1502  LearningRate 0.1581  ProxyLR: 7.9068  Epoch: 2  Global Step: 15750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:12,899-Speed 3894.36 samples/sec  Loss 2.0286  LearningRate 0.1581  ProxyLR: 7.9055  Epoch: 2  Global Step: 15760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:15,529-Speed 3894.39 samples/sec  Loss 2.0485  LearningRate 0.1581  ProxyLR: 7.9043  Epoch: 2  Global Step: 15770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:18,159-Speed 3894.26 samples/sec  Loss 2.1825  LearningRate 0.1581  ProxyLR: 7.9030  Epoch: 2  Global Step: 15780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:20,792-Speed 3890.67 samples/sec  Loss 2.0475  LearningRate 0.1580  ProxyLR: 7.9018  Epoch: 2  Global Step: 15790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:23,423-Speed 3893.12 samples/sec  Loss 1.9159  LearningRate 0.1580  ProxyLR: 7.9005  Epoch: 2  Global Step: 15800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:26,053-Speed 3894.35 samples/sec  Loss 1.9763  LearningRate 0.1580  ProxyLR: 7.8993  Epoch: 2  Global Step: 15810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:28,685-Speed 3891.75 samples/sec  Loss 2.1617  LearningRate 0.1580  ProxyLR: 7.8980  Epoch: 2  Global Step: 15820   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:27:31,317-Speed 3891.02 samples/sec  Loss 2.0538  LearningRate 0.1579  ProxyLR: 7.8968  Epoch: 2  Global Step: 15830   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:27:33,948-Speed 3893.86 samples/sec  Loss 2.1110  LearningRate 0.1579  ProxyLR: 7.8955  Epoch: 2  Global Step: 15840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:27:36,566-Speed 3911.74 samples/sec  Loss 2.0512  LearningRate 0.1579  ProxyLR: 7.8943  Epoch: 2  Global Step: 15850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:39,196-Speed 3893.73 samples/sec  Loss 2.0391  LearningRate 0.1579  ProxyLR: 7.8930  Epoch: 2  Global Step: 15860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:41,827-Speed 3893.85 samples/sec  Loss 2.0503  LearningRate 0.1578  ProxyLR: 7.8918  Epoch: 2  Global Step: 15870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:44,457-Speed 3894.09 samples/sec  Loss 2.0999  LearningRate 0.1578  ProxyLR: 7.8905  Epoch: 2  Global Step: 15880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:47,087-Speed 3894.27 samples/sec  Loss 1.8916  LearningRate 0.1578  ProxyLR: 7.8893  Epoch: 2  Global Step: 15890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:49,718-Speed 3893.67 samples/sec  Loss 1.9586  LearningRate 0.1578  ProxyLR: 7.8880  Epoch: 2  Global Step: 15900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:52,349-Speed 3892.34 samples/sec  Loss 1.9904  LearningRate 0.1577  ProxyLR: 7.8868  Epoch: 2  Global Step: 15910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:54,980-Speed 3893.72 samples/sec  Loss 2.0292  LearningRate 0.1577  ProxyLR: 7.8855  Epoch: 2  Global Step: 15920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:27:57,612-Speed 3891.37 samples/sec  Loss 1.9656  LearningRate 0.1577  ProxyLR: 7.8843  Epoch: 2  Global Step: 15930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:00,241-Speed 3895.68 samples/sec  Loss 2.1263  LearningRate 0.1577  ProxyLR: 7.8830  Epoch: 2  Global Step: 15940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:02,873-Speed 3891.61 samples/sec  Loss 2.1197  LearningRate 0.1576  ProxyLR: 7.8818  Epoch: 2  Global Step: 15950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:05,504-Speed 3893.00 samples/sec  Loss 2.0180  LearningRate 0.1576  ProxyLR: 7.8805  Epoch: 2  Global Step: 15960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:08,136-Speed 3892.28 samples/sec  Loss 1.9757  LearningRate 0.1576  ProxyLR: 7.8793  Epoch: 2  Global Step: 15970   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:10,754-Speed 3911.93 samples/sec  Loss 2.0982  LearningRate 0.1576  ProxyLR: 7.8780  Epoch: 2  Global Step: 15980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:13,384-Speed 3893.63 samples/sec  Loss 1.9996  LearningRate 0.1575  ProxyLR: 7.8768  Epoch: 2  Global Step: 15990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:16,015-Speed 3893.33 samples/sec  Loss 2.0664  LearningRate 0.1575  ProxyLR: 7.8755  Epoch: 2  Global Step: 16000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:18,646-Speed 3893.62 samples/sec  Loss 2.1787  LearningRate 0.1575  ProxyLR: 7.8743  Epoch: 2  Global Step: 16010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:21,289-Speed 3875.05 samples/sec  Loss 2.1320  LearningRate 0.1575  ProxyLR: 7.8731  Epoch: 2  Global Step: 16020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:23,928-Speed 3880.38 samples/sec  Loss 2.0371  LearningRate 0.1574  ProxyLR: 7.8718  Epoch: 2  Global Step: 16030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:26,567-Speed 3882.20 samples/sec  Loss 2.1077  LearningRate 0.1574  ProxyLR: 7.8706  Epoch: 2  Global Step: 16040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:29,205-Speed 3882.09 samples/sec  Loss 2.0448  LearningRate 0.1574  ProxyLR: 7.8693  Epoch: 2  Global Step: 16050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:31,844-Speed 3881.43 samples/sec  Loss 2.1030  LearningRate 0.1574  ProxyLR: 7.8681  Epoch: 2  Global Step: 16060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:34,483-Speed 3880.81 samples/sec  Loss 2.0169  LearningRate 0.1573  ProxyLR: 7.8668  Epoch: 2  Global Step: 16070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:28:37,121-Speed 3882.91 samples/sec  Loss 2.0686  LearningRate 0.1573  ProxyLR: 7.8656  Epoch: 2  Global Step: 16080   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:39,761-Speed 3879.43 samples/sec  Loss 2.0056  LearningRate 0.1573  ProxyLR: 7.8643  Epoch: 2  Global Step: 16090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:42,400-Speed 3881.64 samples/sec  Loss 2.0999  LearningRate 0.1573  ProxyLR: 7.8631  Epoch: 2  Global Step: 16100   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:45,040-Speed 3879.81 samples/sec  Loss 2.0293  LearningRate 0.1572  ProxyLR: 7.8618  Epoch: 2  Global Step: 16110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:47,680-Speed 3879.45 samples/sec  Loss 2.0111  LearningRate 0.1572  ProxyLR: 7.8606  Epoch: 2  Global Step: 16120   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:50,320-Speed 3879.43 samples/sec  Loss 2.1160  LearningRate 0.1572  ProxyLR: 7.8593  Epoch: 2  Global Step: 16130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:52,961-Speed 3879.02 samples/sec  Loss 1.8548  LearningRate 0.1572  ProxyLR: 7.8581  Epoch: 2  Global Step: 16140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:55,599-Speed 3882.55 samples/sec  Loss 2.1997  LearningRate 0.1571  ProxyLR: 7.8568  Epoch: 2  Global Step: 16150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:28:58,239-Speed 3880.20 samples/sec  Loss 2.1381  LearningRate 0.1571  ProxyLR: 7.8556  Epoch: 2  Global Step: 16160   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:29:00,878-Speed 3880.36 samples/sec  Loss 2.0744  LearningRate 0.1571  ProxyLR: 7.8543  Epoch: 2  Global Step: 16170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:29:03,503-Speed 3902.20 samples/sec  Loss 1.9294  LearningRate 0.1571  ProxyLR: 7.8531  Epoch: 2  Global Step: 16180   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:29:06,129-Speed 3899.66 samples/sec  Loss 2.2422  LearningRate 0.1570  ProxyLR: 7.8518  Epoch: 2  Global Step: 16190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:08,769-Speed 3880.19 samples/sec  Loss 2.0587  LearningRate 0.1570  ProxyLR: 7.8506  Epoch: 2  Global Step: 16200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:11,409-Speed 3879.73 samples/sec  Loss 2.1313  LearningRate 0.1570  ProxyLR: 7.8493  Epoch: 2  Global Step: 16210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:14,049-Speed 3880.53 samples/sec  Loss 1.9503  LearningRate 0.1570  ProxyLR: 7.8481  Epoch: 2  Global Step: 16220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:16,688-Speed 3881.08 samples/sec  Loss 2.0615  LearningRate 0.1569  ProxyLR: 7.8469  Epoch: 2  Global Step: 16230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:19,326-Speed 3882.58 samples/sec  Loss 2.0125  LearningRate 0.1569  ProxyLR: 7.8456  Epoch: 2  Global Step: 16240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:21,965-Speed 3880.83 samples/sec  Loss 2.0371  LearningRate 0.1569  ProxyLR: 7.8444  Epoch: 2  Global Step: 16250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:24,603-Speed 3883.09 samples/sec  Loss 2.0009  LearningRate 0.1569  ProxyLR: 7.8431  Epoch: 2  Global Step: 16260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:27,239-Speed 3884.29 samples/sec  Loss 1.9448  LearningRate 0.1568  ProxyLR: 7.8419  Epoch: 2  Global Step: 16270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:29,881-Speed 3878.15 samples/sec  Loss 2.1019  LearningRate 0.1568  ProxyLR: 7.8406  Epoch: 2  Global Step: 16280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:32,499-Speed 3911.72 samples/sec  Loss 2.0420  LearningRate 0.1568  ProxyLR: 7.8394  Epoch: 2  Global Step: 16290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:35,130-Speed 3892.78 samples/sec  Loss 2.0945  LearningRate 0.1568  ProxyLR: 7.8381  Epoch: 2  Global Step: 16300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:37,760-Speed 3894.43 samples/sec  Loss 1.9663  LearningRate 0.1567  ProxyLR: 7.8369  Epoch: 2  Global Step: 16310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:40,390-Speed 3894.55 samples/sec  Loss 2.0968  LearningRate 0.1567  ProxyLR: 7.8356  Epoch: 2  Global Step: 16320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:43,024-Speed 3889.62 samples/sec  Loss 2.0144  LearningRate 0.1567  ProxyLR: 7.8344  Epoch: 2  Global Step: 16330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:45,655-Speed 3892.81 samples/sec  Loss 2.1537  LearningRate 0.1567  ProxyLR: 7.8332  Epoch: 2  Global Step: 16340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:48,287-Speed 3891.75 samples/sec  Loss 2.0938  LearningRate 0.1566  ProxyLR: 7.8319  Epoch: 2  Global Step: 16350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:50,918-Speed 3893.33 samples/sec  Loss 2.1503  LearningRate 0.1566  ProxyLR: 7.8307  Epoch: 2  Global Step: 16360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:53,549-Speed 3891.72 samples/sec  Loss 2.0797  LearningRate 0.1566  ProxyLR: 7.8294  Epoch: 2  Global Step: 16370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:56,181-Speed 3892.25 samples/sec  Loss 2.0724  LearningRate 0.1566  ProxyLR: 7.8282  Epoch: 2  Global Step: 16380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:29:58,813-Speed 3891.23 samples/sec  Loss 1.9419  LearningRate 0.1565  ProxyLR: 7.8269  Epoch: 2  Global Step: 16390   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:01,445-Speed 3891.01 samples/sec  Loss 1.9022  LearningRate 0.1565  ProxyLR: 7.8257  Epoch: 2  Global Step: 16400   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:04,079-Speed 3889.11 samples/sec  Loss 1.8845  LearningRate 0.1565  ProxyLR: 7.8244  Epoch: 2  Global Step: 16410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:06,710-Speed 3892.69 samples/sec  Loss 1.9695  LearningRate 0.1565  ProxyLR: 7.8232  Epoch: 2  Global Step: 16420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:09,341-Speed 3893.43 samples/sec  Loss 1.9955  LearningRate 0.1564  ProxyLR: 7.8219  Epoch: 2  Global Step: 16430   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:11,971-Speed 3894.00 samples/sec  Loss 2.0560  LearningRate 0.1564  ProxyLR: 7.8207  Epoch: 2  Global Step: 16440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:14,590-Speed 3911.42 samples/sec  Loss 1.9047  LearningRate 0.1564  ProxyLR: 7.8195  Epoch: 2  Global Step: 16450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:17,222-Speed 3890.82 samples/sec  Loss 2.0799  LearningRate 0.1564  ProxyLR: 7.8182  Epoch: 2  Global Step: 16460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:19,854-Speed 3892.10 samples/sec  Loss 1.9447  LearningRate 0.1563  ProxyLR: 7.8170  Epoch: 2  Global Step: 16470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:22,484-Speed 3894.32 samples/sec  Loss 1.9699  LearningRate 0.1563  ProxyLR: 7.8157  Epoch: 2  Global Step: 16480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:25,115-Speed 3893.92 samples/sec  Loss 2.1037  LearningRate 0.1563  ProxyLR: 7.8145  Epoch: 2  Global Step: 16490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:27,744-Speed 3895.60 samples/sec  Loss 1.9758  LearningRate 0.1563  ProxyLR: 7.8132  Epoch: 2  Global Step: 16500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:30,373-Speed 3895.96 samples/sec  Loss 1.8911  LearningRate 0.1562  ProxyLR: 7.8120  Epoch: 2  Global Step: 16510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:33,003-Speed 3895.25 samples/sec  Loss 1.9301  LearningRate 0.1562  ProxyLR: 7.8108  Epoch: 2  Global Step: 16520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:35,633-Speed 3894.12 samples/sec  Loss 2.0026  LearningRate 0.1562  ProxyLR: 7.8095  Epoch: 2  Global Step: 16530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:38,262-Speed 3896.12 samples/sec  Loss 2.0683  LearningRate 0.1562  ProxyLR: 7.8083  Epoch: 2  Global Step: 16540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:30:40,892-Speed 3894.55 samples/sec  Loss 1.9848  LearningRate 0.1561  ProxyLR: 7.8070  Epoch: 2  Global Step: 16550   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:43,521-Speed 3895.90 samples/sec  Loss 2.1211  LearningRate 0.1561  ProxyLR: 7.8058  Epoch: 2  Global Step: 16560   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:46,149-Speed 3896.56 samples/sec  Loss 1.9658  LearningRate 0.1561  ProxyLR: 7.8045  Epoch: 2  Global Step: 16570   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:48,780-Speed 3894.11 samples/sec  Loss 1.9520  LearningRate 0.1561  ProxyLR: 7.8033  Epoch: 2  Global Step: 16580   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:51,409-Speed 3894.97 samples/sec  Loss 2.0575  LearningRate 0.1560  ProxyLR: 7.8021  Epoch: 2  Global Step: 16590   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:54,040-Speed 3893.50 samples/sec  Loss 1.9977  LearningRate 0.1560  ProxyLR: 7.8008  Epoch: 2  Global Step: 16600   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:56,670-Speed 3894.79 samples/sec  Loss 2.0074  LearningRate 0.1560  ProxyLR: 7.7996  Epoch: 2  Global Step: 16610   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:30:59,299-Speed 3895.79 samples/sec  Loss 2.0055  LearningRate 0.1560  ProxyLR: 7.7983  Epoch: 2  Global Step: 16620   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:31:01,916-Speed 3914.06 samples/sec  Loss 2.1311  LearningRate 0.1559  ProxyLR: 7.7971  Epoch: 2  Global Step: 16630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:04,547-Speed 3893.38 samples/sec  Loss 1.9914  LearningRate 0.1559  ProxyLR: 7.7958  Epoch: 2  Global Step: 16640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:07,177-Speed 3893.48 samples/sec  Loss 2.1297  LearningRate 0.1559  ProxyLR: 7.7946  Epoch: 2  Global Step: 16650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:09,807-Speed 3894.91 samples/sec  Loss 2.0034  LearningRate 0.1559  ProxyLR: 7.7934  Epoch: 2  Global Step: 16660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:12,437-Speed 3894.33 samples/sec  Loss 2.0012  LearningRate 0.1558  ProxyLR: 7.7921  Epoch: 2  Global Step: 16670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:15,068-Speed 3893.21 samples/sec  Loss 2.1701  LearningRate 0.1558  ProxyLR: 7.7909  Epoch: 2  Global Step: 16680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:17,698-Speed 3894.43 samples/sec  Loss 2.0974  LearningRate 0.1558  ProxyLR: 7.7896  Epoch: 2  Global Step: 16690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:20,328-Speed 3895.27 samples/sec  Loss 2.1000  LearningRate 0.1558  ProxyLR: 7.7884  Epoch: 2  Global Step: 16700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:22,958-Speed 3893.78 samples/sec  Loss 2.1282  LearningRate 0.1557  ProxyLR: 7.7871  Epoch: 2  Global Step: 16710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:25,588-Speed 3894.99 samples/sec  Loss 2.2490  LearningRate 0.1557  ProxyLR: 7.7859  Epoch: 2  Global Step: 16720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:28,218-Speed 3894.77 samples/sec  Loss 2.1451  LearningRate 0.1557  ProxyLR: 7.7847  Epoch: 2  Global Step: 16730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:31:30,834-Speed 3915.01 samples/sec  Loss 2.1064  LearningRate 0.1557  ProxyLR: 7.7834  Epoch: 2  Global Step: 16740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:33,463-Speed 3894.90 samples/sec  Loss 1.9840  LearningRate 0.1556  ProxyLR: 7.7822  Epoch: 2  Global Step: 16750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:36,093-Speed 3895.44 samples/sec  Loss 2.0040  LearningRate 0.1556  ProxyLR: 7.7809  Epoch: 2  Global Step: 16760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:38,720-Speed 3898.32 samples/sec  Loss 1.9505  LearningRate 0.1556  ProxyLR: 7.7797  Epoch: 2  Global Step: 16770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:41,350-Speed 3895.10 samples/sec  Loss 2.0065  LearningRate 0.1556  ProxyLR: 7.7785  Epoch: 2  Global Step: 16780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:43,979-Speed 3895.87 samples/sec  Loss 2.0321  LearningRate 0.1555  ProxyLR: 7.7772  Epoch: 2  Global Step: 16790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:46,607-Speed 3897.93 samples/sec  Loss 2.1740  LearningRate 0.1555  ProxyLR: 7.7760  Epoch: 2  Global Step: 16800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:49,238-Speed 3892.75 samples/sec  Loss 2.0757  LearningRate 0.1555  ProxyLR: 7.7747  Epoch: 2  Global Step: 16810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:51,869-Speed 3892.14 samples/sec  Loss 1.9926  LearningRate 0.1555  ProxyLR: 7.7735  Epoch: 2  Global Step: 16820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:54,499-Speed 3895.03 samples/sec  Loss 2.1271  LearningRate 0.1554  ProxyLR: 7.7723  Epoch: 2  Global Step: 16830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:31:57,128-Speed 3896.85 samples/sec  Loss 1.9660  LearningRate 0.1554  ProxyLR: 7.7710  Epoch: 2  Global Step: 16840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:31:59,743-Speed 3916.50 samples/sec  Loss 2.0644  LearningRate 0.1554  ProxyLR: 7.7698  Epoch: 2  Global Step: 16850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:02,375-Speed 3891.80 samples/sec  Loss 2.0080  LearningRate 0.1554  ProxyLR: 7.7685  Epoch: 2  Global Step: 16860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:05,005-Speed 3894.77 samples/sec  Loss 2.2063  LearningRate 0.1553  ProxyLR: 7.7673  Epoch: 2  Global Step: 16870   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:07,634-Speed 3895.75 samples/sec  Loss 1.9767  LearningRate 0.1553  ProxyLR: 7.7661  Epoch: 2  Global Step: 16880   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:10,263-Speed 3896.24 samples/sec  Loss 2.1208  LearningRate 0.1553  ProxyLR: 7.7648  Epoch: 2  Global Step: 16890   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:12,892-Speed 3895.89 samples/sec  Loss 2.0830  LearningRate 0.1553  ProxyLR: 7.7636  Epoch: 2  Global Step: 16900   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:15,521-Speed 3894.94 samples/sec  Loss 2.0971  LearningRate 0.1552  ProxyLR: 7.7623  Epoch: 2  Global Step: 16910   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:18,150-Speed 3896.08 samples/sec  Loss 1.9742  LearningRate 0.1552  ProxyLR: 7.7611  Epoch: 2  Global Step: 16920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:20,780-Speed 3895.01 samples/sec  Loss 2.0734  LearningRate 0.1552  ProxyLR: 7.7599  Epoch: 2  Global Step: 16930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:23,409-Speed 3896.19 samples/sec  Loss 2.0576  LearningRate 0.1552  ProxyLR: 7.7586  Epoch: 2  Global Step: 16940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:26,041-Speed 3891.15 samples/sec  Loss 2.0136  LearningRate 0.1551  ProxyLR: 7.7574  Epoch: 2  Global Step: 16950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:32:28,673-Speed 3891.11 samples/sec  Loss 2.0843  LearningRate 0.1551  ProxyLR: 7.7561  Epoch: 2  Global Step: 16960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:32:31,291-Speed 3912.09 samples/sec  Loss 1.9930  LearningRate 0.1551  ProxyLR: 7.7549  Epoch: 2  Global Step: 16970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:33,925-Speed 3889.87 samples/sec  Loss 1.9119  LearningRate 0.1551  ProxyLR: 7.7537  Epoch: 2  Global Step: 16980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:36,563-Speed 3882.66 samples/sec  Loss 1.8990  LearningRate 0.1550  ProxyLR: 7.7524  Epoch: 2  Global Step: 16990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:39,195-Speed 3890.68 samples/sec  Loss 1.9912  LearningRate 0.1550  ProxyLR: 7.7512  Epoch: 2  Global Step: 17000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:41,831-Speed 3885.62 samples/sec  Loss 2.0482  LearningRate 0.1550  ProxyLR: 7.7499  Epoch: 2  Global Step: 17010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:44,473-Speed 3877.02 samples/sec  Loss 2.1294  LearningRate 0.1550  ProxyLR: 7.7487  Epoch: 2  Global Step: 17020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:47,115-Speed 3877.71 samples/sec  Loss 2.0206  LearningRate 0.1549  ProxyLR: 7.7475  Epoch: 2  Global Step: 17030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:49,755-Speed 3879.79 samples/sec  Loss 1.9992  LearningRate 0.1549  ProxyLR: 7.7462  Epoch: 2  Global Step: 17040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:32:52,449-Speed 3801.13 samples/sec  Loss 2.2069  LearningRate 0.1549  ProxyLR: 7.7450  Epoch: 2  Global Step: 17050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:01,822-Speed 1092.66 samples/sec  Loss 1.8425  LearningRate 0.1549  ProxyLR: 7.7438  Epoch: 3  Global Step: 17060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:04,472-Speed 3865.12 samples/sec  Loss 1.2941  LearningRate 0.1549  ProxyLR: 7.7425  Epoch: 3  Global Step: 17070   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:07,131-Speed 3852.49 samples/sec  Loss 1.1318  LearningRate 0.1548  ProxyLR: 7.7413  Epoch: 3  Global Step: 17080   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:09,760-Speed 3895.24 samples/sec  Loss 1.1888  LearningRate 0.1548  ProxyLR: 7.7400  Epoch: 3  Global Step: 17090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:12,390-Speed 3894.09 samples/sec  Loss 1.2059  LearningRate 0.1548  ProxyLR: 7.7388  Epoch: 3  Global Step: 17100   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:15,023-Speed 3890.22 samples/sec  Loss 1.1218  LearningRate 0.1548  ProxyLR: 7.7376  Epoch: 3  Global Step: 17110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:17,659-Speed 3885.46 samples/sec  Loss 1.2195  LearningRate 0.1547  ProxyLR: 7.7363  Epoch: 3  Global Step: 17120   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:20,343-Speed 3816.50 samples/sec  Loss 1.1120  LearningRate 0.1547  ProxyLR: 7.7351  Epoch: 3  Global Step: 17130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:22,979-Speed 3885.60 samples/sec  Loss 1.2342  LearningRate 0.1547  ProxyLR: 7.7339  Epoch: 3  Global Step: 17140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:25,608-Speed 3895.92 samples/sec  Loss 1.1466  LearningRate 0.1547  ProxyLR: 7.7326  Epoch: 3  Global Step: 17150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:28,239-Speed 3892.22 samples/sec  Loss 1.2014  LearningRate 0.1546  ProxyLR: 7.7314  Epoch: 3  Global Step: 17160   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:30,855-Speed 3915.78 samples/sec  Loss 1.1891  LearningRate 0.1546  ProxyLR: 7.7301  Epoch: 3  Global Step: 17170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:33:33,470-Speed 3916.37 samples/sec  Loss 1.1413  LearningRate 0.1546  ProxyLR: 7.7289  Epoch: 3  Global Step: 17180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:36,102-Speed 3892.47 samples/sec  Loss 1.1894  LearningRate 0.1546  ProxyLR: 7.7277  Epoch: 3  Global Step: 17190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:38,735-Speed 3890.37 samples/sec  Loss 1.1373  LearningRate 0.1545  ProxyLR: 7.7264  Epoch: 3  Global Step: 17200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:41,364-Speed 3895.77 samples/sec  Loss 1.1707  LearningRate 0.1545  ProxyLR: 7.7252  Epoch: 3  Global Step: 17210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:43,991-Speed 3898.05 samples/sec  Loss 1.1683  LearningRate 0.1545  ProxyLR: 7.7240  Epoch: 3  Global Step: 17220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:46,617-Speed 3901.05 samples/sec  Loss 1.2131  LearningRate 0.1545  ProxyLR: 7.7227  Epoch: 3  Global Step: 17230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:49,245-Speed 3896.78 samples/sec  Loss 1.1824  LearningRate 0.1544  ProxyLR: 7.7215  Epoch: 3  Global Step: 17240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:51,874-Speed 3895.82 samples/sec  Loss 1.2943  LearningRate 0.1544  ProxyLR: 7.7202  Epoch: 3  Global Step: 17250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:54,503-Speed 3896.34 samples/sec  Loss 1.1580  LearningRate 0.1544  ProxyLR: 7.7190  Epoch: 3  Global Step: 17260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:57,130-Speed 3898.60 samples/sec  Loss 1.3110  LearningRate 0.1544  ProxyLR: 7.7178  Epoch: 3  Global Step: 17270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:33:59,757-Speed 3899.71 samples/sec  Loss 1.3240  LearningRate 0.1543  ProxyLR: 7.7165  Epoch: 3  Global Step: 17280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:02,383-Speed 3900.13 samples/sec  Loss 1.1961  LearningRate 0.1543  ProxyLR: 7.7153  Epoch: 3  Global Step: 17290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:05,008-Speed 3901.27 samples/sec  Loss 1.2550  LearningRate 0.1543  ProxyLR: 7.7141  Epoch: 3  Global Step: 17300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:07,638-Speed 3894.69 samples/sec  Loss 1.2471  LearningRate 0.1543  ProxyLR: 7.7128  Epoch: 3  Global Step: 17310   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:10,301-Speed 3846.63 samples/sec  Loss 1.2893  LearningRate 0.1542  ProxyLR: 7.7116  Epoch: 3  Global Step: 17320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:12,926-Speed 3901.62 samples/sec  Loss 1.1961  LearningRate 0.1542  ProxyLR: 7.7104  Epoch: 3  Global Step: 17330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:15,555-Speed 3895.98 samples/sec  Loss 1.1216  LearningRate 0.1542  ProxyLR: 7.7091  Epoch: 3  Global Step: 17340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:18,180-Speed 3901.19 samples/sec  Loss 1.2643  LearningRate 0.1542  ProxyLR: 7.7079  Epoch: 3  Global Step: 17350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:20,858-Speed 3825.11 samples/sec  Loss 1.2826  LearningRate 0.1541  ProxyLR: 7.7067  Epoch: 3  Global Step: 17360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:23,483-Speed 3901.84 samples/sec  Loss 1.2929  LearningRate 0.1541  ProxyLR: 7.7054  Epoch: 3  Global Step: 17370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:26,109-Speed 3900.61 samples/sec  Loss 1.2608  LearningRate 0.1541  ProxyLR: 7.7042  Epoch: 3  Global Step: 17380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:28,733-Speed 3903.51 samples/sec  Loss 1.1874  LearningRate 0.1541  ProxyLR: 7.7029  Epoch: 3  Global Step: 17390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:31,366-Speed 3888.87 samples/sec  Loss 1.2143  LearningRate 0.1540  ProxyLR: 7.7017  Epoch: 3  Global Step: 17400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:34,002-Speed 3886.23 samples/sec  Loss 1.1524  LearningRate 0.1540  ProxyLR: 7.7005  Epoch: 3  Global Step: 17410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:34:36,637-Speed 3887.75 samples/sec  Loss 1.2647  LearningRate 0.1540  ProxyLR: 7.6992  Epoch: 3  Global Step: 17420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:39,273-Speed 3885.88 samples/sec  Loss 1.1186  LearningRate 0.1540  ProxyLR: 7.6980  Epoch: 3  Global Step: 17430   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:41,908-Speed 3886.14 samples/sec  Loss 1.1702  LearningRate 0.1539  ProxyLR: 7.6968  Epoch: 3  Global Step: 17440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:44,546-Speed 3883.53 samples/sec  Loss 1.1939  LearningRate 0.1539  ProxyLR: 7.6955  Epoch: 3  Global Step: 17450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:47,184-Speed 3882.18 samples/sec  Loss 1.2191  LearningRate 0.1539  ProxyLR: 7.6943  Epoch: 3  Global Step: 17460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:49,821-Speed 3884.63 samples/sec  Loss 1.2302  LearningRate 0.1539  ProxyLR: 7.6931  Epoch: 3  Global Step: 17470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:52,455-Speed 3887.55 samples/sec  Loss 1.2475  LearningRate 0.1538  ProxyLR: 7.6918  Epoch: 3  Global Step: 17480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:55,091-Speed 3885.92 samples/sec  Loss 1.1879  LearningRate 0.1538  ProxyLR: 7.6906  Epoch: 3  Global Step: 17490   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:34:57,718-Speed 3898.86 samples/sec  Loss 1.3227  LearningRate 0.1538  ProxyLR: 7.6894  Epoch: 3  Global Step: 17500   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:00,346-Speed 3897.88 samples/sec  Loss 1.1906  LearningRate 0.1538  ProxyLR: 7.6881  Epoch: 3  Global Step: 17510   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:02,959-Speed 3918.96 samples/sec  Loss 1.2540  LearningRate 0.1537  ProxyLR: 7.6869  Epoch: 3  Global Step: 17520   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:05,588-Speed 3896.21 samples/sec  Loss 1.2204  LearningRate 0.1537  ProxyLR: 7.6857  Epoch: 3  Global Step: 17530   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:08,215-Speed 3898.40 samples/sec  Loss 1.2547  LearningRate 0.1537  ProxyLR: 7.6844  Epoch: 3  Global Step: 17540   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:10,844-Speed 3895.98 samples/sec  Loss 1.3572  LearningRate 0.1537  ProxyLR: 7.6832  Epoch: 3  Global Step: 17550   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:13,459-Speed 3916.94 samples/sec  Loss 1.2781  LearningRate 0.1536  ProxyLR: 7.6820  Epoch: 3  Global Step: 17560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:16,091-Speed 3891.56 samples/sec  Loss 1.2988  LearningRate 0.1536  ProxyLR: 7.6807  Epoch: 3  Global Step: 17570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:18,722-Speed 3893.68 samples/sec  Loss 1.2102  LearningRate 0.1536  ProxyLR: 7.6795  Epoch: 3  Global Step: 17580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:21,351-Speed 3895.40 samples/sec  Loss 1.2768  LearningRate 0.1536  ProxyLR: 7.6783  Epoch: 3  Global Step: 17590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:23,981-Speed 3894.88 samples/sec  Loss 1.3234  LearningRate 0.1535  ProxyLR: 7.6770  Epoch: 3  Global Step: 17600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:26,611-Speed 3894.59 samples/sec  Loss 1.4028  LearningRate 0.1535  ProxyLR: 7.6758  Epoch: 3  Global Step: 17610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:29,240-Speed 3895.45 samples/sec  Loss 1.2629  LearningRate 0.1535  ProxyLR: 7.6746  Epoch: 3  Global Step: 17620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:31,868-Speed 3896.87 samples/sec  Loss 1.3124  LearningRate 0.1535  ProxyLR: 7.6733  Epoch: 3  Global Step: 17630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:34,498-Speed 3894.73 samples/sec  Loss 1.3352  LearningRate 0.1534  ProxyLR: 7.6721  Epoch: 3  Global Step: 17640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:37,127-Speed 3895.53 samples/sec  Loss 1.1582  LearningRate 0.1534  ProxyLR: 7.6709  Epoch: 3  Global Step: 17650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:35:39,759-Speed 3892.24 samples/sec  Loss 1.2146  LearningRate 0.1534  ProxyLR: 7.6696  Epoch: 3  Global Step: 17660   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:42,391-Speed 3891.59 samples/sec  Loss 1.2906  LearningRate 0.1534  ProxyLR: 7.6684  Epoch: 3  Global Step: 17670   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:45,022-Speed 3892.54 samples/sec  Loss 1.4324  LearningRate 0.1533  ProxyLR: 7.6672  Epoch: 3  Global Step: 17680   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:47,655-Speed 3890.08 samples/sec  Loss 1.4084  LearningRate 0.1533  ProxyLR: 7.6659  Epoch: 3  Global Step: 17690   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:50,287-Speed 3891.75 samples/sec  Loss 1.3573  LearningRate 0.1533  ProxyLR: 7.6647  Epoch: 3  Global Step: 17700   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:52,917-Speed 3894.98 samples/sec  Loss 1.2534  LearningRate 0.1533  ProxyLR: 7.6635  Epoch: 3  Global Step: 17710   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:55,547-Speed 3893.71 samples/sec  Loss 1.2828  LearningRate 0.1532  ProxyLR: 7.6623  Epoch: 3  Global Step: 17720   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:35:58,181-Speed 3888.30 samples/sec  Loss 1.3144  LearningRate 0.1532  ProxyLR: 7.6610  Epoch: 3  Global Step: 17730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:00,819-Speed 3883.06 samples/sec  Loss 1.2412  LearningRate 0.1532  ProxyLR: 7.6598  Epoch: 3  Global Step: 17740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:03,457-Speed 3882.81 samples/sec  Loss 1.2658  LearningRate 0.1532  ProxyLR: 7.6586  Epoch: 3  Global Step: 17750   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:06,090-Speed 3890.19 samples/sec  Loss 1.3239  LearningRate 0.1531  ProxyLR: 7.6573  Epoch: 3  Global Step: 17760   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 15:36:08,709-Speed 3910.88 samples/sec  Loss 1.2673  LearningRate 0.1531  ProxyLR: 7.6561  Epoch: 3  Global Step: 17770   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:11,341-Speed 3890.40 samples/sec  Loss 1.2627  LearningRate 0.1531  ProxyLR: 7.6549  Epoch: 3  Global Step: 17780   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:13,976-Speed 3888.62 samples/sec  Loss 1.3849  LearningRate 0.1531  ProxyLR: 7.6536  Epoch: 3  Global Step: 17790   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:16,607-Speed 3892.07 samples/sec  Loss 1.3281  LearningRate 0.1530  ProxyLR: 7.6524  Epoch: 3  Global Step: 17800   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:19,238-Speed 3893.76 samples/sec  Loss 1.2743  LearningRate 0.1530  ProxyLR: 7.6512  Epoch: 3  Global Step: 17810   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:21,870-Speed 3891.46 samples/sec  Loss 1.3101  LearningRate 0.1530  ProxyLR: 7.6499  Epoch: 3  Global Step: 17820   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:24,502-Speed 3891.63 samples/sec  Loss 1.2638  LearningRate 0.1530  ProxyLR: 7.6487  Epoch: 3  Global Step: 17830   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:27,133-Speed 3891.65 samples/sec  Loss 1.3639  LearningRate 0.1529  ProxyLR: 7.6475  Epoch: 3  Global Step: 17840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:29,764-Speed 3892.86 samples/sec  Loss 1.3356  LearningRate 0.1529  ProxyLR: 7.6463  Epoch: 3  Global Step: 17850   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:32,396-Speed 3891.77 samples/sec  Loss 1.4293  LearningRate 0.1529  ProxyLR: 7.6450  Epoch: 3  Global Step: 17860   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:35,028-Speed 3892.43 samples/sec  Loss 1.3703  LearningRate 0.1529  ProxyLR: 7.6438  Epoch: 3  Global Step: 17870   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 15:36:37,659-Speed 3892.68 samples/sec  Loss 1.2653  LearningRate 0.1529  ProxyLR: 7.6426  Epoch: 3  Global Step: 17880   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 15:36:40,278-Speed 3910.83 samples/sec  Loss 1.3592  LearningRate 0.1528  ProxyLR: 7.6413  Epoch: 3  Global Step: 17890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:42,909-Speed 3891.92 samples/sec  Loss 1.4428  LearningRate 0.1528  ProxyLR: 7.6401  Epoch: 3  Global Step: 17900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:45,542-Speed 3890.49 samples/sec  Loss 1.4232  LearningRate 0.1528  ProxyLR: 7.6389  Epoch: 3  Global Step: 17910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:48,174-Speed 3890.98 samples/sec  Loss 1.3205  LearningRate 0.1528  ProxyLR: 7.6376  Epoch: 3  Global Step: 17920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:50,809-Speed 3887.77 samples/sec  Loss 1.3500  LearningRate 0.1527  ProxyLR: 7.6364  Epoch: 3  Global Step: 17930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:53,439-Speed 3893.80 samples/sec  Loss 1.3896  LearningRate 0.1527  ProxyLR: 7.6352  Epoch: 3  Global Step: 17940   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:56,071-Speed 3891.81 samples/sec  Loss 1.2596  LearningRate 0.1527  ProxyLR: 7.6340  Epoch: 3  Global Step: 17950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:36:58,689-Speed 3912.22 samples/sec  Loss 1.2739  LearningRate 0.1527  ProxyLR: 7.6327  Epoch: 3  Global Step: 17960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:01,322-Speed 3891.12 samples/sec  Loss 1.3635  LearningRate 0.1526  ProxyLR: 7.6315  Epoch: 3  Global Step: 17970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:03,954-Speed 3891.30 samples/sec  Loss 1.4302  LearningRate 0.1526  ProxyLR: 7.6303  Epoch: 3  Global Step: 17980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:06,586-Speed 3890.29 samples/sec  Loss 1.3242  LearningRate 0.1526  ProxyLR: 7.6290  Epoch: 3  Global Step: 17990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:09,219-Speed 3890.83 samples/sec  Loss 1.4142  LearningRate 0.1526  ProxyLR: 7.6278  Epoch: 3  Global Step: 18000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:11,851-Speed 3891.39 samples/sec  Loss 1.3527  LearningRate 0.1525  ProxyLR: 7.6266  Epoch: 3  Global Step: 18010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:14,482-Speed 3892.74 samples/sec  Loss 1.3040  LearningRate 0.1525  ProxyLR: 7.6254  Epoch: 3  Global Step: 18020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:17,113-Speed 3893.15 samples/sec  Loss 1.4143  LearningRate 0.1525  ProxyLR: 7.6241  Epoch: 3  Global Step: 18030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:19,744-Speed 3892.13 samples/sec  Loss 1.3539  LearningRate 0.1525  ProxyLR: 7.6229  Epoch: 3  Global Step: 18040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:22,376-Speed 3892.60 samples/sec  Loss 1.2932  LearningRate 0.1524  ProxyLR: 7.6217  Epoch: 3  Global Step: 18050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:37:25,006-Speed 3893.98 samples/sec  Loss 1.3745  LearningRate 0.1524  ProxyLR: 7.6204  Epoch: 3  Global Step: 18060   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:27,637-Speed 3892.73 samples/sec  Loss 1.4797  LearningRate 0.1524  ProxyLR: 7.6192  Epoch: 3  Global Step: 18070   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:30,269-Speed 3891.54 samples/sec  Loss 1.3991  LearningRate 0.1524  ProxyLR: 7.6180  Epoch: 3  Global Step: 18080   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:32,906-Speed 3884.49 samples/sec  Loss 1.4490  LearningRate 0.1523  ProxyLR: 7.6168  Epoch: 3  Global Step: 18090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:35,541-Speed 3886.70 samples/sec  Loss 1.3457  LearningRate 0.1523  ProxyLR: 7.6155  Epoch: 3  Global Step: 18100   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:38,173-Speed 3891.69 samples/sec  Loss 1.3618  LearningRate 0.1523  ProxyLR: 7.6143  Epoch: 3  Global Step: 18110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:40,805-Speed 3891.02 samples/sec  Loss 1.5236  LearningRate 0.1523  ProxyLR: 7.6131  Epoch: 3  Global Step: 18120   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:43,438-Speed 3890.97 samples/sec  Loss 1.4184  LearningRate 0.1522  ProxyLR: 7.6118  Epoch: 3  Global Step: 18130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:46,070-Speed 3890.51 samples/sec  Loss 1.3168  LearningRate 0.1522  ProxyLR: 7.6106  Epoch: 3  Global Step: 18140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:48,702-Speed 3891.32 samples/sec  Loss 1.3166  LearningRate 0.1522  ProxyLR: 7.6094  Epoch: 3  Global Step: 18150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:51,334-Speed 3891.82 samples/sec  Loss 1.4261  LearningRate 0.1522  ProxyLR: 7.6082  Epoch: 3  Global Step: 18160   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 15:37:53,953-Speed 3910.20 samples/sec  Loss 1.3997  LearningRate 0.1521  ProxyLR: 7.6069  Epoch: 3  Global Step: 18170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:56,585-Speed 3892.59 samples/sec  Loss 1.4258  LearningRate 0.1521  ProxyLR: 7.6057  Epoch: 3  Global Step: 18180   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:37:59,216-Speed 3893.16 samples/sec  Loss 1.3954  LearningRate 0.1521  ProxyLR: 7.6045  Epoch: 3  Global Step: 18190   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:01,834-Speed 3911.35 samples/sec  Loss 1.3065  LearningRate 0.1521  ProxyLR: 7.6033  Epoch: 3  Global Step: 18200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:04,468-Speed 3889.41 samples/sec  Loss 1.3821  LearningRate 0.1520  ProxyLR: 7.6020  Epoch: 3  Global Step: 18210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:07,101-Speed 3889.80 samples/sec  Loss 1.3844  LearningRate 0.1520  ProxyLR: 7.6008  Epoch: 3  Global Step: 18220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:09,732-Speed 3892.84 samples/sec  Loss 1.5737  LearningRate 0.1520  ProxyLR: 7.5996  Epoch: 3  Global Step: 18230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:12,363-Speed 3893.55 samples/sec  Loss 1.3472  LearningRate 0.1520  ProxyLR: 7.5983  Epoch: 3  Global Step: 18240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:14,994-Speed 3892.94 samples/sec  Loss 1.4657  LearningRate 0.1519  ProxyLR: 7.5971  Epoch: 3  Global Step: 18250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:17,624-Speed 3893.71 samples/sec  Loss 1.4537  LearningRate 0.1519  ProxyLR: 7.5959  Epoch: 3  Global Step: 18260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:20,255-Speed 3893.39 samples/sec  Loss 1.3854  LearningRate 0.1519  ProxyLR: 7.5947  Epoch: 3  Global Step: 18270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:22,887-Speed 3891.10 samples/sec  Loss 1.3819  LearningRate 0.1519  ProxyLR: 7.5934  Epoch: 3  Global Step: 18280   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:25,519-Speed 3891.99 samples/sec  Loss 1.3953  LearningRate 0.1518  ProxyLR: 7.5922  Epoch: 3  Global Step: 18290   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:38:28,150-Speed 3892.48 samples/sec  Loss 1.4121  LearningRate 0.1518  ProxyLR: 7.5910  Epoch: 3  Global Step: 18300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:30,781-Speed 3892.43 samples/sec  Loss 1.4786  LearningRate 0.1518  ProxyLR: 7.5898  Epoch: 3  Global Step: 18310   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:33,413-Speed 3891.61 samples/sec  Loss 1.4481  LearningRate 0.1518  ProxyLR: 7.5885  Epoch: 3  Global Step: 18320   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:36,045-Speed 3891.12 samples/sec  Loss 1.4205  LearningRate 0.1517  ProxyLR: 7.5873  Epoch: 3  Global Step: 18330   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:38,677-Speed 3891.39 samples/sec  Loss 1.4574  LearningRate 0.1517  ProxyLR: 7.5861  Epoch: 3  Global Step: 18340   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:41,310-Speed 3890.22 samples/sec  Loss 1.3989  LearningRate 0.1517  ProxyLR: 7.5849  Epoch: 3  Global Step: 18350   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:43,942-Speed 3892.58 samples/sec  Loss 1.4637  LearningRate 0.1517  ProxyLR: 7.5836  Epoch: 3  Global Step: 18360   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:46,574-Speed 3891.00 samples/sec  Loss 1.5753  LearningRate 0.1516  ProxyLR: 7.5824  Epoch: 3  Global Step: 18370   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:49,205-Speed 3893.34 samples/sec  Loss 1.4367  LearningRate 0.1516  ProxyLR: 7.5812  Epoch: 3  Global Step: 18380   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:51,836-Speed 3892.07 samples/sec  Loss 1.3731  LearningRate 0.1516  ProxyLR: 7.5800  Epoch: 3  Global Step: 18390   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:54,455-Speed 3911.60 samples/sec  Loss 1.4040  LearningRate 0.1516  ProxyLR: 7.5787  Epoch: 3  Global Step: 18400   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:57,087-Speed 3890.76 samples/sec  Loss 1.4027  LearningRate 0.1516  ProxyLR: 7.5775  Epoch: 3  Global Step: 18410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:38:59,719-Speed 3892.55 samples/sec  Loss 1.2841  LearningRate 0.1515  ProxyLR: 7.5763  Epoch: 3  Global Step: 18420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:02,354-Speed 3886.92 samples/sec  Loss 1.4097  LearningRate 0.1515  ProxyLR: 7.5751  Epoch: 3  Global Step: 18430   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:04,992-Speed 3881.76 samples/sec  Loss 1.3735  LearningRate 0.1515  ProxyLR: 7.5738  Epoch: 3  Global Step: 18440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:07,626-Speed 3888.35 samples/sec  Loss 1.4667  LearningRate 0.1515  ProxyLR: 7.5726  Epoch: 3  Global Step: 18450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:10,263-Speed 3884.53 samples/sec  Loss 1.4994  LearningRate 0.1514  ProxyLR: 7.5714  Epoch: 3  Global Step: 18460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:12,897-Speed 3888.42 samples/sec  Loss 1.4598  LearningRate 0.1514  ProxyLR: 7.5702  Epoch: 3  Global Step: 18470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:15,531-Speed 3887.83 samples/sec  Loss 1.5306  LearningRate 0.1514  ProxyLR: 7.5689  Epoch: 3  Global Step: 18480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:18,166-Speed 3888.17 samples/sec  Loss 1.4156  LearningRate 0.1514  ProxyLR: 7.5677  Epoch: 3  Global Step: 18490   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:20,786-Speed 3908.74 samples/sec  Loss 1.5347  LearningRate 0.1513  ProxyLR: 7.5665  Epoch: 3  Global Step: 18500   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:23,424-Speed 3883.06 samples/sec  Loss 1.4962  LearningRate 0.1513  ProxyLR: 7.5653  Epoch: 3  Global Step: 18510   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:26,057-Speed 3889.78 samples/sec  Loss 1.4920  LearningRate 0.1513  ProxyLR: 7.5640  Epoch: 3  Global Step: 18520   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:28,690-Speed 3889.41 samples/sec  Loss 1.4545  LearningRate 0.1513  ProxyLR: 7.5628  Epoch: 3  Global Step: 18530   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:31,327-Speed 3885.47 samples/sec  Loss 1.3880  LearningRate 0.1512  ProxyLR: 7.5616  Epoch: 3  Global Step: 18540   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:33,961-Speed 3888.63 samples/sec  Loss 1.5048  LearningRate 0.1512  ProxyLR: 7.5604  Epoch: 3  Global Step: 18550   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:36,593-Speed 3890.28 samples/sec  Loss 1.4131  LearningRate 0.1512  ProxyLR: 7.5591  Epoch: 3  Global Step: 18560   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:39,228-Speed 3887.48 samples/sec  Loss 1.4898  LearningRate 0.1512  ProxyLR: 7.5579  Epoch: 3  Global Step: 18570   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:41,863-Speed 3887.02 samples/sec  Loss 1.6418  LearningRate 0.1511  ProxyLR: 7.5567  Epoch: 3  Global Step: 18580   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:44,498-Speed 3887.50 samples/sec  Loss 1.3943  LearningRate 0.1511  ProxyLR: 7.5555  Epoch: 3  Global Step: 18590   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:39:47,120-Speed 3906.61 samples/sec  Loss 1.4663  LearningRate 0.1511  ProxyLR: 7.5543  Epoch: 3  Global Step: 18600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:39:49,754-Speed 3888.39 samples/sec  Loss 1.5132  LearningRate 0.1511  ProxyLR: 7.5530  Epoch: 3  Global Step: 18610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:39:52,388-Speed 3888.03 samples/sec  Loss 1.4933  LearningRate 0.1510  ProxyLR: 7.5518  Epoch: 3  Global Step: 18620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:39:55,024-Speed 3885.22 samples/sec  Loss 1.6627  LearningRate 0.1510  ProxyLR: 7.5506  Epoch: 3  Global Step: 18630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:39:57,657-Speed 3890.87 samples/sec  Loss 1.5385  LearningRate 0.1510  ProxyLR: 7.5494  Epoch: 3  Global Step: 18640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:00,292-Speed 3886.82 samples/sec  Loss 1.4795  LearningRate 0.1510  ProxyLR: 7.5481  Epoch: 3  Global Step: 18650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:02,926-Speed 3887.99 samples/sec  Loss 1.6017  LearningRate 0.1509  ProxyLR: 7.5469  Epoch: 3  Global Step: 18660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:05,561-Speed 3887.51 samples/sec  Loss 1.4779  LearningRate 0.1509  ProxyLR: 7.5457  Epoch: 3  Global Step: 18670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:08,195-Speed 3887.70 samples/sec  Loss 1.3783  LearningRate 0.1509  ProxyLR: 7.5445  Epoch: 3  Global Step: 18680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:10,829-Speed 3888.61 samples/sec  Loss 1.6133  LearningRate 0.1509  ProxyLR: 7.5433  Epoch: 3  Global Step: 18690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:13,451-Speed 3906.86 samples/sec  Loss 1.4011  LearningRate 0.1508  ProxyLR: 7.5420  Epoch: 3  Global Step: 18700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:16,086-Speed 3886.13 samples/sec  Loss 1.3802  LearningRate 0.1508  ProxyLR: 7.5408  Epoch: 3  Global Step: 18710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:18,720-Speed 3888.98 samples/sec  Loss 1.3684  LearningRate 0.1508  ProxyLR: 7.5396  Epoch: 3  Global Step: 18720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:21,353-Speed 3890.50 samples/sec  Loss 1.5164  LearningRate 0.1508  ProxyLR: 7.5384  Epoch: 3  Global Step: 18730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:23,989-Speed 3885.60 samples/sec  Loss 1.4683  LearningRate 0.1507  ProxyLR: 7.5371  Epoch: 3  Global Step: 18740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:26,624-Speed 3887.63 samples/sec  Loss 1.4270  LearningRate 0.1507  ProxyLR: 7.5359  Epoch: 3  Global Step: 18750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:29,259-Speed 3886.07 samples/sec  Loss 1.4724  LearningRate 0.1507  ProxyLR: 7.5347  Epoch: 3  Global Step: 18760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:31,894-Speed 3886.91 samples/sec  Loss 1.4636  LearningRate 0.1507  ProxyLR: 7.5335  Epoch: 3  Global Step: 18770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:34,529-Speed 3887.20 samples/sec  Loss 1.5236  LearningRate 0.1506  ProxyLR: 7.5323  Epoch: 3  Global Step: 18780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:37,165-Speed 3886.38 samples/sec  Loss 1.3929  LearningRate 0.1506  ProxyLR: 7.5310  Epoch: 3  Global Step: 18790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:40:39,796-Speed 3892.54 samples/sec  Loss 1.5111  LearningRate 0.1506  ProxyLR: 7.5298  Epoch: 3  Global Step: 18800   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:42,430-Speed 3888.58 samples/sec  Loss 1.5233  LearningRate 0.1506  ProxyLR: 7.5286  Epoch: 3  Global Step: 18810   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:45,066-Speed 3885.22 samples/sec  Loss 1.4695  LearningRate 0.1505  ProxyLR: 7.5274  Epoch: 3  Global Step: 18820   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:47,699-Speed 3890.77 samples/sec  Loss 1.4262  LearningRate 0.1505  ProxyLR: 7.5262  Epoch: 3  Global Step: 18830   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:50,334-Speed 3886.55 samples/sec  Loss 1.5337  LearningRate 0.1505  ProxyLR: 7.5249  Epoch: 3  Global Step: 18840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:52,964-Speed 3893.72 samples/sec  Loss 1.4558  LearningRate 0.1505  ProxyLR: 7.5237  Epoch: 3  Global Step: 18850   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:55,593-Speed 3896.21 samples/sec  Loss 1.5082  LearningRate 0.1504  ProxyLR: 7.5225  Epoch: 3  Global Step: 18860   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:40:58,222-Speed 3896.01 samples/sec  Loss 1.6762  LearningRate 0.1504  ProxyLR: 7.5213  Epoch: 3  Global Step: 18870   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:00,854-Speed 3892.11 samples/sec  Loss 1.5323  LearningRate 0.1504  ProxyLR: 7.5201  Epoch: 3  Global Step: 18880   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:03,486-Speed 3891.49 samples/sec  Loss 1.5010  LearningRate 0.1504  ProxyLR: 7.5188  Epoch: 3  Global Step: 18890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:06,104-Speed 3911.90 samples/sec  Loss 1.5126  LearningRate 0.1504  ProxyLR: 7.5176  Epoch: 3  Global Step: 18900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:08,734-Speed 3894.79 samples/sec  Loss 1.5281  LearningRate 0.1503  ProxyLR: 7.5164  Epoch: 3  Global Step: 18910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:11,365-Speed 3892.83 samples/sec  Loss 1.4426  LearningRate 0.1503  ProxyLR: 7.5152  Epoch: 3  Global Step: 18920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:13,997-Speed 3892.15 samples/sec  Loss 1.5718  LearningRate 0.1503  ProxyLR: 7.5140  Epoch: 3  Global Step: 18930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:16,625-Speed 3896.82 samples/sec  Loss 1.5768  LearningRate 0.1503  ProxyLR: 7.5127  Epoch: 3  Global Step: 18940   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:19,257-Speed 3891.62 samples/sec  Loss 1.4839  LearningRate 0.1502  ProxyLR: 7.5115  Epoch: 3  Global Step: 18950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:21,887-Speed 3894.68 samples/sec  Loss 1.5783  LearningRate 0.1502  ProxyLR: 7.5103  Epoch: 3  Global Step: 18960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:24,516-Speed 3895.36 samples/sec  Loss 1.4082  LearningRate 0.1502  ProxyLR: 7.5091  Epoch: 3  Global Step: 18970   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:27,143-Speed 3898.51 samples/sec  Loss 1.4131  LearningRate 0.1502  ProxyLR: 7.5079  Epoch: 3  Global Step: 18980   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:29,772-Speed 3896.36 samples/sec  Loss 1.5172  LearningRate 0.1501  ProxyLR: 7.5066  Epoch: 3  Global Step: 18990   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:32,403-Speed 3892.79 samples/sec  Loss 1.4654  LearningRate 0.1501  ProxyLR: 7.5054  Epoch: 3  Global Step: 19000   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 15:41:35,020-Speed 3914.15 samples/sec  Loss 1.6074  LearningRate 0.1501  ProxyLR: 7.5042  Epoch: 3  Global Step: 19010   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:37,650-Speed 3893.87 samples/sec  Loss 1.4891  LearningRate 0.1501  ProxyLR: 7.5030  Epoch: 3  Global Step: 19020   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:40,282-Speed 3891.43 samples/sec  Loss 1.5710  LearningRate 0.1500  ProxyLR: 7.5018  Epoch: 3  Global Step: 19030   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:42,916-Speed 3888.93 samples/sec  Loss 1.5892  LearningRate 0.1500  ProxyLR: 7.5005  Epoch: 3  Global Step: 19040   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:45,548-Speed 3891.84 samples/sec  Loss 1.4273  LearningRate 0.1500  ProxyLR: 7.4993  Epoch: 3  Global Step: 19050   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:48,178-Speed 3894.97 samples/sec  Loss 1.5656  LearningRate 0.1500  ProxyLR: 7.4981  Epoch: 3  Global Step: 19060   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:50,808-Speed 3893.90 samples/sec  Loss 1.5570  LearningRate 0.1499  ProxyLR: 7.4969  Epoch: 3  Global Step: 19070   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:53,439-Speed 3892.58 samples/sec  Loss 1.5971  LearningRate 0.1499  ProxyLR: 7.4957  Epoch: 3  Global Step: 19080   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:56,070-Speed 3892.99 samples/sec  Loss 1.4771  LearningRate 0.1499  ProxyLR: 7.4945  Epoch: 3  Global Step: 19090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:41:58,701-Speed 3894.34 samples/sec  Loss 1.4905  LearningRate 0.1499  ProxyLR: 7.4932  Epoch: 3  Global Step: 19100   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:01,318-Speed 3912.70 samples/sec  Loss 1.4035  LearningRate 0.1498  ProxyLR: 7.4920  Epoch: 3  Global Step: 19110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:03,950-Speed 3891.62 samples/sec  Loss 1.5837  LearningRate 0.1498  ProxyLR: 7.4908  Epoch: 3  Global Step: 19120   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:06,581-Speed 3892.96 samples/sec  Loss 1.5200  LearningRate 0.1498  ProxyLR: 7.4896  Epoch: 3  Global Step: 19130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:09,212-Speed 3893.42 samples/sec  Loss 1.6112  LearningRate 0.1498  ProxyLR: 7.4884  Epoch: 3  Global Step: 19140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:11,845-Speed 3890.48 samples/sec  Loss 1.5394  LearningRate 0.1497  ProxyLR: 7.4871  Epoch: 3  Global Step: 19150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:14,477-Speed 3891.56 samples/sec  Loss 1.6460  LearningRate 0.1497  ProxyLR: 7.4859  Epoch: 3  Global Step: 19160   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:17,110-Speed 3889.34 samples/sec  Loss 1.8197  LearningRate 0.1497  ProxyLR: 7.4847  Epoch: 3  Global Step: 19170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:19,727-Speed 3913.80 samples/sec  Loss 1.6757  LearningRate 0.1497  ProxyLR: 7.4835  Epoch: 3  Global Step: 19180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:22,359-Speed 3891.19 samples/sec  Loss 1.4712  LearningRate 0.1496  ProxyLR: 7.4823  Epoch: 3  Global Step: 19190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:24,991-Speed 3891.84 samples/sec  Loss 1.5372  LearningRate 0.1496  ProxyLR: 7.4811  Epoch: 3  Global Step: 19200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:27,622-Speed 3893.23 samples/sec  Loss 1.6641  LearningRate 0.1496  ProxyLR: 7.4798  Epoch: 3  Global Step: 19210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:30,253-Speed 3893.15 samples/sec  Loss 1.6141  LearningRate 0.1496  ProxyLR: 7.4786  Epoch: 3  Global Step: 19220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:32,883-Speed 3894.30 samples/sec  Loss 1.6571  LearningRate 0.1495  ProxyLR: 7.4774  Epoch: 3  Global Step: 19230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:35,513-Speed 3894.94 samples/sec  Loss 1.6383  LearningRate 0.1495  ProxyLR: 7.4762  Epoch: 3  Global Step: 19240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:38,143-Speed 3894.29 samples/sec  Loss 1.6592  LearningRate 0.1495  ProxyLR: 7.4750  Epoch: 3  Global Step: 19250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:40,772-Speed 3895.25 samples/sec  Loss 1.6428  LearningRate 0.1495  ProxyLR: 7.4738  Epoch: 3  Global Step: 19260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:43,403-Speed 3893.42 samples/sec  Loss 1.5840  LearningRate 0.1495  ProxyLR: 7.4725  Epoch: 3  Global Step: 19270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:42:46,030-Speed 3898.50 samples/sec  Loss 1.5240  LearningRate 0.1494  ProxyLR: 7.4713  Epoch: 3  Global Step: 19280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:48,656-Speed 3900.55 samples/sec  Loss 1.6559  LearningRate 0.1494  ProxyLR: 7.4701  Epoch: 3  Global Step: 19290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:51,283-Speed 3900.43 samples/sec  Loss 1.5674  LearningRate 0.1494  ProxyLR: 7.4689  Epoch: 3  Global Step: 19300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:53,910-Speed 3897.95 samples/sec  Loss 1.5064  LearningRate 0.1494  ProxyLR: 7.4677  Epoch: 3  Global Step: 19310   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:56,539-Speed 3896.23 samples/sec  Loss 1.5117  LearningRate 0.1493  ProxyLR: 7.4665  Epoch: 3  Global Step: 19320   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:42:59,165-Speed 3900.50 samples/sec  Loss 1.4852  LearningRate 0.1493  ProxyLR: 7.4653  Epoch: 3  Global Step: 19330   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:01,793-Speed 3898.26 samples/sec  Loss 1.6307  LearningRate 0.1493  ProxyLR: 7.4640  Epoch: 3  Global Step: 19340   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:04,420-Speed 3898.43 samples/sec  Loss 1.6726  LearningRate 0.1493  ProxyLR: 7.4628  Epoch: 3  Global Step: 19350   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:07,049-Speed 3896.64 samples/sec  Loss 1.5715  LearningRate 0.1492  ProxyLR: 7.4616  Epoch: 3  Global Step: 19360   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:09,676-Speed 3898.77 samples/sec  Loss 1.5194  LearningRate 0.1492  ProxyLR: 7.4604  Epoch: 3  Global Step: 19370   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:12,295-Speed 3911.11 samples/sec  Loss 1.5019  LearningRate 0.1492  ProxyLR: 7.4592  Epoch: 3  Global Step: 19380   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:14,921-Speed 3899.36 samples/sec  Loss 1.7900  LearningRate 0.1492  ProxyLR: 7.4580  Epoch: 3  Global Step: 19390   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:17,536-Speed 3917.92 samples/sec  Loss 1.6331  LearningRate 0.1491  ProxyLR: 7.4567  Epoch: 3  Global Step: 19400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:20,163-Speed 3898.73 samples/sec  Loss 1.6892  LearningRate 0.1491  ProxyLR: 7.4555  Epoch: 3  Global Step: 19410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:22,791-Speed 3897.92 samples/sec  Loss 1.7895  LearningRate 0.1491  ProxyLR: 7.4543  Epoch: 3  Global Step: 19420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:25,418-Speed 3898.42 samples/sec  Loss 1.6769  LearningRate 0.1491  ProxyLR: 7.4531  Epoch: 3  Global Step: 19430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:28,043-Speed 3901.90 samples/sec  Loss 1.6766  LearningRate 0.1490  ProxyLR: 7.4519  Epoch: 3  Global Step: 19440   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:30,670-Speed 3899.02 samples/sec  Loss 1.6057  LearningRate 0.1490  ProxyLR: 7.4507  Epoch: 3  Global Step: 19450   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:33,300-Speed 3894.12 samples/sec  Loss 1.6072  LearningRate 0.1490  ProxyLR: 7.4495  Epoch: 3  Global Step: 19460   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:35,927-Speed 3898.60 samples/sec  Loss 1.5839  LearningRate 0.1490  ProxyLR: 7.4482  Epoch: 3  Global Step: 19470   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:38,554-Speed 3900.20 samples/sec  Loss 1.5849  LearningRate 0.1489  ProxyLR: 7.4470  Epoch: 3  Global Step: 19480   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:41,182-Speed 3896.68 samples/sec  Loss 1.4851  LearningRate 0.1489  ProxyLR: 7.4458  Epoch: 3  Global Step: 19490   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:43:43,808-Speed 3900.59 samples/sec  Loss 1.6776  LearningRate 0.1489  ProxyLR: 7.4446  Epoch: 3  Global Step: 19500   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:46,440-Speed 3892.15 samples/sec  Loss 1.6591  LearningRate 0.1489  ProxyLR: 7.4434  Epoch: 3  Global Step: 19510   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:49,072-Speed 3891.12 samples/sec  Loss 1.6134  LearningRate 0.1488  ProxyLR: 7.4422  Epoch: 3  Global Step: 19520   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:51,705-Speed 3891.17 samples/sec  Loss 1.5447  LearningRate 0.1488  ProxyLR: 7.4410  Epoch: 3  Global Step: 19530   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:54,334-Speed 3894.64 samples/sec  Loss 1.5291  LearningRate 0.1488  ProxyLR: 7.4397  Epoch: 3  Global Step: 19540   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:56,967-Speed 3889.98 samples/sec  Loss 1.5008  LearningRate 0.1488  ProxyLR: 7.4385  Epoch: 3  Global Step: 19550   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:43:59,602-Speed 3887.88 samples/sec  Loss 1.6218  LearningRate 0.1487  ProxyLR: 7.4373  Epoch: 3  Global Step: 19560   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:02,235-Speed 3890.35 samples/sec  Loss 1.6337  LearningRate 0.1487  ProxyLR: 7.4361  Epoch: 3  Global Step: 19570   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:04,868-Speed 3889.32 samples/sec  Loss 1.7212  LearningRate 0.1487  ProxyLR: 7.4349  Epoch: 3  Global Step: 19580   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:07,500-Speed 3891.72 samples/sec  Loss 1.5522  LearningRate 0.1487  ProxyLR: 7.4337  Epoch: 3  Global Step: 19590   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:10,118-Speed 3913.23 samples/sec  Loss 1.5293  LearningRate 0.1486  ProxyLR: 7.4325  Epoch: 3  Global Step: 19600   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:12,750-Speed 3891.64 samples/sec  Loss 1.6543  LearningRate 0.1486  ProxyLR: 7.4313  Epoch: 3  Global Step: 19610   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:15,368-Speed 3911.22 samples/sec  Loss 1.5954  LearningRate 0.1486  ProxyLR: 7.4300  Epoch: 3  Global Step: 19620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:18,001-Speed 3890.14 samples/sec  Loss 1.6347  LearningRate 0.1486  ProxyLR: 7.4288  Epoch: 3  Global Step: 19630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:20,635-Speed 3889.70 samples/sec  Loss 1.6551  LearningRate 0.1486  ProxyLR: 7.4276  Epoch: 3  Global Step: 19640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:23,267-Speed 3891.06 samples/sec  Loss 1.6597  LearningRate 0.1485  ProxyLR: 7.4264  Epoch: 3  Global Step: 19650   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:25,898-Speed 3892.14 samples/sec  Loss 1.5565  LearningRate 0.1485  ProxyLR: 7.4252  Epoch: 3  Global Step: 19660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:28,532-Speed 3889.13 samples/sec  Loss 1.5269  LearningRate 0.1485  ProxyLR: 7.4240  Epoch: 3  Global Step: 19670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:31,164-Speed 3891.74 samples/sec  Loss 1.5257  LearningRate 0.1485  ProxyLR: 7.4228  Epoch: 3  Global Step: 19680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:33,794-Speed 3893.83 samples/sec  Loss 1.5696  LearningRate 0.1484  ProxyLR: 7.4216  Epoch: 3  Global Step: 19690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:36,427-Speed 3890.46 samples/sec  Loss 1.6574  LearningRate 0.1484  ProxyLR: 7.4203  Epoch: 3  Global Step: 19700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:39,059-Speed 3892.14 samples/sec  Loss 1.6943  LearningRate 0.1484  ProxyLR: 7.4191  Epoch: 3  Global Step: 19710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:41,690-Speed 3892.32 samples/sec  Loss 1.5679  LearningRate 0.1484  ProxyLR: 7.4179  Epoch: 3  Global Step: 19720   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:44,326-Speed 3886.08 samples/sec  Loss 1.7163  LearningRate 0.1483  ProxyLR: 7.4167  Epoch: 3  Global Step: 19730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:46,958-Speed 3891.46 samples/sec  Loss 1.7741  LearningRate 0.1483  ProxyLR: 7.4155  Epoch: 3  Global Step: 19740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:49,592-Speed 3889.06 samples/sec  Loss 1.5475  LearningRate 0.1483  ProxyLR: 7.4143  Epoch: 3  Global Step: 19750   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:52,226-Speed 3889.18 samples/sec  Loss 1.5753  LearningRate 0.1483  ProxyLR: 7.4131  Epoch: 3  Global Step: 19760   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:44:54,846-Speed 3909.35 samples/sec  Loss 1.6269  LearningRate 0.1482  ProxyLR: 7.4119  Epoch: 3  Global Step: 19770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:44:57,480-Speed 3887.15 samples/sec  Loss 1.7376  LearningRate 0.1482  ProxyLR: 7.4106  Epoch: 3  Global Step: 19780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:00,115-Speed 3888.39 samples/sec  Loss 1.6549  LearningRate 0.1482  ProxyLR: 7.4094  Epoch: 3  Global Step: 19790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:02,749-Speed 3888.18 samples/sec  Loss 1.6925  LearningRate 0.1482  ProxyLR: 7.4082  Epoch: 3  Global Step: 19800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:05,383-Speed 3889.17 samples/sec  Loss 1.7531  LearningRate 0.1481  ProxyLR: 7.4070  Epoch: 3  Global Step: 19810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:08,016-Speed 3888.63 samples/sec  Loss 1.6082  LearningRate 0.1481  ProxyLR: 7.4058  Epoch: 3  Global Step: 19820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:10,649-Speed 3890.75 samples/sec  Loss 1.6587  LearningRate 0.1481  ProxyLR: 7.4046  Epoch: 3  Global Step: 19830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:13,284-Speed 3887.31 samples/sec  Loss 1.6767  LearningRate 0.1481  ProxyLR: 7.4034  Epoch: 3  Global Step: 19840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:15,917-Speed 3890.46 samples/sec  Loss 1.6469  LearningRate 0.1480  ProxyLR: 7.4022  Epoch: 3  Global Step: 19850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:18,549-Speed 3891.33 samples/sec  Loss 1.7151  LearningRate 0.1480  ProxyLR: 7.4010  Epoch: 3  Global Step: 19860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:45:21,180-Speed 3893.35 samples/sec  Loss 1.6539  LearningRate 0.1480  ProxyLR: 7.3998  Epoch: 3  Global Step: 19870   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:23,810-Speed 3894.59 samples/sec  Loss 1.6331  LearningRate 0.1480  ProxyLR: 7.3985  Epoch: 3  Global Step: 19880   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:26,441-Speed 3892.33 samples/sec  Loss 1.6689  LearningRate 0.1479  ProxyLR: 7.3973  Epoch: 3  Global Step: 19890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:29,069-Speed 3897.43 samples/sec  Loss 1.6754  LearningRate 0.1479  ProxyLR: 7.3961  Epoch: 3  Global Step: 19900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:31,698-Speed 3896.35 samples/sec  Loss 1.6579  LearningRate 0.1479  ProxyLR: 7.3949  Epoch: 3  Global Step: 19910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:34,327-Speed 3896.39 samples/sec  Loss 1.6325  LearningRate 0.1479  ProxyLR: 7.3937  Epoch: 3  Global Step: 19920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:36,956-Speed 3896.57 samples/sec  Loss 1.5170  LearningRate 0.1478  ProxyLR: 7.3925  Epoch: 3  Global Step: 19930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:39,585-Speed 3895.98 samples/sec  Loss 1.4723  LearningRate 0.1478  ProxyLR: 7.3913  Epoch: 3  Global Step: 19940   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:45:42,215-Speed 3893.42 samples/sec  Loss 1.5872  LearningRate 0.1478  ProxyLR: 7.3901  Epoch: 3  Global Step: 19950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 15:45:44,844-Speed 3895.99 samples/sec  Loss 1.6720  LearningRate 0.1478  ProxyLR: 7.3889  Epoch: 3  Global Step: 19960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 15:45:47,462-Speed 3912.68 samples/sec  Loss 1.6546  LearningRate 0.1478  ProxyLR: 7.3877  Epoch: 3  Global Step: 19970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 15:45:50,092-Speed 3894.82 samples/sec  Loss 1.6690  LearningRate 0.1477  ProxyLR: 7.3864  Epoch: 3  Global Step: 19980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 15:45:52,721-Speed 3895.52 samples/sec  Loss 1.5810  LearningRate 0.1477  ProxyLR: 7.3852  Epoch: 3  Global Step: 19990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 15:45:55,338-Speed 3913.94 samples/sec  Loss 1.6469  LearningRate 0.1477  ProxyLR: 7.3840  Epoch: 3  Global Step: 20000   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 15:46:44,723-[lfw][20000]XNorm: 20.102915
Training: 2023-05-04 15:46:44,723-[lfw][20000]Accuracy-Flip: 0.99350+-0.00425
Training: 2023-05-04 15:46:44,723-[lfw][20000]Accuracy-Highest: 0.99350
Training: 2023-05-04 15:46:44,724-[lfw][20000]TPR@1stNon-Zero-FPR of 0.00033: 0.95833
Training: 2023-05-04 15:46:44,724-[lfw][20000]Highest TPR@FPR: 0.95833
Training: 2023-05-04 15:47:41,450-[cfp_fp][20000]XNorm: 19.388011
Training: 2023-05-04 15:47:41,450-[cfp_fp][20000]Accuracy-Flip: 0.91171+-0.01626
Training: 2023-05-04 15:47:41,451-[cfp_fp][20000]Accuracy-Highest: 0.91171
Training: 2023-05-04 15:47:41,451-[cfp_fp][20000]TPR@1stNon-Zero-FPR of 0.00029: 0.47257
Training: 2023-05-04 15:47:41,451-[cfp_fp][20000]Highest TPR@FPR: 0.47257
Training: 2023-05-04 15:48:30,736-[agedb_30][20000]XNorm: 19.976718
Training: 2023-05-04 15:48:30,737-[agedb_30][20000]Accuracy-Flip: 0.91367+-0.01320
Training: 2023-05-04 15:48:30,737-[agedb_30][20000]Accuracy-Highest: 0.91367
Training: 2023-05-04 15:48:30,737-[agedb_30][20000]TPR@1stNon-Zero-FPR of 0.00033: 0.25867
Training: 2023-05-04 15:48:30,737-[agedb_30][20000]Highest TPR@FPR: 0.25867
Training: 2023-05-04 15:49:21,364-[calfw][20000]XNorm: 20.330048
Training: 2023-05-04 15:49:21,364-[calfw][20000]Accuracy-Flip: 0.93967+-0.01103
Training: 2023-05-04 15:49:21,364-[calfw][20000]Accuracy-Highest: 0.93967
Training: 2023-05-04 15:49:21,364-[calfw][20000]TPR@1stNon-Zero-FPR of 0.00033: 0.67500
Training: 2023-05-04 15:49:21,364-[calfw][20000]Highest TPR@FPR: 0.67500
Training: 2023-05-04 15:50:12,010-[cplfw][20000]XNorm: 18.997268
Training: 2023-05-04 15:50:12,011-[cplfw][20000]Accuracy-Flip: 0.86283+-0.02338
Training: 2023-05-04 15:50:12,011-[cplfw][20000]Accuracy-Highest: 0.86283
Training: 2023-05-04 15:50:12,011-[cplfw][20000]TPR@1stNon-Zero-FPR of 0.00033: 0.00100
Training: 2023-05-04 15:50:12,011-[cplfw][20000]Highest TPR@FPR: 0.00167
Training: 2023-05-04 15:50:14,658-Speed 39.49 samples/sec  Loss 1.6392  LearningRate 0.1477  ProxyLR: 7.3828  Epoch: 3  Global Step: 20010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:17,279-Speed 3908.91 samples/sec  Loss 1.5906  LearningRate 0.1476  ProxyLR: 7.3816  Epoch: 3  Global Step: 20020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:19,901-Speed 3906.50 samples/sec  Loss 1.5887  LearningRate 0.1476  ProxyLR: 7.3804  Epoch: 3  Global Step: 20030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:22,523-Speed 3906.01 samples/sec  Loss 1.6342  LearningRate 0.1476  ProxyLR: 7.3792  Epoch: 3  Global Step: 20040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:25,148-Speed 3902.26 samples/sec  Loss 1.6785  LearningRate 0.1476  ProxyLR: 7.3780  Epoch: 3  Global Step: 20050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:27,772-Speed 3903.52 samples/sec  Loss 1.6746  LearningRate 0.1475  ProxyLR: 7.3768  Epoch: 3  Global Step: 20060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:30,396-Speed 3902.88 samples/sec  Loss 1.6569  LearningRate 0.1475  ProxyLR: 7.3756  Epoch: 3  Global Step: 20070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:50:33,008-Speed 3921.27 samples/sec  Loss 1.6442  LearningRate 0.1475  ProxyLR: 7.3744  Epoch: 3  Global Step: 20080   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:35,633-Speed 3902.20 samples/sec  Loss 1.6881  LearningRate 0.1475  ProxyLR: 7.3731  Epoch: 3  Global Step: 20090   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:38,259-Speed 3900.97 samples/sec  Loss 1.6577  LearningRate 0.1474  ProxyLR: 7.3719  Epoch: 3  Global Step: 20100   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:40,884-Speed 3901.60 samples/sec  Loss 1.7579  LearningRate 0.1474  ProxyLR: 7.3707  Epoch: 3  Global Step: 20110   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:43,512-Speed 3897.02 samples/sec  Loss 1.5590  LearningRate 0.1474  ProxyLR: 7.3695  Epoch: 3  Global Step: 20120   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:46,140-Speed 3898.24 samples/sec  Loss 1.7188  LearningRate 0.1474  ProxyLR: 7.3683  Epoch: 3  Global Step: 20130   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:48,768-Speed 3896.99 samples/sec  Loss 1.6967  LearningRate 0.1473  ProxyLR: 7.3671  Epoch: 3  Global Step: 20140   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:51,398-Speed 3894.54 samples/sec  Loss 1.6245  LearningRate 0.1473  ProxyLR: 7.3659  Epoch: 3  Global Step: 20150   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:54,027-Speed 3895.73 samples/sec  Loss 1.6350  LearningRate 0.1473  ProxyLR: 7.3647  Epoch: 3  Global Step: 20160   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:56,657-Speed 3895.36 samples/sec  Loss 1.7471  LearningRate 0.1473  ProxyLR: 7.3635  Epoch: 3  Global Step: 20170   Fp16 Grad Scale: 131072  Required: 10 hours
Training: 2023-05-04 15:50:59,286-Speed 3895.67 samples/sec  Loss 1.6674  LearningRate 0.1472  ProxyLR: 7.3623  Epoch: 3  Global Step: 20180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:01,917-Speed 3894.07 samples/sec  Loss 1.7512  LearningRate 0.1472  ProxyLR: 7.3611  Epoch: 3  Global Step: 20190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:04,546-Speed 3894.84 samples/sec  Loss 1.7770  LearningRate 0.1472  ProxyLR: 7.3599  Epoch: 3  Global Step: 20200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:07,175-Speed 3896.14 samples/sec  Loss 1.6850  LearningRate 0.1472  ProxyLR: 7.3587  Epoch: 3  Global Step: 20210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:09,804-Speed 3897.18 samples/sec  Loss 1.6218  LearningRate 0.1471  ProxyLR: 7.3575  Epoch: 3  Global Step: 20220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:12,432-Speed 3896.58 samples/sec  Loss 1.7130  LearningRate 0.1471  ProxyLR: 7.3562  Epoch: 3  Global Step: 20230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:15,063-Speed 3893.39 samples/sec  Loss 1.7586  LearningRate 0.1471  ProxyLR: 7.3550  Epoch: 3  Global Step: 20240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:17,691-Speed 3898.02 samples/sec  Loss 1.7642  LearningRate 0.1471  ProxyLR: 7.3538  Epoch: 3  Global Step: 20250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:20,319-Speed 3897.47 samples/sec  Loss 1.5720  LearningRate 0.1471  ProxyLR: 7.3526  Epoch: 3  Global Step: 20260   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:22,947-Speed 3896.93 samples/sec  Loss 1.4905  LearningRate 0.1470  ProxyLR: 7.3514  Epoch: 3  Global Step: 20270   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:25,575-Speed 3897.98 samples/sec  Loss 1.5984  LearningRate 0.1470  ProxyLR: 7.3502  Epoch: 3  Global Step: 20280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:51:28,203-Speed 3896.64 samples/sec  Loss 1.6770  LearningRate 0.1470  ProxyLR: 7.3490  Epoch: 3  Global Step: 20290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:51:30,832-Speed 3897.29 samples/sec  Loss 1.7086  LearningRate 0.1470  ProxyLR: 7.3478  Epoch: 3  Global Step: 20300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:51:33,448-Speed 3915.09 samples/sec  Loss 1.6776  LearningRate 0.1469  ProxyLR: 7.3466  Epoch: 3  Global Step: 20310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:36,077-Speed 3895.81 samples/sec  Loss 1.6277  LearningRate 0.1469  ProxyLR: 7.3454  Epoch: 3  Global Step: 20320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:38,706-Speed 3896.34 samples/sec  Loss 1.6655  LearningRate 0.1469  ProxyLR: 7.3442  Epoch: 3  Global Step: 20330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:41,335-Speed 3895.13 samples/sec  Loss 1.6937  LearningRate 0.1469  ProxyLR: 7.3430  Epoch: 3  Global Step: 20340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:43,964-Speed 3896.56 samples/sec  Loss 1.7829  LearningRate 0.1468  ProxyLR: 7.3418  Epoch: 3  Global Step: 20350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:46,591-Speed 3898.76 samples/sec  Loss 1.7394  LearningRate 0.1468  ProxyLR: 7.3406  Epoch: 3  Global Step: 20360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:49,220-Speed 3895.67 samples/sec  Loss 1.6311  LearningRate 0.1468  ProxyLR: 7.3394  Epoch: 3  Global Step: 20370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:51,850-Speed 3895.13 samples/sec  Loss 1.6270  LearningRate 0.1468  ProxyLR: 7.3382  Epoch: 3  Global Step: 20380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:54,478-Speed 3897.36 samples/sec  Loss 1.7041  LearningRate 0.1467  ProxyLR: 7.3369  Epoch: 3  Global Step: 20390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:57,105-Speed 3899.56 samples/sec  Loss 1.6809  LearningRate 0.1467  ProxyLR: 7.3357  Epoch: 3  Global Step: 20400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:51:59,734-Speed 3895.46 samples/sec  Loss 1.7569  LearningRate 0.1467  ProxyLR: 7.3345  Epoch: 3  Global Step: 20410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:02,363-Speed 3896.81 samples/sec  Loss 1.5937  LearningRate 0.1467  ProxyLR: 7.3333  Epoch: 3  Global Step: 20420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:04,995-Speed 3891.10 samples/sec  Loss 1.8007  LearningRate 0.1466  ProxyLR: 7.3321  Epoch: 3  Global Step: 20430   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:07,624-Speed 3895.64 samples/sec  Loss 1.6354  LearningRate 0.1466  ProxyLR: 7.3309  Epoch: 3  Global Step: 20440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:10,251-Speed 3898.81 samples/sec  Loss 1.8334  LearningRate 0.1466  ProxyLR: 7.3297  Epoch: 3  Global Step: 20450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:12,880-Speed 3896.35 samples/sec  Loss 1.5312  LearningRate 0.1466  ProxyLR: 7.3285  Epoch: 3  Global Step: 20460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:15,508-Speed 3897.77 samples/sec  Loss 1.6693  LearningRate 0.1465  ProxyLR: 7.3273  Epoch: 3  Global Step: 20470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:18,135-Speed 3898.23 samples/sec  Loss 1.7679  LearningRate 0.1465  ProxyLR: 7.3261  Epoch: 3  Global Step: 20480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:20,766-Speed 3893.95 samples/sec  Loss 1.8033  LearningRate 0.1465  ProxyLR: 7.3249  Epoch: 3  Global Step: 20490   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:23,395-Speed 3895.26 samples/sec  Loss 1.5942  LearningRate 0.1465  ProxyLR: 7.3237  Epoch: 3  Global Step: 20500   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:26,010-Speed 3916.62 samples/sec  Loss 1.6512  LearningRate 0.1464  ProxyLR: 7.3225  Epoch: 3  Global Step: 20510   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:28,644-Speed 3889.40 samples/sec  Loss 1.7119  LearningRate 0.1464  ProxyLR: 7.3213  Epoch: 3  Global Step: 20520   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:31,276-Speed 3890.92 samples/sec  Loss 1.6520  LearningRate 0.1464  ProxyLR: 7.3201  Epoch: 3  Global Step: 20530   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:33,911-Speed 3887.65 samples/sec  Loss 1.6631  LearningRate 0.1464  ProxyLR: 7.3189  Epoch: 3  Global Step: 20540   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:52:36,537-Speed 3900.36 samples/sec  Loss 1.7632  LearningRate 0.1464  ProxyLR: 7.3177  Epoch: 3  Global Step: 20550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:39,174-Speed 3885.53 samples/sec  Loss 1.7538  LearningRate 0.1463  ProxyLR: 7.3165  Epoch: 3  Global Step: 20560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:41,808-Speed 3888.11 samples/sec  Loss 1.5837  LearningRate 0.1463  ProxyLR: 7.3153  Epoch: 3  Global Step: 20570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:44,443-Speed 3886.91 samples/sec  Loss 1.7357  LearningRate 0.1463  ProxyLR: 7.3141  Epoch: 3  Global Step: 20580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:47,078-Speed 3886.38 samples/sec  Loss 1.6337  LearningRate 0.1463  ProxyLR: 7.3129  Epoch: 3  Global Step: 20590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:49,712-Speed 3889.11 samples/sec  Loss 1.6108  LearningRate 0.1462  ProxyLR: 7.3117  Epoch: 3  Global Step: 20600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:52,343-Speed 3893.84 samples/sec  Loss 1.6686  LearningRate 0.1462  ProxyLR: 7.3105  Epoch: 3  Global Step: 20610   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:54,968-Speed 3900.67 samples/sec  Loss 1.7643  LearningRate 0.1462  ProxyLR: 7.3093  Epoch: 3  Global Step: 20620   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:52:57,598-Speed 3895.45 samples/sec  Loss 1.6678  LearningRate 0.1462  ProxyLR: 7.3081  Epoch: 3  Global Step: 20630   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:00,227-Speed 3895.90 samples/sec  Loss 1.6681  LearningRate 0.1461  ProxyLR: 7.3069  Epoch: 3  Global Step: 20640   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:02,856-Speed 3896.24 samples/sec  Loss 1.7843  LearningRate 0.1461  ProxyLR: 7.3056  Epoch: 3  Global Step: 20650   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:05,486-Speed 3894.31 samples/sec  Loss 1.8363  LearningRate 0.1461  ProxyLR: 7.3044  Epoch: 3  Global Step: 20660   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:08,118-Speed 3892.11 samples/sec  Loss 1.7455  LearningRate 0.1461  ProxyLR: 7.3032  Epoch: 3  Global Step: 20670   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:10,746-Speed 3897.12 samples/sec  Loss 1.8906  LearningRate 0.1460  ProxyLR: 7.3020  Epoch: 3  Global Step: 20680   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:13,379-Speed 3890.30 samples/sec  Loss 1.8289  LearningRate 0.1460  ProxyLR: 7.3008  Epoch: 3  Global Step: 20690   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:16,011-Speed 3891.69 samples/sec  Loss 1.6505  LearningRate 0.1460  ProxyLR: 7.2996  Epoch: 3  Global Step: 20700   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:18,642-Speed 3891.77 samples/sec  Loss 1.7492  LearningRate 0.1460  ProxyLR: 7.2984  Epoch: 3  Global Step: 20710   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:21,276-Speed 3889.18 samples/sec  Loss 1.7566  LearningRate 0.1459  ProxyLR: 7.2972  Epoch: 3  Global Step: 20720   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:23,907-Speed 3892.69 samples/sec  Loss 1.6773  LearningRate 0.1459  ProxyLR: 7.2960  Epoch: 3  Global Step: 20730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:26,526-Speed 3911.49 samples/sec  Loss 1.7233  LearningRate 0.1459  ProxyLR: 7.2948  Epoch: 3  Global Step: 20740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:29,160-Speed 3889.01 samples/sec  Loss 1.7202  LearningRate 0.1459  ProxyLR: 7.2936  Epoch: 3  Global Step: 20750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:31,794-Speed 3887.42 samples/sec  Loss 1.7772  LearningRate 0.1458  ProxyLR: 7.2924  Epoch: 3  Global Step: 20760   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:34,427-Speed 3890.76 samples/sec  Loss 1.7217  LearningRate 0.1458  ProxyLR: 7.2912  Epoch: 3  Global Step: 20770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:37,059-Speed 3891.76 samples/sec  Loss 1.7903  LearningRate 0.1458  ProxyLR: 7.2900  Epoch: 3  Global Step: 20780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:39,687-Speed 3897.58 samples/sec  Loss 1.6301  LearningRate 0.1458  ProxyLR: 7.2888  Epoch: 3  Global Step: 20790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:42,324-Speed 3884.38 samples/sec  Loss 1.6153  LearningRate 0.1458  ProxyLR: 7.2876  Epoch: 3  Global Step: 20800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:44,959-Speed 3886.86 samples/sec  Loss 1.7810  LearningRate 0.1457  ProxyLR: 7.2864  Epoch: 3  Global Step: 20810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:47,593-Speed 3887.78 samples/sec  Loss 1.7853  LearningRate 0.1457  ProxyLR: 7.2852  Epoch: 3  Global Step: 20820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:50,223-Speed 3894.57 samples/sec  Loss 1.7452  LearningRate 0.1457  ProxyLR: 7.2840  Epoch: 3  Global Step: 20830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:53:52,856-Speed 3891.37 samples/sec  Loss 1.8377  LearningRate 0.1457  ProxyLR: 7.2828  Epoch: 3  Global Step: 20840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:55,487-Speed 3892.07 samples/sec  Loss 1.6658  LearningRate 0.1456  ProxyLR: 7.2816  Epoch: 3  Global Step: 20850   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:53:58,119-Speed 3891.99 samples/sec  Loss 1.7431  LearningRate 0.1456  ProxyLR: 7.2804  Epoch: 3  Global Step: 20860   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:00,751-Speed 3890.94 samples/sec  Loss 1.7773  LearningRate 0.1456  ProxyLR: 7.2792  Epoch: 3  Global Step: 20870   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:03,379-Speed 3897.84 samples/sec  Loss 1.6898  LearningRate 0.1456  ProxyLR: 7.2780  Epoch: 3  Global Step: 20880   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:06,009-Speed 3894.28 samples/sec  Loss 1.7727  LearningRate 0.1455  ProxyLR: 7.2768  Epoch: 3  Global Step: 20890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:08,639-Speed 3895.58 samples/sec  Loss 1.7662  LearningRate 0.1455  ProxyLR: 7.2756  Epoch: 3  Global Step: 20900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:11,266-Speed 3898.12 samples/sec  Loss 1.7026  LearningRate 0.1455  ProxyLR: 7.2744  Epoch: 3  Global Step: 20910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:13,896-Speed 3894.96 samples/sec  Loss 1.7587  LearningRate 0.1455  ProxyLR: 7.2732  Epoch: 3  Global Step: 20920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:16,522-Speed 3899.90 samples/sec  Loss 1.7539  LearningRate 0.1454  ProxyLR: 7.2720  Epoch: 3  Global Step: 20930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:19,138-Speed 3916.08 samples/sec  Loss 1.6651  LearningRate 0.1454  ProxyLR: 7.2708  Epoch: 3  Global Step: 20940   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:21,767-Speed 3895.26 samples/sec  Loss 1.6955  LearningRate 0.1454  ProxyLR: 7.2696  Epoch: 3  Global Step: 20950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:24,399-Speed 3891.39 samples/sec  Loss 1.7486  LearningRate 0.1454  ProxyLR: 7.2684  Epoch: 3  Global Step: 20960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:27,030-Speed 3893.42 samples/sec  Loss 1.7053  LearningRate 0.1453  ProxyLR: 7.2672  Epoch: 3  Global Step: 20970   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:29,661-Speed 3893.81 samples/sec  Loss 1.6905  LearningRate 0.1453  ProxyLR: 7.2660  Epoch: 3  Global Step: 20980   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:32,290-Speed 3895.33 samples/sec  Loss 1.6573  LearningRate 0.1453  ProxyLR: 7.2648  Epoch: 3  Global Step: 20990   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:34,920-Speed 3894.40 samples/sec  Loss 1.7417  LearningRate 0.1453  ProxyLR: 7.2636  Epoch: 3  Global Step: 21000   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:54:37,539-Speed 3911.97 samples/sec  Loss 1.5750  LearningRate 0.1452  ProxyLR: 7.2624  Epoch: 3  Global Step: 21010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:40,169-Speed 3894.40 samples/sec  Loss 1.6385  LearningRate 0.1452  ProxyLR: 7.2612  Epoch: 3  Global Step: 21020   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:42,799-Speed 3893.60 samples/sec  Loss 1.6888  LearningRate 0.1452  ProxyLR: 7.2600  Epoch: 3  Global Step: 21030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:45,430-Speed 3893.47 samples/sec  Loss 1.5393  LearningRate 0.1452  ProxyLR: 7.2588  Epoch: 3  Global Step: 21040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:48,060-Speed 3894.18 samples/sec  Loss 1.6644  LearningRate 0.1452  ProxyLR: 7.2576  Epoch: 3  Global Step: 21050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:50,691-Speed 3892.95 samples/sec  Loss 1.7022  LearningRate 0.1451  ProxyLR: 7.2564  Epoch: 3  Global Step: 21060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:53,320-Speed 3895.68 samples/sec  Loss 1.7781  LearningRate 0.1451  ProxyLR: 7.2552  Epoch: 3  Global Step: 21070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:55,951-Speed 3893.44 samples/sec  Loss 1.7530  LearningRate 0.1451  ProxyLR: 7.2540  Epoch: 3  Global Step: 21080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:54:58,583-Speed 3891.98 samples/sec  Loss 1.7072  LearningRate 0.1451  ProxyLR: 7.2528  Epoch: 3  Global Step: 21090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:01,214-Speed 3892.09 samples/sec  Loss 1.8002  LearningRate 0.1450  ProxyLR: 7.2516  Epoch: 3  Global Step: 21100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:03,849-Speed 3887.22 samples/sec  Loss 1.7325  LearningRate 0.1450  ProxyLR: 7.2504  Epoch: 3  Global Step: 21110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:06,485-Speed 3886.34 samples/sec  Loss 1.6297  LearningRate 0.1450  ProxyLR: 7.2492  Epoch: 3  Global Step: 21120   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:09,123-Speed 3882.54 samples/sec  Loss 1.7633  LearningRate 0.1450  ProxyLR: 7.2480  Epoch: 3  Global Step: 21130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:11,759-Speed 3885.40 samples/sec  Loss 1.8129  LearningRate 0.1449  ProxyLR: 7.2468  Epoch: 3  Global Step: 21140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:14,400-Speed 3878.63 samples/sec  Loss 1.8242  LearningRate 0.1449  ProxyLR: 7.2456  Epoch: 3  Global Step: 21150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:17,020-Speed 3910.29 samples/sec  Loss 1.8348  LearningRate 0.1449  ProxyLR: 7.2444  Epoch: 3  Global Step: 21160   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:19,652-Speed 3891.24 samples/sec  Loss 1.7760  LearningRate 0.1449  ProxyLR: 7.2432  Epoch: 3  Global Step: 21170   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:22,283-Speed 3893.05 samples/sec  Loss 1.7398  LearningRate 0.1448  ProxyLR: 7.2421  Epoch: 3  Global Step: 21180   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:24,915-Speed 3891.56 samples/sec  Loss 1.6582  LearningRate 0.1448  ProxyLR: 7.2409  Epoch: 3  Global Step: 21190   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:27,545-Speed 3893.60 samples/sec  Loss 1.7541  LearningRate 0.1448  ProxyLR: 7.2397  Epoch: 3  Global Step: 21200   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:30,175-Speed 3895.03 samples/sec  Loss 1.7934  LearningRate 0.1448  ProxyLR: 7.2385  Epoch: 3  Global Step: 21210   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:32,805-Speed 3894.70 samples/sec  Loss 1.6925  LearningRate 0.1447  ProxyLR: 7.2373  Epoch: 3  Global Step: 21220   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:35,432-Speed 3898.70 samples/sec  Loss 1.6934  LearningRate 0.1447  ProxyLR: 7.2361  Epoch: 3  Global Step: 21230   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:38,060-Speed 3897.70 samples/sec  Loss 1.7285  LearningRate 0.1447  ProxyLR: 7.2349  Epoch: 3  Global Step: 21240   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:40,688-Speed 3897.80 samples/sec  Loss 1.6948  LearningRate 0.1447  ProxyLR: 7.2337  Epoch: 3  Global Step: 21250   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:55:43,314-Speed 3899.16 samples/sec  Loss 1.6185  LearningRate 0.1446  ProxyLR: 7.2325  Epoch: 3  Global Step: 21260   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:45,943-Speed 3896.60 samples/sec  Loss 1.6546  LearningRate 0.1446  ProxyLR: 7.2313  Epoch: 3  Global Step: 21270   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:48,573-Speed 3894.94 samples/sec  Loss 1.6956  LearningRate 0.1446  ProxyLR: 7.2301  Epoch: 3  Global Step: 21280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:51,201-Speed 3896.89 samples/sec  Loss 1.7455  LearningRate 0.1446  ProxyLR: 7.2289  Epoch: 3  Global Step: 21290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:53,829-Speed 3897.74 samples/sec  Loss 1.8618  LearningRate 0.1446  ProxyLR: 7.2277  Epoch: 3  Global Step: 21300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:56,456-Speed 3898.72 samples/sec  Loss 1.6824  LearningRate 0.1445  ProxyLR: 7.2265  Epoch: 3  Global Step: 21310   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:55:59,084-Speed 3897.02 samples/sec  Loss 1.7651  LearningRate 0.1445  ProxyLR: 7.2253  Epoch: 3  Global Step: 21320   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:01,711-Speed 3898.66 samples/sec  Loss 1.6878  LearningRate 0.1445  ProxyLR: 7.2241  Epoch: 3  Global Step: 21330   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:04,325-Speed 3918.86 samples/sec  Loss 1.6887  LearningRate 0.1445  ProxyLR: 7.2229  Epoch: 3  Global Step: 21340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:06,954-Speed 3896.65 samples/sec  Loss 1.6882  LearningRate 0.1444  ProxyLR: 7.2217  Epoch: 3  Global Step: 21350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:09,580-Speed 3900.65 samples/sec  Loss 1.6882  LearningRate 0.1444  ProxyLR: 7.2205  Epoch: 3  Global Step: 21360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:12,208-Speed 3897.75 samples/sec  Loss 1.5926  LearningRate 0.1444  ProxyLR: 7.2193  Epoch: 3  Global Step: 21370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:14,837-Speed 3895.98 samples/sec  Loss 1.8244  LearningRate 0.1444  ProxyLR: 7.2181  Epoch: 3  Global Step: 21380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:17,464-Speed 3898.06 samples/sec  Loss 1.7651  LearningRate 0.1443  ProxyLR: 7.2169  Epoch: 3  Global Step: 21390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:20,092-Speed 3897.25 samples/sec  Loss 1.9493  LearningRate 0.1443  ProxyLR: 7.2157  Epoch: 3  Global Step: 21400   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:22,719-Speed 3899.85 samples/sec  Loss 1.8089  LearningRate 0.1443  ProxyLR: 7.2145  Epoch: 3  Global Step: 21410   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:25,346-Speed 3898.64 samples/sec  Loss 1.7840  LearningRate 0.1443  ProxyLR: 7.2133  Epoch: 3  Global Step: 21420   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:27,973-Speed 3899.33 samples/sec  Loss 1.7196  LearningRate 0.1442  ProxyLR: 7.2121  Epoch: 3  Global Step: 21430   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:30,599-Speed 3899.84 samples/sec  Loss 1.7317  LearningRate 0.1442  ProxyLR: 7.2110  Epoch: 3  Global Step: 21440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:33,225-Speed 3900.92 samples/sec  Loss 1.8142  LearningRate 0.1442  ProxyLR: 7.2098  Epoch: 3  Global Step: 21450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:35,854-Speed 3895.31 samples/sec  Loss 1.9089  LearningRate 0.1442  ProxyLR: 7.2086  Epoch: 3  Global Step: 21460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:38,482-Speed 3898.23 samples/sec  Loss 1.7252  LearningRate 0.1441  ProxyLR: 7.2074  Epoch: 3  Global Step: 21470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:41,107-Speed 3901.77 samples/sec  Loss 1.7263  LearningRate 0.1441  ProxyLR: 7.2062  Epoch: 3  Global Step: 21480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:43,733-Speed 3899.28 samples/sec  Loss 1.7225  LearningRate 0.1441  ProxyLR: 7.2050  Epoch: 3  Global Step: 21490   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:56:46,346-Speed 3920.18 samples/sec  Loss 1.7462  LearningRate 0.1441  ProxyLR: 7.2038  Epoch: 3  Global Step: 21500   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:48,972-Speed 3901.42 samples/sec  Loss 1.7237  LearningRate 0.1441  ProxyLR: 7.2026  Epoch: 3  Global Step: 21510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:51,597-Speed 3900.68 samples/sec  Loss 1.6951  LearningRate 0.1440  ProxyLR: 7.2014  Epoch: 3  Global Step: 21520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:54,222-Speed 3902.65 samples/sec  Loss 1.7159  LearningRate 0.1440  ProxyLR: 7.2002  Epoch: 3  Global Step: 21530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:56,846-Speed 3904.06 samples/sec  Loss 1.6898  LearningRate 0.1440  ProxyLR: 7.1990  Epoch: 3  Global Step: 21540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:56:59,469-Speed 3905.14 samples/sec  Loss 1.8129  LearningRate 0.1440  ProxyLR: 7.1978  Epoch: 3  Global Step: 21550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:57:02,092-Speed 3903.88 samples/sec  Loss 1.8261  LearningRate 0.1439  ProxyLR: 7.1966  Epoch: 3  Global Step: 21560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:57:04,716-Speed 3903.11 samples/sec  Loss 1.6484  LearningRate 0.1439  ProxyLR: 7.1954  Epoch: 3  Global Step: 21570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:57:07,339-Speed 3905.80 samples/sec  Loss 1.7544  LearningRate 0.1439  ProxyLR: 7.1942  Epoch: 3  Global Step: 21580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:57:09,963-Speed 3903.75 samples/sec  Loss 1.6817  LearningRate 0.1439  ProxyLR: 7.1930  Epoch: 3  Global Step: 21590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:57:12,584-Speed 3906.79 samples/sec  Loss 1.7356  LearningRate 0.1438  ProxyLR: 7.1919  Epoch: 3  Global Step: 21600   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:15,207-Speed 3904.74 samples/sec  Loss 1.7965  LearningRate 0.1438  ProxyLR: 7.1907  Epoch: 3  Global Step: 21610   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:17,830-Speed 3905.75 samples/sec  Loss 1.7447  LearningRate 0.1438  ProxyLR: 7.1895  Epoch: 3  Global Step: 21620   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:20,454-Speed 3902.85 samples/sec  Loss 1.7641  LearningRate 0.1438  ProxyLR: 7.1883  Epoch: 3  Global Step: 21630   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:23,077-Speed 3905.80 samples/sec  Loss 1.6522  LearningRate 0.1437  ProxyLR: 7.1871  Epoch: 3  Global Step: 21640   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:25,700-Speed 3903.83 samples/sec  Loss 1.8111  LearningRate 0.1437  ProxyLR: 7.1859  Epoch: 3  Global Step: 21650   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:28,329-Speed 3896.18 samples/sec  Loss 1.6956  LearningRate 0.1437  ProxyLR: 7.1847  Epoch: 3  Global Step: 21660   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:30,959-Speed 3894.10 samples/sec  Loss 1.6249  LearningRate 0.1437  ProxyLR: 7.1835  Epoch: 3  Global Step: 21670   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:33,590-Speed 3893.15 samples/sec  Loss 1.7736  LearningRate 0.1436  ProxyLR: 7.1823  Epoch: 3  Global Step: 21680   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:36,220-Speed 3895.71 samples/sec  Loss 1.9944  LearningRate 0.1436  ProxyLR: 7.1811  Epoch: 3  Global Step: 21690   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:38,851-Speed 3892.25 samples/sec  Loss 1.6646  LearningRate 0.1436  ProxyLR: 7.1799  Epoch: 3  Global Step: 21700   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 15:57:41,469-Speed 3912.93 samples/sec  Loss 1.7089  LearningRate 0.1436  ProxyLR: 7.1787  Epoch: 3  Global Step: 21710   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:44,099-Speed 3893.93 samples/sec  Loss 1.6375  LearningRate 0.1436  ProxyLR: 7.1775  Epoch: 3  Global Step: 21720   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:46,730-Speed 3894.09 samples/sec  Loss 1.6158  LearningRate 0.1435  ProxyLR: 7.1763  Epoch: 3  Global Step: 21730   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:49,359-Speed 3894.87 samples/sec  Loss 1.6828  LearningRate 0.1435  ProxyLR: 7.1752  Epoch: 3  Global Step: 21740   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:51,989-Speed 3895.26 samples/sec  Loss 1.5552  LearningRate 0.1435  ProxyLR: 7.1740  Epoch: 3  Global Step: 21750   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:54,619-Speed 3893.73 samples/sec  Loss 1.6929  LearningRate 0.1435  ProxyLR: 7.1728  Epoch: 3  Global Step: 21760   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:57:57,235-Speed 3915.50 samples/sec  Loss 1.8825  LearningRate 0.1434  ProxyLR: 7.1716  Epoch: 3  Global Step: 21770   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:57:59,865-Speed 3895.31 samples/sec  Loss 1.8451  LearningRate 0.1434  ProxyLR: 7.1704  Epoch: 3  Global Step: 21780   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:02,498-Speed 3889.44 samples/sec  Loss 1.7216  LearningRate 0.1434  ProxyLR: 7.1692  Epoch: 3  Global Step: 21790   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:05,128-Speed 3894.38 samples/sec  Loss 1.7518  LearningRate 0.1434  ProxyLR: 7.1680  Epoch: 3  Global Step: 21800   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:07,757-Speed 3895.75 samples/sec  Loss 1.7544  LearningRate 0.1433  ProxyLR: 7.1668  Epoch: 3  Global Step: 21810   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:10,386-Speed 3896.02 samples/sec  Loss 1.8381  LearningRate 0.1433  ProxyLR: 7.1656  Epoch: 3  Global Step: 21820   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:13,017-Speed 3894.06 samples/sec  Loss 1.7737  LearningRate 0.1433  ProxyLR: 7.1644  Epoch: 3  Global Step: 21830   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:15,646-Speed 3895.66 samples/sec  Loss 1.7746  LearningRate 0.1433  ProxyLR: 7.1632  Epoch: 3  Global Step: 21840   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:18,275-Speed 3895.24 samples/sec  Loss 1.7072  LearningRate 0.1432  ProxyLR: 7.1621  Epoch: 3  Global Step: 21850   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:20,905-Speed 3894.78 samples/sec  Loss 1.6978  LearningRate 0.1432  ProxyLR: 7.1609  Epoch: 3  Global Step: 21860   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:23,530-Speed 3902.04 samples/sec  Loss 1.6861  LearningRate 0.1432  ProxyLR: 7.1597  Epoch: 3  Global Step: 21870   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:58:26,155-Speed 3901.47 samples/sec  Loss 1.7439  LearningRate 0.1432  ProxyLR: 7.1585  Epoch: 3  Global Step: 21880   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:58:28,781-Speed 3901.17 samples/sec  Loss 1.8294  LearningRate 0.1431  ProxyLR: 7.1573  Epoch: 3  Global Step: 21890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:58:31,406-Speed 3902.50 samples/sec  Loss 1.7918  LearningRate 0.1431  ProxyLR: 7.1561  Epoch: 3  Global Step: 21900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:58:34,029-Speed 3904.30 samples/sec  Loss 1.7169  LearningRate 0.1431  ProxyLR: 7.1549  Epoch: 3  Global Step: 21910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:58:36,639-Speed 3924.11 samples/sec  Loss 1.8279  LearningRate 0.1431  ProxyLR: 7.1537  Epoch: 3  Global Step: 21920   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:39,264-Speed 3901.76 samples/sec  Loss 1.8761  LearningRate 0.1431  ProxyLR: 7.1525  Epoch: 3  Global Step: 21930   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:41,890-Speed 3901.13 samples/sec  Loss 1.7762  LearningRate 0.1430  ProxyLR: 7.1513  Epoch: 3  Global Step: 21940   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:44,514-Speed 3903.73 samples/sec  Loss 1.8058  LearningRate 0.1430  ProxyLR: 7.1502  Epoch: 3  Global Step: 21950   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:47,137-Speed 3904.20 samples/sec  Loss 1.7010  LearningRate 0.1430  ProxyLR: 7.1490  Epoch: 3  Global Step: 21960   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:49,760-Speed 3905.74 samples/sec  Loss 1.6960  LearningRate 0.1430  ProxyLR: 7.1478  Epoch: 3  Global Step: 21970   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:52,383-Speed 3905.12 samples/sec  Loss 1.7426  LearningRate 0.1429  ProxyLR: 7.1466  Epoch: 3  Global Step: 21980   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:55,006-Speed 3904.73 samples/sec  Loss 1.7253  LearningRate 0.1429  ProxyLR: 7.1454  Epoch: 3  Global Step: 21990   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:58:57,631-Speed 3900.60 samples/sec  Loss 1.7860  LearningRate 0.1429  ProxyLR: 7.1442  Epoch: 3  Global Step: 22000   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:00,253-Speed 3906.26 samples/sec  Loss 1.7944  LearningRate 0.1429  ProxyLR: 7.1430  Epoch: 3  Global Step: 22010   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:02,876-Speed 3904.89 samples/sec  Loss 1.7205  LearningRate 0.1428  ProxyLR: 7.1418  Epoch: 3  Global Step: 22020   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:05,489-Speed 3920.35 samples/sec  Loss 1.8061  LearningRate 0.1428  ProxyLR: 7.1406  Epoch: 3  Global Step: 22030   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:08,113-Speed 3904.15 samples/sec  Loss 1.7136  LearningRate 0.1428  ProxyLR: 7.1394  Epoch: 3  Global Step: 22040   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:10,734-Speed 3907.88 samples/sec  Loss 1.6907  LearningRate 0.1428  ProxyLR: 7.1383  Epoch: 3  Global Step: 22050   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:13,356-Speed 3906.05 samples/sec  Loss 1.7360  LearningRate 0.1427  ProxyLR: 7.1371  Epoch: 3  Global Step: 22060   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:15,980-Speed 3902.67 samples/sec  Loss 1.6402  LearningRate 0.1427  ProxyLR: 7.1359  Epoch: 3  Global Step: 22070   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:18,603-Speed 3905.76 samples/sec  Loss 1.6060  LearningRate 0.1427  ProxyLR: 7.1347  Epoch: 3  Global Step: 22080   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:21,227-Speed 3903.14 samples/sec  Loss 1.8754  LearningRate 0.1427  ProxyLR: 7.1335  Epoch: 3  Global Step: 22090   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:23,850-Speed 3904.56 samples/sec  Loss 1.7169  LearningRate 0.1426  ProxyLR: 7.1323  Epoch: 3  Global Step: 22100   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:26,473-Speed 3905.60 samples/sec  Loss 1.6336  LearningRate 0.1426  ProxyLR: 7.1311  Epoch: 3  Global Step: 22110   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:29,097-Speed 3903.13 samples/sec  Loss 1.7874  LearningRate 0.1426  ProxyLR: 7.1299  Epoch: 3  Global Step: 22120   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 15:59:31,721-Speed 3902.87 samples/sec  Loss 1.8169  LearningRate 0.1426  ProxyLR: 7.1288  Epoch: 3  Global Step: 22130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:34,347-Speed 3901.33 samples/sec  Loss 1.7374  LearningRate 0.1426  ProxyLR: 7.1276  Epoch: 3  Global Step: 22140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:36,971-Speed 3903.11 samples/sec  Loss 1.8287  LearningRate 0.1425  ProxyLR: 7.1264  Epoch: 3  Global Step: 22150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:39,595-Speed 3904.03 samples/sec  Loss 1.7129  LearningRate 0.1425  ProxyLR: 7.1252  Epoch: 3  Global Step: 22160   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:42,218-Speed 3904.08 samples/sec  Loss 1.7509  LearningRate 0.1425  ProxyLR: 7.1240  Epoch: 3  Global Step: 22170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:44,841-Speed 3904.98 samples/sec  Loss 1.8559  LearningRate 0.1425  ProxyLR: 7.1228  Epoch: 3  Global Step: 22180   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:47,465-Speed 3902.94 samples/sec  Loss 1.7555  LearningRate 0.1424  ProxyLR: 7.1216  Epoch: 3  Global Step: 22190   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:50,089-Speed 3904.09 samples/sec  Loss 1.6665  LearningRate 0.1424  ProxyLR: 7.1204  Epoch: 3  Global Step: 22200   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:52,711-Speed 3905.70 samples/sec  Loss 1.7586  LearningRate 0.1424  ProxyLR: 7.1193  Epoch: 3  Global Step: 22210   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:55,334-Speed 3904.65 samples/sec  Loss 1.7066  LearningRate 0.1424  ProxyLR: 7.1181  Epoch: 3  Global Step: 22220   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 15:59:57,946-Speed 3922.56 samples/sec  Loss 1.7510  LearningRate 0.1423  ProxyLR: 7.1169  Epoch: 3  Global Step: 22230   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:00,570-Speed 3902.02 samples/sec  Loss 1.8254  LearningRate 0.1423  ProxyLR: 7.1157  Epoch: 3  Global Step: 22240   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:03,194-Speed 3904.15 samples/sec  Loss 1.7763  LearningRate 0.1423  ProxyLR: 7.1145  Epoch: 3  Global Step: 22250   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:05,818-Speed 3902.76 samples/sec  Loss 1.7454  LearningRate 0.1423  ProxyLR: 7.1133  Epoch: 3  Global Step: 22260   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:08,442-Speed 3904.53 samples/sec  Loss 1.7274  LearningRate 0.1422  ProxyLR: 7.1121  Epoch: 3  Global Step: 22270   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:11,064-Speed 3905.63 samples/sec  Loss 1.7104  LearningRate 0.1422  ProxyLR: 7.1109  Epoch: 3  Global Step: 22280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:13,687-Speed 3905.69 samples/sec  Loss 1.7490  LearningRate 0.1422  ProxyLR: 7.1098  Epoch: 3  Global Step: 22290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:16,298-Speed 3922.37 samples/sec  Loss 1.7956  LearningRate 0.1422  ProxyLR: 7.1086  Epoch: 3  Global Step: 22300   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:18,921-Speed 3905.18 samples/sec  Loss 1.6285  LearningRate 0.1421  ProxyLR: 7.1074  Epoch: 3  Global Step: 22310   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:21,545-Speed 3903.45 samples/sec  Loss 1.7670  LearningRate 0.1421  ProxyLR: 7.1062  Epoch: 3  Global Step: 22320   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:24,167-Speed 3906.59 samples/sec  Loss 1.7430  LearningRate 0.1421  ProxyLR: 7.1050  Epoch: 3  Global Step: 22330   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:26,790-Speed 3905.30 samples/sec  Loss 1.8327  LearningRate 0.1421  ProxyLR: 7.1038  Epoch: 3  Global Step: 22340   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:29,414-Speed 3903.18 samples/sec  Loss 1.8778  LearningRate 0.1421  ProxyLR: 7.1026  Epoch: 3  Global Step: 22350   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:32,038-Speed 3902.61 samples/sec  Loss 1.7937  LearningRate 0.1420  ProxyLR: 7.1015  Epoch: 3  Global Step: 22360   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:34,661-Speed 3904.79 samples/sec  Loss 1.8255  LearningRate 0.1420  ProxyLR: 7.1003  Epoch: 3  Global Step: 22370   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:37,284-Speed 3905.20 samples/sec  Loss 1.8032  LearningRate 0.1420  ProxyLR: 7.0991  Epoch: 3  Global Step: 22380   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:39,906-Speed 3905.83 samples/sec  Loss 1.7889  LearningRate 0.1420  ProxyLR: 7.0979  Epoch: 3  Global Step: 22390   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:00:42,530-Speed 3903.26 samples/sec  Loss 1.7855  LearningRate 0.1419  ProxyLR: 7.0967  Epoch: 3  Global Step: 22400   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:45,153-Speed 3904.93 samples/sec  Loss 1.6667  LearningRate 0.1419  ProxyLR: 7.0955  Epoch: 3  Global Step: 22410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:47,778-Speed 3902.98 samples/sec  Loss 1.8180  LearningRate 0.1419  ProxyLR: 7.0943  Epoch: 3  Global Step: 22420   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:50,401-Speed 3904.95 samples/sec  Loss 1.8274  LearningRate 0.1419  ProxyLR: 7.0932  Epoch: 3  Global Step: 22430   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:53,024-Speed 3903.88 samples/sec  Loss 1.7212  LearningRate 0.1418  ProxyLR: 7.0920  Epoch: 3  Global Step: 22440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:55,646-Speed 3906.32 samples/sec  Loss 1.7853  LearningRate 0.1418  ProxyLR: 7.0908  Epoch: 3  Global Step: 22450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:00:58,270-Speed 3903.79 samples/sec  Loss 1.7452  LearningRate 0.1418  ProxyLR: 7.0896  Epoch: 3  Global Step: 22460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:00,895-Speed 3902.63 samples/sec  Loss 1.8139  LearningRate 0.1418  ProxyLR: 7.0884  Epoch: 3  Global Step: 22470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:03,517-Speed 3905.69 samples/sec  Loss 1.7616  LearningRate 0.1417  ProxyLR: 7.0872  Epoch: 3  Global Step: 22480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:06,141-Speed 3903.97 samples/sec  Loss 1.6915  LearningRate 0.1417  ProxyLR: 7.0861  Epoch: 3  Global Step: 22490   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:08,751-Speed 3923.76 samples/sec  Loss 1.6685  LearningRate 0.1417  ProxyLR: 7.0849  Epoch: 3  Global Step: 22500   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:11,362-Speed 3922.23 samples/sec  Loss 1.7662  LearningRate 0.1417  ProxyLR: 7.0837  Epoch: 3  Global Step: 22510   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:13,987-Speed 3902.80 samples/sec  Loss 1.6891  LearningRate 0.1416  ProxyLR: 7.0825  Epoch: 3  Global Step: 22520   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:16,609-Speed 3906.67 samples/sec  Loss 1.7195  LearningRate 0.1416  ProxyLR: 7.0813  Epoch: 3  Global Step: 22530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:19,232-Speed 3904.56 samples/sec  Loss 1.7284  LearningRate 0.1416  ProxyLR: 7.0801  Epoch: 3  Global Step: 22540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:21,854-Speed 3905.70 samples/sec  Loss 1.7741  LearningRate 0.1416  ProxyLR: 7.0789  Epoch: 3  Global Step: 22550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:24,477-Speed 3905.55 samples/sec  Loss 1.7140  LearningRate 0.1416  ProxyLR: 7.0778  Epoch: 3  Global Step: 22560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:27,100-Speed 3904.66 samples/sec  Loss 1.7161  LearningRate 0.1415  ProxyLR: 7.0766  Epoch: 3  Global Step: 22570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:29,724-Speed 3903.09 samples/sec  Loss 1.7902  LearningRate 0.1415  ProxyLR: 7.0754  Epoch: 3  Global Step: 22580   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:32,347-Speed 3904.89 samples/sec  Loss 1.7195  LearningRate 0.1415  ProxyLR: 7.0742  Epoch: 3  Global Step: 22590   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:34,971-Speed 3904.21 samples/sec  Loss 1.8829  LearningRate 0.1415  ProxyLR: 7.0730  Epoch: 3  Global Step: 22600   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:37,593-Speed 3905.63 samples/sec  Loss 1.7675  LearningRate 0.1414  ProxyLR: 7.0718  Epoch: 3  Global Step: 22610   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:40,219-Speed 3900.61 samples/sec  Loss 1.7141  LearningRate 0.1414  ProxyLR: 7.0707  Epoch: 3  Global Step: 22620   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:42,844-Speed 3901.50 samples/sec  Loss 1.6493  LearningRate 0.1414  ProxyLR: 7.0695  Epoch: 3  Global Step: 22630   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:45,468-Speed 3903.79 samples/sec  Loss 1.6592  LearningRate 0.1414  ProxyLR: 7.0683  Epoch: 3  Global Step: 22640   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:48,091-Speed 3905.45 samples/sec  Loss 1.6465  LearningRate 0.1413  ProxyLR: 7.0671  Epoch: 3  Global Step: 22650   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:01:50,701-Speed 3923.31 samples/sec  Loss 1.7521  LearningRate 0.1413  ProxyLR: 7.0659  Epoch: 3  Global Step: 22660   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:53,326-Speed 3902.50 samples/sec  Loss 1.7215  LearningRate 0.1413  ProxyLR: 7.0647  Epoch: 3  Global Step: 22670   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:55,949-Speed 3904.21 samples/sec  Loss 1.8677  LearningRate 0.1413  ProxyLR: 7.0636  Epoch: 3  Global Step: 22680   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:01:58,572-Speed 3905.77 samples/sec  Loss 1.6726  LearningRate 0.1412  ProxyLR: 7.0624  Epoch: 3  Global Step: 22690   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:01,194-Speed 3905.62 samples/sec  Loss 1.8593  LearningRate 0.1412  ProxyLR: 7.0612  Epoch: 3  Global Step: 22700   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:03,817-Speed 3905.19 samples/sec  Loss 1.6688  LearningRate 0.1412  ProxyLR: 7.0600  Epoch: 3  Global Step: 22710   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:06,441-Speed 3903.94 samples/sec  Loss 1.7135  LearningRate 0.1412  ProxyLR: 7.0588  Epoch: 3  Global Step: 22720   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:09,066-Speed 3901.87 samples/sec  Loss 1.8869  LearningRate 0.1412  ProxyLR: 7.0577  Epoch: 3  Global Step: 22730   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:11,750-Speed 3816.97 samples/sec  Loss 1.7194  LearningRate 0.1411  ProxyLR: 7.0565  Epoch: 3  Global Step: 22740   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:21,499-Speed 1050.38 samples/sec  Loss 1.3152  LearningRate 0.1411  ProxyLR: 7.0553  Epoch: 4  Global Step: 22750   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:02:24,190-Speed 3806.88 samples/sec  Loss 1.1107  LearningRate 0.1411  ProxyLR: 7.0541  Epoch: 4  Global Step: 22760   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:26,818-Speed 3897.29 samples/sec  Loss 1.0314  LearningRate 0.1411  ProxyLR: 7.0529  Epoch: 4  Global Step: 22770   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:29,450-Speed 3891.70 samples/sec  Loss 1.0505  LearningRate 0.1410  ProxyLR: 7.0517  Epoch: 4  Global Step: 22780   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:32,082-Speed 3892.17 samples/sec  Loss 0.9722  LearningRate 0.1410  ProxyLR: 7.0506  Epoch: 4  Global Step: 22790   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:34,711-Speed 3895.36 samples/sec  Loss 0.9322  LearningRate 0.1410  ProxyLR: 7.0494  Epoch: 4  Global Step: 22800   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:37,342-Speed 3892.95 samples/sec  Loss 1.0791  LearningRate 0.1410  ProxyLR: 7.0482  Epoch: 4  Global Step: 22810   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:39,973-Speed 3892.80 samples/sec  Loss 1.0006  LearningRate 0.1409  ProxyLR: 7.0470  Epoch: 4  Global Step: 22820   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:42,601-Speed 3897.89 samples/sec  Loss 0.9246  LearningRate 0.1409  ProxyLR: 7.0458  Epoch: 4  Global Step: 22830   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:45,227-Speed 3900.06 samples/sec  Loss 0.9340  LearningRate 0.1409  ProxyLR: 7.0447  Epoch: 4  Global Step: 22840   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:47,858-Speed 3893.16 samples/sec  Loss 1.0713  LearningRate 0.1409  ProxyLR: 7.0435  Epoch: 4  Global Step: 22850   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:50,487-Speed 3895.07 samples/sec  Loss 1.0203  LearningRate 0.1408  ProxyLR: 7.0423  Epoch: 4  Global Step: 22860   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:02:53,109-Speed 3906.93 samples/sec  Loss 1.0056  LearningRate 0.1408  ProxyLR: 7.0411  Epoch: 4  Global Step: 22870   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:55,806-Speed 3797.24 samples/sec  Loss 1.0028  LearningRate 0.1408  ProxyLR: 7.0399  Epoch: 4  Global Step: 22880   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:02:58,466-Speed 3850.50 samples/sec  Loss 0.9995  LearningRate 0.1408  ProxyLR: 7.0388  Epoch: 4  Global Step: 22890   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:01,100-Speed 3889.43 samples/sec  Loss 1.0944  LearningRate 0.1408  ProxyLR: 7.0376  Epoch: 4  Global Step: 22900   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:03,733-Speed 3890.24 samples/sec  Loss 1.0633  LearningRate 0.1407  ProxyLR: 7.0364  Epoch: 4  Global Step: 22910   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:06,367-Speed 3889.04 samples/sec  Loss 1.0686  LearningRate 0.1407  ProxyLR: 7.0352  Epoch: 4  Global Step: 22920   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:09,055-Speed 3810.20 samples/sec  Loss 0.9851  LearningRate 0.1407  ProxyLR: 7.0340  Epoch: 4  Global Step: 22930   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:11,688-Speed 3889.90 samples/sec  Loss 1.1154  LearningRate 0.1407  ProxyLR: 7.0329  Epoch: 4  Global Step: 22940   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:14,323-Speed 3886.94 samples/sec  Loss 0.9746  LearningRate 0.1406  ProxyLR: 7.0317  Epoch: 4  Global Step: 22950   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:16,954-Speed 3892.11 samples/sec  Loss 0.9135  LearningRate 0.1406  ProxyLR: 7.0305  Epoch: 4  Global Step: 22960   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:19,586-Speed 3891.78 samples/sec  Loss 0.9936  LearningRate 0.1406  ProxyLR: 7.0293  Epoch: 4  Global Step: 22970   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:03:22,223-Speed 3884.04 samples/sec  Loss 1.0343  LearningRate 0.1406  ProxyLR: 7.0281  Epoch: 4  Global Step: 22980   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:03:24,851-Speed 3897.33 samples/sec  Loss 1.0751  LearningRate 0.1405  ProxyLR: 7.0270  Epoch: 4  Global Step: 22990   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:03:27,466-Speed 3916.46 samples/sec  Loss 0.9396  LearningRate 0.1405  ProxyLR: 7.0258  Epoch: 4  Global Step: 23000   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:30,096-Speed 3895.15 samples/sec  Loss 1.0187  LearningRate 0.1405  ProxyLR: 7.0246  Epoch: 4  Global Step: 23010   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:32,724-Speed 3896.96 samples/sec  Loss 1.0048  LearningRate 0.1405  ProxyLR: 7.0234  Epoch: 4  Global Step: 23020   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:35,354-Speed 3894.76 samples/sec  Loss 0.9217  LearningRate 0.1404  ProxyLR: 7.0222  Epoch: 4  Global Step: 23030   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:37,984-Speed 3894.32 samples/sec  Loss 0.9621  LearningRate 0.1404  ProxyLR: 7.0211  Epoch: 4  Global Step: 23040   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:40,614-Speed 3894.49 samples/sec  Loss 1.0648  LearningRate 0.1404  ProxyLR: 7.0199  Epoch: 4  Global Step: 23050   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:43,242-Speed 3897.94 samples/sec  Loss 1.0337  LearningRate 0.1404  ProxyLR: 7.0187  Epoch: 4  Global Step: 23060   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:45,870-Speed 3897.69 samples/sec  Loss 1.0289  LearningRate 0.1404  ProxyLR: 7.0175  Epoch: 4  Global Step: 23070   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:48,498-Speed 3897.67 samples/sec  Loss 1.0273  LearningRate 0.1403  ProxyLR: 7.0163  Epoch: 4  Global Step: 23080   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:51,126-Speed 3897.53 samples/sec  Loss 1.0626  LearningRate 0.1403  ProxyLR: 7.0152  Epoch: 4  Global Step: 23090   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:53,754-Speed 3896.08 samples/sec  Loss 1.0150  LearningRate 0.1403  ProxyLR: 7.0140  Epoch: 4  Global Step: 23100   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:03:56,370-Speed 3916.49 samples/sec  Loss 1.0834  LearningRate 0.1403  ProxyLR: 7.0128  Epoch: 4  Global Step: 23110   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:03:58,997-Speed 3897.68 samples/sec  Loss 0.9378  LearningRate 0.1402  ProxyLR: 7.0116  Epoch: 4  Global Step: 23120   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:01,625-Speed 3898.80 samples/sec  Loss 1.0010  LearningRate 0.1402  ProxyLR: 7.0105  Epoch: 4  Global Step: 23130   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:04,252-Speed 3898.41 samples/sec  Loss 1.0444  LearningRate 0.1402  ProxyLR: 7.0093  Epoch: 4  Global Step: 23140   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:06,879-Speed 3899.22 samples/sec  Loss 1.0420  LearningRate 0.1402  ProxyLR: 7.0081  Epoch: 4  Global Step: 23150   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:09,508-Speed 3896.04 samples/sec  Loss 1.0882  LearningRate 0.1401  ProxyLR: 7.0069  Epoch: 4  Global Step: 23160   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:12,136-Speed 3896.87 samples/sec  Loss 1.1730  LearningRate 0.1401  ProxyLR: 7.0057  Epoch: 4  Global Step: 23170   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:14,764-Speed 3897.33 samples/sec  Loss 1.0438  LearningRate 0.1401  ProxyLR: 7.0046  Epoch: 4  Global Step: 23180   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:17,391-Speed 3899.13 samples/sec  Loss 1.1447  LearningRate 0.1401  ProxyLR: 7.0034  Epoch: 4  Global Step: 23190   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:20,019-Speed 3897.68 samples/sec  Loss 1.1145  LearningRate 0.1400  ProxyLR: 7.0022  Epoch: 4  Global Step: 23200   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:22,633-Speed 3917.51 samples/sec  Loss 1.1790  LearningRate 0.1400  ProxyLR: 7.0010  Epoch: 4  Global Step: 23210   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:25,263-Speed 3895.62 samples/sec  Loss 1.0668  LearningRate 0.1400  ProxyLR: 6.9999  Epoch: 4  Global Step: 23220   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:27,888-Speed 3900.99 samples/sec  Loss 1.0920  LearningRate 0.1400  ProxyLR: 6.9987  Epoch: 4  Global Step: 23230   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:30,514-Speed 3900.10 samples/sec  Loss 1.0612  LearningRate 0.1400  ProxyLR: 6.9975  Epoch: 4  Global Step: 23240   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:33,141-Speed 3899.64 samples/sec  Loss 1.0626  LearningRate 0.1399  ProxyLR: 6.9963  Epoch: 4  Global Step: 23250   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:35,767-Speed 3900.19 samples/sec  Loss 1.0417  LearningRate 0.1399  ProxyLR: 6.9951  Epoch: 4  Global Step: 23260   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:38,393-Speed 3899.75 samples/sec  Loss 1.2458  LearningRate 0.1399  ProxyLR: 6.9940  Epoch: 4  Global Step: 23270   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:41,020-Speed 3898.73 samples/sec  Loss 1.0906  LearningRate 0.1399  ProxyLR: 6.9928  Epoch: 4  Global Step: 23280   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:43,646-Speed 3901.68 samples/sec  Loss 1.0780  LearningRate 0.1398  ProxyLR: 6.9916  Epoch: 4  Global Step: 23290   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:46,272-Speed 3900.06 samples/sec  Loss 1.0333  LearningRate 0.1398  ProxyLR: 6.9904  Epoch: 4  Global Step: 23300   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:48,900-Speed 3898.33 samples/sec  Loss 1.0511  LearningRate 0.1398  ProxyLR: 6.9893  Epoch: 4  Global Step: 23310   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:04:51,514-Speed 3917.64 samples/sec  Loss 1.0612  LearningRate 0.1398  ProxyLR: 6.9881  Epoch: 4  Global Step: 23320   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:54,140-Speed 3899.80 samples/sec  Loss 1.0551  LearningRate 0.1397  ProxyLR: 6.9869  Epoch: 4  Global Step: 23330   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:56,769-Speed 3897.25 samples/sec  Loss 1.0972  LearningRate 0.1397  ProxyLR: 6.9857  Epoch: 4  Global Step: 23340   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:04:59,394-Speed 3901.03 samples/sec  Loss 1.1088  LearningRate 0.1397  ProxyLR: 6.9846  Epoch: 4  Global Step: 23350   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:02,022-Speed 3897.29 samples/sec  Loss 1.0985  LearningRate 0.1397  ProxyLR: 6.9834  Epoch: 4  Global Step: 23360   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:04,649-Speed 3899.30 samples/sec  Loss 1.1131  LearningRate 0.1396  ProxyLR: 6.9822  Epoch: 4  Global Step: 23370   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:07,276-Speed 3898.64 samples/sec  Loss 1.1471  LearningRate 0.1396  ProxyLR: 6.9810  Epoch: 4  Global Step: 23380   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:09,904-Speed 3898.19 samples/sec  Loss 1.1770  LearningRate 0.1396  ProxyLR: 6.9799  Epoch: 4  Global Step: 23390   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:12,530-Speed 3899.40 samples/sec  Loss 1.1345  LearningRate 0.1396  ProxyLR: 6.9787  Epoch: 4  Global Step: 23400   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:15,158-Speed 3897.88 samples/sec  Loss 1.0936  LearningRate 0.1396  ProxyLR: 6.9775  Epoch: 4  Global Step: 23410   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:17,785-Speed 3899.51 samples/sec  Loss 1.1059  LearningRate 0.1395  ProxyLR: 6.9763  Epoch: 4  Global Step: 23420   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:05:20,413-Speed 3897.69 samples/sec  Loss 1.1570  LearningRate 0.1395  ProxyLR: 6.9752  Epoch: 4  Global Step: 23430   Fp16 Grad Scale: 1048576  Required: 10 hours
Training: 2023-05-04 16:05:23,026-Speed 3919.24 samples/sec  Loss 1.0748  LearningRate 0.1395  ProxyLR: 6.9740  Epoch: 4  Global Step: 23440   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:25,653-Speed 3898.64 samples/sec  Loss 1.2152  LearningRate 0.1395  ProxyLR: 6.9728  Epoch: 4  Global Step: 23450   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:28,280-Speed 3898.56 samples/sec  Loss 1.2055  LearningRate 0.1394  ProxyLR: 6.9716  Epoch: 4  Global Step: 23460   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:30,906-Speed 3900.44 samples/sec  Loss 1.0824  LearningRate 0.1394  ProxyLR: 6.9705  Epoch: 4  Global Step: 23470   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:33,533-Speed 3899.44 samples/sec  Loss 1.0492  LearningRate 0.1394  ProxyLR: 6.9693  Epoch: 4  Global Step: 23480   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:36,160-Speed 3898.56 samples/sec  Loss 1.0686  LearningRate 0.1394  ProxyLR: 6.9681  Epoch: 4  Global Step: 23490   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:38,787-Speed 3899.47 samples/sec  Loss 1.1377  LearningRate 0.1393  ProxyLR: 6.9669  Epoch: 4  Global Step: 23500   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:41,414-Speed 3898.54 samples/sec  Loss 1.1922  LearningRate 0.1393  ProxyLR: 6.9658  Epoch: 4  Global Step: 23510   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:44,041-Speed 3898.32 samples/sec  Loss 1.0885  LearningRate 0.1393  ProxyLR: 6.9646  Epoch: 4  Global Step: 23520   Fp16 Grad Scale: 524288  Required: 10 hours
Training: 2023-05-04 16:05:46,654-Speed 3920.67 samples/sec  Loss 1.1982  LearningRate 0.1393  ProxyLR: 6.9634  Epoch: 4  Global Step: 23530   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:05:49,282-Speed 3897.77 samples/sec  Loss 1.1434  LearningRate 0.1392  ProxyLR: 6.9622  Epoch: 4  Global Step: 23540   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:05:51,908-Speed 3899.80 samples/sec  Loss 1.0511  LearningRate 0.1392  ProxyLR: 6.9611  Epoch: 4  Global Step: 23550   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:05:54,536-Speed 3897.74 samples/sec  Loss 1.1001  LearningRate 0.1392  ProxyLR: 6.9599  Epoch: 4  Global Step: 23560   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:05:57,163-Speed 3899.29 samples/sec  Loss 1.1935  LearningRate 0.1392  ProxyLR: 6.9587  Epoch: 4  Global Step: 23570   Fp16 Grad Scale: 262144  Required: 10 hours
Training: 2023-05-04 16:05:59,788-Speed 3900.84 samples/sec  Loss 1.1871  LearningRate 0.1392  ProxyLR: 6.9575  Epoch: 4  Global Step: 23580   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:06:02,416-Speed 3898.17 samples/sec  Loss 1.1235  LearningRate 0.1391  ProxyLR: 6.9564  Epoch: 4  Global Step: 23590   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:06:05,043-Speed 3898.26 samples/sec  Loss 1.1594  LearningRate 0.1391  ProxyLR: 6.9552  Epoch: 4  Global Step: 23600   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:06:07,673-Speed 3895.57 samples/sec  Loss 1.1848  LearningRate 0.1391  ProxyLR: 6.9540  Epoch: 4  Global Step: 23610   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:06:10,306-Speed 3889.70 samples/sec  Loss 1.1267  LearningRate 0.1391  ProxyLR: 6.9528  Epoch: 4  Global Step: 23620   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:06:12,937-Speed 3892.45 samples/sec  Loss 1.0913  LearningRate 0.1390  ProxyLR: 6.9517  Epoch: 4  Global Step: 23630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:15,569-Speed 3890.88 samples/sec  Loss 1.1911  LearningRate 0.1390  ProxyLR: 6.9505  Epoch: 4  Global Step: 23640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:18,201-Speed 3891.72 samples/sec  Loss 1.1614  LearningRate 0.1390  ProxyLR: 6.9493  Epoch: 4  Global Step: 23650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:20,836-Speed 3887.46 samples/sec  Loss 1.2020  LearningRate 0.1390  ProxyLR: 6.9482  Epoch: 4  Global Step: 23660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:23,468-Speed 3890.77 samples/sec  Loss 1.1734  LearningRate 0.1389  ProxyLR: 6.9470  Epoch: 4  Global Step: 23670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:26,100-Speed 3892.79 samples/sec  Loss 1.2200  LearningRate 0.1389  ProxyLR: 6.9458  Epoch: 4  Global Step: 23680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:28,730-Speed 3894.28 samples/sec  Loss 1.1555  LearningRate 0.1389  ProxyLR: 6.9446  Epoch: 4  Global Step: 23690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:31,361-Speed 3892.97 samples/sec  Loss 1.1361  LearningRate 0.1389  ProxyLR: 6.9435  Epoch: 4  Global Step: 23700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:33,992-Speed 3892.94 samples/sec  Loss 1.0579  LearningRate 0.1388  ProxyLR: 6.9423  Epoch: 4  Global Step: 23710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:36,622-Speed 3894.37 samples/sec  Loss 1.2591  LearningRate 0.1388  ProxyLR: 6.9411  Epoch: 4  Global Step: 23720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:39,253-Speed 3893.33 samples/sec  Loss 1.2383  LearningRate 0.1388  ProxyLR: 6.9400  Epoch: 4  Global Step: 23730   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:06:41,884-Speed 3892.32 samples/sec  Loss 1.1673  LearningRate 0.1388  ProxyLR: 6.9388  Epoch: 4  Global Step: 23740   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:06:44,516-Speed 3891.58 samples/sec  Loss 1.1878  LearningRate 0.1388  ProxyLR: 6.9376  Epoch: 4  Global Step: 23750   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:06:47,133-Speed 3913.57 samples/sec  Loss 1.1131  LearningRate 0.1387  ProxyLR: 6.9364  Epoch: 4  Global Step: 23760   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:49,763-Speed 3894.02 samples/sec  Loss 1.1857  LearningRate 0.1387  ProxyLR: 6.9353  Epoch: 4  Global Step: 23770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:52,395-Speed 3891.82 samples/sec  Loss 1.2074  LearningRate 0.1387  ProxyLR: 6.9341  Epoch: 4  Global Step: 23780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:55,025-Speed 3894.81 samples/sec  Loss 1.1750  LearningRate 0.1387  ProxyLR: 6.9329  Epoch: 4  Global Step: 23790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:06:57,654-Speed 3895.65 samples/sec  Loss 1.1401  LearningRate 0.1386  ProxyLR: 6.9317  Epoch: 4  Global Step: 23800   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:00,285-Speed 3893.25 samples/sec  Loss 1.1721  LearningRate 0.1386  ProxyLR: 6.9306  Epoch: 4  Global Step: 23810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:02,915-Speed 3894.73 samples/sec  Loss 1.1645  LearningRate 0.1386  ProxyLR: 6.9294  Epoch: 4  Global Step: 23820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:05,545-Speed 3894.01 samples/sec  Loss 1.1420  LearningRate 0.1386  ProxyLR: 6.9282  Epoch: 4  Global Step: 23830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:08,172-Speed 3899.51 samples/sec  Loss 1.1830  LearningRate 0.1385  ProxyLR: 6.9271  Epoch: 4  Global Step: 23840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:10,798-Speed 3900.34 samples/sec  Loss 1.2391  LearningRate 0.1385  ProxyLR: 6.9259  Epoch: 4  Global Step: 23850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:13,426-Speed 3897.07 samples/sec  Loss 1.2356  LearningRate 0.1385  ProxyLR: 6.9247  Epoch: 4  Global Step: 23860   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:07:16,052-Speed 3899.69 samples/sec  Loss 1.2184  LearningRate 0.1385  ProxyLR: 6.9236  Epoch: 4  Global Step: 23870   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:07:18,667-Speed 3917.62 samples/sec  Loss 1.2245  LearningRate 0.1384  ProxyLR: 6.9224  Epoch: 4  Global Step: 23880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:21,295-Speed 3897.81 samples/sec  Loss 1.1718  LearningRate 0.1384  ProxyLR: 6.9212  Epoch: 4  Global Step: 23890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:23,921-Speed 3899.59 samples/sec  Loss 1.2215  LearningRate 0.1384  ProxyLR: 6.9200  Epoch: 4  Global Step: 23900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:26,550-Speed 3897.06 samples/sec  Loss 1.1976  LearningRate 0.1384  ProxyLR: 6.9189  Epoch: 4  Global Step: 23910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:29,176-Speed 3899.98 samples/sec  Loss 1.2045  LearningRate 0.1384  ProxyLR: 6.9177  Epoch: 4  Global Step: 23920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:31,803-Speed 3898.87 samples/sec  Loss 1.1466  LearningRate 0.1383  ProxyLR: 6.9165  Epoch: 4  Global Step: 23930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:34,430-Speed 3899.41 samples/sec  Loss 1.2889  LearningRate 0.1383  ProxyLR: 6.9154  Epoch: 4  Global Step: 23940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:37,055-Speed 3900.90 samples/sec  Loss 1.1867  LearningRate 0.1383  ProxyLR: 6.9142  Epoch: 4  Global Step: 23950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:39,682-Speed 3898.99 samples/sec  Loss 1.2199  LearningRate 0.1383  ProxyLR: 6.9130  Epoch: 4  Global Step: 23960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:42,309-Speed 3900.15 samples/sec  Loss 1.3243  LearningRate 0.1382  ProxyLR: 6.9118  Epoch: 4  Global Step: 23970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:44,935-Speed 3900.15 samples/sec  Loss 1.2090  LearningRate 0.1382  ProxyLR: 6.9107  Epoch: 4  Global Step: 23980   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:07:47,547-Speed 3920.59 samples/sec  Loss 1.2465  LearningRate 0.1382  ProxyLR: 6.9095  Epoch: 4  Global Step: 23990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:50,175-Speed 3897.67 samples/sec  Loss 1.2014  LearningRate 0.1382  ProxyLR: 6.9083  Epoch: 4  Global Step: 24000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:52,805-Speed 3894.50 samples/sec  Loss 1.3082  LearningRate 0.1381  ProxyLR: 6.9072  Epoch: 4  Global Step: 24010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:55,433-Speed 3897.05 samples/sec  Loss 1.2751  LearningRate 0.1381  ProxyLR: 6.9060  Epoch: 4  Global Step: 24020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:07:58,060-Speed 3899.79 samples/sec  Loss 1.2243  LearningRate 0.1381  ProxyLR: 6.9048  Epoch: 4  Global Step: 24030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:00,686-Speed 3900.28 samples/sec  Loss 1.2165  LearningRate 0.1381  ProxyLR: 6.9037  Epoch: 4  Global Step: 24040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:03,297-Speed 3921.93 samples/sec  Loss 1.2315  LearningRate 0.1380  ProxyLR: 6.9025  Epoch: 4  Global Step: 24050   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:05,923-Speed 3901.21 samples/sec  Loss 1.3223  LearningRate 0.1380  ProxyLR: 6.9013  Epoch: 4  Global Step: 24060   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:08,551-Speed 3897.28 samples/sec  Loss 1.1498  LearningRate 0.1380  ProxyLR: 6.9002  Epoch: 4  Global Step: 24070   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:11,176-Speed 3901.06 samples/sec  Loss 1.2510  LearningRate 0.1380  ProxyLR: 6.8990  Epoch: 4  Global Step: 24080   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:13,803-Speed 3898.79 samples/sec  Loss 1.2104  LearningRate 0.1380  ProxyLR: 6.8978  Epoch: 4  Global Step: 24090   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:16,431-Speed 3897.87 samples/sec  Loss 1.2078  LearningRate 0.1379  ProxyLR: 6.8967  Epoch: 4  Global Step: 24100   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:19,059-Speed 3897.35 samples/sec  Loss 1.2585  LearningRate 0.1379  ProxyLR: 6.8955  Epoch: 4  Global Step: 24110   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:21,687-Speed 3898.65 samples/sec  Loss 1.2634  LearningRate 0.1379  ProxyLR: 6.8943  Epoch: 4  Global Step: 24120   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:24,315-Speed 3897.30 samples/sec  Loss 1.1981  LearningRate 0.1379  ProxyLR: 6.8931  Epoch: 4  Global Step: 24130   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:26,941-Speed 3899.28 samples/sec  Loss 1.2137  LearningRate 0.1378  ProxyLR: 6.8920  Epoch: 4  Global Step: 24140   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:08:29,569-Speed 3897.54 samples/sec  Loss 1.2623  LearningRate 0.1378  ProxyLR: 6.8908  Epoch: 4  Global Step: 24150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:32,195-Speed 3901.49 samples/sec  Loss 1.1927  LearningRate 0.1378  ProxyLR: 6.8896  Epoch: 4  Global Step: 24160   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:34,822-Speed 3898.42 samples/sec  Loss 1.2802  LearningRate 0.1378  ProxyLR: 6.8885  Epoch: 4  Global Step: 24170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:37,448-Speed 3900.43 samples/sec  Loss 1.3005  LearningRate 0.1377  ProxyLR: 6.8873  Epoch: 4  Global Step: 24180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:40,075-Speed 3898.85 samples/sec  Loss 1.2860  LearningRate 0.1377  ProxyLR: 6.8861  Epoch: 4  Global Step: 24190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:42,702-Speed 3899.25 samples/sec  Loss 1.3596  LearningRate 0.1377  ProxyLR: 6.8850  Epoch: 4  Global Step: 24200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:45,328-Speed 3899.28 samples/sec  Loss 1.1914  LearningRate 0.1377  ProxyLR: 6.8838  Epoch: 4  Global Step: 24210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:47,955-Speed 3899.98 samples/sec  Loss 1.1814  LearningRate 0.1377  ProxyLR: 6.8826  Epoch: 4  Global Step: 24220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:50,581-Speed 3899.43 samples/sec  Loss 1.2974  LearningRate 0.1376  ProxyLR: 6.8815  Epoch: 4  Global Step: 24230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:53,207-Speed 3900.70 samples/sec  Loss 1.3356  LearningRate 0.1376  ProxyLR: 6.8803  Epoch: 4  Global Step: 24240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:08:55,832-Speed 3901.68 samples/sec  Loss 1.2953  LearningRate 0.1376  ProxyLR: 6.8791  Epoch: 4  Global Step: 24250   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:08:58,445-Speed 3919.69 samples/sec  Loss 1.2190  LearningRate 0.1376  ProxyLR: 6.8780  Epoch: 4  Global Step: 24260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:01,070-Speed 3902.78 samples/sec  Loss 1.3374  LearningRate 0.1375  ProxyLR: 6.8768  Epoch: 4  Global Step: 24270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:03,695-Speed 3901.74 samples/sec  Loss 1.3590  LearningRate 0.1375  ProxyLR: 6.8756  Epoch: 4  Global Step: 24280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:06,321-Speed 3900.41 samples/sec  Loss 1.3379  LearningRate 0.1375  ProxyLR: 6.8745  Epoch: 4  Global Step: 24290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:08,947-Speed 3899.91 samples/sec  Loss 1.3354  LearningRate 0.1375  ProxyLR: 6.8733  Epoch: 4  Global Step: 24300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:11,574-Speed 3899.85 samples/sec  Loss 1.3320  LearningRate 0.1374  ProxyLR: 6.8721  Epoch: 4  Global Step: 24310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:14,199-Speed 3901.42 samples/sec  Loss 1.2886  LearningRate 0.1374  ProxyLR: 6.8710  Epoch: 4  Global Step: 24320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:16,825-Speed 3899.78 samples/sec  Loss 1.3215  LearningRate 0.1374  ProxyLR: 6.8698  Epoch: 4  Global Step: 24330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:19,453-Speed 3897.76 samples/sec  Loss 1.3355  LearningRate 0.1374  ProxyLR: 6.8686  Epoch: 4  Global Step: 24340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:22,080-Speed 3899.41 samples/sec  Loss 1.1486  LearningRate 0.1373  ProxyLR: 6.8675  Epoch: 4  Global Step: 24350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:24,693-Speed 3919.05 samples/sec  Loss 1.1895  LearningRate 0.1373  ProxyLR: 6.8663  Epoch: 4  Global Step: 24360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:27,321-Speed 3897.69 samples/sec  Loss 1.3861  LearningRate 0.1373  ProxyLR: 6.8651  Epoch: 4  Global Step: 24370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:29,948-Speed 3899.45 samples/sec  Loss 1.4407  LearningRate 0.1373  ProxyLR: 6.8640  Epoch: 4  Global Step: 24380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:32,575-Speed 3898.73 samples/sec  Loss 1.2996  LearningRate 0.1373  ProxyLR: 6.8628  Epoch: 4  Global Step: 24390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:35,204-Speed 3896.53 samples/sec  Loss 1.2132  LearningRate 0.1372  ProxyLR: 6.8616  Epoch: 4  Global Step: 24400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:37,832-Speed 3897.59 samples/sec  Loss 1.2571  LearningRate 0.1372  ProxyLR: 6.8605  Epoch: 4  Global Step: 24410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:40,459-Speed 3898.88 samples/sec  Loss 1.3496  LearningRate 0.1372  ProxyLR: 6.8593  Epoch: 4  Global Step: 24420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:43,086-Speed 3898.80 samples/sec  Loss 1.2201  LearningRate 0.1372  ProxyLR: 6.8581  Epoch: 4  Global Step: 24430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:45,713-Speed 3897.91 samples/sec  Loss 1.2128  LearningRate 0.1371  ProxyLR: 6.8570  Epoch: 4  Global Step: 24440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:48,343-Speed 3894.70 samples/sec  Loss 1.3510  LearningRate 0.1371  ProxyLR: 6.8558  Epoch: 4  Global Step: 24450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:50,971-Speed 3897.78 samples/sec  Loss 1.3501  LearningRate 0.1371  ProxyLR: 6.8547  Epoch: 4  Global Step: 24460   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:09:53,586-Speed 3917.27 samples/sec  Loss 1.3811  LearningRate 0.1371  ProxyLR: 6.8535  Epoch: 4  Global Step: 24470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:56,214-Speed 3897.72 samples/sec  Loss 1.2966  LearningRate 0.1370  ProxyLR: 6.8523  Epoch: 4  Global Step: 24480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:09:58,842-Speed 3897.01 samples/sec  Loss 1.3325  LearningRate 0.1370  ProxyLR: 6.8512  Epoch: 4  Global Step: 24490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:01,472-Speed 3895.31 samples/sec  Loss 1.4588  LearningRate 0.1370  ProxyLR: 6.8500  Epoch: 4  Global Step: 24500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:04,101-Speed 3895.78 samples/sec  Loss 1.2646  LearningRate 0.1370  ProxyLR: 6.8488  Epoch: 4  Global Step: 24510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:06,730-Speed 3895.18 samples/sec  Loss 1.2854  LearningRate 0.1370  ProxyLR: 6.8477  Epoch: 4  Global Step: 24520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:09,362-Speed 3892.26 samples/sec  Loss 1.2709  LearningRate 0.1369  ProxyLR: 6.8465  Epoch: 4  Global Step: 24530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:11,991-Speed 3896.03 samples/sec  Loss 1.2387  LearningRate 0.1369  ProxyLR: 6.8453  Epoch: 4  Global Step: 24540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:14,620-Speed 3895.83 samples/sec  Loss 1.3851  LearningRate 0.1369  ProxyLR: 6.8442  Epoch: 4  Global Step: 24550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:17,248-Speed 3896.95 samples/sec  Loss 1.3741  LearningRate 0.1369  ProxyLR: 6.8430  Epoch: 4  Global Step: 24560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:19,861-Speed 3919.53 samples/sec  Loss 1.4238  LearningRate 0.1368  ProxyLR: 6.8418  Epoch: 4  Global Step: 24570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:22,488-Speed 3899.19 samples/sec  Loss 1.4556  LearningRate 0.1368  ProxyLR: 6.8407  Epoch: 4  Global Step: 24580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:25,116-Speed 3897.25 samples/sec  Loss 1.3857  LearningRate 0.1368  ProxyLR: 6.8395  Epoch: 4  Global Step: 24590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:27,745-Speed 3895.72 samples/sec  Loss 1.3229  LearningRate 0.1368  ProxyLR: 6.8384  Epoch: 4  Global Step: 24600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:30,374-Speed 3897.04 samples/sec  Loss 1.2463  LearningRate 0.1367  ProxyLR: 6.8372  Epoch: 4  Global Step: 24610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:33,001-Speed 3897.78 samples/sec  Loss 1.2830  LearningRate 0.1367  ProxyLR: 6.8360  Epoch: 4  Global Step: 24620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:35,634-Speed 3890.95 samples/sec  Loss 1.2726  LearningRate 0.1367  ProxyLR: 6.8349  Epoch: 4  Global Step: 24630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:38,268-Speed 3888.39 samples/sec  Loss 1.3441  LearningRate 0.1367  ProxyLR: 6.8337  Epoch: 4  Global Step: 24640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:40,902-Speed 3888.68 samples/sec  Loss 1.2914  LearningRate 0.1367  ProxyLR: 6.8325  Epoch: 4  Global Step: 24650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:43,536-Speed 3888.43 samples/sec  Loss 1.3802  LearningRate 0.1366  ProxyLR: 6.8314  Epoch: 4  Global Step: 24660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:10:46,174-Speed 3882.35 samples/sec  Loss 1.3078  LearningRate 0.1366  ProxyLR: 6.8302  Epoch: 4  Global Step: 24670   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:10:48,783-Speed 3925.50 samples/sec  Loss 1.3461  LearningRate 0.1366  ProxyLR: 6.8290  Epoch: 4  Global Step: 24680   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:10:51,419-Speed 3887.02 samples/sec  Loss 1.2997  LearningRate 0.1366  ProxyLR: 6.8279  Epoch: 4  Global Step: 24690   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:10:54,051-Speed 3890.76 samples/sec  Loss 1.3466  LearningRate 0.1365  ProxyLR: 6.8267  Epoch: 4  Global Step: 24700   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:10:56,689-Speed 3882.91 samples/sec  Loss 1.2286  LearningRate 0.1365  ProxyLR: 6.8256  Epoch: 4  Global Step: 24710   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:10:59,323-Speed 3888.44 samples/sec  Loss 1.2628  LearningRate 0.1365  ProxyLR: 6.8244  Epoch: 4  Global Step: 24720   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:01,957-Speed 3888.27 samples/sec  Loss 1.2883  LearningRate 0.1365  ProxyLR: 6.8232  Epoch: 4  Global Step: 24730   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:04,591-Speed 3888.79 samples/sec  Loss 1.3297  LearningRate 0.1364  ProxyLR: 6.8221  Epoch: 4  Global Step: 24740   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:07,226-Speed 3887.14 samples/sec  Loss 1.3378  LearningRate 0.1364  ProxyLR: 6.8209  Epoch: 4  Global Step: 24750   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:09,860-Speed 3888.12 samples/sec  Loss 1.3707  LearningRate 0.1364  ProxyLR: 6.8198  Epoch: 4  Global Step: 24760   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:12,494-Speed 3888.46 samples/sec  Loss 1.3202  LearningRate 0.1364  ProxyLR: 6.8186  Epoch: 4  Global Step: 24770   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:15,127-Speed 3889.85 samples/sec  Loss 1.3653  LearningRate 0.1363  ProxyLR: 6.8174  Epoch: 4  Global Step: 24780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:17,760-Speed 3890.54 samples/sec  Loss 1.4049  LearningRate 0.1363  ProxyLR: 6.8163  Epoch: 4  Global Step: 24790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:20,380-Speed 3909.14 samples/sec  Loss 1.3216  LearningRate 0.1363  ProxyLR: 6.8151  Epoch: 4  Global Step: 24800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:23,013-Speed 3889.43 samples/sec  Loss 1.4274  LearningRate 0.1363  ProxyLR: 6.8139  Epoch: 4  Global Step: 24810   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:25,646-Speed 3890.16 samples/sec  Loss 1.4091  LearningRate 0.1363  ProxyLR: 6.8128  Epoch: 4  Global Step: 24820   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:28,279-Speed 3890.37 samples/sec  Loss 1.2891  LearningRate 0.1362  ProxyLR: 6.8116  Epoch: 4  Global Step: 24830   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:30,912-Speed 3889.86 samples/sec  Loss 1.2867  LearningRate 0.1362  ProxyLR: 6.8105  Epoch: 4  Global Step: 24840   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:33,544-Speed 3891.94 samples/sec  Loss 1.2695  LearningRate 0.1362  ProxyLR: 6.8093  Epoch: 4  Global Step: 24850   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:36,175-Speed 3892.66 samples/sec  Loss 1.3785  LearningRate 0.1362  ProxyLR: 6.8081  Epoch: 4  Global Step: 24860   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:38,809-Speed 3889.47 samples/sec  Loss 1.3656  LearningRate 0.1361  ProxyLR: 6.8070  Epoch: 4  Global Step: 24870   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:41,442-Speed 3890.15 samples/sec  Loss 1.4183  LearningRate 0.1361  ProxyLR: 6.8058  Epoch: 4  Global Step: 24880   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:44,074-Speed 3891.52 samples/sec  Loss 1.2098  LearningRate 0.1361  ProxyLR: 6.8047  Epoch: 4  Global Step: 24890   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:11:46,706-Speed 3890.66 samples/sec  Loss 1.2732  LearningRate 0.1361  ProxyLR: 6.8035  Epoch: 4  Global Step: 24900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:49,338-Speed 3891.99 samples/sec  Loss 1.3093  LearningRate 0.1360  ProxyLR: 6.8023  Epoch: 4  Global Step: 24910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:51,970-Speed 3891.64 samples/sec  Loss 1.4796  LearningRate 0.1360  ProxyLR: 6.8012  Epoch: 4  Global Step: 24920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:54,603-Speed 3890.19 samples/sec  Loss 1.4070  LearningRate 0.1360  ProxyLR: 6.8000  Epoch: 4  Global Step: 24930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:57,235-Speed 3890.96 samples/sec  Loss 1.2719  LearningRate 0.1360  ProxyLR: 6.7989  Epoch: 4  Global Step: 24940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:11:59,869-Speed 3889.35 samples/sec  Loss 1.2585  LearningRate 0.1360  ProxyLR: 6.7977  Epoch: 4  Global Step: 24950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:02,502-Speed 3889.40 samples/sec  Loss 1.4125  LearningRate 0.1359  ProxyLR: 6.7965  Epoch: 4  Global Step: 24960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:05,135-Speed 3889.99 samples/sec  Loss 1.4011  LearningRate 0.1359  ProxyLR: 6.7954  Epoch: 4  Global Step: 24970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:07,769-Speed 3889.28 samples/sec  Loss 1.3563  LearningRate 0.1359  ProxyLR: 6.7942  Epoch: 4  Global Step: 24980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:10,401-Speed 3890.46 samples/sec  Loss 1.3954  LearningRate 0.1359  ProxyLR: 6.7931  Epoch: 4  Global Step: 24990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:13,033-Speed 3891.34 samples/sec  Loss 1.3577  LearningRate 0.1358  ProxyLR: 6.7919  Epoch: 4  Global Step: 25000   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:12:15,651-Speed 3912.93 samples/sec  Loss 1.4098  LearningRate 0.1358  ProxyLR: 6.7907  Epoch: 4  Global Step: 25010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:18,281-Speed 3893.81 samples/sec  Loss 1.4254  LearningRate 0.1358  ProxyLR: 6.7896  Epoch: 4  Global Step: 25020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:20,912-Speed 3893.33 samples/sec  Loss 1.3468  LearningRate 0.1358  ProxyLR: 6.7884  Epoch: 4  Global Step: 25030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:23,541-Speed 3895.96 samples/sec  Loss 1.3509  LearningRate 0.1357  ProxyLR: 6.7873  Epoch: 4  Global Step: 25040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:26,169-Speed 3896.89 samples/sec  Loss 1.2818  LearningRate 0.1357  ProxyLR: 6.7861  Epoch: 4  Global Step: 25050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:28,797-Speed 3897.44 samples/sec  Loss 1.2894  LearningRate 0.1357  ProxyLR: 6.7849  Epoch: 4  Global Step: 25060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:31,425-Speed 3897.61 samples/sec  Loss 1.3495  LearningRate 0.1357  ProxyLR: 6.7838  Epoch: 4  Global Step: 25070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:34,055-Speed 3894.35 samples/sec  Loss 1.3076  LearningRate 0.1357  ProxyLR: 6.7826  Epoch: 4  Global Step: 25080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:36,683-Speed 3898.50 samples/sec  Loss 1.4380  LearningRate 0.1356  ProxyLR: 6.7815  Epoch: 4  Global Step: 25090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:39,310-Speed 3897.85 samples/sec  Loss 1.5038  LearningRate 0.1356  ProxyLR: 6.7803  Epoch: 4  Global Step: 25100   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:41,924-Speed 3918.40 samples/sec  Loss 1.3905  LearningRate 0.1356  ProxyLR: 6.7791  Epoch: 4  Global Step: 25110   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:44,553-Speed 3897.24 samples/sec  Loss 1.3755  LearningRate 0.1356  ProxyLR: 6.7780  Epoch: 4  Global Step: 25120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:47,182-Speed 3895.02 samples/sec  Loss 1.4302  LearningRate 0.1355  ProxyLR: 6.7768  Epoch: 4  Global Step: 25130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:49,811-Speed 3896.74 samples/sec  Loss 1.2696  LearningRate 0.1355  ProxyLR: 6.7757  Epoch: 4  Global Step: 25140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:52,438-Speed 3898.58 samples/sec  Loss 1.3922  LearningRate 0.1355  ProxyLR: 6.7745  Epoch: 4  Global Step: 25150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:55,064-Speed 3899.48 samples/sec  Loss 1.4188  LearningRate 0.1355  ProxyLR: 6.7734  Epoch: 4  Global Step: 25160   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:12:57,689-Speed 3901.77 samples/sec  Loss 1.3592  LearningRate 0.1354  ProxyLR: 6.7722  Epoch: 4  Global Step: 25170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:00,316-Speed 3899.80 samples/sec  Loss 1.5321  LearningRate 0.1354  ProxyLR: 6.7710  Epoch: 4  Global Step: 25180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:02,942-Speed 3900.03 samples/sec  Loss 1.4057  LearningRate 0.1354  ProxyLR: 6.7699  Epoch: 4  Global Step: 25190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:05,567-Speed 3902.48 samples/sec  Loss 1.3508  LearningRate 0.1354  ProxyLR: 6.7687  Epoch: 4  Global Step: 25200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:08,179-Speed 3920.92 samples/sec  Loss 1.4323  LearningRate 0.1354  ProxyLR: 6.7676  Epoch: 4  Global Step: 25210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:10,806-Speed 3899.32 samples/sec  Loss 1.3814  LearningRate 0.1353  ProxyLR: 6.7664  Epoch: 4  Global Step: 25220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:13,436-Speed 3894.24 samples/sec  Loss 1.3821  LearningRate 0.1353  ProxyLR: 6.7653  Epoch: 4  Global Step: 25230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:16,052-Speed 3915.39 samples/sec  Loss 1.3233  LearningRate 0.1353  ProxyLR: 6.7641  Epoch: 4  Global Step: 25240   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:18,679-Speed 3899.07 samples/sec  Loss 1.3612  LearningRate 0.1353  ProxyLR: 6.7629  Epoch: 4  Global Step: 25250   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:21,307-Speed 3897.30 samples/sec  Loss 1.4442  LearningRate 0.1352  ProxyLR: 6.7618  Epoch: 4  Global Step: 25260   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:23,936-Speed 3895.82 samples/sec  Loss 1.4680  LearningRate 0.1352  ProxyLR: 6.7606  Epoch: 4  Global Step: 25270   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:26,562-Speed 3899.83 samples/sec  Loss 1.3968  LearningRate 0.1352  ProxyLR: 6.7595  Epoch: 4  Global Step: 25280   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:29,189-Speed 3898.51 samples/sec  Loss 1.4701  LearningRate 0.1352  ProxyLR: 6.7583  Epoch: 4  Global Step: 25290   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:31,816-Speed 3899.40 samples/sec  Loss 1.3713  LearningRate 0.1351  ProxyLR: 6.7572  Epoch: 4  Global Step: 25300   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:34,443-Speed 3899.19 samples/sec  Loss 1.3667  LearningRate 0.1351  ProxyLR: 6.7560  Epoch: 4  Global Step: 25310   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:37,070-Speed 3898.88 samples/sec  Loss 1.5019  LearningRate 0.1351  ProxyLR: 6.7548  Epoch: 4  Global Step: 25320   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:39,697-Speed 3899.38 samples/sec  Loss 1.4080  LearningRate 0.1351  ProxyLR: 6.7537  Epoch: 4  Global Step: 25330   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:13:42,324-Speed 3898.35 samples/sec  Loss 1.3511  LearningRate 0.1351  ProxyLR: 6.7525  Epoch: 4  Global Step: 25340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:44,952-Speed 3898.31 samples/sec  Loss 1.3915  LearningRate 0.1350  ProxyLR: 6.7514  Epoch: 4  Global Step: 25350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:47,579-Speed 3897.99 samples/sec  Loss 1.3418  LearningRate 0.1350  ProxyLR: 6.7502  Epoch: 4  Global Step: 25360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:50,206-Speed 3898.60 samples/sec  Loss 1.3547  LearningRate 0.1350  ProxyLR: 6.7491  Epoch: 4  Global Step: 25370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:52,834-Speed 3898.39 samples/sec  Loss 1.4388  LearningRate 0.1350  ProxyLR: 6.7479  Epoch: 4  Global Step: 25380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:55,462-Speed 3897.26 samples/sec  Loss 1.5327  LearningRate 0.1349  ProxyLR: 6.7467  Epoch: 4  Global Step: 25390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:13:58,090-Speed 3897.13 samples/sec  Loss 1.3930  LearningRate 0.1349  ProxyLR: 6.7456  Epoch: 4  Global Step: 25400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:00,718-Speed 3897.94 samples/sec  Loss 1.3806  LearningRate 0.1349  ProxyLR: 6.7444  Epoch: 4  Global Step: 25410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:03,334-Speed 3915.71 samples/sec  Loss 1.3776  LearningRate 0.1349  ProxyLR: 6.7433  Epoch: 4  Global Step: 25420   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:05,961-Speed 3897.63 samples/sec  Loss 1.2845  LearningRate 0.1348  ProxyLR: 6.7421  Epoch: 4  Global Step: 25430   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:08,591-Speed 3895.02 samples/sec  Loss 1.4047  LearningRate 0.1348  ProxyLR: 6.7410  Epoch: 4  Global Step: 25440   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:11,222-Speed 3892.79 samples/sec  Loss 1.4244  LearningRate 0.1348  ProxyLR: 6.7398  Epoch: 4  Global Step: 25450   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:13,851-Speed 3895.85 samples/sec  Loss 1.6468  LearningRate 0.1348  ProxyLR: 6.7387  Epoch: 4  Global Step: 25460   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:16,478-Speed 3898.65 samples/sec  Loss 1.4394  LearningRate 0.1348  ProxyLR: 6.7375  Epoch: 4  Global Step: 25470   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:19,104-Speed 3901.04 samples/sec  Loss 1.4421  LearningRate 0.1347  ProxyLR: 6.7364  Epoch: 4  Global Step: 25480   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:21,730-Speed 3900.84 samples/sec  Loss 1.3711  LearningRate 0.1347  ProxyLR: 6.7352  Epoch: 4  Global Step: 25490   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:24,356-Speed 3900.42 samples/sec  Loss 1.4100  LearningRate 0.1347  ProxyLR: 6.7340  Epoch: 4  Global Step: 25500   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:26,982-Speed 3899.83 samples/sec  Loss 1.4583  LearningRate 0.1347  ProxyLR: 6.7329  Epoch: 4  Global Step: 25510   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:14:29,610-Speed 3897.21 samples/sec  Loss 1.4268  LearningRate 0.1346  ProxyLR: 6.7317  Epoch: 4  Global Step: 25520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:32,238-Speed 3897.83 samples/sec  Loss 1.4005  LearningRate 0.1346  ProxyLR: 6.7306  Epoch: 4  Global Step: 25530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:34,865-Speed 3898.72 samples/sec  Loss 1.4701  LearningRate 0.1346  ProxyLR: 6.7294  Epoch: 4  Global Step: 25540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:37,494-Speed 3895.56 samples/sec  Loss 1.3287  LearningRate 0.1346  ProxyLR: 6.7283  Epoch: 4  Global Step: 25550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:40,122-Speed 3898.29 samples/sec  Loss 1.4522  LearningRate 0.1345  ProxyLR: 6.7271  Epoch: 4  Global Step: 25560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:42,748-Speed 3899.63 samples/sec  Loss 1.3178  LearningRate 0.1345  ProxyLR: 6.7260  Epoch: 4  Global Step: 25570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:45,374-Speed 3900.49 samples/sec  Loss 1.2837  LearningRate 0.1345  ProxyLR: 6.7248  Epoch: 4  Global Step: 25580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:48,001-Speed 3898.86 samples/sec  Loss 1.4595  LearningRate 0.1345  ProxyLR: 6.7237  Epoch: 4  Global Step: 25590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:50,628-Speed 3900.01 samples/sec  Loss 1.2694  LearningRate 0.1345  ProxyLR: 6.7225  Epoch: 4  Global Step: 25600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:53,257-Speed 3895.50 samples/sec  Loss 1.3153  LearningRate 0.1344  ProxyLR: 6.7213  Epoch: 4  Global Step: 25610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:55,874-Speed 3913.72 samples/sec  Loss 1.3911  LearningRate 0.1344  ProxyLR: 6.7202  Epoch: 4  Global Step: 25620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:14:58,501-Speed 3898.88 samples/sec  Loss 1.4159  LearningRate 0.1344  ProxyLR: 6.7190  Epoch: 4  Global Step: 25630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:01,130-Speed 3896.15 samples/sec  Loss 1.4239  LearningRate 0.1344  ProxyLR: 6.7179  Epoch: 4  Global Step: 25640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:03,756-Speed 3900.72 samples/sec  Loss 1.3668  LearningRate 0.1343  ProxyLR: 6.7167  Epoch: 4  Global Step: 25650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:06,382-Speed 3899.97 samples/sec  Loss 1.4098  LearningRate 0.1343  ProxyLR: 6.7156  Epoch: 4  Global Step: 25660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:09,010-Speed 3897.89 samples/sec  Loss 1.4379  LearningRate 0.1343  ProxyLR: 6.7144  Epoch: 4  Global Step: 25670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:11,636-Speed 3900.25 samples/sec  Loss 1.3955  LearningRate 0.1343  ProxyLR: 6.7133  Epoch: 4  Global Step: 25680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:14,262-Speed 3900.06 samples/sec  Loss 1.3769  LearningRate 0.1342  ProxyLR: 6.7121  Epoch: 4  Global Step: 25690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:16,890-Speed 3896.95 samples/sec  Loss 1.5152  LearningRate 0.1342  ProxyLR: 6.7110  Epoch: 4  Global Step: 25700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:19,518-Speed 3897.96 samples/sec  Loss 1.4488  LearningRate 0.1342  ProxyLR: 6.7098  Epoch: 4  Global Step: 25710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:22,132-Speed 3918.21 samples/sec  Loss 1.4538  LearningRate 0.1342  ProxyLR: 6.7087  Epoch: 4  Global Step: 25720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:24,756-Speed 3903.51 samples/sec  Loss 1.4072  LearningRate 0.1342  ProxyLR: 6.7075  Epoch: 4  Global Step: 25730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:27,383-Speed 3899.27 samples/sec  Loss 1.4604  LearningRate 0.1341  ProxyLR: 6.7064  Epoch: 4  Global Step: 25740   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:30,010-Speed 3898.08 samples/sec  Loss 1.5137  LearningRate 0.1341  ProxyLR: 6.7052  Epoch: 4  Global Step: 25750   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:32,638-Speed 3898.05 samples/sec  Loss 1.4707  LearningRate 0.1341  ProxyLR: 6.7041  Epoch: 4  Global Step: 25760   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:35,265-Speed 3899.13 samples/sec  Loss 1.3370  LearningRate 0.1341  ProxyLR: 6.7029  Epoch: 4  Global Step: 25770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:37,890-Speed 3901.17 samples/sec  Loss 1.5717  LearningRate 0.1340  ProxyLR: 6.7018  Epoch: 4  Global Step: 25780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:40,516-Speed 3900.93 samples/sec  Loss 1.3640  LearningRate 0.1340  ProxyLR: 6.7006  Epoch: 4  Global Step: 25790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:43,142-Speed 3899.82 samples/sec  Loss 1.4225  LearningRate 0.1340  ProxyLR: 6.6994  Epoch: 4  Global Step: 25800   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:45,768-Speed 3901.43 samples/sec  Loss 1.4831  LearningRate 0.1340  ProxyLR: 6.6983  Epoch: 4  Global Step: 25810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:48,381-Speed 3919.38 samples/sec  Loss 1.4748  LearningRate 0.1339  ProxyLR: 6.6971  Epoch: 4  Global Step: 25820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:51,007-Speed 3900.07 samples/sec  Loss 1.3747  LearningRate 0.1339  ProxyLR: 6.6960  Epoch: 4  Global Step: 25830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:53,637-Speed 3895.49 samples/sec  Loss 1.4864  LearningRate 0.1339  ProxyLR: 6.6948  Epoch: 4  Global Step: 25840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:56,266-Speed 3895.62 samples/sec  Loss 1.4781  LearningRate 0.1339  ProxyLR: 6.6937  Epoch: 4  Global Step: 25850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:15:58,881-Speed 3916.38 samples/sec  Loss 1.4451  LearningRate 0.1339  ProxyLR: 6.6925  Epoch: 4  Global Step: 25860   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:01,510-Speed 3895.80 samples/sec  Loss 1.4826  LearningRate 0.1338  ProxyLR: 6.6914  Epoch: 4  Global Step: 25870   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:04,140-Speed 3894.44 samples/sec  Loss 1.4728  LearningRate 0.1338  ProxyLR: 6.6902  Epoch: 4  Global Step: 25880   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:06,770-Speed 3895.05 samples/sec  Loss 1.4808  LearningRate 0.1338  ProxyLR: 6.6891  Epoch: 4  Global Step: 25890   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:09,403-Speed 3889.14 samples/sec  Loss 1.4881  LearningRate 0.1338  ProxyLR: 6.6879  Epoch: 4  Global Step: 25900   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:12,039-Speed 3885.72 samples/sec  Loss 1.4867  LearningRate 0.1337  ProxyLR: 6.6868  Epoch: 4  Global Step: 25910   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:14,674-Speed 3887.99 samples/sec  Loss 1.3346  LearningRate 0.1337  ProxyLR: 6.6856  Epoch: 4  Global Step: 25920   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:17,309-Speed 3886.70 samples/sec  Loss 1.4555  LearningRate 0.1337  ProxyLR: 6.6845  Epoch: 4  Global Step: 25930   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:19,945-Speed 3885.15 samples/sec  Loss 1.5661  LearningRate 0.1337  ProxyLR: 6.6833  Epoch: 4  Global Step: 25940   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:22,579-Speed 3888.35 samples/sec  Loss 1.4924  LearningRate 0.1336  ProxyLR: 6.6822  Epoch: 4  Global Step: 25950   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:25,215-Speed 3886.01 samples/sec  Loss 1.5731  LearningRate 0.1336  ProxyLR: 6.6810  Epoch: 4  Global Step: 25960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:27,849-Speed 3888.74 samples/sec  Loss 1.6464  LearningRate 0.1336  ProxyLR: 6.6799  Epoch: 4  Global Step: 25970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:30,482-Speed 3889.79 samples/sec  Loss 1.4025  LearningRate 0.1336  ProxyLR: 6.6787  Epoch: 4  Global Step: 25980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:33,117-Speed 3887.92 samples/sec  Loss 1.5795  LearningRate 0.1336  ProxyLR: 6.6776  Epoch: 4  Global Step: 25990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:35,752-Speed 3886.77 samples/sec  Loss 1.5658  LearningRate 0.1335  ProxyLR: 6.6764  Epoch: 4  Global Step: 26000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:38,387-Speed 3887.79 samples/sec  Loss 1.5212  LearningRate 0.1335  ProxyLR: 6.6753  Epoch: 4  Global Step: 26010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:41,021-Speed 3888.14 samples/sec  Loss 1.5127  LearningRate 0.1335  ProxyLR: 6.6741  Epoch: 4  Global Step: 26020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:43,656-Speed 3886.55 samples/sec  Loss 1.5087  LearningRate 0.1335  ProxyLR: 6.6730  Epoch: 4  Global Step: 26030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:46,295-Speed 3880.85 samples/sec  Loss 1.5522  LearningRate 0.1334  ProxyLR: 6.6718  Epoch: 4  Global Step: 26040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:48,934-Speed 3881.52 samples/sec  Loss 1.4882  LearningRate 0.1334  ProxyLR: 6.6707  Epoch: 4  Global Step: 26050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:16:51,575-Speed 3879.20 samples/sec  Loss 1.4472  LearningRate 0.1334  ProxyLR: 6.6695  Epoch: 4  Global Step: 26060   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:16:54,186-Speed 3922.16 samples/sec  Loss 1.3556  LearningRate 0.1334  ProxyLR: 6.6684  Epoch: 4  Global Step: 26070   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:56,826-Speed 3880.10 samples/sec  Loss 1.4193  LearningRate 0.1333  ProxyLR: 6.6672  Epoch: 4  Global Step: 26080   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:16:59,464-Speed 3882.97 samples/sec  Loss 1.4442  LearningRate 0.1333  ProxyLR: 6.6661  Epoch: 4  Global Step: 26090   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:02,101-Speed 3883.32 samples/sec  Loss 1.4964  LearningRate 0.1333  ProxyLR: 6.6649  Epoch: 4  Global Step: 26100   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:04,740-Speed 3882.02 samples/sec  Loss 1.5330  LearningRate 0.1333  ProxyLR: 6.6638  Epoch: 4  Global Step: 26110   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:07,377-Speed 3883.76 samples/sec  Loss 1.5162  LearningRate 0.1333  ProxyLR: 6.6626  Epoch: 4  Global Step: 26120   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:10,015-Speed 3882.32 samples/sec  Loss 1.5239  LearningRate 0.1332  ProxyLR: 6.6615  Epoch: 4  Global Step: 26130   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:12,652-Speed 3884.16 samples/sec  Loss 1.3757  LearningRate 0.1332  ProxyLR: 6.6604  Epoch: 4  Global Step: 26140   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:15,289-Speed 3884.36 samples/sec  Loss 1.4393  LearningRate 0.1332  ProxyLR: 6.6592  Epoch: 4  Global Step: 26150   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:17,926-Speed 3884.31 samples/sec  Loss 1.4764  LearningRate 0.1332  ProxyLR: 6.6581  Epoch: 4  Global Step: 26160   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:20,561-Speed 3886.31 samples/sec  Loss 1.5030  LearningRate 0.1331  ProxyLR: 6.6569  Epoch: 4  Global Step: 26170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:23,193-Speed 3891.97 samples/sec  Loss 1.5227  LearningRate 0.1331  ProxyLR: 6.6558  Epoch: 4  Global Step: 26180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:25,825-Speed 3891.20 samples/sec  Loss 1.4834  LearningRate 0.1331  ProxyLR: 6.6546  Epoch: 4  Global Step: 26190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:28,457-Speed 3892.65 samples/sec  Loss 1.5061  LearningRate 0.1331  ProxyLR: 6.6535  Epoch: 4  Global Step: 26200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:31,087-Speed 3894.18 samples/sec  Loss 1.4422  LearningRate 0.1330  ProxyLR: 6.6523  Epoch: 4  Global Step: 26210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:33,718-Speed 3893.15 samples/sec  Loss 1.5995  LearningRate 0.1330  ProxyLR: 6.6512  Epoch: 4  Global Step: 26220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:36,348-Speed 3894.10 samples/sec  Loss 1.4449  LearningRate 0.1330  ProxyLR: 6.6500  Epoch: 4  Global Step: 26230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:38,978-Speed 3894.24 samples/sec  Loss 1.4399  LearningRate 0.1330  ProxyLR: 6.6489  Epoch: 4  Global Step: 26240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:41,608-Speed 3894.80 samples/sec  Loss 1.5410  LearningRate 0.1330  ProxyLR: 6.6477  Epoch: 4  Global Step: 26250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:44,239-Speed 3893.11 samples/sec  Loss 1.6318  LearningRate 0.1329  ProxyLR: 6.6466  Epoch: 4  Global Step: 26260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:17:46,856-Speed 3913.89 samples/sec  Loss 1.4834  LearningRate 0.1329  ProxyLR: 6.6454  Epoch: 4  Global Step: 26270   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:49,487-Speed 3893.30 samples/sec  Loss 1.4509  LearningRate 0.1329  ProxyLR: 6.6443  Epoch: 4  Global Step: 26280   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:52,118-Speed 3892.79 samples/sec  Loss 1.4623  LearningRate 0.1329  ProxyLR: 6.6431  Epoch: 4  Global Step: 26290   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:54,747-Speed 3895.86 samples/sec  Loss 1.4745  LearningRate 0.1328  ProxyLR: 6.6420  Epoch: 4  Global Step: 26300   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:17:57,375-Speed 3896.71 samples/sec  Loss 1.3839  LearningRate 0.1328  ProxyLR: 6.6408  Epoch: 4  Global Step: 26310   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:18:00,006-Speed 3893.81 samples/sec  Loss 1.4994  LearningRate 0.1328  ProxyLR: 6.6397  Epoch: 4  Global Step: 26320   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:18:02,635-Speed 3895.10 samples/sec  Loss 1.5797  LearningRate 0.1328  ProxyLR: 6.6386  Epoch: 4  Global Step: 26330   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:18:05,266-Speed 3893.63 samples/sec  Loss 1.5669  LearningRate 0.1327  ProxyLR: 6.6374  Epoch: 4  Global Step: 26340   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:18:07,896-Speed 3894.99 samples/sec  Loss 1.4669  LearningRate 0.1327  ProxyLR: 6.6363  Epoch: 4  Global Step: 26350   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:18:10,526-Speed 3894.42 samples/sec  Loss 1.4058  LearningRate 0.1327  ProxyLR: 6.6351  Epoch: 4  Global Step: 26360   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:18:13,155-Speed 3895.71 samples/sec  Loss 1.5083  LearningRate 0.1327  ProxyLR: 6.6340  Epoch: 4  Global Step: 26370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:15,785-Speed 3894.70 samples/sec  Loss 1.5176  LearningRate 0.1327  ProxyLR: 6.6328  Epoch: 4  Global Step: 26380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:18,413-Speed 3896.25 samples/sec  Loss 1.5297  LearningRate 0.1326  ProxyLR: 6.6317  Epoch: 4  Global Step: 26390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:21,044-Speed 3894.08 samples/sec  Loss 1.4010  LearningRate 0.1326  ProxyLR: 6.6305  Epoch: 4  Global Step: 26400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:23,674-Speed 3894.27 samples/sec  Loss 1.4266  LearningRate 0.1326  ProxyLR: 6.6294  Epoch: 4  Global Step: 26410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:26,304-Speed 3893.88 samples/sec  Loss 1.5422  LearningRate 0.1326  ProxyLR: 6.6282  Epoch: 4  Global Step: 26420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:28,934-Speed 3894.37 samples/sec  Loss 1.5207  LearningRate 0.1325  ProxyLR: 6.6271  Epoch: 4  Global Step: 26430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:31,565-Speed 3893.79 samples/sec  Loss 1.4478  LearningRate 0.1325  ProxyLR: 6.6259  Epoch: 4  Global Step: 26440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:34,194-Speed 3895.89 samples/sec  Loss 1.5947  LearningRate 0.1325  ProxyLR: 6.6248  Epoch: 4  Global Step: 26450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:36,824-Speed 3894.87 samples/sec  Loss 1.4674  LearningRate 0.1325  ProxyLR: 6.6237  Epoch: 4  Global Step: 26460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:39,439-Speed 3915.69 samples/sec  Loss 1.5559  LearningRate 0.1325  ProxyLR: 6.6225  Epoch: 4  Global Step: 26470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:42,069-Speed 3894.73 samples/sec  Loss 1.5036  LearningRate 0.1324  ProxyLR: 6.6214  Epoch: 4  Global Step: 26480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:44,699-Speed 3894.88 samples/sec  Loss 1.5522  LearningRate 0.1324  ProxyLR: 6.6202  Epoch: 4  Global Step: 26490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:47,328-Speed 3896.55 samples/sec  Loss 1.4091  LearningRate 0.1324  ProxyLR: 6.6191  Epoch: 4  Global Step: 26500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:49,957-Speed 3895.26 samples/sec  Loss 1.5357  LearningRate 0.1324  ProxyLR: 6.6179  Epoch: 4  Global Step: 26510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:52,588-Speed 3892.81 samples/sec  Loss 1.4913  LearningRate 0.1323  ProxyLR: 6.6168  Epoch: 4  Global Step: 26520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:55,219-Speed 3893.65 samples/sec  Loss 1.5438  LearningRate 0.1323  ProxyLR: 6.6156  Epoch: 4  Global Step: 26530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:18:57,851-Speed 3891.16 samples/sec  Loss 1.4841  LearningRate 0.1323  ProxyLR: 6.6145  Epoch: 4  Global Step: 26540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:00,484-Speed 3890.20 samples/sec  Loss 1.5123  LearningRate 0.1323  ProxyLR: 6.6134  Epoch: 4  Global Step: 26550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:03,117-Speed 3889.37 samples/sec  Loss 1.4900  LearningRate 0.1322  ProxyLR: 6.6122  Epoch: 4  Global Step: 26560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:05,748-Speed 3892.91 samples/sec  Loss 1.6339  LearningRate 0.1322  ProxyLR: 6.6111  Epoch: 4  Global Step: 26570   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:19:08,354-Speed 3930.44 samples/sec  Loss 1.6029  LearningRate 0.1322  ProxyLR: 6.6099  Epoch: 4  Global Step: 26580   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:10,988-Speed 3888.64 samples/sec  Loss 1.4744  LearningRate 0.1322  ProxyLR: 6.6088  Epoch: 4  Global Step: 26590   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:13,621-Speed 3890.53 samples/sec  Loss 1.4146  LearningRate 0.1322  ProxyLR: 6.6076  Epoch: 4  Global Step: 26600   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:16,255-Speed 3888.46 samples/sec  Loss 1.5435  LearningRate 0.1321  ProxyLR: 6.6065  Epoch: 4  Global Step: 26610   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:18,887-Speed 3891.03 samples/sec  Loss 1.4630  LearningRate 0.1321  ProxyLR: 6.6053  Epoch: 4  Global Step: 26620   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:21,521-Speed 3888.55 samples/sec  Loss 1.4272  LearningRate 0.1321  ProxyLR: 6.6042  Epoch: 4  Global Step: 26630   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:24,157-Speed 3886.45 samples/sec  Loss 1.5090  LearningRate 0.1321  ProxyLR: 6.6031  Epoch: 4  Global Step: 26640   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:26,794-Speed 3884.36 samples/sec  Loss 1.5399  LearningRate 0.1320  ProxyLR: 6.6019  Epoch: 4  Global Step: 26650   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:29,428-Speed 3887.67 samples/sec  Loss 1.5298  LearningRate 0.1320  ProxyLR: 6.6008  Epoch: 4  Global Step: 26660   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:32,063-Speed 3887.09 samples/sec  Loss 1.4832  LearningRate 0.1320  ProxyLR: 6.5996  Epoch: 4  Global Step: 26670   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:34,699-Speed 3886.46 samples/sec  Loss 1.4779  LearningRate 0.1320  ProxyLR: 6.5985  Epoch: 4  Global Step: 26680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:37,336-Speed 3884.12 samples/sec  Loss 1.4902  LearningRate 0.1319  ProxyLR: 6.5973  Epoch: 4  Global Step: 26690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:39,971-Speed 3886.59 samples/sec  Loss 1.4734  LearningRate 0.1319  ProxyLR: 6.5962  Epoch: 4  Global Step: 26700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:42,608-Speed 3884.10 samples/sec  Loss 1.6442  LearningRate 0.1319  ProxyLR: 6.5951  Epoch: 4  Global Step: 26710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:19:45,229-Speed 3907.50 samples/sec  Loss 1.5572  LearningRate 0.1319  ProxyLR: 6.5939  Epoch: 4  Global Step: 26720   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:47,864-Speed 3887.65 samples/sec  Loss 1.4288  LearningRate 0.1319  ProxyLR: 6.5928  Epoch: 4  Global Step: 26730   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:50,499-Speed 3886.32 samples/sec  Loss 1.5449  LearningRate 0.1318  ProxyLR: 6.5916  Epoch: 4  Global Step: 26740   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:53,134-Speed 3887.21 samples/sec  Loss 1.5251  LearningRate 0.1318  ProxyLR: 6.5905  Epoch: 4  Global Step: 26750   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:55,768-Speed 3889.36 samples/sec  Loss 1.5063  LearningRate 0.1318  ProxyLR: 6.5894  Epoch: 4  Global Step: 26760   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:19:58,404-Speed 3884.88 samples/sec  Loss 1.4742  LearningRate 0.1318  ProxyLR: 6.5882  Epoch: 4  Global Step: 26770   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:01,037-Speed 3890.69 samples/sec  Loss 1.4047  LearningRate 0.1317  ProxyLR: 6.5871  Epoch: 4  Global Step: 26780   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:03,670-Speed 3889.79 samples/sec  Loss 1.4618  LearningRate 0.1317  ProxyLR: 6.5859  Epoch: 4  Global Step: 26790   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:06,301-Speed 3892.56 samples/sec  Loss 1.4832  LearningRate 0.1317  ProxyLR: 6.5848  Epoch: 4  Global Step: 26800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:08,934-Speed 3891.37 samples/sec  Loss 1.6705  LearningRate 0.1317  ProxyLR: 6.5836  Epoch: 4  Global Step: 26810   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:11,566-Speed 3890.83 samples/sec  Loss 1.6331  LearningRate 0.1316  ProxyLR: 6.5825  Epoch: 4  Global Step: 26820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:14,201-Speed 3887.00 samples/sec  Loss 1.5408  LearningRate 0.1316  ProxyLR: 6.5814  Epoch: 4  Global Step: 26830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:16,834-Speed 3889.84 samples/sec  Loss 1.4981  LearningRate 0.1316  ProxyLR: 6.5802  Epoch: 4  Global Step: 26840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:19,468-Speed 3889.04 samples/sec  Loss 1.5098  LearningRate 0.1316  ProxyLR: 6.5791  Epoch: 4  Global Step: 26850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:22,102-Speed 3888.23 samples/sec  Loss 1.5336  LearningRate 0.1316  ProxyLR: 6.5779  Epoch: 4  Global Step: 26860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:24,736-Speed 3888.99 samples/sec  Loss 1.4955  LearningRate 0.1315  ProxyLR: 6.5768  Epoch: 4  Global Step: 26870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:27,370-Speed 3888.68 samples/sec  Loss 1.5686  LearningRate 0.1315  ProxyLR: 6.5757  Epoch: 4  Global Step: 26880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:30,003-Speed 3889.67 samples/sec  Loss 1.5223  LearningRate 0.1315  ProxyLR: 6.5745  Epoch: 4  Global Step: 26890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:32,637-Speed 3888.41 samples/sec  Loss 1.5558  LearningRate 0.1315  ProxyLR: 6.5734  Epoch: 4  Global Step: 26900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:35,272-Speed 3887.71 samples/sec  Loss 1.5211  LearningRate 0.1314  ProxyLR: 6.5722  Epoch: 4  Global Step: 26910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:37,905-Speed 3889.62 samples/sec  Loss 1.5228  LearningRate 0.1314  ProxyLR: 6.5711  Epoch: 4  Global Step: 26920   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:20:40,527-Speed 3906.39 samples/sec  Loss 1.5626  LearningRate 0.1314  ProxyLR: 6.5699  Epoch: 4  Global Step: 26930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:43,161-Speed 3888.70 samples/sec  Loss 1.5541  LearningRate 0.1314  ProxyLR: 6.5688  Epoch: 4  Global Step: 26940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:45,796-Speed 3886.47 samples/sec  Loss 1.4885  LearningRate 0.1314  ProxyLR: 6.5677  Epoch: 4  Global Step: 26950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:48,430-Speed 3889.07 samples/sec  Loss 1.5251  LearningRate 0.1313  ProxyLR: 6.5665  Epoch: 4  Global Step: 26960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:20:51,048-Speed 3911.53 samples/sec  Loss 1.6518  LearningRate 0.1313  ProxyLR: 6.5654  Epoch: 4  Global Step: 26970   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:53,680-Speed 3891.35 samples/sec  Loss 1.6220  LearningRate 0.1313  ProxyLR: 6.5642  Epoch: 4  Global Step: 26980   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:56,313-Speed 3890.40 samples/sec  Loss 1.5376  LearningRate 0.1313  ProxyLR: 6.5631  Epoch: 4  Global Step: 26990   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:20:58,944-Speed 3892.85 samples/sec  Loss 1.5660  LearningRate 0.1312  ProxyLR: 6.5620  Epoch: 4  Global Step: 27000   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:01,577-Speed 3890.14 samples/sec  Loss 1.5197  LearningRate 0.1312  ProxyLR: 6.5608  Epoch: 4  Global Step: 27010   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:04,212-Speed 3887.45 samples/sec  Loss 1.4637  LearningRate 0.1312  ProxyLR: 6.5597  Epoch: 4  Global Step: 27020   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:06,844-Speed 3891.77 samples/sec  Loss 1.5016  LearningRate 0.1312  ProxyLR: 6.5586  Epoch: 4  Global Step: 27030   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:09,477-Speed 3890.44 samples/sec  Loss 1.5230  LearningRate 0.1311  ProxyLR: 6.5574  Epoch: 4  Global Step: 27040   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:12,109-Speed 3891.82 samples/sec  Loss 1.5559  LearningRate 0.1311  ProxyLR: 6.5563  Epoch: 4  Global Step: 27050   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:14,742-Speed 3888.89 samples/sec  Loss 1.5149  LearningRate 0.1311  ProxyLR: 6.5551  Epoch: 4  Global Step: 27060   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:17,375-Speed 3890.20 samples/sec  Loss 1.4239  LearningRate 0.1311  ProxyLR: 6.5540  Epoch: 4  Global Step: 27070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:21:20,008-Speed 3890.61 samples/sec  Loss 1.4853  LearningRate 0.1311  ProxyLR: 6.5529  Epoch: 4  Global Step: 27080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:21:22,626-Speed 3912.06 samples/sec  Loss 1.6049  LearningRate 0.1310  ProxyLR: 6.5517  Epoch: 4  Global Step: 27090   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:25,259-Speed 3889.60 samples/sec  Loss 1.4341  LearningRate 0.1310  ProxyLR: 6.5506  Epoch: 4  Global Step: 27100   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:27,892-Speed 3890.11 samples/sec  Loss 1.5202  LearningRate 0.1310  ProxyLR: 6.5494  Epoch: 4  Global Step: 27110   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:30,527-Speed 3887.11 samples/sec  Loss 1.4771  LearningRate 0.1310  ProxyLR: 6.5483  Epoch: 4  Global Step: 27120   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:33,159-Speed 3892.35 samples/sec  Loss 1.4728  LearningRate 0.1309  ProxyLR: 6.5472  Epoch: 4  Global Step: 27130   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:35,791-Speed 3891.16 samples/sec  Loss 1.5530  LearningRate 0.1309  ProxyLR: 6.5460  Epoch: 4  Global Step: 27140   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:38,422-Speed 3893.02 samples/sec  Loss 1.5506  LearningRate 0.1309  ProxyLR: 6.5449  Epoch: 4  Global Step: 27150   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:41,054-Speed 3890.97 samples/sec  Loss 1.5383  LearningRate 0.1309  ProxyLR: 6.5437  Epoch: 4  Global Step: 27160   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:43,686-Speed 3891.57 samples/sec  Loss 1.5681  LearningRate 0.1309  ProxyLR: 6.5426  Epoch: 4  Global Step: 27170   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:46,320-Speed 3889.24 samples/sec  Loss 1.5349  LearningRate 0.1308  ProxyLR: 6.5415  Epoch: 4  Global Step: 27180   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:21:48,953-Speed 3890.53 samples/sec  Loss 1.4709  LearningRate 0.1308  ProxyLR: 6.5403  Epoch: 4  Global Step: 27190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:21:51,585-Speed 3890.60 samples/sec  Loss 1.4407  LearningRate 0.1308  ProxyLR: 6.5392  Epoch: 4  Global Step: 27200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:21:54,216-Speed 3893.37 samples/sec  Loss 1.4467  LearningRate 0.1308  ProxyLR: 6.5381  Epoch: 4  Global Step: 27210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:21:56,845-Speed 3895.69 samples/sec  Loss 1.4317  LearningRate 0.1307  ProxyLR: 6.5369  Epoch: 4  Global Step: 27220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:21:59,476-Speed 3893.25 samples/sec  Loss 1.6820  LearningRate 0.1307  ProxyLR: 6.5358  Epoch: 4  Global Step: 27230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:02,106-Speed 3894.37 samples/sec  Loss 1.5339  LearningRate 0.1307  ProxyLR: 6.5346  Epoch: 4  Global Step: 27240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:04,737-Speed 3894.35 samples/sec  Loss 1.5221  LearningRate 0.1307  ProxyLR: 6.5335  Epoch: 4  Global Step: 27250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:07,367-Speed 3894.13 samples/sec  Loss 1.5778  LearningRate 0.1306  ProxyLR: 6.5324  Epoch: 4  Global Step: 27260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:09,997-Speed 3894.25 samples/sec  Loss 1.5003  LearningRate 0.1306  ProxyLR: 6.5312  Epoch: 4  Global Step: 27270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:12,627-Speed 3895.06 samples/sec  Loss 1.4538  LearningRate 0.1306  ProxyLR: 6.5301  Epoch: 4  Global Step: 27280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:15,256-Speed 3895.07 samples/sec  Loss 1.5132  LearningRate 0.1306  ProxyLR: 6.5290  Epoch: 4  Global Step: 27290   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:22:17,873-Speed 3915.00 samples/sec  Loss 1.5036  LearningRate 0.1306  ProxyLR: 6.5278  Epoch: 4  Global Step: 27300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:20,503-Speed 3893.86 samples/sec  Loss 1.5383  LearningRate 0.1305  ProxyLR: 6.5267  Epoch: 4  Global Step: 27310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:23,132-Speed 3895.41 samples/sec  Loss 1.4824  LearningRate 0.1305  ProxyLR: 6.5255  Epoch: 4  Global Step: 27320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:25,760-Speed 3897.91 samples/sec  Loss 1.6889  LearningRate 0.1305  ProxyLR: 6.5244  Epoch: 4  Global Step: 27330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:28,387-Speed 3898.54 samples/sec  Loss 1.5006  LearningRate 0.1305  ProxyLR: 6.5233  Epoch: 4  Global Step: 27340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:31,017-Speed 3895.17 samples/sec  Loss 1.6047  LearningRate 0.1304  ProxyLR: 6.5221  Epoch: 4  Global Step: 27350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:33,648-Speed 3892.32 samples/sec  Loss 1.4809  LearningRate 0.1304  ProxyLR: 6.5210  Epoch: 4  Global Step: 27360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:36,276-Speed 3898.03 samples/sec  Loss 1.4847  LearningRate 0.1304  ProxyLR: 6.5199  Epoch: 4  Global Step: 27370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:38,903-Speed 3898.98 samples/sec  Loss 1.5028  LearningRate 0.1304  ProxyLR: 6.5187  Epoch: 4  Global Step: 27380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:41,532-Speed 3895.26 samples/sec  Loss 1.6232  LearningRate 0.1304  ProxyLR: 6.5176  Epoch: 4  Global Step: 27390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:44,149-Speed 3914.48 samples/sec  Loss 1.5657  LearningRate 0.1303  ProxyLR: 6.5165  Epoch: 4  Global Step: 27400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:46,777-Speed 3897.53 samples/sec  Loss 1.5508  LearningRate 0.1303  ProxyLR: 6.5153  Epoch: 4  Global Step: 27410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:49,407-Speed 3893.51 samples/sec  Loss 1.5377  LearningRate 0.1303  ProxyLR: 6.5142  Epoch: 4  Global Step: 27420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:52,037-Speed 3894.81 samples/sec  Loss 1.5241  LearningRate 0.1303  ProxyLR: 6.5131  Epoch: 4  Global Step: 27430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:54,666-Speed 3896.05 samples/sec  Loss 1.6342  LearningRate 0.1302  ProxyLR: 6.5119  Epoch: 4  Global Step: 27440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:57,297-Speed 3893.18 samples/sec  Loss 1.4461  LearningRate 0.1302  ProxyLR: 6.5108  Epoch: 4  Global Step: 27450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:22:59,925-Speed 3897.03 samples/sec  Loss 1.6082  LearningRate 0.1302  ProxyLR: 6.5096  Epoch: 4  Global Step: 27460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:02,555-Speed 3894.52 samples/sec  Loss 1.5433  LearningRate 0.1302  ProxyLR: 6.5085  Epoch: 4  Global Step: 27470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:05,186-Speed 3893.16 samples/sec  Loss 1.5189  LearningRate 0.1301  ProxyLR: 6.5074  Epoch: 4  Global Step: 27480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:07,814-Speed 3897.53 samples/sec  Loss 1.4641  LearningRate 0.1301  ProxyLR: 6.5062  Epoch: 4  Global Step: 27490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:10,430-Speed 3916.69 samples/sec  Loss 1.6330  LearningRate 0.1301  ProxyLR: 6.5051  Epoch: 4  Global Step: 27500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:13,060-Speed 3894.40 samples/sec  Loss 1.4673  LearningRate 0.1301  ProxyLR: 6.5040  Epoch: 4  Global Step: 27510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:15,692-Speed 3891.29 samples/sec  Loss 1.5413  LearningRate 0.1301  ProxyLR: 6.5028  Epoch: 4  Global Step: 27520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:18,319-Speed 3899.17 samples/sec  Loss 1.4036  LearningRate 0.1300  ProxyLR: 6.5017  Epoch: 4  Global Step: 27530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:20,947-Speed 3897.20 samples/sec  Loss 1.4853  LearningRate 0.1300  ProxyLR: 6.5006  Epoch: 4  Global Step: 27540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:23,576-Speed 3895.63 samples/sec  Loss 1.5817  LearningRate 0.1300  ProxyLR: 6.4994  Epoch: 4  Global Step: 27550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:26,205-Speed 3895.79 samples/sec  Loss 1.5043  LearningRate 0.1300  ProxyLR: 6.4983  Epoch: 4  Global Step: 27560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:28,833-Speed 3897.18 samples/sec  Loss 1.5242  LearningRate 0.1299  ProxyLR: 6.4972  Epoch: 4  Global Step: 27570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:31,462-Speed 3896.10 samples/sec  Loss 1.5933  LearningRate 0.1299  ProxyLR: 6.4960  Epoch: 4  Global Step: 27580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:34,090-Speed 3898.22 samples/sec  Loss 1.5190  LearningRate 0.1299  ProxyLR: 6.4949  Epoch: 4  Global Step: 27590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:36,716-Speed 3899.26 samples/sec  Loss 1.5570  LearningRate 0.1299  ProxyLR: 6.4938  Epoch: 4  Global Step: 27600   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:23:39,333-Speed 3914.78 samples/sec  Loss 1.5357  LearningRate 0.1299  ProxyLR: 6.4926  Epoch: 4  Global Step: 27610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:41,962-Speed 3896.68 samples/sec  Loss 1.6011  LearningRate 0.1298  ProxyLR: 6.4915  Epoch: 4  Global Step: 27620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:44,590-Speed 3897.35 samples/sec  Loss 1.6344  LearningRate 0.1298  ProxyLR: 6.4904  Epoch: 4  Global Step: 27630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:47,217-Speed 3897.90 samples/sec  Loss 1.5565  LearningRate 0.1298  ProxyLR: 6.4892  Epoch: 4  Global Step: 27640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:49,847-Speed 3895.24 samples/sec  Loss 1.5526  LearningRate 0.1298  ProxyLR: 6.4881  Epoch: 4  Global Step: 27650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:52,476-Speed 3895.06 samples/sec  Loss 1.5271  LearningRate 0.1297  ProxyLR: 6.4870  Epoch: 4  Global Step: 27660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:55,104-Speed 3897.41 samples/sec  Loss 1.5958  LearningRate 0.1297  ProxyLR: 6.4858  Epoch: 4  Global Step: 27670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:23:57,733-Speed 3896.31 samples/sec  Loss 1.5795  LearningRate 0.1297  ProxyLR: 6.4847  Epoch: 4  Global Step: 27680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:00,361-Speed 3897.99 samples/sec  Loss 1.6536  LearningRate 0.1297  ProxyLR: 6.4836  Epoch: 4  Global Step: 27690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:02,991-Speed 3893.80 samples/sec  Loss 1.5212  LearningRate 0.1296  ProxyLR: 6.4824  Epoch: 4  Global Step: 27700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:05,610-Speed 3911.52 samples/sec  Loss 1.5173  LearningRate 0.1296  ProxyLR: 6.4813  Epoch: 4  Global Step: 27710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:08,226-Speed 3915.36 samples/sec  Loss 1.4792  LearningRate 0.1296  ProxyLR: 6.4802  Epoch: 4  Global Step: 27720   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:10,855-Speed 3895.95 samples/sec  Loss 1.4476  LearningRate 0.1296  ProxyLR: 6.4790  Epoch: 4  Global Step: 27730   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:13,485-Speed 3893.24 samples/sec  Loss 1.5869  LearningRate 0.1296  ProxyLR: 6.4779  Epoch: 4  Global Step: 27740   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:16,119-Speed 3889.33 samples/sec  Loss 1.4403  LearningRate 0.1295  ProxyLR: 6.4768  Epoch: 4  Global Step: 27750   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:18,751-Speed 3891.23 samples/sec  Loss 1.4653  LearningRate 0.1295  ProxyLR: 6.4756  Epoch: 4  Global Step: 27760   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:21,384-Speed 3890.13 samples/sec  Loss 1.5374  LearningRate 0.1295  ProxyLR: 6.4745  Epoch: 4  Global Step: 27770   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:24,013-Speed 3896.22 samples/sec  Loss 1.4729  LearningRate 0.1295  ProxyLR: 6.4734  Epoch: 4  Global Step: 27780   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:26,642-Speed 3894.83 samples/sec  Loss 1.4644  LearningRate 0.1294  ProxyLR: 6.4722  Epoch: 4  Global Step: 27790   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:29,271-Speed 3896.88 samples/sec  Loss 1.4713  LearningRate 0.1294  ProxyLR: 6.4711  Epoch: 4  Global Step: 27800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:31,899-Speed 3897.29 samples/sec  Loss 1.4756  LearningRate 0.1294  ProxyLR: 6.4700  Epoch: 4  Global Step: 27810   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:24:34,527-Speed 3897.44 samples/sec  Loss 1.4832  LearningRate 0.1294  ProxyLR: 6.4688  Epoch: 4  Global Step: 27820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:37,156-Speed 3896.51 samples/sec  Loss 1.6167  LearningRate 0.1294  ProxyLR: 6.4677  Epoch: 4  Global Step: 27830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:39,784-Speed 3897.72 samples/sec  Loss 1.4960  LearningRate 0.1293  ProxyLR: 6.4666  Epoch: 4  Global Step: 27840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:42,411-Speed 3898.84 samples/sec  Loss 1.5255  LearningRate 0.1293  ProxyLR: 6.4654  Epoch: 4  Global Step: 27850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:45,039-Speed 3896.31 samples/sec  Loss 1.4781  LearningRate 0.1293  ProxyLR: 6.4643  Epoch: 4  Global Step: 27860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:47,670-Speed 3893.80 samples/sec  Loss 1.5882  LearningRate 0.1293  ProxyLR: 6.4632  Epoch: 4  Global Step: 27870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:50,298-Speed 3897.44 samples/sec  Loss 1.5198  LearningRate 0.1292  ProxyLR: 6.4621  Epoch: 4  Global Step: 27880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:52,926-Speed 3897.35 samples/sec  Loss 1.4720  LearningRate 0.1292  ProxyLR: 6.4609  Epoch: 4  Global Step: 27890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:55,555-Speed 3895.16 samples/sec  Loss 1.5385  LearningRate 0.1292  ProxyLR: 6.4598  Epoch: 4  Global Step: 27900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:24:58,186-Speed 3893.66 samples/sec  Loss 1.5733  LearningRate 0.1292  ProxyLR: 6.4587  Epoch: 4  Global Step: 27910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:00,814-Speed 3896.65 samples/sec  Loss 1.3485  LearningRate 0.1292  ProxyLR: 6.4575  Epoch: 4  Global Step: 27920   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:25:03,430-Speed 3914.90 samples/sec  Loss 1.5986  LearningRate 0.1291  ProxyLR: 6.4564  Epoch: 4  Global Step: 27930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:06,059-Speed 3896.83 samples/sec  Loss 1.6572  LearningRate 0.1291  ProxyLR: 6.4553  Epoch: 4  Global Step: 27940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:08,686-Speed 3898.54 samples/sec  Loss 1.5450  LearningRate 0.1291  ProxyLR: 6.4541  Epoch: 4  Global Step: 27950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:11,315-Speed 3895.28 samples/sec  Loss 1.5932  LearningRate 0.1291  ProxyLR: 6.4530  Epoch: 4  Global Step: 27960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:13,944-Speed 3896.66 samples/sec  Loss 1.4636  LearningRate 0.1290  ProxyLR: 6.4519  Epoch: 4  Global Step: 27970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:16,572-Speed 3897.32 samples/sec  Loss 1.4986  LearningRate 0.1290  ProxyLR: 6.4508  Epoch: 4  Global Step: 27980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:19,202-Speed 3894.18 samples/sec  Loss 1.6790  LearningRate 0.1290  ProxyLR: 6.4496  Epoch: 4  Global Step: 27990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:21,831-Speed 3896.32 samples/sec  Loss 1.5247  LearningRate 0.1290  ProxyLR: 6.4485  Epoch: 4  Global Step: 28000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:24,460-Speed 3895.32 samples/sec  Loss 1.5798  LearningRate 0.1289  ProxyLR: 6.4474  Epoch: 4  Global Step: 28010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:27,091-Speed 3893.57 samples/sec  Loss 1.6215  LearningRate 0.1289  ProxyLR: 6.4462  Epoch: 4  Global Step: 28020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:29,720-Speed 3895.73 samples/sec  Loss 1.5222  LearningRate 0.1289  ProxyLR: 6.4451  Epoch: 4  Global Step: 28030   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:25:32,335-Speed 3916.98 samples/sec  Loss 1.4951  LearningRate 0.1289  ProxyLR: 6.4440  Epoch: 4  Global Step: 28040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:34,963-Speed 3898.65 samples/sec  Loss 1.5567  LearningRate 0.1289  ProxyLR: 6.4428  Epoch: 4  Global Step: 28050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:37,591-Speed 3897.23 samples/sec  Loss 1.5517  LearningRate 0.1288  ProxyLR: 6.4417  Epoch: 4  Global Step: 28060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:40,221-Speed 3894.30 samples/sec  Loss 1.5310  LearningRate 0.1288  ProxyLR: 6.4406  Epoch: 4  Global Step: 28070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:42,848-Speed 3898.98 samples/sec  Loss 1.4920  LearningRate 0.1288  ProxyLR: 6.4395  Epoch: 4  Global Step: 28080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:45,477-Speed 3895.72 samples/sec  Loss 1.5527  LearningRate 0.1288  ProxyLR: 6.4383  Epoch: 4  Global Step: 28090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:25:48,095-Speed 3911.25 samples/sec  Loss 1.6402  LearningRate 0.1287  ProxyLR: 6.4372  Epoch: 4  Global Step: 28100   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:25:50,727-Speed 3892.56 samples/sec  Loss 1.5763  LearningRate 0.1287  ProxyLR: 6.4361  Epoch: 4  Global Step: 28110   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:25:53,355-Speed 3896.77 samples/sec  Loss 1.5294  LearningRate 0.1287  ProxyLR: 6.4349  Epoch: 4  Global Step: 28120   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:25:55,982-Speed 3899.16 samples/sec  Loss 1.5720  LearningRate 0.1287  ProxyLR: 6.4338  Epoch: 4  Global Step: 28130   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:25:58,613-Speed 3892.66 samples/sec  Loss 1.6204  LearningRate 0.1287  ProxyLR: 6.4327  Epoch: 4  Global Step: 28140   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:26:01,244-Speed 3892.88 samples/sec  Loss 1.5498  LearningRate 0.1286  ProxyLR: 6.4316  Epoch: 4  Global Step: 28150   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:26:03,874-Speed 3895.30 samples/sec  Loss 1.4390  LearningRate 0.1286  ProxyLR: 6.4304  Epoch: 4  Global Step: 28160   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:26:06,504-Speed 3894.52 samples/sec  Loss 1.4735  LearningRate 0.1286  ProxyLR: 6.4293  Epoch: 4  Global Step: 28170   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:26:09,134-Speed 3894.52 samples/sec  Loss 1.5300  LearningRate 0.1286  ProxyLR: 6.4282  Epoch: 4  Global Step: 28180   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:26:11,763-Speed 3895.89 samples/sec  Loss 1.4362  LearningRate 0.1285  ProxyLR: 6.4270  Epoch: 4  Global Step: 28190   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:26:14,393-Speed 3894.52 samples/sec  Loss 1.4998  LearningRate 0.1285  ProxyLR: 6.4259  Epoch: 4  Global Step: 28200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:17,022-Speed 3895.73 samples/sec  Loss 1.5039  LearningRate 0.1285  ProxyLR: 6.4248  Epoch: 4  Global Step: 28210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:19,654-Speed 3891.53 samples/sec  Loss 1.4675  LearningRate 0.1285  ProxyLR: 6.4237  Epoch: 4  Global Step: 28220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:22,285-Speed 3893.51 samples/sec  Loss 1.6354  LearningRate 0.1285  ProxyLR: 6.4225  Epoch: 4  Global Step: 28230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:24,913-Speed 3897.31 samples/sec  Loss 1.5444  LearningRate 0.1284  ProxyLR: 6.4214  Epoch: 4  Global Step: 28240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:27,542-Speed 3895.60 samples/sec  Loss 1.6175  LearningRate 0.1284  ProxyLR: 6.4203  Epoch: 4  Global Step: 28250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:30,172-Speed 3895.37 samples/sec  Loss 1.4469  LearningRate 0.1284  ProxyLR: 6.4191  Epoch: 4  Global Step: 28260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:32,803-Speed 3893.11 samples/sec  Loss 1.4217  LearningRate 0.1284  ProxyLR: 6.4180  Epoch: 4  Global Step: 28270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:35,436-Speed 3889.70 samples/sec  Loss 1.5253  LearningRate 0.1283  ProxyLR: 6.4169  Epoch: 4  Global Step: 28280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:38,069-Speed 3889.71 samples/sec  Loss 1.5035  LearningRate 0.1283  ProxyLR: 6.4158  Epoch: 4  Global Step: 28290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:40,691-Speed 3907.32 samples/sec  Loss 1.4642  LearningRate 0.1283  ProxyLR: 6.4146  Epoch: 4  Global Step: 28300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:43,324-Speed 3889.77 samples/sec  Loss 1.5091  LearningRate 0.1283  ProxyLR: 6.4135  Epoch: 4  Global Step: 28310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:45,959-Speed 3887.47 samples/sec  Loss 1.6312  LearningRate 0.1282  ProxyLR: 6.4124  Epoch: 4  Global Step: 28320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:48,590-Speed 3891.92 samples/sec  Loss 1.7170  LearningRate 0.1282  ProxyLR: 6.4113  Epoch: 4  Global Step: 28330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:51,221-Speed 3893.93 samples/sec  Loss 1.4483  LearningRate 0.1282  ProxyLR: 6.4101  Epoch: 4  Global Step: 28340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:53,855-Speed 3888.29 samples/sec  Loss 1.4841  LearningRate 0.1282  ProxyLR: 6.4090  Epoch: 4  Global Step: 28350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:56,488-Speed 3889.62 samples/sec  Loss 1.6571  LearningRate 0.1282  ProxyLR: 6.4079  Epoch: 4  Global Step: 28360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:26:59,123-Speed 3886.98 samples/sec  Loss 1.6531  LearningRate 0.1281  ProxyLR: 6.4068  Epoch: 4  Global Step: 28370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:27:01,759-Speed 3885.93 samples/sec  Loss 1.5498  LearningRate 0.1281  ProxyLR: 6.4056  Epoch: 4  Global Step: 28380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:27:04,382-Speed 3904.64 samples/sec  Loss 1.5126  LearningRate 0.1281  ProxyLR: 6.4045  Epoch: 4  Global Step: 28390   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:07,018-Speed 3886.70 samples/sec  Loss 1.5578  LearningRate 0.1281  ProxyLR: 6.4034  Epoch: 4  Global Step: 28400   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:09,655-Speed 3884.44 samples/sec  Loss 1.5725  LearningRate 0.1280  ProxyLR: 6.4023  Epoch: 4  Global Step: 28410   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:12,347-Speed 3803.80 samples/sec  Loss 1.5404  LearningRate 0.1280  ProxyLR: 6.4011  Epoch: 4  Global Step: 28420   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:14,975-Speed 3898.54 samples/sec  Loss 1.5698  LearningRate 0.1280  ProxyLR: 6.4000  Epoch: 4  Global Step: 28430   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:41,594-Speed 384.70 samples/sec  Loss 0.9616  LearningRate 0.1280  ProxyLR: 6.3989  Epoch: 5  Global Step: 28440   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:44,250-Speed 3856.42 samples/sec  Loss 0.9447  LearningRate 0.1280  ProxyLR: 6.3977  Epoch: 5  Global Step: 28450   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:46,878-Speed 3898.02 samples/sec  Loss 0.8954  LearningRate 0.1279  ProxyLR: 6.3966  Epoch: 5  Global Step: 28460   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:49,505-Speed 3898.38 samples/sec  Loss 0.8959  LearningRate 0.1279  ProxyLR: 6.3955  Epoch: 5  Global Step: 28470   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:52,137-Speed 3892.05 samples/sec  Loss 0.9011  LearningRate 0.1279  ProxyLR: 6.3944  Epoch: 5  Global Step: 28480   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:27:54,769-Speed 3891.49 samples/sec  Loss 0.8572  LearningRate 0.1279  ProxyLR: 6.3932  Epoch: 5  Global Step: 28490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:27:57,427-Speed 3853.51 samples/sec  Loss 0.8827  LearningRate 0.1278  ProxyLR: 6.3921  Epoch: 5  Global Step: 28500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:00,058-Speed 3893.20 samples/sec  Loss 0.9050  LearningRate 0.1278  ProxyLR: 6.3910  Epoch: 5  Global Step: 28510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:02,691-Speed 3889.05 samples/sec  Loss 0.8961  LearningRate 0.1278  ProxyLR: 6.3899  Epoch: 5  Global Step: 28520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:05,326-Speed 3888.23 samples/sec  Loss 0.9461  LearningRate 0.1278  ProxyLR: 6.3887  Epoch: 5  Global Step: 28530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:07,959-Speed 3889.95 samples/sec  Loss 0.9088  LearningRate 0.1278  ProxyLR: 6.3876  Epoch: 5  Global Step: 28540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:10,592-Speed 3890.39 samples/sec  Loss 0.9457  LearningRate 0.1277  ProxyLR: 6.3865  Epoch: 5  Global Step: 28550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:13,274-Speed 3818.62 samples/sec  Loss 0.8871  LearningRate 0.1277  ProxyLR: 6.3854  Epoch: 5  Global Step: 28560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:15,906-Speed 3890.96 samples/sec  Loss 0.8218  LearningRate 0.1277  ProxyLR: 6.3843  Epoch: 5  Global Step: 28570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:18,541-Speed 3888.21 samples/sec  Loss 0.8294  LearningRate 0.1277  ProxyLR: 6.3831  Epoch: 5  Global Step: 28580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:21,179-Speed 3882.97 samples/sec  Loss 0.9527  LearningRate 0.1276  ProxyLR: 6.3820  Epoch: 5  Global Step: 28590   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:28:23,800-Speed 3907.45 samples/sec  Loss 0.9130  LearningRate 0.1276  ProxyLR: 6.3809  Epoch: 5  Global Step: 28600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:26,436-Speed 3886.06 samples/sec  Loss 0.9131  LearningRate 0.1276  ProxyLR: 6.3798  Epoch: 5  Global Step: 28610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:29,071-Speed 3886.84 samples/sec  Loss 0.7641  LearningRate 0.1276  ProxyLR: 6.3786  Epoch: 5  Global Step: 28620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:31,761-Speed 3877.80 samples/sec  Loss 0.9609  LearningRate 0.1276  ProxyLR: 6.3775  Epoch: 5  Global Step: 28630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:34,396-Speed 3887.78 samples/sec  Loss 0.9360  LearningRate 0.1275  ProxyLR: 6.3764  Epoch: 5  Global Step: 28640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:37,028-Speed 3891.01 samples/sec  Loss 0.9189  LearningRate 0.1275  ProxyLR: 6.3753  Epoch: 5  Global Step: 28650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:39,661-Speed 3889.78 samples/sec  Loss 0.9757  LearningRate 0.1275  ProxyLR: 6.3741  Epoch: 5  Global Step: 28660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:42,293-Speed 3891.94 samples/sec  Loss 0.9163  LearningRate 0.1275  ProxyLR: 6.3730  Epoch: 5  Global Step: 28670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:44,925-Speed 3891.47 samples/sec  Loss 0.9156  LearningRate 0.1274  ProxyLR: 6.3719  Epoch: 5  Global Step: 28680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:47,557-Speed 3891.94 samples/sec  Loss 0.8149  LearningRate 0.1274  ProxyLR: 6.3708  Epoch: 5  Global Step: 28690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:28:50,188-Speed 3892.76 samples/sec  Loss 0.8637  LearningRate 0.1274  ProxyLR: 6.3696  Epoch: 5  Global Step: 28700   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:28:52,815-Speed 3899.55 samples/sec  Loss 0.9449  LearningRate 0.1274  ProxyLR: 6.3685  Epoch: 5  Global Step: 28710   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:28:55,447-Speed 3892.01 samples/sec  Loss 0.9782  LearningRate 0.1273  ProxyLR: 6.3674  Epoch: 5  Global Step: 28720   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:28:58,080-Speed 3890.25 samples/sec  Loss 0.9231  LearningRate 0.1273  ProxyLR: 6.3663  Epoch: 5  Global Step: 28730   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:00,712-Speed 3891.40 samples/sec  Loss 0.9283  LearningRate 0.1273  ProxyLR: 6.3652  Epoch: 5  Global Step: 28740   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:03,344-Speed 3892.05 samples/sec  Loss 0.9322  LearningRate 0.1273  ProxyLR: 6.3640  Epoch: 5  Global Step: 28750   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:05,974-Speed 3894.16 samples/sec  Loss 0.8940  LearningRate 0.1273  ProxyLR: 6.3629  Epoch: 5  Global Step: 28760   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:08,606-Speed 3891.37 samples/sec  Loss 0.9872  LearningRate 0.1272  ProxyLR: 6.3618  Epoch: 5  Global Step: 28770   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:11,238-Speed 3892.20 samples/sec  Loss 0.8954  LearningRate 0.1272  ProxyLR: 6.3607  Epoch: 5  Global Step: 28780   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:13,870-Speed 3890.99 samples/sec  Loss 0.9158  LearningRate 0.1272  ProxyLR: 6.3595  Epoch: 5  Global Step: 28790   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:16,501-Speed 3893.34 samples/sec  Loss 0.8811  LearningRate 0.1272  ProxyLR: 6.3584  Epoch: 5  Global Step: 28800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:29:19,135-Speed 3889.00 samples/sec  Loss 1.0198  LearningRate 0.1271  ProxyLR: 6.3573  Epoch: 5  Global Step: 28810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:21,767-Speed 3891.49 samples/sec  Loss 0.9683  LearningRate 0.1271  ProxyLR: 6.3562  Epoch: 5  Global Step: 28820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:24,398-Speed 3893.20 samples/sec  Loss 1.0143  LearningRate 0.1271  ProxyLR: 6.3551  Epoch: 5  Global Step: 28830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:27,029-Speed 3892.24 samples/sec  Loss 0.8638  LearningRate 0.1271  ProxyLR: 6.3539  Epoch: 5  Global Step: 28840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:29,663-Speed 3889.99 samples/sec  Loss 0.9456  LearningRate 0.1271  ProxyLR: 6.3528  Epoch: 5  Global Step: 28850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:32,294-Speed 3892.35 samples/sec  Loss 0.8945  LearningRate 0.1270  ProxyLR: 6.3517  Epoch: 5  Global Step: 28860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:34,928-Speed 3889.28 samples/sec  Loss 0.9058  LearningRate 0.1270  ProxyLR: 6.3506  Epoch: 5  Global Step: 28870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:37,561-Speed 3889.75 samples/sec  Loss 0.8683  LearningRate 0.1270  ProxyLR: 6.3494  Epoch: 5  Global Step: 28880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:40,191-Speed 3894.65 samples/sec  Loss 0.9560  LearningRate 0.1270  ProxyLR: 6.3483  Epoch: 5  Global Step: 28890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:42,821-Speed 3894.32 samples/sec  Loss 0.9204  LearningRate 0.1269  ProxyLR: 6.3472  Epoch: 5  Global Step: 28900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:45,438-Speed 3914.51 samples/sec  Loss 0.9654  LearningRate 0.1269  ProxyLR: 6.3461  Epoch: 5  Global Step: 28910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:48,069-Speed 3893.65 samples/sec  Loss 0.9028  LearningRate 0.1269  ProxyLR: 6.3450  Epoch: 5  Global Step: 28920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:50,699-Speed 3893.56 samples/sec  Loss 0.9719  LearningRate 0.1269  ProxyLR: 6.3438  Epoch: 5  Global Step: 28930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:53,333-Speed 3888.89 samples/sec  Loss 0.9833  LearningRate 0.1269  ProxyLR: 6.3427  Epoch: 5  Global Step: 28940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:55,966-Speed 3890.32 samples/sec  Loss 0.9976  LearningRate 0.1268  ProxyLR: 6.3416  Epoch: 5  Global Step: 28950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:29:58,598-Speed 3891.80 samples/sec  Loss 0.8905  LearningRate 0.1268  ProxyLR: 6.3405  Epoch: 5  Global Step: 28960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:01,231-Speed 3889.47 samples/sec  Loss 0.9752  LearningRate 0.1268  ProxyLR: 6.3394  Epoch: 5  Global Step: 28970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:03,907-Speed 3888.90 samples/sec  Loss 0.9380  LearningRate 0.1268  ProxyLR: 6.3382  Epoch: 5  Global Step: 28980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:06,540-Speed 3889.50 samples/sec  Loss 0.9818  LearningRate 0.1267  ProxyLR: 6.3371  Epoch: 5  Global Step: 28990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:09,172-Speed 3891.86 samples/sec  Loss 0.9706  LearningRate 0.1267  ProxyLR: 6.3360  Epoch: 5  Global Step: 29000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:11,804-Speed 3891.83 samples/sec  Loss 1.0164  LearningRate 0.1267  ProxyLR: 6.3349  Epoch: 5  Global Step: 29010   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:14,433-Speed 3896.06 samples/sec  Loss 0.9498  LearningRate 0.1267  ProxyLR: 6.3338  Epoch: 5  Global Step: 29020   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:17,064-Speed 3893.82 samples/sec  Loss 1.0662  LearningRate 0.1267  ProxyLR: 6.3326  Epoch: 5  Global Step: 29030   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:19,693-Speed 3895.01 samples/sec  Loss 0.9200  LearningRate 0.1266  ProxyLR: 6.3315  Epoch: 5  Global Step: 29040   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:22,325-Speed 3892.38 samples/sec  Loss 1.0093  LearningRate 0.1266  ProxyLR: 6.3304  Epoch: 5  Global Step: 29050   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:24,942-Speed 3913.46 samples/sec  Loss 0.9512  LearningRate 0.1266  ProxyLR: 6.3293  Epoch: 5  Global Step: 29060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:27,573-Speed 3893.70 samples/sec  Loss 1.0465  LearningRate 0.1266  ProxyLR: 6.3282  Epoch: 5  Global Step: 29070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:30,203-Speed 3895.16 samples/sec  Loss 0.9591  LearningRate 0.1265  ProxyLR: 6.3270  Epoch: 5  Global Step: 29080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:32,830-Speed 3898.37 samples/sec  Loss 0.9290  LearningRate 0.1265  ProxyLR: 6.3259  Epoch: 5  Global Step: 29090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:35,461-Speed 3893.59 samples/sec  Loss 0.9364  LearningRate 0.1265  ProxyLR: 6.3248  Epoch: 5  Global Step: 29100   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:38,090-Speed 3894.99 samples/sec  Loss 0.9462  LearningRate 0.1265  ProxyLR: 6.3237  Epoch: 5  Global Step: 29110   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:40,720-Speed 3894.87 samples/sec  Loss 0.9791  LearningRate 0.1265  ProxyLR: 6.3226  Epoch: 5  Global Step: 29120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:43,350-Speed 3894.73 samples/sec  Loss 1.0116  LearningRate 0.1264  ProxyLR: 6.3215  Epoch: 5  Global Step: 29130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:45,980-Speed 3894.04 samples/sec  Loss 0.9042  LearningRate 0.1264  ProxyLR: 6.3203  Epoch: 5  Global Step: 29140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:48,610-Speed 3894.25 samples/sec  Loss 1.0616  LearningRate 0.1264  ProxyLR: 6.3192  Epoch: 5  Global Step: 29150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:51,242-Speed 3892.47 samples/sec  Loss 0.9606  LearningRate 0.1264  ProxyLR: 6.3181  Epoch: 5  Global Step: 29160   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:53,872-Speed 3894.18 samples/sec  Loss 1.0105  LearningRate 0.1263  ProxyLR: 6.3170  Epoch: 5  Global Step: 29170   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:30:56,489-Speed 3914.11 samples/sec  Loss 1.0265  LearningRate 0.1263  ProxyLR: 6.3159  Epoch: 5  Global Step: 29180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:30:59,119-Speed 3894.25 samples/sec  Loss 1.0591  LearningRate 0.1263  ProxyLR: 6.3147  Epoch: 5  Global Step: 29190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:01,749-Speed 3894.71 samples/sec  Loss 1.0203  LearningRate 0.1263  ProxyLR: 6.3136  Epoch: 5  Global Step: 29200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:04,378-Speed 3896.60 samples/sec  Loss 1.1087  LearningRate 0.1263  ProxyLR: 6.3125  Epoch: 5  Global Step: 29210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:07,007-Speed 3895.55 samples/sec  Loss 1.0169  LearningRate 0.1262  ProxyLR: 6.3114  Epoch: 5  Global Step: 29220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:09,637-Speed 3895.06 samples/sec  Loss 0.8916  LearningRate 0.1262  ProxyLR: 6.3103  Epoch: 5  Global Step: 29230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:12,267-Speed 3894.09 samples/sec  Loss 1.0019  LearningRate 0.1262  ProxyLR: 6.3092  Epoch: 5  Global Step: 29240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:14,899-Speed 3891.78 samples/sec  Loss 0.9578  LearningRate 0.1262  ProxyLR: 6.3080  Epoch: 5  Global Step: 29250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:17,529-Speed 3894.47 samples/sec  Loss 0.9776  LearningRate 0.1261  ProxyLR: 6.3069  Epoch: 5  Global Step: 29260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:20,160-Speed 3893.93 samples/sec  Loss 0.9552  LearningRate 0.1261  ProxyLR: 6.3058  Epoch: 5  Global Step: 29270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:22,777-Speed 3913.39 samples/sec  Loss 1.0158  LearningRate 0.1261  ProxyLR: 6.3047  Epoch: 5  Global Step: 29280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:25,407-Speed 3894.98 samples/sec  Loss 0.9681  LearningRate 0.1261  ProxyLR: 6.3036  Epoch: 5  Global Step: 29290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:28,036-Speed 3895.05 samples/sec  Loss 0.9665  LearningRate 0.1260  ProxyLR: 6.3024  Epoch: 5  Global Step: 29300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:30,666-Speed 3895.24 samples/sec  Loss 0.9303  LearningRate 0.1260  ProxyLR: 6.3013  Epoch: 5  Global Step: 29310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:33,293-Speed 3899.31 samples/sec  Loss 1.0227  LearningRate 0.1260  ProxyLR: 6.3002  Epoch: 5  Global Step: 29320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:35,924-Speed 3892.84 samples/sec  Loss 0.8772  LearningRate 0.1260  ProxyLR: 6.2991  Epoch: 5  Global Step: 29330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:38,555-Speed 3893.71 samples/sec  Loss 0.9363  LearningRate 0.1260  ProxyLR: 6.2980  Epoch: 5  Global Step: 29340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:31:41,169-Speed 3917.72 samples/sec  Loss 1.0143  LearningRate 0.1259  ProxyLR: 6.2969  Epoch: 5  Global Step: 29350   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:43,798-Speed 3895.79 samples/sec  Loss 0.9929  LearningRate 0.1259  ProxyLR: 6.2957  Epoch: 5  Global Step: 29360   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:46,428-Speed 3894.48 samples/sec  Loss 0.9126  LearningRate 0.1259  ProxyLR: 6.2946  Epoch: 5  Global Step: 29370   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:49,061-Speed 3890.26 samples/sec  Loss 0.9407  LearningRate 0.1259  ProxyLR: 6.2935  Epoch: 5  Global Step: 29380   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:51,695-Speed 3888.54 samples/sec  Loss 1.0322  LearningRate 0.1258  ProxyLR: 6.2924  Epoch: 5  Global Step: 29390   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:54,331-Speed 3886.48 samples/sec  Loss 1.0166  LearningRate 0.1258  ProxyLR: 6.2913  Epoch: 5  Global Step: 29400   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:56,962-Speed 3891.88 samples/sec  Loss 0.9881  LearningRate 0.1258  ProxyLR: 6.2902  Epoch: 5  Global Step: 29410   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:31:59,584-Speed 3906.71 samples/sec  Loss 0.9784  LearningRate 0.1258  ProxyLR: 6.2891  Epoch: 5  Global Step: 29420   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:32:02,208-Speed 3903.16 samples/sec  Loss 0.9301  LearningRate 0.1258  ProxyLR: 6.2879  Epoch: 5  Global Step: 29430   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:32:04,835-Speed 3899.79 samples/sec  Loss 0.9810  LearningRate 0.1257  ProxyLR: 6.2868  Epoch: 5  Global Step: 29440   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:32:07,459-Speed 3903.14 samples/sec  Loss 1.0299  LearningRate 0.1257  ProxyLR: 6.2857  Epoch: 5  Global Step: 29450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:10,083-Speed 3904.63 samples/sec  Loss 0.9627  LearningRate 0.1257  ProxyLR: 6.2846  Epoch: 5  Global Step: 29460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:12,707-Speed 3903.11 samples/sec  Loss 1.0880  LearningRate 0.1257  ProxyLR: 6.2835  Epoch: 5  Global Step: 29470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:15,331-Speed 3904.17 samples/sec  Loss 1.0550  LearningRate 0.1256  ProxyLR: 6.2824  Epoch: 5  Global Step: 29480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:17,953-Speed 3906.41 samples/sec  Loss 1.0152  LearningRate 0.1256  ProxyLR: 6.2812  Epoch: 5  Global Step: 29490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:20,576-Speed 3903.92 samples/sec  Loss 1.1244  LearningRate 0.1256  ProxyLR: 6.2801  Epoch: 5  Global Step: 29500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:23,200-Speed 3904.82 samples/sec  Loss 0.9803  LearningRate 0.1256  ProxyLR: 6.2790  Epoch: 5  Global Step: 29510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:25,822-Speed 3906.11 samples/sec  Loss 1.0996  LearningRate 0.1256  ProxyLR: 6.2779  Epoch: 5  Global Step: 29520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:28,444-Speed 3906.12 samples/sec  Loss 0.9832  LearningRate 0.1255  ProxyLR: 6.2768  Epoch: 5  Global Step: 29530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:31,069-Speed 3901.45 samples/sec  Loss 1.0628  LearningRate 0.1255  ProxyLR: 6.2757  Epoch: 5  Global Step: 29540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:33,681-Speed 3922.36 samples/sec  Loss 1.0238  LearningRate 0.1255  ProxyLR: 6.2746  Epoch: 5  Global Step: 29550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:36,305-Speed 3903.24 samples/sec  Loss 1.0888  LearningRate 0.1255  ProxyLR: 6.2734  Epoch: 5  Global Step: 29560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:38,930-Speed 3901.84 samples/sec  Loss 1.0839  LearningRate 0.1254  ProxyLR: 6.2723  Epoch: 5  Global Step: 29570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:41,555-Speed 3903.21 samples/sec  Loss 1.0567  LearningRate 0.1254  ProxyLR: 6.2712  Epoch: 5  Global Step: 29580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:44,179-Speed 3903.03 samples/sec  Loss 1.0656  LearningRate 0.1254  ProxyLR: 6.2701  Epoch: 5  Global Step: 29590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:46,803-Speed 3903.95 samples/sec  Loss 1.0694  LearningRate 0.1254  ProxyLR: 6.2690  Epoch: 5  Global Step: 29600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:49,427-Speed 3903.28 samples/sec  Loss 1.0825  LearningRate 0.1254  ProxyLR: 6.2679  Epoch: 5  Global Step: 29610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:52,050-Speed 3904.31 samples/sec  Loss 1.0208  LearningRate 0.1253  ProxyLR: 6.2668  Epoch: 5  Global Step: 29620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:54,672-Speed 3906.81 samples/sec  Loss 1.0028  LearningRate 0.1253  ProxyLR: 6.2656  Epoch: 5  Global Step: 29630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:57,296-Speed 3902.81 samples/sec  Loss 1.1157  LearningRate 0.1253  ProxyLR: 6.2645  Epoch: 5  Global Step: 29640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:32:59,921-Speed 3901.86 samples/sec  Loss 1.1153  LearningRate 0.1253  ProxyLR: 6.2634  Epoch: 5  Global Step: 29650   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:33:02,534-Speed 3920.69 samples/sec  Loss 1.1723  LearningRate 0.1252  ProxyLR: 6.2623  Epoch: 5  Global Step: 29660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:05,157-Speed 3904.05 samples/sec  Loss 1.0374  LearningRate 0.1252  ProxyLR: 6.2612  Epoch: 5  Global Step: 29670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:07,780-Speed 3905.46 samples/sec  Loss 1.1302  LearningRate 0.1252  ProxyLR: 6.2601  Epoch: 5  Global Step: 29680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:10,405-Speed 3902.18 samples/sec  Loss 1.0164  LearningRate 0.1252  ProxyLR: 6.2590  Epoch: 5  Global Step: 29690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:13,029-Speed 3904.14 samples/sec  Loss 1.0958  LearningRate 0.1252  ProxyLR: 6.2579  Epoch: 5  Global Step: 29700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:15,650-Speed 3907.02 samples/sec  Loss 1.0976  LearningRate 0.1251  ProxyLR: 6.2567  Epoch: 5  Global Step: 29710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:18,272-Speed 3906.55 samples/sec  Loss 1.0274  LearningRate 0.1251  ProxyLR: 6.2556  Epoch: 5  Global Step: 29720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:20,896-Speed 3903.19 samples/sec  Loss 1.1101  LearningRate 0.1251  ProxyLR: 6.2545  Epoch: 5  Global Step: 29730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:23,519-Speed 3906.24 samples/sec  Loss 1.1861  LearningRate 0.1251  ProxyLR: 6.2534  Epoch: 5  Global Step: 29740   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:26,144-Speed 3901.57 samples/sec  Loss 1.0764  LearningRate 0.1250  ProxyLR: 6.2523  Epoch: 5  Global Step: 29750   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:28,754-Speed 3924.13 samples/sec  Loss 0.9992  LearningRate 0.1250  ProxyLR: 6.2512  Epoch: 5  Global Step: 29760   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:31,379-Speed 3901.49 samples/sec  Loss 1.1238  LearningRate 0.1250  ProxyLR: 6.2501  Epoch: 5  Global Step: 29770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:34,003-Speed 3904.59 samples/sec  Loss 1.1361  LearningRate 0.1250  ProxyLR: 6.2489  Epoch: 5  Global Step: 29780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:36,627-Speed 3903.30 samples/sec  Loss 1.1026  LearningRate 0.1250  ProxyLR: 6.2478  Epoch: 5  Global Step: 29790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:39,250-Speed 3904.69 samples/sec  Loss 1.1071  LearningRate 0.1249  ProxyLR: 6.2467  Epoch: 5  Global Step: 29800   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:41,872-Speed 3905.99 samples/sec  Loss 1.0531  LearningRate 0.1249  ProxyLR: 6.2456  Epoch: 5  Global Step: 29810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:44,498-Speed 3900.60 samples/sec  Loss 1.0776  LearningRate 0.1249  ProxyLR: 6.2445  Epoch: 5  Global Step: 29820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:47,122-Speed 3904.63 samples/sec  Loss 1.1416  LearningRate 0.1249  ProxyLR: 6.2434  Epoch: 5  Global Step: 29830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:49,744-Speed 3906.35 samples/sec  Loss 1.1667  LearningRate 0.1248  ProxyLR: 6.2423  Epoch: 5  Global Step: 29840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:52,368-Speed 3903.30 samples/sec  Loss 1.0424  LearningRate 0.1248  ProxyLR: 6.2412  Epoch: 5  Global Step: 29850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:54,978-Speed 3924.67 samples/sec  Loss 1.0642  LearningRate 0.1248  ProxyLR: 6.2401  Epoch: 5  Global Step: 29860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:33:57,602-Speed 3903.60 samples/sec  Loss 1.0237  LearningRate 0.1248  ProxyLR: 6.2389  Epoch: 5  Global Step: 29870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:00,226-Speed 3903.60 samples/sec  Loss 1.0792  LearningRate 0.1248  ProxyLR: 6.2378  Epoch: 5  Global Step: 29880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:02,850-Speed 3903.48 samples/sec  Loss 1.1923  LearningRate 0.1247  ProxyLR: 6.2367  Epoch: 5  Global Step: 29890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:05,473-Speed 3905.02 samples/sec  Loss 1.0570  LearningRate 0.1247  ProxyLR: 6.2356  Epoch: 5  Global Step: 29900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:08,101-Speed 3896.45 samples/sec  Loss 1.1152  LearningRate 0.1247  ProxyLR: 6.2345  Epoch: 5  Global Step: 29910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:10,731-Speed 3895.25 samples/sec  Loss 1.1376  LearningRate 0.1247  ProxyLR: 6.2334  Epoch: 5  Global Step: 29920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:13,357-Speed 3899.68 samples/sec  Loss 1.0175  LearningRate 0.1246  ProxyLR: 6.2323  Epoch: 5  Global Step: 29930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:15,989-Speed 3892.01 samples/sec  Loss 1.1871  LearningRate 0.1246  ProxyLR: 6.2312  Epoch: 5  Global Step: 29940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:18,619-Speed 3894.45 samples/sec  Loss 1.1336  LearningRate 0.1246  ProxyLR: 6.2301  Epoch: 5  Global Step: 29950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:21,235-Speed 3914.77 samples/sec  Loss 1.1540  LearningRate 0.1246  ProxyLR: 6.2289  Epoch: 5  Global Step: 29960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:23,864-Speed 3896.62 samples/sec  Loss 1.0821  LearningRate 0.1246  ProxyLR: 6.2278  Epoch: 5  Global Step: 29970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:26,494-Speed 3895.76 samples/sec  Loss 1.1010  LearningRate 0.1245  ProxyLR: 6.2267  Epoch: 5  Global Step: 29980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:29,121-Speed 3898.91 samples/sec  Loss 1.0497  LearningRate 0.1245  ProxyLR: 6.2256  Epoch: 5  Global Step: 29990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:34:31,746-Speed 3901.38 samples/sec  Loss 1.1519  LearningRate 0.1245  ProxyLR: 6.2245  Epoch: 5  Global Step: 30000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:35:21,281-[lfw][30000]XNorm: 21.077794
Training: 2023-05-04 16:35:21,282-[lfw][30000]Accuracy-Flip: 0.99633+-0.00221
Training: 2023-05-04 16:35:21,282-[lfw][30000]Accuracy-Highest: 0.99633
Training: 2023-05-04 16:35:21,282-[lfw][30000]TPR@1stNon-Zero-FPR of 0.00033: 0.97867
Training: 2023-05-04 16:35:21,282-[lfw][30000]Highest TPR@FPR: 0.97867
Training: 2023-05-04 16:36:18,245-[cfp_fp][30000]XNorm: 20.786628
Training: 2023-05-04 16:36:18,245-[cfp_fp][30000]Accuracy-Flip: 0.92143+-0.01939
Training: 2023-05-04 16:36:18,245-[cfp_fp][30000]Accuracy-Highest: 0.92143
Training: 2023-05-04 16:36:18,245-[cfp_fp][30000]TPR@1stNon-Zero-FPR of 0.00029: 0.47800
Training: 2023-05-04 16:36:18,245-[cfp_fp][30000]Highest TPR@FPR: 0.47800
Training: 2023-05-04 16:37:07,786-[agedb_30][30000]XNorm: 21.286106
Training: 2023-05-04 16:37:07,786-[agedb_30][30000]Accuracy-Flip: 0.93750+-0.01153
Training: 2023-05-04 16:37:07,786-[agedb_30][30000]Accuracy-Highest: 0.93750
Training: 2023-05-04 16:37:07,786-[agedb_30][30000]TPR@1stNon-Zero-FPR of 0.00033: 0.43967
Training: 2023-05-04 16:37:07,786-[agedb_30][30000]Highest TPR@FPR: 0.43967
Training: 2023-05-04 16:37:58,722-[calfw][30000]XNorm: 21.328838
Training: 2023-05-04 16:37:58,722-[calfw][30000]Accuracy-Flip: 0.94183+-0.01172
Training: 2023-05-04 16:37:58,723-[calfw][30000]Accuracy-Highest: 0.94183
Training: 2023-05-04 16:37:58,723-[calfw][30000]TPR@1stNon-Zero-FPR of 0.00033: 0.60133
Training: 2023-05-04 16:37:58,723-[calfw][30000]Highest TPR@FPR: 0.67500
Training: 2023-05-04 16:38:49,651-[cplfw][30000]XNorm: 20.034300
Training: 2023-05-04 16:38:49,652-[cplfw][30000]Accuracy-Flip: 0.88383+-0.02116
Training: 2023-05-04 16:38:49,652-[cplfw][30000]Accuracy-Highest: 0.88383
Training: 2023-05-04 16:38:49,652-[cplfw][30000]TPR@1stNon-Zero-FPR of 0.00033: 0.00267
Training: 2023-05-04 16:38:49,652-[cplfw][30000]Highest TPR@FPR: 0.00267
Training: 2023-05-04 16:38:52,308-Speed 39.30 samples/sec  Loss 1.0988  LearningRate 0.1245  ProxyLR: 6.2234  Epoch: 5  Global Step: 30010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:38:54,926-Speed 3912.90 samples/sec  Loss 1.1637  LearningRate 0.1244  ProxyLR: 6.2223  Epoch: 5  Global Step: 30020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:38:57,543-Speed 3913.08 samples/sec  Loss 1.0649  LearningRate 0.1244  ProxyLR: 6.2212  Epoch: 5  Global Step: 30030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:00,161-Speed 3912.18 samples/sec  Loss 1.1882  LearningRate 0.1244  ProxyLR: 6.2201  Epoch: 5  Global Step: 30040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:02,780-Speed 3911.67 samples/sec  Loss 1.1998  LearningRate 0.1244  ProxyLR: 6.2190  Epoch: 5  Global Step: 30050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:05,398-Speed 3911.67 samples/sec  Loss 1.2252  LearningRate 0.1244  ProxyLR: 6.2178  Epoch: 5  Global Step: 30060   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:39:08,004-Speed 3931.15 samples/sec  Loss 1.1045  LearningRate 0.1243  ProxyLR: 6.2167  Epoch: 5  Global Step: 30070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:10,624-Speed 3908.96 samples/sec  Loss 1.1005  LearningRate 0.1243  ProxyLR: 6.2156  Epoch: 5  Global Step: 30080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:13,243-Speed 3911.88 samples/sec  Loss 1.2095  LearningRate 0.1243  ProxyLR: 6.2145  Epoch: 5  Global Step: 30090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:15,861-Speed 3911.64 samples/sec  Loss 1.1129  LearningRate 0.1243  ProxyLR: 6.2134  Epoch: 5  Global Step: 30100   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:18,481-Speed 3909.88 samples/sec  Loss 1.1688  LearningRate 0.1242  ProxyLR: 6.2123  Epoch: 5  Global Step: 30110   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:21,101-Speed 3909.48 samples/sec  Loss 1.1564  LearningRate 0.1242  ProxyLR: 6.2112  Epoch: 5  Global Step: 30120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:23,722-Speed 3908.75 samples/sec  Loss 1.1147  LearningRate 0.1242  ProxyLR: 6.2101  Epoch: 5  Global Step: 30130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:26,343-Speed 3907.94 samples/sec  Loss 1.2536  LearningRate 0.1242  ProxyLR: 6.2090  Epoch: 5  Global Step: 30140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:28,964-Speed 3907.26 samples/sec  Loss 1.1209  LearningRate 0.1242  ProxyLR: 6.2079  Epoch: 5  Global Step: 30150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:31,587-Speed 3904.65 samples/sec  Loss 1.1194  LearningRate 0.1241  ProxyLR: 6.2068  Epoch: 5  Global Step: 30160   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:34,210-Speed 3904.87 samples/sec  Loss 1.1828  LearningRate 0.1241  ProxyLR: 6.2056  Epoch: 5  Global Step: 30170   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:39:36,821-Speed 3924.09 samples/sec  Loss 1.2117  LearningRate 0.1241  ProxyLR: 6.2045  Epoch: 5  Global Step: 30180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:39,443-Speed 3905.58 samples/sec  Loss 1.1658  LearningRate 0.1241  ProxyLR: 6.2034  Epoch: 5  Global Step: 30190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:42,067-Speed 3903.74 samples/sec  Loss 1.1552  LearningRate 0.1240  ProxyLR: 6.2023  Epoch: 5  Global Step: 30200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:44,690-Speed 3905.75 samples/sec  Loss 1.1377  LearningRate 0.1240  ProxyLR: 6.2012  Epoch: 5  Global Step: 30210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:47,313-Speed 3904.25 samples/sec  Loss 1.1935  LearningRate 0.1240  ProxyLR: 6.2001  Epoch: 5  Global Step: 30220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:49,937-Speed 3904.05 samples/sec  Loss 1.1355  LearningRate 0.1240  ProxyLR: 6.1990  Epoch: 5  Global Step: 30230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:52,560-Speed 3904.24 samples/sec  Loss 1.0615  LearningRate 0.1240  ProxyLR: 6.1979  Epoch: 5  Global Step: 30240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:55,184-Speed 3903.74 samples/sec  Loss 1.2178  LearningRate 0.1239  ProxyLR: 6.1968  Epoch: 5  Global Step: 30250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:39:57,807-Speed 3905.55 samples/sec  Loss 1.1974  LearningRate 0.1239  ProxyLR: 6.1957  Epoch: 5  Global Step: 30260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:00,432-Speed 3902.64 samples/sec  Loss 1.1732  LearningRate 0.1239  ProxyLR: 6.1946  Epoch: 5  Global Step: 30270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:03,041-Speed 3924.83 samples/sec  Loss 1.1449  LearningRate 0.1239  ProxyLR: 6.1935  Epoch: 5  Global Step: 30280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:05,664-Speed 3905.42 samples/sec  Loss 1.2138  LearningRate 0.1238  ProxyLR: 6.1924  Epoch: 5  Global Step: 30290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:08,288-Speed 3903.97 samples/sec  Loss 1.1264  LearningRate 0.1238  ProxyLR: 6.1912  Epoch: 5  Global Step: 30300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:10,912-Speed 3902.84 samples/sec  Loss 1.1492  LearningRate 0.1238  ProxyLR: 6.1901  Epoch: 5  Global Step: 30310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:13,536-Speed 3903.53 samples/sec  Loss 1.1274  LearningRate 0.1238  ProxyLR: 6.1890  Epoch: 5  Global Step: 30320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:16,160-Speed 3903.24 samples/sec  Loss 1.0825  LearningRate 0.1238  ProxyLR: 6.1879  Epoch: 5  Global Step: 30330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:18,783-Speed 3904.63 samples/sec  Loss 1.2482  LearningRate 0.1237  ProxyLR: 6.1868  Epoch: 5  Global Step: 30340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:21,408-Speed 3903.24 samples/sec  Loss 1.2036  LearningRate 0.1237  ProxyLR: 6.1857  Epoch: 5  Global Step: 30350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:24,031-Speed 3903.97 samples/sec  Loss 1.1303  LearningRate 0.1237  ProxyLR: 6.1846  Epoch: 5  Global Step: 30360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:26,656-Speed 3901.97 samples/sec  Loss 1.1476  LearningRate 0.1237  ProxyLR: 6.1835  Epoch: 5  Global Step: 30370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:29,266-Speed 3924.46 samples/sec  Loss 1.1935  LearningRate 0.1236  ProxyLR: 6.1824  Epoch: 5  Global Step: 30380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:31,896-Speed 3894.71 samples/sec  Loss 1.1176  LearningRate 0.1236  ProxyLR: 6.1813  Epoch: 5  Global Step: 30390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:34,529-Speed 3890.24 samples/sec  Loss 1.1185  LearningRate 0.1236  ProxyLR: 6.1802  Epoch: 5  Global Step: 30400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:37,161-Speed 3892.24 samples/sec  Loss 1.1782  LearningRate 0.1236  ProxyLR: 6.1791  Epoch: 5  Global Step: 30410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:39,792-Speed 3892.63 samples/sec  Loss 1.1938  LearningRate 0.1236  ProxyLR: 6.1780  Epoch: 5  Global Step: 30420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:42,419-Speed 3898.67 samples/sec  Loss 1.1976  LearningRate 0.1235  ProxyLR: 6.1769  Epoch: 5  Global Step: 30430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:45,048-Speed 3895.30 samples/sec  Loss 1.2244  LearningRate 0.1235  ProxyLR: 6.1758  Epoch: 5  Global Step: 30440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:47,681-Speed 3891.26 samples/sec  Loss 1.0872  LearningRate 0.1235  ProxyLR: 6.1747  Epoch: 5  Global Step: 30450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:50,311-Speed 3893.39 samples/sec  Loss 1.2178  LearningRate 0.1235  ProxyLR: 6.1735  Epoch: 5  Global Step: 30460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:52,942-Speed 3893.27 samples/sec  Loss 1.3189  LearningRate 0.1234  ProxyLR: 6.1724  Epoch: 5  Global Step: 30470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:40:55,573-Speed 3893.08 samples/sec  Loss 1.2021  LearningRate 0.1234  ProxyLR: 6.1713  Epoch: 5  Global Step: 30480   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:40:58,208-Speed 3887.48 samples/sec  Loss 1.1529  LearningRate 0.1234  ProxyLR: 6.1702  Epoch: 5  Global Step: 30490   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:41:00,826-Speed 3912.87 samples/sec  Loss 1.1212  LearningRate 0.1234  ProxyLR: 6.1691  Epoch: 5  Global Step: 30500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:03,458-Speed 3891.53 samples/sec  Loss 1.1716  LearningRate 0.1234  ProxyLR: 6.1680  Epoch: 5  Global Step: 30510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:06,090-Speed 3891.53 samples/sec  Loss 1.1959  LearningRate 0.1233  ProxyLR: 6.1669  Epoch: 5  Global Step: 30520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:08,722-Speed 3891.01 samples/sec  Loss 1.2039  LearningRate 0.1233  ProxyLR: 6.1658  Epoch: 5  Global Step: 30530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:11,357-Speed 3886.52 samples/sec  Loss 1.3172  LearningRate 0.1233  ProxyLR: 6.1647  Epoch: 5  Global Step: 30540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:13,990-Speed 3889.83 samples/sec  Loss 1.2817  LearningRate 0.1233  ProxyLR: 6.1636  Epoch: 5  Global Step: 30550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:16,623-Speed 3890.13 samples/sec  Loss 1.1447  LearningRate 0.1232  ProxyLR: 6.1625  Epoch: 5  Global Step: 30560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:19,256-Speed 3890.29 samples/sec  Loss 1.2158  LearningRate 0.1232  ProxyLR: 6.1614  Epoch: 5  Global Step: 30570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:21,888-Speed 3891.34 samples/sec  Loss 1.2746  LearningRate 0.1232  ProxyLR: 6.1603  Epoch: 5  Global Step: 30580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:24,521-Speed 3889.84 samples/sec  Loss 1.1962  LearningRate 0.1232  ProxyLR: 6.1592  Epoch: 5  Global Step: 30590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:27,143-Speed 3906.99 samples/sec  Loss 1.3065  LearningRate 0.1232  ProxyLR: 6.1581  Epoch: 5  Global Step: 30600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:29,777-Speed 3888.12 samples/sec  Loss 1.1989  LearningRate 0.1231  ProxyLR: 6.1570  Epoch: 5  Global Step: 30610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:32,413-Speed 3886.09 samples/sec  Loss 1.2174  LearningRate 0.1231  ProxyLR: 6.1559  Epoch: 5  Global Step: 30620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:35,041-Speed 3897.29 samples/sec  Loss 1.2414  LearningRate 0.1231  ProxyLR: 6.1548  Epoch: 5  Global Step: 30630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:37,670-Speed 3896.56 samples/sec  Loss 1.2197  LearningRate 0.1231  ProxyLR: 6.1537  Epoch: 5  Global Step: 30640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:40,296-Speed 3899.14 samples/sec  Loss 1.1876  LearningRate 0.1231  ProxyLR: 6.1526  Epoch: 5  Global Step: 30650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:42,926-Speed 3895.57 samples/sec  Loss 1.1093  LearningRate 0.1230  ProxyLR: 6.1515  Epoch: 5  Global Step: 30660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:45,554-Speed 3897.45 samples/sec  Loss 1.2666  LearningRate 0.1230  ProxyLR: 6.1504  Epoch: 5  Global Step: 30670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:48,183-Speed 3895.66 samples/sec  Loss 1.1043  LearningRate 0.1230  ProxyLR: 6.1493  Epoch: 5  Global Step: 30680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:50,811-Speed 3897.69 samples/sec  Loss 1.1154  LearningRate 0.1230  ProxyLR: 6.1481  Epoch: 5  Global Step: 30690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:53,438-Speed 3898.08 samples/sec  Loss 1.2067  LearningRate 0.1229  ProxyLR: 6.1470  Epoch: 5  Global Step: 30700   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:41:56,054-Speed 3916.05 samples/sec  Loss 1.2262  LearningRate 0.1229  ProxyLR: 6.1459  Epoch: 5  Global Step: 30710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:41:58,679-Speed 3901.32 samples/sec  Loss 1.2060  LearningRate 0.1229  ProxyLR: 6.1448  Epoch: 5  Global Step: 30720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:01,304-Speed 3901.55 samples/sec  Loss 1.1925  LearningRate 0.1229  ProxyLR: 6.1437  Epoch: 5  Global Step: 30730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:03,930-Speed 3900.93 samples/sec  Loss 1.2014  LearningRate 0.1229  ProxyLR: 6.1426  Epoch: 5  Global Step: 30740   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:06,541-Speed 3922.16 samples/sec  Loss 1.1901  LearningRate 0.1228  ProxyLR: 6.1415  Epoch: 5  Global Step: 30750   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:09,167-Speed 3901.29 samples/sec  Loss 1.2814  LearningRate 0.1228  ProxyLR: 6.1404  Epoch: 5  Global Step: 30760   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:11,792-Speed 3901.32 samples/sec  Loss 1.2635  LearningRate 0.1228  ProxyLR: 6.1393  Epoch: 5  Global Step: 30770   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:14,419-Speed 3899.22 samples/sec  Loss 1.2657  LearningRate 0.1228  ProxyLR: 6.1382  Epoch: 5  Global Step: 30780   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:17,046-Speed 3898.84 samples/sec  Loss 1.1335  LearningRate 0.1227  ProxyLR: 6.1371  Epoch: 5  Global Step: 30790   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:19,672-Speed 3900.99 samples/sec  Loss 1.1488  LearningRate 0.1227  ProxyLR: 6.1360  Epoch: 5  Global Step: 30800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:22,297-Speed 3900.56 samples/sec  Loss 1.2623  LearningRate 0.1227  ProxyLR: 6.1349  Epoch: 5  Global Step: 30810   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:25,159-Speed 3579.82 samples/sec  Loss 1.3290  LearningRate 0.1227  ProxyLR: 6.1338  Epoch: 5  Global Step: 30820   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:27,783-Speed 3902.03 samples/sec  Loss 1.2182  LearningRate 0.1227  ProxyLR: 6.1327  Epoch: 5  Global Step: 30830   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:30,408-Speed 3902.46 samples/sec  Loss 1.2371  LearningRate 0.1226  ProxyLR: 6.1316  Epoch: 5  Global Step: 30840   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:42:33,034-Speed 3901.26 samples/sec  Loss 1.2113  LearningRate 0.1226  ProxyLR: 6.1305  Epoch: 5  Global Step: 30850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:35,660-Speed 3900.27 samples/sec  Loss 1.2981  LearningRate 0.1226  ProxyLR: 6.1294  Epoch: 5  Global Step: 30860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:38,284-Speed 3903.32 samples/sec  Loss 1.1960  LearningRate 0.1226  ProxyLR: 6.1283  Epoch: 5  Global Step: 30870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:40,909-Speed 3902.14 samples/sec  Loss 1.3745  LearningRate 0.1225  ProxyLR: 6.1272  Epoch: 5  Global Step: 30880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:43,535-Speed 3899.57 samples/sec  Loss 1.1240  LearningRate 0.1225  ProxyLR: 6.1261  Epoch: 5  Global Step: 30890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:46,159-Speed 3904.18 samples/sec  Loss 1.1190  LearningRate 0.1225  ProxyLR: 6.1250  Epoch: 5  Global Step: 30900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:48,783-Speed 3902.81 samples/sec  Loss 1.3062  LearningRate 0.1225  ProxyLR: 6.1239  Epoch: 5  Global Step: 30910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:51,408-Speed 3901.96 samples/sec  Loss 1.3468  LearningRate 0.1225  ProxyLR: 6.1228  Epoch: 5  Global Step: 30920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:54,033-Speed 3902.54 samples/sec  Loss 1.1967  LearningRate 0.1224  ProxyLR: 6.1217  Epoch: 5  Global Step: 30930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:56,656-Speed 3904.64 samples/sec  Loss 1.2597  LearningRate 0.1224  ProxyLR: 6.1206  Epoch: 5  Global Step: 30940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:42:59,282-Speed 3900.33 samples/sec  Loss 1.2047  LearningRate 0.1224  ProxyLR: 6.1195  Epoch: 5  Global Step: 30950   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:43:01,893-Speed 3922.96 samples/sec  Loss 1.2997  LearningRate 0.1224  ProxyLR: 6.1184  Epoch: 5  Global Step: 30960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:04,757-Speed 3575.53 samples/sec  Loss 1.2780  LearningRate 0.1223  ProxyLR: 6.1173  Epoch: 5  Global Step: 30970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:07,381-Speed 3903.58 samples/sec  Loss 1.2931  LearningRate 0.1223  ProxyLR: 6.1162  Epoch: 5  Global Step: 30980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:10,005-Speed 3904.55 samples/sec  Loss 1.2909  LearningRate 0.1223  ProxyLR: 6.1151  Epoch: 5  Global Step: 30990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:12,629-Speed 3902.99 samples/sec  Loss 1.1564  LearningRate 0.1223  ProxyLR: 6.1140  Epoch: 5  Global Step: 31000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:15,253-Speed 3902.82 samples/sec  Loss 1.1990  LearningRate 0.1223  ProxyLR: 6.1129  Epoch: 5  Global Step: 31010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:17,881-Speed 3898.07 samples/sec  Loss 1.1519  LearningRate 0.1222  ProxyLR: 6.1118  Epoch: 5  Global Step: 31020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:20,512-Speed 3891.87 samples/sec  Loss 1.2407  LearningRate 0.1222  ProxyLR: 6.1107  Epoch: 5  Global Step: 31030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:23,138-Speed 3900.20 samples/sec  Loss 1.2311  LearningRate 0.1222  ProxyLR: 6.1096  Epoch: 5  Global Step: 31040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:25,762-Speed 3903.55 samples/sec  Loss 1.1823  LearningRate 0.1222  ProxyLR: 6.1085  Epoch: 5  Global Step: 31050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:28,614-Speed 3591.99 samples/sec  Loss 1.1525  LearningRate 0.1221  ProxyLR: 6.1074  Epoch: 5  Global Step: 31060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:31,238-Speed 3902.21 samples/sec  Loss 1.2854  LearningRate 0.1221  ProxyLR: 6.1063  Epoch: 5  Global Step: 31070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:33,861-Speed 3905.06 samples/sec  Loss 1.1521  LearningRate 0.1221  ProxyLR: 6.1052  Epoch: 5  Global Step: 31080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:36,486-Speed 3902.92 samples/sec  Loss 1.1751  LearningRate 0.1221  ProxyLR: 6.1041  Epoch: 5  Global Step: 31090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:39,109-Speed 3904.18 samples/sec  Loss 1.3135  LearningRate 0.1221  ProxyLR: 6.1030  Epoch: 5  Global Step: 31100   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:41,732-Speed 3904.71 samples/sec  Loss 1.2268  LearningRate 0.1220  ProxyLR: 6.1019  Epoch: 5  Global Step: 31110   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:44,357-Speed 3902.74 samples/sec  Loss 1.3799  LearningRate 0.1220  ProxyLR: 6.1008  Epoch: 5  Global Step: 31120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:46,980-Speed 3904.07 samples/sec  Loss 1.2613  LearningRate 0.1220  ProxyLR: 6.0997  Epoch: 5  Global Step: 31130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:49,605-Speed 3902.87 samples/sec  Loss 1.1690  LearningRate 0.1220  ProxyLR: 6.0986  Epoch: 5  Global Step: 31140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:52,227-Speed 3905.69 samples/sec  Loss 1.2249  LearningRate 0.1220  ProxyLR: 6.0975  Epoch: 5  Global Step: 31150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:43:54,851-Speed 3903.49 samples/sec  Loss 1.3564  LearningRate 0.1219  ProxyLR: 6.0964  Epoch: 5  Global Step: 31160   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:43:57,464-Speed 3920.28 samples/sec  Loss 1.2750  LearningRate 0.1219  ProxyLR: 6.0953  Epoch: 5  Global Step: 31170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:00,091-Speed 3898.50 samples/sec  Loss 1.2733  LearningRate 0.1219  ProxyLR: 6.0942  Epoch: 5  Global Step: 31180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:02,716-Speed 3900.78 samples/sec  Loss 1.2615  LearningRate 0.1219  ProxyLR: 6.0931  Epoch: 5  Global Step: 31190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:05,347-Speed 3893.08 samples/sec  Loss 1.2658  LearningRate 0.1218  ProxyLR: 6.0920  Epoch: 5  Global Step: 31200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:07,977-Speed 3894.68 samples/sec  Loss 1.3312  LearningRate 0.1218  ProxyLR: 6.0909  Epoch: 5  Global Step: 31210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:10,610-Speed 3890.55 samples/sec  Loss 1.2912  LearningRate 0.1218  ProxyLR: 6.0898  Epoch: 5  Global Step: 31220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:13,242-Speed 3891.39 samples/sec  Loss 1.3981  LearningRate 0.1218  ProxyLR: 6.0887  Epoch: 5  Global Step: 31230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:15,874-Speed 3891.40 samples/sec  Loss 1.3019  LearningRate 0.1218  ProxyLR: 6.0876  Epoch: 5  Global Step: 31240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:18,516-Speed 3877.10 samples/sec  Loss 1.3505  LearningRate 0.1217  ProxyLR: 6.0865  Epoch: 5  Global Step: 31250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:21,151-Speed 3887.65 samples/sec  Loss 1.3658  LearningRate 0.1217  ProxyLR: 6.0854  Epoch: 5  Global Step: 31260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:23,786-Speed 3886.28 samples/sec  Loss 1.2113  LearningRate 0.1217  ProxyLR: 6.0843  Epoch: 5  Global Step: 31270   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:44:26,408-Speed 3907.26 samples/sec  Loss 1.3719  LearningRate 0.1217  ProxyLR: 6.0832  Epoch: 5  Global Step: 31280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:29,044-Speed 3885.02 samples/sec  Loss 1.2159  LearningRate 0.1216  ProxyLR: 6.0821  Epoch: 5  Global Step: 31290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:31,677-Speed 3890.37 samples/sec  Loss 1.2609  LearningRate 0.1216  ProxyLR: 6.0810  Epoch: 5  Global Step: 31300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:34,310-Speed 3889.83 samples/sec  Loss 1.3189  LearningRate 0.1216  ProxyLR: 6.0799  Epoch: 5  Global Step: 31310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:36,946-Speed 3885.71 samples/sec  Loss 1.2087  LearningRate 0.1216  ProxyLR: 6.0788  Epoch: 5  Global Step: 31320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:39,581-Speed 3887.03 samples/sec  Loss 1.1628  LearningRate 0.1216  ProxyLR: 6.0777  Epoch: 5  Global Step: 31330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:42,214-Speed 3889.16 samples/sec  Loss 1.2475  LearningRate 0.1215  ProxyLR: 6.0766  Epoch: 5  Global Step: 31340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:44,850-Speed 3886.08 samples/sec  Loss 1.2719  LearningRate 0.1215  ProxyLR: 6.0756  Epoch: 5  Global Step: 31350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:47,487-Speed 3885.31 samples/sec  Loss 1.2881  LearningRate 0.1215  ProxyLR: 6.0745  Epoch: 5  Global Step: 31360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:50,117-Speed 3894.45 samples/sec  Loss 1.3075  LearningRate 0.1215  ProxyLR: 6.0734  Epoch: 5  Global Step: 31370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:52,736-Speed 3910.64 samples/sec  Loss 1.2566  LearningRate 0.1214  ProxyLR: 6.0723  Epoch: 5  Global Step: 31380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:55,367-Speed 3893.10 samples/sec  Loss 1.3788  LearningRate 0.1214  ProxyLR: 6.0712  Epoch: 5  Global Step: 31390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:44:58,000-Speed 3890.36 samples/sec  Loss 1.2801  LearningRate 0.1214  ProxyLR: 6.0701  Epoch: 5  Global Step: 31400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:00,631-Speed 3893.04 samples/sec  Loss 1.3070  LearningRate 0.1214  ProxyLR: 6.0690  Epoch: 5  Global Step: 31410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:03,263-Speed 3890.96 samples/sec  Loss 1.2371  LearningRate 0.1214  ProxyLR: 6.0679  Epoch: 5  Global Step: 31420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:05,895-Speed 3891.68 samples/sec  Loss 1.3750  LearningRate 0.1213  ProxyLR: 6.0668  Epoch: 5  Global Step: 31430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:08,527-Speed 3892.08 samples/sec  Loss 1.2935  LearningRate 0.1213  ProxyLR: 6.0657  Epoch: 5  Global Step: 31440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:11,158-Speed 3891.57 samples/sec  Loss 1.2964  LearningRate 0.1213  ProxyLR: 6.0646  Epoch: 5  Global Step: 31450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:13,778-Speed 3910.76 samples/sec  Loss 1.1949  LearningRate 0.1213  ProxyLR: 6.0635  Epoch: 5  Global Step: 31460   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:16,409-Speed 3892.13 samples/sec  Loss 1.0995  LearningRate 0.1212  ProxyLR: 6.0624  Epoch: 5  Global Step: 31470   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:19,039-Speed 3894.02 samples/sec  Loss 1.2984  LearningRate 0.1212  ProxyLR: 6.0613  Epoch: 5  Global Step: 31480   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:21,671-Speed 3892.34 samples/sec  Loss 1.2298  LearningRate 0.1212  ProxyLR: 6.0602  Epoch: 5  Global Step: 31490   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:24,301-Speed 3893.61 samples/sec  Loss 1.2574  LearningRate 0.1212  ProxyLR: 6.0591  Epoch: 5  Global Step: 31500   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:26,934-Speed 3890.82 samples/sec  Loss 1.3513  LearningRate 0.1212  ProxyLR: 6.0580  Epoch: 5  Global Step: 31510   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:29,567-Speed 3890.81 samples/sec  Loss 1.2304  LearningRate 0.1211  ProxyLR: 6.0569  Epoch: 5  Global Step: 31520   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:32,196-Speed 3895.10 samples/sec  Loss 1.2476  LearningRate 0.1211  ProxyLR: 6.0558  Epoch: 5  Global Step: 31530   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:34,827-Speed 3893.61 samples/sec  Loss 1.4160  LearningRate 0.1211  ProxyLR: 6.0547  Epoch: 5  Global Step: 31540   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:37,457-Speed 3893.79 samples/sec  Loss 1.2704  LearningRate 0.1211  ProxyLR: 6.0536  Epoch: 5  Global Step: 31550   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:45:40,090-Speed 3890.71 samples/sec  Loss 1.2516  LearningRate 0.1211  ProxyLR: 6.0525  Epoch: 5  Global Step: 31560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:42,720-Speed 3893.96 samples/sec  Loss 1.2183  LearningRate 0.1210  ProxyLR: 6.0514  Epoch: 5  Global Step: 31570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:45,351-Speed 3893.16 samples/sec  Loss 1.3334  LearningRate 0.1210  ProxyLR: 6.0504  Epoch: 5  Global Step: 31580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:47,982-Speed 3893.49 samples/sec  Loss 1.2985  LearningRate 0.1210  ProxyLR: 6.0493  Epoch: 5  Global Step: 31590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:50,612-Speed 3893.26 samples/sec  Loss 1.3069  LearningRate 0.1210  ProxyLR: 6.0482  Epoch: 5  Global Step: 31600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:53,244-Speed 3891.47 samples/sec  Loss 1.2943  LearningRate 0.1209  ProxyLR: 6.0471  Epoch: 5  Global Step: 31610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:55,878-Speed 3889.65 samples/sec  Loss 1.2926  LearningRate 0.1209  ProxyLR: 6.0460  Epoch: 5  Global Step: 31620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:45:58,508-Speed 3893.16 samples/sec  Loss 1.2798  LearningRate 0.1209  ProxyLR: 6.0449  Epoch: 5  Global Step: 31630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:01,142-Speed 3888.82 samples/sec  Loss 1.2132  LearningRate 0.1209  ProxyLR: 6.0438  Epoch: 5  Global Step: 31640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:03,774-Speed 3892.28 samples/sec  Loss 1.1965  LearningRate 0.1209  ProxyLR: 6.0427  Epoch: 5  Global Step: 31650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:06,391-Speed 3913.11 samples/sec  Loss 1.2492  LearningRate 0.1208  ProxyLR: 6.0416  Epoch: 5  Global Step: 31660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:09,024-Speed 3890.82 samples/sec  Loss 1.2725  LearningRate 0.1208  ProxyLR: 6.0405  Epoch: 5  Global Step: 31670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:11,655-Speed 3892.91 samples/sec  Loss 1.2028  LearningRate 0.1208  ProxyLR: 6.0394  Epoch: 5  Global Step: 31680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:14,287-Speed 3890.69 samples/sec  Loss 1.2909  LearningRate 0.1208  ProxyLR: 6.0383  Epoch: 5  Global Step: 31690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:16,920-Speed 3890.74 samples/sec  Loss 1.2545  LearningRate 0.1207  ProxyLR: 6.0372  Epoch: 5  Global Step: 31700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:19,550-Speed 3893.40 samples/sec  Loss 1.3241  LearningRate 0.1207  ProxyLR: 6.0361  Epoch: 5  Global Step: 31710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:22,179-Speed 3896.73 samples/sec  Loss 1.2574  LearningRate 0.1207  ProxyLR: 6.0350  Epoch: 5  Global Step: 31720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:24,811-Speed 3890.99 samples/sec  Loss 1.3214  LearningRate 0.1207  ProxyLR: 6.0340  Epoch: 5  Global Step: 31730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:27,444-Speed 3890.31 samples/sec  Loss 1.2677  LearningRate 0.1207  ProxyLR: 6.0329  Epoch: 5  Global Step: 31740   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:30,075-Speed 3893.23 samples/sec  Loss 1.3992  LearningRate 0.1206  ProxyLR: 6.0318  Epoch: 5  Global Step: 31750   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:32,693-Speed 3911.89 samples/sec  Loss 1.2653  LearningRate 0.1206  ProxyLR: 6.0307  Epoch: 5  Global Step: 31760   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:35,326-Speed 3890.81 samples/sec  Loss 1.2915  LearningRate 0.1206  ProxyLR: 6.0296  Epoch: 5  Global Step: 31770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:37,957-Speed 3892.88 samples/sec  Loss 1.2977  LearningRate 0.1206  ProxyLR: 6.0285  Epoch: 5  Global Step: 31780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:40,588-Speed 3892.07 samples/sec  Loss 1.2610  LearningRate 0.1205  ProxyLR: 6.0274  Epoch: 5  Global Step: 31790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:46:43,206-Speed 3912.50 samples/sec  Loss 1.3452  LearningRate 0.1205  ProxyLR: 6.0263  Epoch: 5  Global Step: 31800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:46:45,837-Speed 3893.66 samples/sec  Loss 1.2714  LearningRate 0.1205  ProxyLR: 6.0252  Epoch: 5  Global Step: 31810   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:46:48,470-Speed 3889.51 samples/sec  Loss 1.3189  LearningRate 0.1205  ProxyLR: 6.0241  Epoch: 5  Global Step: 31820   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:46:51,101-Speed 3893.17 samples/sec  Loss 1.4735  LearningRate 0.1205  ProxyLR: 6.0230  Epoch: 5  Global Step: 31830   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:46:53,731-Speed 3894.57 samples/sec  Loss 1.2666  LearningRate 0.1204  ProxyLR: 6.0219  Epoch: 5  Global Step: 31840   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:46:56,361-Speed 3893.55 samples/sec  Loss 1.2303  LearningRate 0.1204  ProxyLR: 6.0208  Epoch: 5  Global Step: 31850   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:46:58,993-Speed 3892.42 samples/sec  Loss 1.2298  LearningRate 0.1204  ProxyLR: 6.0198  Epoch: 5  Global Step: 31860   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:47:01,624-Speed 3893.16 samples/sec  Loss 1.2908  LearningRate 0.1204  ProxyLR: 6.0187  Epoch: 5  Global Step: 31870   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:47:04,259-Speed 3887.39 samples/sec  Loss 1.2541  LearningRate 0.1204  ProxyLR: 6.0176  Epoch: 5  Global Step: 31880   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:47:06,889-Speed 3893.47 samples/sec  Loss 1.2894  LearningRate 0.1203  ProxyLR: 6.0165  Epoch: 5  Global Step: 31890   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:47:09,520-Speed 3892.95 samples/sec  Loss 1.2668  LearningRate 0.1203  ProxyLR: 6.0154  Epoch: 5  Global Step: 31900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:12,154-Speed 3888.63 samples/sec  Loss 1.4034  LearningRate 0.1203  ProxyLR: 6.0143  Epoch: 5  Global Step: 31910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:14,788-Speed 3889.22 samples/sec  Loss 1.2910  LearningRate 0.1203  ProxyLR: 6.0132  Epoch: 5  Global Step: 31920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:17,418-Speed 3893.39 samples/sec  Loss 1.3587  LearningRate 0.1202  ProxyLR: 6.0121  Epoch: 5  Global Step: 31930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:20,051-Speed 3890.86 samples/sec  Loss 1.3268  LearningRate 0.1202  ProxyLR: 6.0110  Epoch: 5  Global Step: 31940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:22,683-Speed 3891.63 samples/sec  Loss 1.3748  LearningRate 0.1202  ProxyLR: 6.0099  Epoch: 5  Global Step: 31950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:25,314-Speed 3893.09 samples/sec  Loss 1.2541  LearningRate 0.1202  ProxyLR: 6.0088  Epoch: 5  Global Step: 31960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:27,944-Speed 3893.14 samples/sec  Loss 1.2923  LearningRate 0.1202  ProxyLR: 6.0077  Epoch: 5  Global Step: 31970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:30,576-Speed 3891.86 samples/sec  Loss 1.2618  LearningRate 0.1201  ProxyLR: 6.0067  Epoch: 5  Global Step: 31980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:33,206-Speed 3893.85 samples/sec  Loss 1.3645  LearningRate 0.1201  ProxyLR: 6.0056  Epoch: 5  Global Step: 31990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:35,838-Speed 3892.40 samples/sec  Loss 1.3939  LearningRate 0.1201  ProxyLR: 6.0045  Epoch: 5  Global Step: 32000   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:47:38,455-Speed 3913.18 samples/sec  Loss 1.2850  LearningRate 0.1201  ProxyLR: 6.0034  Epoch: 5  Global Step: 32010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:41,088-Speed 3890.97 samples/sec  Loss 1.2537  LearningRate 0.1200  ProxyLR: 6.0023  Epoch: 5  Global Step: 32020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:43,720-Speed 3890.63 samples/sec  Loss 1.2631  LearningRate 0.1200  ProxyLR: 6.0012  Epoch: 5  Global Step: 32030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:46,352-Speed 3891.24 samples/sec  Loss 1.2821  LearningRate 0.1200  ProxyLR: 6.0001  Epoch: 5  Global Step: 32040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:48,985-Speed 3890.57 samples/sec  Loss 1.2843  LearningRate 0.1200  ProxyLR: 5.9990  Epoch: 5  Global Step: 32050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:51,615-Speed 3893.88 samples/sec  Loss 1.2694  LearningRate 0.1200  ProxyLR: 5.9979  Epoch: 5  Global Step: 32060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:54,244-Speed 3896.36 samples/sec  Loss 1.4380  LearningRate 0.1199  ProxyLR: 5.9968  Epoch: 5  Global Step: 32070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:56,875-Speed 3893.75 samples/sec  Loss 1.3372  LearningRate 0.1199  ProxyLR: 5.9958  Epoch: 5  Global Step: 32080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:47:59,506-Speed 3892.94 samples/sec  Loss 1.4544  LearningRate 0.1199  ProxyLR: 5.9947  Epoch: 5  Global Step: 32090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:02,136-Speed 3893.94 samples/sec  Loss 1.4807  LearningRate 0.1199  ProxyLR: 5.9936  Epoch: 5  Global Step: 32100   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:04,768-Speed 3891.61 samples/sec  Loss 1.2855  LearningRate 0.1198  ProxyLR: 5.9925  Epoch: 5  Global Step: 32110   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:48:07,387-Speed 3910.41 samples/sec  Loss 1.3260  LearningRate 0.1198  ProxyLR: 5.9914  Epoch: 5  Global Step: 32120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:10,019-Speed 3891.27 samples/sec  Loss 1.2637  LearningRate 0.1198  ProxyLR: 5.9903  Epoch: 5  Global Step: 32130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:12,651-Speed 3891.56 samples/sec  Loss 1.3407  LearningRate 0.1198  ProxyLR: 5.9892  Epoch: 5  Global Step: 32140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:15,283-Speed 3891.38 samples/sec  Loss 1.3614  LearningRate 0.1198  ProxyLR: 5.9881  Epoch: 5  Global Step: 32150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:17,916-Speed 3890.07 samples/sec  Loss 1.3760  LearningRate 0.1197  ProxyLR: 5.9870  Epoch: 5  Global Step: 32160   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:20,549-Speed 3891.84 samples/sec  Loss 1.4644  LearningRate 0.1197  ProxyLR: 5.9860  Epoch: 5  Global Step: 32170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:23,180-Speed 3892.77 samples/sec  Loss 1.3486  LearningRate 0.1197  ProxyLR: 5.9849  Epoch: 5  Global Step: 32180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:25,809-Speed 3895.29 samples/sec  Loss 1.3394  LearningRate 0.1197  ProxyLR: 5.9838  Epoch: 5  Global Step: 32190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:28,441-Speed 3892.15 samples/sec  Loss 1.3509  LearningRate 0.1197  ProxyLR: 5.9827  Epoch: 5  Global Step: 32200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:31,071-Speed 3893.41 samples/sec  Loss 1.3955  LearningRate 0.1196  ProxyLR: 5.9816  Epoch: 5  Global Step: 32210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:33,690-Speed 3911.61 samples/sec  Loss 1.4971  LearningRate 0.1196  ProxyLR: 5.9805  Epoch: 5  Global Step: 32220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:36,321-Speed 3893.41 samples/sec  Loss 1.4711  LearningRate 0.1196  ProxyLR: 5.9794  Epoch: 5  Global Step: 32230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:38,952-Speed 3892.81 samples/sec  Loss 1.3183  LearningRate 0.1196  ProxyLR: 5.9783  Epoch: 5  Global Step: 32240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:41,584-Speed 3891.59 samples/sec  Loss 1.3472  LearningRate 0.1195  ProxyLR: 5.9773  Epoch: 5  Global Step: 32250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:44,214-Speed 3893.43 samples/sec  Loss 1.3576  LearningRate 0.1195  ProxyLR: 5.9762  Epoch: 5  Global Step: 32260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:46,844-Speed 3894.92 samples/sec  Loss 1.4129  LearningRate 0.1195  ProxyLR: 5.9751  Epoch: 5  Global Step: 32270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:49,475-Speed 3892.23 samples/sec  Loss 1.2945  LearningRate 0.1195  ProxyLR: 5.9740  Epoch: 5  Global Step: 32280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:52,108-Speed 3891.44 samples/sec  Loss 1.3662  LearningRate 0.1195  ProxyLR: 5.9729  Epoch: 5  Global Step: 32290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:54,738-Speed 3894.18 samples/sec  Loss 1.3259  LearningRate 0.1194  ProxyLR: 5.9718  Epoch: 5  Global Step: 32300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:57,368-Speed 3893.43 samples/sec  Loss 1.3603  LearningRate 0.1194  ProxyLR: 5.9707  Epoch: 5  Global Step: 32310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:48:59,987-Speed 3911.11 samples/sec  Loss 1.2480  LearningRate 0.1194  ProxyLR: 5.9696  Epoch: 5  Global Step: 32320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:02,620-Speed 3889.85 samples/sec  Loss 1.4026  LearningRate 0.1194  ProxyLR: 5.9686  Epoch: 5  Global Step: 32330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:05,252-Speed 3891.91 samples/sec  Loss 1.2963  LearningRate 0.1193  ProxyLR: 5.9675  Epoch: 5  Global Step: 32340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:07,883-Speed 3892.67 samples/sec  Loss 1.3513  LearningRate 0.1193  ProxyLR: 5.9664  Epoch: 5  Global Step: 32350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:10,514-Speed 3893.65 samples/sec  Loss 1.3366  LearningRate 0.1193  ProxyLR: 5.9653  Epoch: 5  Global Step: 32360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:13,143-Speed 3895.63 samples/sec  Loss 1.3681  LearningRate 0.1193  ProxyLR: 5.9642  Epoch: 5  Global Step: 32370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:15,774-Speed 3892.77 samples/sec  Loss 1.3203  LearningRate 0.1193  ProxyLR: 5.9631  Epoch: 5  Global Step: 32380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:18,404-Speed 3895.17 samples/sec  Loss 1.2958  LearningRate 0.1192  ProxyLR: 5.9620  Epoch: 5  Global Step: 32390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:21,034-Speed 3893.64 samples/sec  Loss 1.3672  LearningRate 0.1192  ProxyLR: 5.9609  Epoch: 5  Global Step: 32400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:23,664-Speed 3894.92 samples/sec  Loss 1.3947  LearningRate 0.1192  ProxyLR: 5.9599  Epoch: 5  Global Step: 32410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:26,295-Speed 3892.85 samples/sec  Loss 1.3807  LearningRate 0.1192  ProxyLR: 5.9588  Epoch: 5  Global Step: 32420   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:49:28,916-Speed 3907.93 samples/sec  Loss 1.3964  LearningRate 0.1192  ProxyLR: 5.9577  Epoch: 5  Global Step: 32430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:31,546-Speed 3893.76 samples/sec  Loss 1.3334  LearningRate 0.1191  ProxyLR: 5.9566  Epoch: 5  Global Step: 32440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:34,177-Speed 3893.64 samples/sec  Loss 1.3865  LearningRate 0.1191  ProxyLR: 5.9555  Epoch: 5  Global Step: 32450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:36,808-Speed 3892.79 samples/sec  Loss 1.3405  LearningRate 0.1191  ProxyLR: 5.9544  Epoch: 5  Global Step: 32460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:39,438-Speed 3894.08 samples/sec  Loss 1.3091  LearningRate 0.1191  ProxyLR: 5.9533  Epoch: 5  Global Step: 32470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:42,069-Speed 3893.78 samples/sec  Loss 1.3824  LearningRate 0.1190  ProxyLR: 5.9523  Epoch: 5  Global Step: 32480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:44,700-Speed 3892.50 samples/sec  Loss 1.3423  LearningRate 0.1190  ProxyLR: 5.9512  Epoch: 5  Global Step: 32490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:47,331-Speed 3893.61 samples/sec  Loss 1.4270  LearningRate 0.1190  ProxyLR: 5.9501  Epoch: 5  Global Step: 32500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:49,961-Speed 3894.80 samples/sec  Loss 1.4234  LearningRate 0.1190  ProxyLR: 5.9490  Epoch: 5  Global Step: 32510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:52,590-Speed 3895.29 samples/sec  Loss 1.3995  LearningRate 0.1190  ProxyLR: 5.9479  Epoch: 5  Global Step: 32520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:55,209-Speed 3911.29 samples/sec  Loss 1.3319  LearningRate 0.1189  ProxyLR: 5.9468  Epoch: 5  Global Step: 32530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:49:57,840-Speed 3892.72 samples/sec  Loss 1.3369  LearningRate 0.1189  ProxyLR: 5.9457  Epoch: 5  Global Step: 32540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:00,471-Speed 3893.21 samples/sec  Loss 1.2124  LearningRate 0.1189  ProxyLR: 5.9447  Epoch: 5  Global Step: 32550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:03,101-Speed 3894.26 samples/sec  Loss 1.4065  LearningRate 0.1189  ProxyLR: 5.9436  Epoch: 5  Global Step: 32560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:05,732-Speed 3893.49 samples/sec  Loss 1.3134  LearningRate 0.1188  ProxyLR: 5.9425  Epoch: 5  Global Step: 32570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:08,363-Speed 3892.47 samples/sec  Loss 1.3363  LearningRate 0.1188  ProxyLR: 5.9414  Epoch: 5  Global Step: 32580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:10,997-Speed 3888.61 samples/sec  Loss 1.3121  LearningRate 0.1188  ProxyLR: 5.9403  Epoch: 5  Global Step: 32590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:13,631-Speed 3888.83 samples/sec  Loss 1.3173  LearningRate 0.1188  ProxyLR: 5.9392  Epoch: 5  Global Step: 32600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:16,262-Speed 3893.29 samples/sec  Loss 1.4382  LearningRate 0.1188  ProxyLR: 5.9382  Epoch: 5  Global Step: 32610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:18,893-Speed 3892.24 samples/sec  Loss 1.3474  LearningRate 0.1187  ProxyLR: 5.9371  Epoch: 5  Global Step: 32620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:21,524-Speed 3893.84 samples/sec  Loss 1.4297  LearningRate 0.1187  ProxyLR: 5.9360  Epoch: 5  Global Step: 32630   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:50:24,140-Speed 3914.98 samples/sec  Loss 1.3565  LearningRate 0.1187  ProxyLR: 5.9349  Epoch: 5  Global Step: 32640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:26,768-Speed 3897.67 samples/sec  Loss 1.4636  LearningRate 0.1187  ProxyLR: 5.9338  Epoch: 5  Global Step: 32650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:29,397-Speed 3895.75 samples/sec  Loss 1.3426  LearningRate 0.1187  ProxyLR: 5.9327  Epoch: 5  Global Step: 32660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:50:32,012-Speed 3915.74 samples/sec  Loss 1.3719  LearningRate 0.1186  ProxyLR: 5.9317  Epoch: 5  Global Step: 32670   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:34,642-Speed 3894.37 samples/sec  Loss 1.3916  LearningRate 0.1186  ProxyLR: 5.9306  Epoch: 5  Global Step: 32680   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:37,273-Speed 3893.54 samples/sec  Loss 1.4165  LearningRate 0.1186  ProxyLR: 5.9295  Epoch: 5  Global Step: 32690   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:39,904-Speed 3892.89 samples/sec  Loss 1.5890  LearningRate 0.1186  ProxyLR: 5.9284  Epoch: 5  Global Step: 32700   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:42,534-Speed 3894.39 samples/sec  Loss 1.4402  LearningRate 0.1185  ProxyLR: 5.9273  Epoch: 5  Global Step: 32710   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:45,163-Speed 3895.60 samples/sec  Loss 1.3060  LearningRate 0.1185  ProxyLR: 5.9262  Epoch: 5  Global Step: 32720   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:47,792-Speed 3896.14 samples/sec  Loss 1.3711  LearningRate 0.1185  ProxyLR: 5.9252  Epoch: 5  Global Step: 32730   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:50,421-Speed 3896.26 samples/sec  Loss 1.2374  LearningRate 0.1185  ProxyLR: 5.9241  Epoch: 5  Global Step: 32740   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:53,048-Speed 3898.58 samples/sec  Loss 1.3859  LearningRate 0.1185  ProxyLR: 5.9230  Epoch: 5  Global Step: 32750   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:55,680-Speed 3892.35 samples/sec  Loss 1.3941  LearningRate 0.1184  ProxyLR: 5.9219  Epoch: 5  Global Step: 32760   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:50:58,309-Speed 3895.21 samples/sec  Loss 1.2367  LearningRate 0.1184  ProxyLR: 5.9208  Epoch: 5  Global Step: 32770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:00,942-Speed 3890.73 samples/sec  Loss 1.3997  LearningRate 0.1184  ProxyLR: 5.9197  Epoch: 5  Global Step: 32780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:03,575-Speed 3890.07 samples/sec  Loss 1.3030  LearningRate 0.1184  ProxyLR: 5.9187  Epoch: 5  Global Step: 32790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:06,208-Speed 3889.25 samples/sec  Loss 1.4337  LearningRate 0.1184  ProxyLR: 5.9176  Epoch: 5  Global Step: 32800   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:08,841-Speed 3890.36 samples/sec  Loss 1.2715  LearningRate 0.1183  ProxyLR: 5.9165  Epoch: 5  Global Step: 32810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:11,472-Speed 3893.52 samples/sec  Loss 1.3071  LearningRate 0.1183  ProxyLR: 5.9154  Epoch: 5  Global Step: 32820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:14,104-Speed 3890.01 samples/sec  Loss 1.2884  LearningRate 0.1183  ProxyLR: 5.9143  Epoch: 5  Global Step: 32830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:16,736-Speed 3891.75 samples/sec  Loss 1.4359  LearningRate 0.1183  ProxyLR: 5.9132  Epoch: 5  Global Step: 32840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:19,366-Speed 3894.12 samples/sec  Loss 1.3802  LearningRate 0.1182  ProxyLR: 5.9122  Epoch: 5  Global Step: 32850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:22,002-Speed 3886.90 samples/sec  Loss 1.4376  LearningRate 0.1182  ProxyLR: 5.9111  Epoch: 5  Global Step: 32860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:24,619-Speed 3913.28 samples/sec  Loss 1.2746  LearningRate 0.1182  ProxyLR: 5.9100  Epoch: 5  Global Step: 32870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:27,251-Speed 3891.77 samples/sec  Loss 1.2854  LearningRate 0.1182  ProxyLR: 5.9089  Epoch: 5  Global Step: 32880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:29,881-Speed 3894.76 samples/sec  Loss 1.4562  LearningRate 0.1182  ProxyLR: 5.9078  Epoch: 5  Global Step: 32890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:32,514-Speed 3889.38 samples/sec  Loss 1.4219  LearningRate 0.1181  ProxyLR: 5.9068  Epoch: 5  Global Step: 32900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:35,148-Speed 3889.38 samples/sec  Loss 1.3008  LearningRate 0.1181  ProxyLR: 5.9057  Epoch: 5  Global Step: 32910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:37,780-Speed 3890.45 samples/sec  Loss 1.4162  LearningRate 0.1181  ProxyLR: 5.9046  Epoch: 5  Global Step: 32920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:40,413-Speed 3890.83 samples/sec  Loss 1.4088  LearningRate 0.1181  ProxyLR: 5.9035  Epoch: 5  Global Step: 32930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:43,045-Speed 3891.04 samples/sec  Loss 1.4135  LearningRate 0.1180  ProxyLR: 5.9024  Epoch: 5  Global Step: 32940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:45,677-Speed 3891.02 samples/sec  Loss 1.2946  LearningRate 0.1180  ProxyLR: 5.9014  Epoch: 5  Global Step: 32950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:48,313-Speed 3886.56 samples/sec  Loss 1.4807  LearningRate 0.1180  ProxyLR: 5.9003  Epoch: 5  Global Step: 32960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:50,947-Speed 3888.69 samples/sec  Loss 1.3728  LearningRate 0.1180  ProxyLR: 5.8992  Epoch: 5  Global Step: 32970   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:51:53,566-Speed 3911.25 samples/sec  Loss 1.3633  LearningRate 0.1180  ProxyLR: 5.8981  Epoch: 5  Global Step: 32980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:56,197-Speed 3892.64 samples/sec  Loss 1.4561  LearningRate 0.1179  ProxyLR: 5.8970  Epoch: 5  Global Step: 32990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:51:58,826-Speed 3896.16 samples/sec  Loss 1.2674  LearningRate 0.1179  ProxyLR: 5.8959  Epoch: 5  Global Step: 33000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:01,458-Speed 3891.70 samples/sec  Loss 1.4249  LearningRate 0.1179  ProxyLR: 5.8949  Epoch: 5  Global Step: 33010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:04,076-Speed 3912.69 samples/sec  Loss 1.4001  LearningRate 0.1179  ProxyLR: 5.8938  Epoch: 5  Global Step: 33020   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:06,706-Speed 3893.61 samples/sec  Loss 1.3677  LearningRate 0.1179  ProxyLR: 5.8927  Epoch: 5  Global Step: 33030   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:09,337-Speed 3893.99 samples/sec  Loss 1.3612  LearningRate 0.1178  ProxyLR: 5.8916  Epoch: 5  Global Step: 33040   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:11,966-Speed 3895.18 samples/sec  Loss 1.4205  LearningRate 0.1178  ProxyLR: 5.8905  Epoch: 5  Global Step: 33050   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:14,597-Speed 3892.94 samples/sec  Loss 1.4620  LearningRate 0.1178  ProxyLR: 5.8895  Epoch: 5  Global Step: 33060   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:17,229-Speed 3891.67 samples/sec  Loss 1.4095  LearningRate 0.1178  ProxyLR: 5.8884  Epoch: 5  Global Step: 33070   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:19,859-Speed 3894.59 samples/sec  Loss 1.3397  LearningRate 0.1177  ProxyLR: 5.8873  Epoch: 5  Global Step: 33080   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:22,491-Speed 3891.49 samples/sec  Loss 1.4534  LearningRate 0.1177  ProxyLR: 5.8862  Epoch: 5  Global Step: 33090   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:25,124-Speed 3890.22 samples/sec  Loss 1.4024  LearningRate 0.1177  ProxyLR: 5.8852  Epoch: 5  Global Step: 33100   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:27,756-Speed 3891.77 samples/sec  Loss 1.3725  LearningRate 0.1177  ProxyLR: 5.8841  Epoch: 5  Global Step: 33110   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:30,387-Speed 3892.51 samples/sec  Loss 1.3200  LearningRate 0.1177  ProxyLR: 5.8830  Epoch: 5  Global Step: 33120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:33,020-Speed 3890.36 samples/sec  Loss 1.3913  LearningRate 0.1176  ProxyLR: 5.8819  Epoch: 5  Global Step: 33130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:35,652-Speed 3891.56 samples/sec  Loss 1.4174  LearningRate 0.1176  ProxyLR: 5.8808  Epoch: 5  Global Step: 33140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:38,283-Speed 3893.68 samples/sec  Loss 1.4148  LearningRate 0.1176  ProxyLR: 5.8798  Epoch: 5  Global Step: 33150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:40,913-Speed 3894.02 samples/sec  Loss 1.3483  LearningRate 0.1176  ProxyLR: 5.8787  Epoch: 5  Global Step: 33160   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:43,544-Speed 3893.41 samples/sec  Loss 1.3133  LearningRate 0.1176  ProxyLR: 5.8776  Epoch: 5  Global Step: 33170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:46,174-Speed 3893.62 samples/sec  Loss 1.2969  LearningRate 0.1175  ProxyLR: 5.8765  Epoch: 5  Global Step: 33180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:52:48,790-Speed 3915.02 samples/sec  Loss 1.4523  LearningRate 0.1175  ProxyLR: 5.8754  Epoch: 5  Global Step: 33190   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:51,418-Speed 3898.27 samples/sec  Loss 1.3823  LearningRate 0.1175  ProxyLR: 5.8744  Epoch: 5  Global Step: 33200   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:54,048-Speed 3894.15 samples/sec  Loss 1.3297  LearningRate 0.1175  ProxyLR: 5.8733  Epoch: 5  Global Step: 33210   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:56,679-Speed 3893.68 samples/sec  Loss 1.4220  LearningRate 0.1174  ProxyLR: 5.8722  Epoch: 5  Global Step: 33220   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:52:59,307-Speed 3896.35 samples/sec  Loss 1.4386  LearningRate 0.1174  ProxyLR: 5.8711  Epoch: 5  Global Step: 33230   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:53:01,937-Speed 3894.62 samples/sec  Loss 1.3535  LearningRate 0.1174  ProxyLR: 5.8700  Epoch: 5  Global Step: 33240   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:53:04,566-Speed 3896.83 samples/sec  Loss 1.3152  LearningRate 0.1174  ProxyLR: 5.8690  Epoch: 5  Global Step: 33250   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:53:07,195-Speed 3896.02 samples/sec  Loss 1.4707  LearningRate 0.1174  ProxyLR: 5.8679  Epoch: 5  Global Step: 33260   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:53:09,824-Speed 3895.29 samples/sec  Loss 1.3971  LearningRate 0.1173  ProxyLR: 5.8668  Epoch: 5  Global Step: 33270   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:53:12,455-Speed 3893.56 samples/sec  Loss 1.4278  LearningRate 0.1173  ProxyLR: 5.8657  Epoch: 5  Global Step: 33280   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:53:15,091-Speed 3884.80 samples/sec  Loss 1.2894  LearningRate 0.1173  ProxyLR: 5.8647  Epoch: 5  Global Step: 33290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:17,730-Speed 3882.11 samples/sec  Loss 1.3250  LearningRate 0.1173  ProxyLR: 5.8636  Epoch: 5  Global Step: 33300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:20,365-Speed 3886.64 samples/sec  Loss 1.3090  LearningRate 0.1173  ProxyLR: 5.8625  Epoch: 5  Global Step: 33310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:23,000-Speed 3886.26 samples/sec  Loss 1.4756  LearningRate 0.1172  ProxyLR: 5.8614  Epoch: 5  Global Step: 33320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:25,634-Speed 3889.76 samples/sec  Loss 1.3456  LearningRate 0.1172  ProxyLR: 5.8604  Epoch: 5  Global Step: 33330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:28,265-Speed 3891.85 samples/sec  Loss 1.3196  LearningRate 0.1172  ProxyLR: 5.8593  Epoch: 5  Global Step: 33340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:30,897-Speed 3892.35 samples/sec  Loss 1.3579  LearningRate 0.1172  ProxyLR: 5.8582  Epoch: 5  Global Step: 33350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:33,528-Speed 3891.79 samples/sec  Loss 1.2742  LearningRate 0.1171  ProxyLR: 5.8571  Epoch: 5  Global Step: 33360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:36,155-Speed 3898.91 samples/sec  Loss 1.2958  LearningRate 0.1171  ProxyLR: 5.8560  Epoch: 5  Global Step: 33370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:38,781-Speed 3901.68 samples/sec  Loss 1.4618  LearningRate 0.1171  ProxyLR: 5.8550  Epoch: 5  Global Step: 33380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:41,407-Speed 3900.61 samples/sec  Loss 1.3597  LearningRate 0.1171  ProxyLR: 5.8539  Epoch: 5  Global Step: 33390   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:53:44,019-Speed 3921.33 samples/sec  Loss 1.3712  LearningRate 0.1171  ProxyLR: 5.8528  Epoch: 5  Global Step: 33400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:46,643-Speed 3902.31 samples/sec  Loss 1.3809  LearningRate 0.1170  ProxyLR: 5.8517  Epoch: 5  Global Step: 33410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:49,267-Speed 3904.46 samples/sec  Loss 1.4277  LearningRate 0.1170  ProxyLR: 5.8507  Epoch: 5  Global Step: 33420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:51,890-Speed 3904.31 samples/sec  Loss 1.4257  LearningRate 0.1170  ProxyLR: 5.8496  Epoch: 5  Global Step: 33430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:54,514-Speed 3903.88 samples/sec  Loss 1.4826  LearningRate 0.1170  ProxyLR: 5.8485  Epoch: 5  Global Step: 33440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:57,138-Speed 3902.84 samples/sec  Loss 1.5420  LearningRate 0.1169  ProxyLR: 5.8474  Epoch: 5  Global Step: 33450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:53:59,762-Speed 3903.28 samples/sec  Loss 1.4331  LearningRate 0.1169  ProxyLR: 5.8464  Epoch: 5  Global Step: 33460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:02,385-Speed 3904.68 samples/sec  Loss 1.3519  LearningRate 0.1169  ProxyLR: 5.8453  Epoch: 5  Global Step: 33470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:05,008-Speed 3904.98 samples/sec  Loss 1.3043  LearningRate 0.1169  ProxyLR: 5.8442  Epoch: 5  Global Step: 33480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:07,631-Speed 3904.09 samples/sec  Loss 1.4267  LearningRate 0.1169  ProxyLR: 5.8431  Epoch: 5  Global Step: 33490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:10,242-Speed 3923.11 samples/sec  Loss 1.4362  LearningRate 0.1168  ProxyLR: 5.8421  Epoch: 5  Global Step: 33500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:12,852-Speed 3925.24 samples/sec  Loss 1.4334  LearningRate 0.1168  ProxyLR: 5.8410  Epoch: 5  Global Step: 33510   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:15,475-Speed 3903.53 samples/sec  Loss 1.2743  LearningRate 0.1168  ProxyLR: 5.8399  Epoch: 5  Global Step: 33520   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:18,100-Speed 3902.95 samples/sec  Loss 1.3509  LearningRate 0.1168  ProxyLR: 5.8388  Epoch: 5  Global Step: 33530   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:20,722-Speed 3906.47 samples/sec  Loss 1.5228  LearningRate 0.1168  ProxyLR: 5.8378  Epoch: 5  Global Step: 33540   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:23,345-Speed 3904.43 samples/sec  Loss 1.4247  LearningRate 0.1167  ProxyLR: 5.8367  Epoch: 5  Global Step: 33550   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:25,968-Speed 3904.57 samples/sec  Loss 1.3983  LearningRate 0.1167  ProxyLR: 5.8356  Epoch: 5  Global Step: 33560   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:28,593-Speed 3902.02 samples/sec  Loss 1.3091  LearningRate 0.1167  ProxyLR: 5.8345  Epoch: 5  Global Step: 33570   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:31,217-Speed 3903.53 samples/sec  Loss 1.4134  LearningRate 0.1167  ProxyLR: 5.8335  Epoch: 5  Global Step: 33580   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:33,841-Speed 3902.75 samples/sec  Loss 1.3726  LearningRate 0.1166  ProxyLR: 5.8324  Epoch: 5  Global Step: 33590   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:36,464-Speed 3905.14 samples/sec  Loss 1.2878  LearningRate 0.1166  ProxyLR: 5.8313  Epoch: 5  Global Step: 33600   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:54:39,087-Speed 3904.92 samples/sec  Loss 1.4295  LearningRate 0.1166  ProxyLR: 5.8302  Epoch: 5  Global Step: 33610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:41,710-Speed 3904.35 samples/sec  Loss 1.2447  LearningRate 0.1166  ProxyLR: 5.8292  Epoch: 5  Global Step: 33620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:44,335-Speed 3902.84 samples/sec  Loss 1.3380  LearningRate 0.1166  ProxyLR: 5.8281  Epoch: 5  Global Step: 33630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:46,959-Speed 3903.79 samples/sec  Loss 1.5579  LearningRate 0.1165  ProxyLR: 5.8270  Epoch: 5  Global Step: 33640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:49,582-Speed 3903.44 samples/sec  Loss 1.4198  LearningRate 0.1165  ProxyLR: 5.8259  Epoch: 5  Global Step: 33650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:52,207-Speed 3902.84 samples/sec  Loss 1.4142  LearningRate 0.1165  ProxyLR: 5.8249  Epoch: 5  Global Step: 33660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:54,831-Speed 3902.97 samples/sec  Loss 1.3876  LearningRate 0.1165  ProxyLR: 5.8238  Epoch: 5  Global Step: 33670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:54:57,784-Speed 3467.84 samples/sec  Loss 1.4095  LearningRate 0.1165  ProxyLR: 5.8227  Epoch: 5  Global Step: 33680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:00,409-Speed 3901.93 samples/sec  Loss 1.4044  LearningRate 0.1164  ProxyLR: 5.8216  Epoch: 5  Global Step: 33690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:03,033-Speed 3904.32 samples/sec  Loss 1.4058  LearningRate 0.1164  ProxyLR: 5.8206  Epoch: 5  Global Step: 33700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:05,658-Speed 3901.67 samples/sec  Loss 1.3120  LearningRate 0.1164  ProxyLR: 5.8195  Epoch: 5  Global Step: 33710   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:55:08,271-Speed 3920.34 samples/sec  Loss 1.3634  LearningRate 0.1164  ProxyLR: 5.8184  Epoch: 5  Global Step: 33720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:10,901-Speed 3893.38 samples/sec  Loss 1.3764  LearningRate 0.1163  ProxyLR: 5.8173  Epoch: 5  Global Step: 33730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:13,531-Speed 3894.43 samples/sec  Loss 1.3114  LearningRate 0.1163  ProxyLR: 5.8163  Epoch: 5  Global Step: 33740   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:16,162-Speed 3893.80 samples/sec  Loss 1.3397  LearningRate 0.1163  ProxyLR: 5.8152  Epoch: 5  Global Step: 33750   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:18,794-Speed 3890.58 samples/sec  Loss 1.3361  LearningRate 0.1163  ProxyLR: 5.8141  Epoch: 5  Global Step: 33760   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:21,413-Speed 3911.71 samples/sec  Loss 1.3084  LearningRate 0.1163  ProxyLR: 5.8131  Epoch: 5  Global Step: 33770   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:24,045-Speed 3890.69 samples/sec  Loss 1.2516  LearningRate 0.1162  ProxyLR: 5.8120  Epoch: 5  Global Step: 33780   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:26,676-Speed 3893.46 samples/sec  Loss 1.3886  LearningRate 0.1162  ProxyLR: 5.8109  Epoch: 5  Global Step: 33790   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:29,305-Speed 3895.46 samples/sec  Loss 1.4073  LearningRate 0.1162  ProxyLR: 5.8098  Epoch: 5  Global Step: 33800   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:31,937-Speed 3892.33 samples/sec  Loss 1.3680  LearningRate 0.1162  ProxyLR: 5.8088  Epoch: 5  Global Step: 33810   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:34,567-Speed 3894.39 samples/sec  Loss 1.3709  LearningRate 0.1162  ProxyLR: 5.8077  Epoch: 5  Global Step: 33820   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:37,196-Speed 3895.02 samples/sec  Loss 1.4018  LearningRate 0.1161  ProxyLR: 5.8066  Epoch: 5  Global Step: 33830   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:39,825-Speed 3895.96 samples/sec  Loss 1.3937  LearningRate 0.1161  ProxyLR: 5.8056  Epoch: 5  Global Step: 33840   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:42,455-Speed 3894.65 samples/sec  Loss 1.4663  LearningRate 0.1161  ProxyLR: 5.8045  Epoch: 5  Global Step: 33850   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:45,084-Speed 3895.60 samples/sec  Loss 1.2954  LearningRate 0.1161  ProxyLR: 5.8034  Epoch: 5  Global Step: 33860   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:55:47,715-Speed 3892.80 samples/sec  Loss 1.3784  LearningRate 0.1160  ProxyLR: 5.8023  Epoch: 5  Global Step: 33870   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:50,348-Speed 3890.59 samples/sec  Loss 1.4215  LearningRate 0.1160  ProxyLR: 5.8013  Epoch: 5  Global Step: 33880   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:52,977-Speed 3896.41 samples/sec  Loss 1.4164  LearningRate 0.1160  ProxyLR: 5.8002  Epoch: 5  Global Step: 33890   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:55,610-Speed 3889.57 samples/sec  Loss 1.3775  LearningRate 0.1160  ProxyLR: 5.7991  Epoch: 5  Global Step: 33900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:55:58,240-Speed 3895.33 samples/sec  Loss 1.3840  LearningRate 0.1160  ProxyLR: 5.7980  Epoch: 5  Global Step: 33910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:00,871-Speed 3893.20 samples/sec  Loss 1.5092  LearningRate 0.1159  ProxyLR: 5.7970  Epoch: 5  Global Step: 33920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:03,502-Speed 3892.29 samples/sec  Loss 1.4387  LearningRate 0.1159  ProxyLR: 5.7959  Epoch: 5  Global Step: 33930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:06,133-Speed 3893.59 samples/sec  Loss 1.3908  LearningRate 0.1159  ProxyLR: 5.7948  Epoch: 5  Global Step: 33940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:08,763-Speed 3894.97 samples/sec  Loss 1.4598  LearningRate 0.1159  ProxyLR: 5.7938  Epoch: 5  Global Step: 33950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:11,392-Speed 3895.42 samples/sec  Loss 1.3622  LearningRate 0.1159  ProxyLR: 5.7927  Epoch: 5  Global Step: 33960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:14,022-Speed 3894.84 samples/sec  Loss 1.4088  LearningRate 0.1158  ProxyLR: 5.7916  Epoch: 5  Global Step: 33970   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:56:16,638-Speed 3914.94 samples/sec  Loss 1.4589  LearningRate 0.1158  ProxyLR: 5.7906  Epoch: 5  Global Step: 33980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:19,267-Speed 3895.32 samples/sec  Loss 1.4467  LearningRate 0.1158  ProxyLR: 5.7895  Epoch: 5  Global Step: 33990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:21,896-Speed 3896.48 samples/sec  Loss 1.3714  LearningRate 0.1158  ProxyLR: 5.7884  Epoch: 5  Global Step: 34000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:24,527-Speed 3893.30 samples/sec  Loss 1.3838  LearningRate 0.1157  ProxyLR: 5.7873  Epoch: 5  Global Step: 34010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:27,156-Speed 3895.24 samples/sec  Loss 1.4491  LearningRate 0.1157  ProxyLR: 5.7863  Epoch: 5  Global Step: 34020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:29,782-Speed 3900.85 samples/sec  Loss 1.3519  LearningRate 0.1157  ProxyLR: 5.7852  Epoch: 5  Global Step: 34030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:32,410-Speed 3897.10 samples/sec  Loss 1.3081  LearningRate 0.1157  ProxyLR: 5.7841  Epoch: 5  Global Step: 34040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:35,035-Speed 3901.63 samples/sec  Loss 1.2940  LearningRate 0.1157  ProxyLR: 5.7831  Epoch: 5  Global Step: 34050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:37,663-Speed 3898.66 samples/sec  Loss 1.3136  LearningRate 0.1156  ProxyLR: 5.7820  Epoch: 5  Global Step: 34060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:56:40,277-Speed 3917.15 samples/sec  Loss 1.4334  LearningRate 0.1156  ProxyLR: 5.7809  Epoch: 5  Global Step: 34070   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:56:42,903-Speed 3901.50 samples/sec  Loss 1.4115  LearningRate 0.1156  ProxyLR: 5.7799  Epoch: 5  Global Step: 34080   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:56:45,529-Speed 3900.41 samples/sec  Loss 1.4485  LearningRate 0.1156  ProxyLR: 5.7788  Epoch: 5  Global Step: 34090   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:56:48,156-Speed 3898.62 samples/sec  Loss 1.4177  LearningRate 0.1156  ProxyLR: 5.7777  Epoch: 5  Global Step: 34100   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:56:50,842-Speed 3813.01 samples/sec  Loss 1.5551  LearningRate 0.1155  ProxyLR: 5.7766  Epoch: 5  Global Step: 34110   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:57:01,620-Speed 950.17 samples/sec  Loss 1.1527  LearningRate 0.1155  ProxyLR: 5.7756  Epoch: 6  Global Step: 34120   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:57:04,283-Speed 3846.62 samples/sec  Loss 0.9233  LearningRate 0.1155  ProxyLR: 5.7745  Epoch: 6  Global Step: 34130   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:57:06,913-Speed 3895.20 samples/sec  Loss 0.8209  LearningRate 0.1155  ProxyLR: 5.7734  Epoch: 6  Global Step: 34140   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:57:09,541-Speed 3897.01 samples/sec  Loss 0.8361  LearningRate 0.1154  ProxyLR: 5.7724  Epoch: 6  Global Step: 34150   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:57:12,170-Speed 3896.40 samples/sec  Loss 0.8054  LearningRate 0.1154  ProxyLR: 5.7713  Epoch: 6  Global Step: 34160   Fp16 Grad Scale: 262144  Required: 9 hours
Training: 2023-05-04 16:57:14,803-Speed 3890.38 samples/sec  Loss 0.8311  LearningRate 0.1154  ProxyLR: 5.7702  Epoch: 6  Global Step: 34170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:17,437-Speed 3887.16 samples/sec  Loss 0.7971  LearningRate 0.1154  ProxyLR: 5.7692  Epoch: 6  Global Step: 34180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:20,073-Speed 3886.77 samples/sec  Loss 0.7188  LearningRate 0.1154  ProxyLR: 5.7681  Epoch: 6  Global Step: 34190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:22,707-Speed 3888.22 samples/sec  Loss 0.8376  LearningRate 0.1153  ProxyLR: 5.7670  Epoch: 6  Global Step: 34200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:25,346-Speed 3881.30 samples/sec  Loss 0.7640  LearningRate 0.1153  ProxyLR: 5.7660  Epoch: 6  Global Step: 34210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:27,980-Speed 3888.67 samples/sec  Loss 0.7871  LearningRate 0.1153  ProxyLR: 5.7649  Epoch: 6  Global Step: 34220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:30,644-Speed 3843.94 samples/sec  Loss 0.8061  LearningRate 0.1153  ProxyLR: 5.7638  Epoch: 6  Global Step: 34230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:33,279-Speed 3888.44 samples/sec  Loss 0.7691  LearningRate 0.1153  ProxyLR: 5.7627  Epoch: 6  Global Step: 34240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:35,910-Speed 3892.78 samples/sec  Loss 0.7785  LearningRate 0.1152  ProxyLR: 5.7617  Epoch: 6  Global Step: 34250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:38,539-Speed 3895.85 samples/sec  Loss 0.8566  LearningRate 0.1152  ProxyLR: 5.7606  Epoch: 6  Global Step: 34260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:41,170-Speed 3893.07 samples/sec  Loss 0.8008  LearningRate 0.1152  ProxyLR: 5.7595  Epoch: 6  Global Step: 34270   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:57:43,803-Speed 3890.71 samples/sec  Loss 0.8137  LearningRate 0.1152  ProxyLR: 5.7585  Epoch: 6  Global Step: 34280   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:57:46,433-Speed 3893.61 samples/sec  Loss 0.7640  LearningRate 0.1151  ProxyLR: 5.7574  Epoch: 6  Global Step: 34290   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:57:49,117-Speed 3816.91 samples/sec  Loss 0.7528  LearningRate 0.1151  ProxyLR: 5.7563  Epoch: 6  Global Step: 34300   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:57:51,747-Speed 3894.05 samples/sec  Loss 0.7537  LearningRate 0.1151  ProxyLR: 5.7553  Epoch: 6  Global Step: 34310   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:57:54,362-Speed 3916.45 samples/sec  Loss 0.8408  LearningRate 0.1151  ProxyLR: 5.7542  Epoch: 6  Global Step: 34320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:56,992-Speed 3895.68 samples/sec  Loss 0.8204  LearningRate 0.1151  ProxyLR: 5.7531  Epoch: 6  Global Step: 34330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:57:59,624-Speed 3890.88 samples/sec  Loss 0.8008  LearningRate 0.1150  ProxyLR: 5.7521  Epoch: 6  Global Step: 34340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:02,257-Speed 3889.71 samples/sec  Loss 0.8058  LearningRate 0.1150  ProxyLR: 5.7510  Epoch: 6  Global Step: 34350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:04,891-Speed 3888.10 samples/sec  Loss 0.8058  LearningRate 0.1150  ProxyLR: 5.7499  Epoch: 6  Global Step: 34360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:07,527-Speed 3886.75 samples/sec  Loss 0.9225  LearningRate 0.1150  ProxyLR: 5.7489  Epoch: 6  Global Step: 34370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:10,157-Speed 3893.23 samples/sec  Loss 0.9090  LearningRate 0.1150  ProxyLR: 5.7478  Epoch: 6  Global Step: 34380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:12,790-Speed 3891.01 samples/sec  Loss 0.8184  LearningRate 0.1149  ProxyLR: 5.7467  Epoch: 6  Global Step: 34390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:15,474-Speed 3815.96 samples/sec  Loss 0.8079  LearningRate 0.1149  ProxyLR: 5.7457  Epoch: 6  Global Step: 34400   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:18,106-Speed 3890.50 samples/sec  Loss 0.7945  LearningRate 0.1149  ProxyLR: 5.7446  Epoch: 6  Global Step: 34410   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:20,723-Speed 3914.91 samples/sec  Loss 0.7213  LearningRate 0.1149  ProxyLR: 5.7435  Epoch: 6  Global Step: 34420   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:23,353-Speed 3894.03 samples/sec  Loss 0.8305  LearningRate 0.1148  ProxyLR: 5.7425  Epoch: 6  Global Step: 34430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:25,986-Speed 3890.50 samples/sec  Loss 0.7534  LearningRate 0.1148  ProxyLR: 5.7414  Epoch: 6  Global Step: 34440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:28,620-Speed 3888.56 samples/sec  Loss 0.8443  LearningRate 0.1148  ProxyLR: 5.7403  Epoch: 6  Global Step: 34450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:31,250-Speed 3893.22 samples/sec  Loss 0.8756  LearningRate 0.1148  ProxyLR: 5.7393  Epoch: 6  Global Step: 34460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:33,881-Speed 3893.24 samples/sec  Loss 0.8158  LearningRate 0.1148  ProxyLR: 5.7382  Epoch: 6  Global Step: 34470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:36,516-Speed 3887.12 samples/sec  Loss 0.8548  LearningRate 0.1147  ProxyLR: 5.7371  Epoch: 6  Global Step: 34480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:39,147-Speed 3892.83 samples/sec  Loss 0.8039  LearningRate 0.1147  ProxyLR: 5.7361  Epoch: 6  Global Step: 34490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:41,777-Speed 3895.08 samples/sec  Loss 0.7327  LearningRate 0.1147  ProxyLR: 5.7350  Epoch: 6  Global Step: 34500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:44,409-Speed 3891.33 samples/sec  Loss 0.8919  LearningRate 0.1147  ProxyLR: 5.7339  Epoch: 6  Global Step: 34510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:47,042-Speed 3889.95 samples/sec  Loss 0.7773  LearningRate 0.1147  ProxyLR: 5.7329  Epoch: 6  Global Step: 34520   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:58:49,674-Speed 3891.16 samples/sec  Loss 0.7910  LearningRate 0.1146  ProxyLR: 5.7318  Epoch: 6  Global Step: 34530   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:58:52,292-Speed 3912.38 samples/sec  Loss 0.8836  LearningRate 0.1146  ProxyLR: 5.7308  Epoch: 6  Global Step: 34540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:54,925-Speed 3890.59 samples/sec  Loss 0.9174  LearningRate 0.1146  ProxyLR: 5.7297  Epoch: 6  Global Step: 34550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:58:57,560-Speed 3886.15 samples/sec  Loss 0.8435  LearningRate 0.1146  ProxyLR: 5.7286  Epoch: 6  Global Step: 34560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:00,197-Speed 3884.59 samples/sec  Loss 0.8563  LearningRate 0.1146  ProxyLR: 5.7276  Epoch: 6  Global Step: 34570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:02,833-Speed 3885.99 samples/sec  Loss 0.8284  LearningRate 0.1145  ProxyLR: 5.7265  Epoch: 6  Global Step: 34580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:05,469-Speed 3884.79 samples/sec  Loss 0.8108  LearningRate 0.1145  ProxyLR: 5.7254  Epoch: 6  Global Step: 34590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:08,103-Speed 3888.95 samples/sec  Loss 0.7473  LearningRate 0.1145  ProxyLR: 5.7244  Epoch: 6  Global Step: 34600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:10,737-Speed 3889.11 samples/sec  Loss 0.8544  LearningRate 0.1145  ProxyLR: 5.7233  Epoch: 6  Global Step: 34610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:13,368-Speed 3892.23 samples/sec  Loss 0.7899  LearningRate 0.1144  ProxyLR: 5.7222  Epoch: 6  Global Step: 34620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:16,001-Speed 3889.78 samples/sec  Loss 0.8149  LearningRate 0.1144  ProxyLR: 5.7212  Epoch: 6  Global Step: 34630   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:18,632-Speed 3893.81 samples/sec  Loss 0.9107  LearningRate 0.1144  ProxyLR: 5.7201  Epoch: 6  Global Step: 34640   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:59:21,263-Speed 3891.78 samples/sec  Loss 0.9455  LearningRate 0.1144  ProxyLR: 5.7190  Epoch: 6  Global Step: 34650   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:59:23,882-Speed 3911.99 samples/sec  Loss 0.8987  LearningRate 0.1144  ProxyLR: 5.7180  Epoch: 6  Global Step: 34660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:26,515-Speed 3888.84 samples/sec  Loss 0.7835  LearningRate 0.1143  ProxyLR: 5.7169  Epoch: 6  Global Step: 34670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:29,147-Speed 3892.16 samples/sec  Loss 0.8082  LearningRate 0.1143  ProxyLR: 5.7158  Epoch: 6  Global Step: 34680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:31,782-Speed 3886.45 samples/sec  Loss 0.8047  LearningRate 0.1143  ProxyLR: 5.7148  Epoch: 6  Global Step: 34690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:34,412-Speed 3895.09 samples/sec  Loss 0.9483  LearningRate 0.1143  ProxyLR: 5.7137  Epoch: 6  Global Step: 34700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:37,043-Speed 3893.70 samples/sec  Loss 0.8822  LearningRate 0.1143  ProxyLR: 5.7127  Epoch: 6  Global Step: 34710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:39,672-Speed 3895.71 samples/sec  Loss 0.8228  LearningRate 0.1142  ProxyLR: 5.7116  Epoch: 6  Global Step: 34720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:42,301-Speed 3895.24 samples/sec  Loss 0.9069  LearningRate 0.1142  ProxyLR: 5.7105  Epoch: 6  Global Step: 34730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:44,932-Speed 3892.86 samples/sec  Loss 0.8358  LearningRate 0.1142  ProxyLR: 5.7095  Epoch: 6  Global Step: 34740   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:47,562-Speed 3895.44 samples/sec  Loss 0.8866  LearningRate 0.1142  ProxyLR: 5.7084  Epoch: 6  Global Step: 34750   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:50,192-Speed 3894.57 samples/sec  Loss 0.9147  LearningRate 0.1141  ProxyLR: 5.7073  Epoch: 6  Global Step: 34760   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 16:59:52,809-Speed 3913.48 samples/sec  Loss 0.8447  LearningRate 0.1141  ProxyLR: 5.7063  Epoch: 6  Global Step: 34770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:55,438-Speed 3895.10 samples/sec  Loss 0.9119  LearningRate 0.1141  ProxyLR: 5.7052  Epoch: 6  Global Step: 34780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 16:59:58,067-Speed 3896.55 samples/sec  Loss 0.8590  LearningRate 0.1141  ProxyLR: 5.7042  Epoch: 6  Global Step: 34790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:00,698-Speed 3892.87 samples/sec  Loss 0.9498  LearningRate 0.1141  ProxyLR: 5.7031  Epoch: 6  Global Step: 34800   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:03,328-Speed 3894.42 samples/sec  Loss 0.7838  LearningRate 0.1140  ProxyLR: 5.7020  Epoch: 6  Global Step: 34810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:05,960-Speed 3891.78 samples/sec  Loss 0.8500  LearningRate 0.1140  ProxyLR: 5.7010  Epoch: 6  Global Step: 34820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:08,591-Speed 3893.68 samples/sec  Loss 0.9521  LearningRate 0.1140  ProxyLR: 5.6999  Epoch: 6  Global Step: 34830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:11,223-Speed 3890.32 samples/sec  Loss 0.8806  LearningRate 0.1140  ProxyLR: 5.6988  Epoch: 6  Global Step: 34840   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:13,856-Speed 3890.75 samples/sec  Loss 0.8689  LearningRate 0.1140  ProxyLR: 5.6978  Epoch: 6  Global Step: 34850   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:16,488-Speed 3892.04 samples/sec  Loss 0.8347  LearningRate 0.1139  ProxyLR: 5.6967  Epoch: 6  Global Step: 34860   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:19,121-Speed 3889.95 samples/sec  Loss 0.8161  LearningRate 0.1139  ProxyLR: 5.6957  Epoch: 6  Global Step: 34870   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:00:21,754-Speed 3889.72 samples/sec  Loss 0.9119  LearningRate 0.1139  ProxyLR: 5.6946  Epoch: 6  Global Step: 34880   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:00:24,387-Speed 3890.08 samples/sec  Loss 0.9291  LearningRate 0.1139  ProxyLR: 5.6935  Epoch: 6  Global Step: 34890   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:00:27,006-Speed 3910.67 samples/sec  Loss 0.9605  LearningRate 0.1138  ProxyLR: 5.6925  Epoch: 6  Global Step: 34900   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:29,640-Speed 3889.18 samples/sec  Loss 0.8276  LearningRate 0.1138  ProxyLR: 5.6914  Epoch: 6  Global Step: 34910   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:32,272-Speed 3890.52 samples/sec  Loss 0.8873  LearningRate 0.1138  ProxyLR: 5.6903  Epoch: 6  Global Step: 34920   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:34,906-Speed 3888.61 samples/sec  Loss 0.9007  LearningRate 0.1138  ProxyLR: 5.6893  Epoch: 6  Global Step: 34930   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:37,537-Speed 3893.40 samples/sec  Loss 0.8905  LearningRate 0.1138  ProxyLR: 5.6882  Epoch: 6  Global Step: 34940   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:40,169-Speed 3891.23 samples/sec  Loss 0.8879  LearningRate 0.1137  ProxyLR: 5.6872  Epoch: 6  Global Step: 34950   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:42,800-Speed 3892.78 samples/sec  Loss 0.8649  LearningRate 0.1137  ProxyLR: 5.6861  Epoch: 6  Global Step: 34960   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:45,432-Speed 3891.18 samples/sec  Loss 0.8830  LearningRate 0.1137  ProxyLR: 5.6850  Epoch: 6  Global Step: 34970   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:48,067-Speed 3886.76 samples/sec  Loss 0.9242  LearningRate 0.1137  ProxyLR: 5.6840  Epoch: 6  Global Step: 34980   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:50,699-Speed 3891.58 samples/sec  Loss 0.9445  LearningRate 0.1137  ProxyLR: 5.6829  Epoch: 6  Global Step: 34990   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:53,316-Speed 3913.53 samples/sec  Loss 0.8836  LearningRate 0.1136  ProxyLR: 5.6819  Epoch: 6  Global Step: 35000   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:55,949-Speed 3890.61 samples/sec  Loss 0.9053  LearningRate 0.1136  ProxyLR: 5.6808  Epoch: 6  Global Step: 35010   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:00:58,580-Speed 3892.36 samples/sec  Loss 0.9660  LearningRate 0.1136  ProxyLR: 5.6797  Epoch: 6  Global Step: 35020   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:01,214-Speed 3888.81 samples/sec  Loss 0.9274  LearningRate 0.1136  ProxyLR: 5.6787  Epoch: 6  Global Step: 35030   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:03,847-Speed 3890.32 samples/sec  Loss 0.8286  LearningRate 0.1136  ProxyLR: 5.6776  Epoch: 6  Global Step: 35040   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:06,478-Speed 3893.23 samples/sec  Loss 0.8369  LearningRate 0.1135  ProxyLR: 5.6766  Epoch: 6  Global Step: 35050   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:09,108-Speed 3893.35 samples/sec  Loss 0.8882  LearningRate 0.1135  ProxyLR: 5.6755  Epoch: 6  Global Step: 35060   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:11,738-Speed 3894.78 samples/sec  Loss 0.8736  LearningRate 0.1135  ProxyLR: 5.6744  Epoch: 6  Global Step: 35070   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:14,370-Speed 3892.15 samples/sec  Loss 0.9220  LearningRate 0.1135  ProxyLR: 5.6734  Epoch: 6  Global Step: 35080   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:17,002-Speed 3891.07 samples/sec  Loss 0.9497  LearningRate 0.1134  ProxyLR: 5.6723  Epoch: 6  Global Step: 35090   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:19,622-Speed 3909.18 samples/sec  Loss 0.8062  LearningRate 0.1134  ProxyLR: 5.6713  Epoch: 6  Global Step: 35100   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:22,255-Speed 3889.90 samples/sec  Loss 0.9014  LearningRate 0.1134  ProxyLR: 5.6702  Epoch: 6  Global Step: 35110   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:24,888-Speed 3890.90 samples/sec  Loss 1.0063  LearningRate 0.1134  ProxyLR: 5.6691  Epoch: 6  Global Step: 35120   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:27,521-Speed 3890.00 samples/sec  Loss 0.8634  LearningRate 0.1134  ProxyLR: 5.6681  Epoch: 6  Global Step: 35130   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:30,150-Speed 3895.25 samples/sec  Loss 0.9521  LearningRate 0.1133  ProxyLR: 5.6670  Epoch: 6  Global Step: 35140   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:32,784-Speed 3888.25 samples/sec  Loss 0.8901  LearningRate 0.1133  ProxyLR: 5.6660  Epoch: 6  Global Step: 35150   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:35,416-Speed 3892.52 samples/sec  Loss 0.8876  LearningRate 0.1133  ProxyLR: 5.6649  Epoch: 6  Global Step: 35160   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:38,048-Speed 3890.49 samples/sec  Loss 0.8695  LearningRate 0.1133  ProxyLR: 5.6638  Epoch: 6  Global Step: 35170   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:40,682-Speed 3889.51 samples/sec  Loss 0.9040  LearningRate 0.1133  ProxyLR: 5.6628  Epoch: 6  Global Step: 35180   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:43,312-Speed 3894.59 samples/sec  Loss 0.9755  LearningRate 0.1132  ProxyLR: 5.6617  Epoch: 6  Global Step: 35190   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:45,929-Speed 3912.65 samples/sec  Loss 0.9976  LearningRate 0.1132  ProxyLR: 5.6607  Epoch: 6  Global Step: 35200   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:48,560-Speed 3892.65 samples/sec  Loss 0.8956  LearningRate 0.1132  ProxyLR: 5.6596  Epoch: 6  Global Step: 35210   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:51,192-Speed 3892.08 samples/sec  Loss 0.9239  LearningRate 0.1132  ProxyLR: 5.6586  Epoch: 6  Global Step: 35220   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:53,824-Speed 3891.34 samples/sec  Loss 0.8681  LearningRate 0.1131  ProxyLR: 5.6575  Epoch: 6  Global Step: 35230   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:56,455-Speed 3893.26 samples/sec  Loss 0.9104  LearningRate 0.1131  ProxyLR: 5.6564  Epoch: 6  Global Step: 35240   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:01:59,087-Speed 3890.74 samples/sec  Loss 0.9364  LearningRate 0.1131  ProxyLR: 5.6554  Epoch: 6  Global Step: 35250   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:01,720-Speed 3891.13 samples/sec  Loss 0.9375  LearningRate 0.1131  ProxyLR: 5.6543  Epoch: 6  Global Step: 35260   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:04,350-Speed 3893.68 samples/sec  Loss 1.0885  LearningRate 0.1131  ProxyLR: 5.6533  Epoch: 6  Global Step: 35270   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:06,980-Speed 3894.58 samples/sec  Loss 0.9988  LearningRate 0.1130  ProxyLR: 5.6522  Epoch: 6  Global Step: 35280   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:09,615-Speed 3888.19 samples/sec  Loss 0.9473  LearningRate 0.1130  ProxyLR: 5.6511  Epoch: 6  Global Step: 35290   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:12,234-Speed 3910.40 samples/sec  Loss 0.9491  LearningRate 0.1130  ProxyLR: 5.6501  Epoch: 6  Global Step: 35300   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:14,867-Speed 3890.33 samples/sec  Loss 0.9788  LearningRate 0.1130  ProxyLR: 5.6490  Epoch: 6  Global Step: 35310   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:17,501-Speed 3888.62 samples/sec  Loss 0.9543  LearningRate 0.1130  ProxyLR: 5.6480  Epoch: 6  Global Step: 35320   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:20,135-Speed 3888.61 samples/sec  Loss 0.9433  LearningRate 0.1129  ProxyLR: 5.6469  Epoch: 6  Global Step: 35330   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:22,767-Speed 3891.38 samples/sec  Loss 0.9460  LearningRate 0.1129  ProxyLR: 5.6459  Epoch: 6  Global Step: 35340   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:25,398-Speed 3891.97 samples/sec  Loss 0.9265  LearningRate 0.1129  ProxyLR: 5.6448  Epoch: 6  Global Step: 35350   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:28,031-Speed 3891.10 samples/sec  Loss 1.0460  LearningRate 0.1129  ProxyLR: 5.6437  Epoch: 6  Global Step: 35360   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:30,663-Speed 3890.68 samples/sec  Loss 0.9433  LearningRate 0.1129  ProxyLR: 5.6427  Epoch: 6  Global Step: 35370   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:33,295-Speed 3891.60 samples/sec  Loss 0.9361  LearningRate 0.1128  ProxyLR: 5.6416  Epoch: 6  Global Step: 35380   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:35,928-Speed 3890.35 samples/sec  Loss 0.9985  LearningRate 0.1128  ProxyLR: 5.6406  Epoch: 6  Global Step: 35390   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:38,559-Speed 3892.68 samples/sec  Loss 0.8925  LearningRate 0.1128  ProxyLR: 5.6395  Epoch: 6  Global Step: 35400   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:02:41,194-Speed 3886.81 samples/sec  Loss 0.9299  LearningRate 0.1128  ProxyLR: 5.6385  Epoch: 6  Global Step: 35410   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:02:43,831-Speed 3883.67 samples/sec  Loss 0.9522  LearningRate 0.1127  ProxyLR: 5.6374  Epoch: 6  Global Step: 35420   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:02:46,452-Speed 3908.49 samples/sec  Loss 0.9157  LearningRate 0.1127  ProxyLR: 5.6363  Epoch: 6  Global Step: 35430   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:49,085-Speed 3890.35 samples/sec  Loss 0.8740  LearningRate 0.1127  ProxyLR: 5.6353  Epoch: 6  Global Step: 35440   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:51,716-Speed 3892.77 samples/sec  Loss 0.9765  LearningRate 0.1127  ProxyLR: 5.6342  Epoch: 6  Global Step: 35450   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:54,348-Speed 3890.60 samples/sec  Loss 0.9933  LearningRate 0.1127  ProxyLR: 5.6332  Epoch: 6  Global Step: 35460   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:56,985-Speed 3885.08 samples/sec  Loss 0.9979  LearningRate 0.1126  ProxyLR: 5.6321  Epoch: 6  Global Step: 35470   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:02:59,616-Speed 3892.22 samples/sec  Loss 0.9933  LearningRate 0.1126  ProxyLR: 5.6311  Epoch: 6  Global Step: 35480   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:02,248-Speed 3891.62 samples/sec  Loss 1.0340  LearningRate 0.1126  ProxyLR: 5.6300  Epoch: 6  Global Step: 35490   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:04,881-Speed 3890.87 samples/sec  Loss 0.9741  LearningRate 0.1126  ProxyLR: 5.6290  Epoch: 6  Global Step: 35500   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:07,514-Speed 3890.03 samples/sec  Loss 1.0021  LearningRate 0.1126  ProxyLR: 5.6279  Epoch: 6  Global Step: 35510   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:10,144-Speed 3894.19 samples/sec  Loss 0.9135  LearningRate 0.1125  ProxyLR: 5.6268  Epoch: 6  Global Step: 35520   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:12,763-Speed 3910.30 samples/sec  Loss 0.9973  LearningRate 0.1125  ProxyLR: 5.6258  Epoch: 6  Global Step: 35530   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:15,394-Speed 3892.86 samples/sec  Loss 0.9920  LearningRate 0.1125  ProxyLR: 5.6247  Epoch: 6  Global Step: 35540   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:18,024-Speed 3894.21 samples/sec  Loss 1.0019  LearningRate 0.1125  ProxyLR: 5.6237  Epoch: 6  Global Step: 35550   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:20,654-Speed 3894.11 samples/sec  Loss 1.0477  LearningRate 0.1125  ProxyLR: 5.6226  Epoch: 6  Global Step: 35560   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:23,285-Speed 3894.09 samples/sec  Loss 0.9743  LearningRate 0.1124  ProxyLR: 5.6216  Epoch: 6  Global Step: 35570   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:25,915-Speed 3894.47 samples/sec  Loss 1.0184  LearningRate 0.1124  ProxyLR: 5.6205  Epoch: 6  Global Step: 35580   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:28,546-Speed 3892.99 samples/sec  Loss 1.0592  LearningRate 0.1124  ProxyLR: 5.6195  Epoch: 6  Global Step: 35590   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:31,177-Speed 3892.47 samples/sec  Loss 1.0312  LearningRate 0.1124  ProxyLR: 5.6184  Epoch: 6  Global Step: 35600   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:33,808-Speed 3893.08 samples/sec  Loss 0.9644  LearningRate 0.1123  ProxyLR: 5.6174  Epoch: 6  Global Step: 35610   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:36,438-Speed 3894.85 samples/sec  Loss 1.0573  LearningRate 0.1123  ProxyLR: 5.6163  Epoch: 6  Global Step: 35620   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:39,066-Speed 3897.44 samples/sec  Loss 0.9664  LearningRate 0.1123  ProxyLR: 5.6152  Epoch: 6  Global Step: 35630   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:03:41,681-Speed 3915.72 samples/sec  Loss 1.0584  LearningRate 0.1123  ProxyLR: 5.6142  Epoch: 6  Global Step: 35640   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:44,311-Speed 3895.22 samples/sec  Loss 1.0255  LearningRate 0.1123  ProxyLR: 5.6131  Epoch: 6  Global Step: 35650   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:46,940-Speed 3895.38 samples/sec  Loss 1.0092  LearningRate 0.1122  ProxyLR: 5.6121  Epoch: 6  Global Step: 35660   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:49,568-Speed 3898.47 samples/sec  Loss 1.0054  LearningRate 0.1122  ProxyLR: 5.6110  Epoch: 6  Global Step: 35670   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:52,197-Speed 3895.50 samples/sec  Loss 1.0213  LearningRate 0.1122  ProxyLR: 5.6100  Epoch: 6  Global Step: 35680   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:54,824-Speed 3898.20 samples/sec  Loss 0.9821  LearningRate 0.1122  ProxyLR: 5.6089  Epoch: 6  Global Step: 35690   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:03:57,453-Speed 3896.16 samples/sec  Loss 1.0267  LearningRate 0.1122  ProxyLR: 5.6079  Epoch: 6  Global Step: 35700   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:00,082-Speed 3896.77 samples/sec  Loss 0.9075  LearningRate 0.1121  ProxyLR: 5.6068  Epoch: 6  Global Step: 35710   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:02,710-Speed 3896.55 samples/sec  Loss 0.9122  LearningRate 0.1121  ProxyLR: 5.6058  Epoch: 6  Global Step: 35720   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:05,342-Speed 3892.01 samples/sec  Loss 1.0689  LearningRate 0.1121  ProxyLR: 5.6047  Epoch: 6  Global Step: 35730   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:07,972-Speed 3893.79 samples/sec  Loss 1.0802  LearningRate 0.1121  ProxyLR: 5.6037  Epoch: 6  Global Step: 35740   Fp16 Grad Scale: 1048576  Required: 9 hours
Training: 2023-05-04 17:04:10,591-Speed 3911.19 samples/sec  Loss 1.0023  LearningRate 0.1121  ProxyLR: 5.6026  Epoch: 6  Global Step: 35750   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:13,222-Speed 3892.47 samples/sec  Loss 0.9636  LearningRate 0.1120  ProxyLR: 5.6015  Epoch: 6  Global Step: 35760   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:15,852-Speed 3894.56 samples/sec  Loss 0.9313  LearningRate 0.1120  ProxyLR: 5.6005  Epoch: 6  Global Step: 35770   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:18,482-Speed 3894.48 samples/sec  Loss 0.9318  LearningRate 0.1120  ProxyLR: 5.5994  Epoch: 6  Global Step: 35780   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:21,110-Speed 3896.98 samples/sec  Loss 0.9789  LearningRate 0.1120  ProxyLR: 5.5984  Epoch: 6  Global Step: 35790   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:23,739-Speed 3896.09 samples/sec  Loss 1.0059  LearningRate 0.1119  ProxyLR: 5.5973  Epoch: 6  Global Step: 35800   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:26,368-Speed 3895.80 samples/sec  Loss 0.9330  LearningRate 0.1119  ProxyLR: 5.5963  Epoch: 6  Global Step: 35810   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:28,995-Speed 3899.31 samples/sec  Loss 0.9671  LearningRate 0.1119  ProxyLR: 5.5952  Epoch: 6  Global Step: 35820   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:31,622-Speed 3898.86 samples/sec  Loss 1.0230  LearningRate 0.1119  ProxyLR: 5.5942  Epoch: 6  Global Step: 35830   Fp16 Grad Scale: 524288  Required: 9 hours
Training: 2023-05-04 17:04:34,249-Speed 3899.75 samples/sec  Loss 0.9867  LearningRate 0.1119  ProxyLR: 5.5931  Epoch: 6  Global Step: 35840   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:36,874-Speed 3901.05 samples/sec  Loss 1.0932  LearningRate 0.1118  ProxyLR: 5.5921  Epoch: 6  Global Step: 35850   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:04:39,502-Speed 3898.35 samples/sec  Loss 1.0548  LearningRate 0.1118  ProxyLR: 5.5910  Epoch: 6  Global Step: 35860   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:04:42,114-Speed 3920.59 samples/sec  Loss 1.0259  LearningRate 0.1118  ProxyLR: 5.5900  Epoch: 6  Global Step: 35870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:44,740-Speed 3900.37 samples/sec  Loss 1.0326  LearningRate 0.1118  ProxyLR: 5.5889  Epoch: 6  Global Step: 35880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:47,365-Speed 3902.05 samples/sec  Loss 1.0188  LearningRate 0.1118  ProxyLR: 5.5879  Epoch: 6  Global Step: 35890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:49,992-Speed 3899.53 samples/sec  Loss 1.0052  LearningRate 0.1117  ProxyLR: 5.5868  Epoch: 6  Global Step: 35900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:52,617-Speed 3901.72 samples/sec  Loss 1.0225  LearningRate 0.1117  ProxyLR: 5.5858  Epoch: 6  Global Step: 35910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:55,243-Speed 3899.94 samples/sec  Loss 0.9157  LearningRate 0.1117  ProxyLR: 5.5847  Epoch: 6  Global Step: 35920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:04:57,869-Speed 3899.64 samples/sec  Loss 1.0681  LearningRate 0.1117  ProxyLR: 5.5837  Epoch: 6  Global Step: 35930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:00,494-Speed 3902.15 samples/sec  Loss 0.9565  LearningRate 0.1117  ProxyLR: 5.5826  Epoch: 6  Global Step: 35940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:03,118-Speed 3903.38 samples/sec  Loss 0.9739  LearningRate 0.1116  ProxyLR: 5.5816  Epoch: 6  Global Step: 35950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:05,743-Speed 3902.88 samples/sec  Loss 1.0653  LearningRate 0.1116  ProxyLR: 5.5805  Epoch: 6  Global Step: 35960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:08,369-Speed 3899.85 samples/sec  Loss 0.9877  LearningRate 0.1116  ProxyLR: 5.5795  Epoch: 6  Global Step: 35970   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:05:10,981-Speed 3920.85 samples/sec  Loss 1.1523  LearningRate 0.1116  ProxyLR: 5.5784  Epoch: 6  Global Step: 35980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:13,606-Speed 3902.38 samples/sec  Loss 1.0119  LearningRate 0.1115  ProxyLR: 5.5774  Epoch: 6  Global Step: 35990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:16,230-Speed 3902.95 samples/sec  Loss 1.0197  LearningRate 0.1115  ProxyLR: 5.5763  Epoch: 6  Global Step: 36000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:18,856-Speed 3901.34 samples/sec  Loss 1.0440  LearningRate 0.1115  ProxyLR: 5.5753  Epoch: 6  Global Step: 36010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:21,480-Speed 3903.29 samples/sec  Loss 1.0510  LearningRate 0.1115  ProxyLR: 5.5742  Epoch: 6  Global Step: 36020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:24,104-Speed 3903.10 samples/sec  Loss 1.0165  LearningRate 0.1115  ProxyLR: 5.5732  Epoch: 6  Global Step: 36030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:26,729-Speed 3901.13 samples/sec  Loss 1.0318  LearningRate 0.1114  ProxyLR: 5.5721  Epoch: 6  Global Step: 36040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:29,355-Speed 3901.54 samples/sec  Loss 1.0700  LearningRate 0.1114  ProxyLR: 5.5710  Epoch: 6  Global Step: 36050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:31,978-Speed 3903.56 samples/sec  Loss 1.1004  LearningRate 0.1114  ProxyLR: 5.5700  Epoch: 6  Global Step: 36060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:34,603-Speed 3902.32 samples/sec  Loss 1.1292  LearningRate 0.1114  ProxyLR: 5.5689  Epoch: 6  Global Step: 36070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:05:37,201-Speed 3941.83 samples/sec  Loss 1.1047  LearningRate 0.1114  ProxyLR: 5.5679  Epoch: 6  Global Step: 36080   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:39,825-Speed 3903.32 samples/sec  Loss 0.9982  LearningRate 0.1113  ProxyLR: 5.5668  Epoch: 6  Global Step: 36090   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:42,451-Speed 3901.30 samples/sec  Loss 1.0832  LearningRate 0.1113  ProxyLR: 5.5658  Epoch: 6  Global Step: 36100   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:45,076-Speed 3902.16 samples/sec  Loss 1.0406  LearningRate 0.1113  ProxyLR: 5.5648  Epoch: 6  Global Step: 36110   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:47,700-Speed 3902.67 samples/sec  Loss 1.0635  LearningRate 0.1113  ProxyLR: 5.5637  Epoch: 6  Global Step: 36120   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:50,325-Speed 3901.43 samples/sec  Loss 0.9948  LearningRate 0.1113  ProxyLR: 5.5627  Epoch: 6  Global Step: 36130   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:52,952-Speed 3899.24 samples/sec  Loss 1.0178  LearningRate 0.1112  ProxyLR: 5.5616  Epoch: 6  Global Step: 36140   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:55,579-Speed 3899.67 samples/sec  Loss 1.0773  LearningRate 0.1112  ProxyLR: 5.5606  Epoch: 6  Global Step: 36150   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:05:58,204-Speed 3901.85 samples/sec  Loss 1.0438  LearningRate 0.1112  ProxyLR: 5.5595  Epoch: 6  Global Step: 36160   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:06:00,830-Speed 3900.61 samples/sec  Loss 1.1408  LearningRate 0.1112  ProxyLR: 5.5585  Epoch: 6  Global Step: 36170   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:06:03,454-Speed 3903.04 samples/sec  Loss 1.1549  LearningRate 0.1111  ProxyLR: 5.5574  Epoch: 6  Global Step: 36180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:06,080-Speed 3899.27 samples/sec  Loss 1.1556  LearningRate 0.1111  ProxyLR: 5.5564  Epoch: 6  Global Step: 36190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:08,708-Speed 3898.69 samples/sec  Loss 1.0881  LearningRate 0.1111  ProxyLR: 5.5553  Epoch: 6  Global Step: 36200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:11,334-Speed 3900.31 samples/sec  Loss 1.0159  LearningRate 0.1111  ProxyLR: 5.5543  Epoch: 6  Global Step: 36210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:13,959-Speed 3902.08 samples/sec  Loss 1.0133  LearningRate 0.1111  ProxyLR: 5.5532  Epoch: 6  Global Step: 36220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:16,584-Speed 3902.01 samples/sec  Loss 1.0234  LearningRate 0.1110  ProxyLR: 5.5522  Epoch: 6  Global Step: 36230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:19,209-Speed 3901.30 samples/sec  Loss 1.0942  LearningRate 0.1110  ProxyLR: 5.5511  Epoch: 6  Global Step: 36240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:21,834-Speed 3902.18 samples/sec  Loss 1.0714  LearningRate 0.1110  ProxyLR: 5.5501  Epoch: 6  Global Step: 36250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:24,460-Speed 3899.48 samples/sec  Loss 1.0829  LearningRate 0.1110  ProxyLR: 5.5490  Epoch: 6  Global Step: 36260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:27,084-Speed 3903.44 samples/sec  Loss 0.9470  LearningRate 0.1110  ProxyLR: 5.5480  Epoch: 6  Global Step: 36270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:29,709-Speed 3902.64 samples/sec  Loss 0.9834  LearningRate 0.1109  ProxyLR: 5.5469  Epoch: 6  Global Step: 36280   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:06:32,320-Speed 3923.10 samples/sec  Loss 1.0687  LearningRate 0.1109  ProxyLR: 5.5459  Epoch: 6  Global Step: 36290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:34,946-Speed 3899.45 samples/sec  Loss 1.0958  LearningRate 0.1109  ProxyLR: 5.5448  Epoch: 6  Global Step: 36300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:37,571-Speed 3902.27 samples/sec  Loss 1.0169  LearningRate 0.1109  ProxyLR: 5.5438  Epoch: 6  Global Step: 36310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:40,198-Speed 3898.77 samples/sec  Loss 0.9602  LearningRate 0.1109  ProxyLR: 5.5427  Epoch: 6  Global Step: 36320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:42,823-Speed 3902.29 samples/sec  Loss 0.9656  LearningRate 0.1108  ProxyLR: 5.5417  Epoch: 6  Global Step: 36330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:45,448-Speed 3901.53 samples/sec  Loss 1.1444  LearningRate 0.1108  ProxyLR: 5.5406  Epoch: 6  Global Step: 36340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:48,074-Speed 3900.76 samples/sec  Loss 1.0459  LearningRate 0.1108  ProxyLR: 5.5396  Epoch: 6  Global Step: 36350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:50,699-Speed 3902.18 samples/sec  Loss 1.0930  LearningRate 0.1108  ProxyLR: 5.5385  Epoch: 6  Global Step: 36360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:53,325-Speed 3899.76 samples/sec  Loss 1.0617  LearningRate 0.1107  ProxyLR: 5.5375  Epoch: 6  Global Step: 36370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:55,957-Speed 3892.85 samples/sec  Loss 1.0607  LearningRate 0.1107  ProxyLR: 5.5364  Epoch: 6  Global Step: 36380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:06:58,588-Speed 3892.28 samples/sec  Loss 1.1369  LearningRate 0.1107  ProxyLR: 5.5354  Epoch: 6  Global Step: 36390   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:07:01,209-Speed 3908.84 samples/sec  Loss 1.0371  LearningRate 0.1107  ProxyLR: 5.5344  Epoch: 6  Global Step: 36400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:03,844-Speed 3886.97 samples/sec  Loss 1.1288  LearningRate 0.1107  ProxyLR: 5.5333  Epoch: 6  Global Step: 36410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:06,477-Speed 3889.25 samples/sec  Loss 1.0823  LearningRate 0.1106  ProxyLR: 5.5323  Epoch: 6  Global Step: 36420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:09,108-Speed 3893.09 samples/sec  Loss 1.0311  LearningRate 0.1106  ProxyLR: 5.5312  Epoch: 6  Global Step: 36430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:11,740-Speed 3892.34 samples/sec  Loss 1.0341  LearningRate 0.1106  ProxyLR: 5.5302  Epoch: 6  Global Step: 36440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:14,375-Speed 3887.03 samples/sec  Loss 1.0195  LearningRate 0.1106  ProxyLR: 5.5291  Epoch: 6  Global Step: 36450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:17,012-Speed 3883.29 samples/sec  Loss 1.1633  LearningRate 0.1106  ProxyLR: 5.5281  Epoch: 6  Global Step: 36460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:19,643-Speed 3893.35 samples/sec  Loss 1.0576  LearningRate 0.1105  ProxyLR: 5.5270  Epoch: 6  Global Step: 36470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:22,274-Speed 3892.71 samples/sec  Loss 1.1023  LearningRate 0.1105  ProxyLR: 5.5260  Epoch: 6  Global Step: 36480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:24,908-Speed 3889.00 samples/sec  Loss 1.0274  LearningRate 0.1105  ProxyLR: 5.5249  Epoch: 6  Global Step: 36490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:27,526-Speed 3911.97 samples/sec  Loss 1.1005  LearningRate 0.1105  ProxyLR: 5.5239  Epoch: 6  Global Step: 36500   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:30,160-Speed 3889.02 samples/sec  Loss 1.0461  LearningRate 0.1105  ProxyLR: 5.5228  Epoch: 6  Global Step: 36510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:32,794-Speed 3889.40 samples/sec  Loss 1.1260  LearningRate 0.1104  ProxyLR: 5.5218  Epoch: 6  Global Step: 36520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:35,426-Speed 3891.35 samples/sec  Loss 1.0557  LearningRate 0.1104  ProxyLR: 5.5208  Epoch: 6  Global Step: 36530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:38,059-Speed 3889.85 samples/sec  Loss 1.1345  LearningRate 0.1104  ProxyLR: 5.5197  Epoch: 6  Global Step: 36540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:40,693-Speed 3889.06 samples/sec  Loss 1.1237  LearningRate 0.1104  ProxyLR: 5.5187  Epoch: 6  Global Step: 36550   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:43,323-Speed 3893.31 samples/sec  Loss 1.1335  LearningRate 0.1104  ProxyLR: 5.5176  Epoch: 6  Global Step: 36560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:45,957-Speed 3889.73 samples/sec  Loss 1.0475  LearningRate 0.1103  ProxyLR: 5.5166  Epoch: 6  Global Step: 36570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:48,591-Speed 3887.68 samples/sec  Loss 1.0504  LearningRate 0.1103  ProxyLR: 5.5155  Epoch: 6  Global Step: 36580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:51,219-Speed 3897.06 samples/sec  Loss 1.1319  LearningRate 0.1103  ProxyLR: 5.5145  Epoch: 6  Global Step: 36590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:53,846-Speed 3899.80 samples/sec  Loss 1.0549  LearningRate 0.1103  ProxyLR: 5.5134  Epoch: 6  Global Step: 36600   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:07:56,459-Speed 3919.13 samples/sec  Loss 1.0227  LearningRate 0.1102  ProxyLR: 5.5124  Epoch: 6  Global Step: 36610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:07:59,084-Speed 3902.77 samples/sec  Loss 1.1357  LearningRate 0.1102  ProxyLR: 5.5114  Epoch: 6  Global Step: 36620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:01,711-Speed 3899.48 samples/sec  Loss 1.1775  LearningRate 0.1102  ProxyLR: 5.5103  Epoch: 6  Global Step: 36630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:04,336-Speed 3901.78 samples/sec  Loss 1.1962  LearningRate 0.1102  ProxyLR: 5.5093  Epoch: 6  Global Step: 36640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:06,960-Speed 3902.63 samples/sec  Loss 1.1351  LearningRate 0.1102  ProxyLR: 5.5082  Epoch: 6  Global Step: 36650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:09,586-Speed 3901.35 samples/sec  Loss 1.1459  LearningRate 0.1101  ProxyLR: 5.5072  Epoch: 6  Global Step: 36660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:12,210-Speed 3902.50 samples/sec  Loss 1.1354  LearningRate 0.1101  ProxyLR: 5.5061  Epoch: 6  Global Step: 36670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:14,836-Speed 3900.62 samples/sec  Loss 1.0729  LearningRate 0.1101  ProxyLR: 5.5051  Epoch: 6  Global Step: 36680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:17,461-Speed 3901.57 samples/sec  Loss 1.0876  LearningRate 0.1101  ProxyLR: 5.5040  Epoch: 6  Global Step: 36690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:20,087-Speed 3901.38 samples/sec  Loss 1.0515  LearningRate 0.1101  ProxyLR: 5.5030  Epoch: 6  Global Step: 36700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:22,710-Speed 3904.22 samples/sec  Loss 1.0978  LearningRate 0.1100  ProxyLR: 5.5020  Epoch: 6  Global Step: 36710   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:08:25,337-Speed 3899.71 samples/sec  Loss 1.2053  LearningRate 0.1100  ProxyLR: 5.5009  Epoch: 6  Global Step: 36720   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:08:27,963-Speed 3900.19 samples/sec  Loss 1.2055  LearningRate 0.1100  ProxyLR: 5.4999  Epoch: 6  Global Step: 36730   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:08:30,592-Speed 3896.24 samples/sec  Loss 1.0346  LearningRate 0.1100  ProxyLR: 5.4988  Epoch: 6  Global Step: 36740   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:08:33,219-Speed 3898.80 samples/sec  Loss 1.0374  LearningRate 0.1100  ProxyLR: 5.4978  Epoch: 6  Global Step: 36750   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:08:35,846-Speed 3899.07 samples/sec  Loss 1.0971  LearningRate 0.1099  ProxyLR: 5.4967  Epoch: 6  Global Step: 36760   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:08:38,458-Speed 3921.11 samples/sec  Loss 1.1674  LearningRate 0.1099  ProxyLR: 5.4957  Epoch: 6  Global Step: 36770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:41,083-Speed 3901.72 samples/sec  Loss 1.2144  LearningRate 0.1099  ProxyLR: 5.4947  Epoch: 6  Global Step: 36780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:43,708-Speed 3901.43 samples/sec  Loss 1.1623  LearningRate 0.1099  ProxyLR: 5.4936  Epoch: 6  Global Step: 36790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:08:46,321-Speed 3920.16 samples/sec  Loss 1.1797  LearningRate 0.1099  ProxyLR: 5.4926  Epoch: 6  Global Step: 36800   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:08:48,947-Speed 3900.96 samples/sec  Loss 1.0449  LearningRate 0.1098  ProxyLR: 5.4915  Epoch: 6  Global Step: 36810   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:08:51,572-Speed 3900.94 samples/sec  Loss 1.1924  LearningRate 0.1098  ProxyLR: 5.4905  Epoch: 6  Global Step: 36820   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:08:54,197-Speed 3901.99 samples/sec  Loss 1.1046  LearningRate 0.1098  ProxyLR: 5.4894  Epoch: 6  Global Step: 36830   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:08:56,823-Speed 3901.57 samples/sec  Loss 1.0786  LearningRate 0.1098  ProxyLR: 5.4884  Epoch: 6  Global Step: 36840   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:08:59,449-Speed 3900.51 samples/sec  Loss 1.2115  LearningRate 0.1097  ProxyLR: 5.4874  Epoch: 6  Global Step: 36850   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:09:02,075-Speed 3900.34 samples/sec  Loss 1.1285  LearningRate 0.1097  ProxyLR: 5.4863  Epoch: 6  Global Step: 36860   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:09:04,701-Speed 3900.34 samples/sec  Loss 1.1046  LearningRate 0.1097  ProxyLR: 5.4853  Epoch: 6  Global Step: 36870   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:09:07,328-Speed 3898.86 samples/sec  Loss 1.1261  LearningRate 0.1097  ProxyLR: 5.4842  Epoch: 6  Global Step: 36880   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:09:09,954-Speed 3900.57 samples/sec  Loss 1.1424  LearningRate 0.1097  ProxyLR: 5.4832  Epoch: 6  Global Step: 36890   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:09:12,580-Speed 3901.01 samples/sec  Loss 1.1395  LearningRate 0.1096  ProxyLR: 5.4821  Epoch: 6  Global Step: 36900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:15,206-Speed 3900.22 samples/sec  Loss 1.1978  LearningRate 0.1096  ProxyLR: 5.4811  Epoch: 6  Global Step: 36910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:17,832-Speed 3900.01 samples/sec  Loss 1.2213  LearningRate 0.1096  ProxyLR: 5.4801  Epoch: 6  Global Step: 36920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:20,457-Speed 3902.30 samples/sec  Loss 1.1072  LearningRate 0.1096  ProxyLR: 5.4790  Epoch: 6  Global Step: 36930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:23,082-Speed 3901.89 samples/sec  Loss 1.1933  LearningRate 0.1096  ProxyLR: 5.4780  Epoch: 6  Global Step: 36940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:25,707-Speed 3902.02 samples/sec  Loss 1.2029  LearningRate 0.1095  ProxyLR: 5.4769  Epoch: 6  Global Step: 36950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:28,333-Speed 3900.28 samples/sec  Loss 1.1504  LearningRate 0.1095  ProxyLR: 5.4759  Epoch: 6  Global Step: 36960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:30,957-Speed 3902.78 samples/sec  Loss 1.1179  LearningRate 0.1095  ProxyLR: 5.4749  Epoch: 6  Global Step: 36970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:33,583-Speed 3900.19 samples/sec  Loss 1.1011  LearningRate 0.1095  ProxyLR: 5.4738  Epoch: 6  Global Step: 36980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:36,208-Speed 3902.14 samples/sec  Loss 1.1080  LearningRate 0.1095  ProxyLR: 5.4728  Epoch: 6  Global Step: 36990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:38,819-Speed 3922.97 samples/sec  Loss 1.1384  LearningRate 0.1094  ProxyLR: 5.4717  Epoch: 6  Global Step: 37000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:41,446-Speed 3899.37 samples/sec  Loss 1.1038  LearningRate 0.1094  ProxyLR: 5.4707  Epoch: 6  Global Step: 37010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:44,069-Speed 3905.05 samples/sec  Loss 1.2149  LearningRate 0.1094  ProxyLR: 5.4697  Epoch: 6  Global Step: 37020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:46,695-Speed 3900.66 samples/sec  Loss 1.2152  LearningRate 0.1094  ProxyLR: 5.4686  Epoch: 6  Global Step: 37030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:49,319-Speed 3902.03 samples/sec  Loss 1.1276  LearningRate 0.1094  ProxyLR: 5.4676  Epoch: 6  Global Step: 37040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:51,945-Speed 3901.73 samples/sec  Loss 1.1000  LearningRate 0.1093  ProxyLR: 5.4665  Epoch: 6  Global Step: 37050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:54,570-Speed 3900.71 samples/sec  Loss 1.1585  LearningRate 0.1093  ProxyLR: 5.4655  Epoch: 6  Global Step: 37060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:57,195-Speed 3902.68 samples/sec  Loss 1.1001  LearningRate 0.1093  ProxyLR: 5.4644  Epoch: 6  Global Step: 37070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:09:59,822-Speed 3899.33 samples/sec  Loss 1.2161  LearningRate 0.1093  ProxyLR: 5.4634  Epoch: 6  Global Step: 37080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:02,448-Speed 3899.72 samples/sec  Loss 1.0764  LearningRate 0.1092  ProxyLR: 5.4624  Epoch: 6  Global Step: 37090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:05,061-Speed 3919.81 samples/sec  Loss 1.1726  LearningRate 0.1092  ProxyLR: 5.4613  Epoch: 6  Global Step: 37100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:07,687-Speed 3900.96 samples/sec  Loss 1.1835  LearningRate 0.1092  ProxyLR: 5.4603  Epoch: 6  Global Step: 37110   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:10,313-Speed 3900.36 samples/sec  Loss 1.1965  LearningRate 0.1092  ProxyLR: 5.4593  Epoch: 6  Global Step: 37120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:12,938-Speed 3902.25 samples/sec  Loss 1.1994  LearningRate 0.1092  ProxyLR: 5.4582  Epoch: 6  Global Step: 37130   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:15,563-Speed 3900.79 samples/sec  Loss 1.1942  LearningRate 0.1091  ProxyLR: 5.4572  Epoch: 6  Global Step: 37140   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:18,187-Speed 3903.21 samples/sec  Loss 1.0718  LearningRate 0.1091  ProxyLR: 5.4561  Epoch: 6  Global Step: 37150   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:20,814-Speed 3900.02 samples/sec  Loss 1.1089  LearningRate 0.1091  ProxyLR: 5.4551  Epoch: 6  Global Step: 37160   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:23,438-Speed 3903.85 samples/sec  Loss 1.2518  LearningRate 0.1091  ProxyLR: 5.4541  Epoch: 6  Global Step: 37170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:26,064-Speed 3900.01 samples/sec  Loss 1.1895  LearningRate 0.1091  ProxyLR: 5.4530  Epoch: 6  Global Step: 37180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:28,690-Speed 3900.84 samples/sec  Loss 1.1099  LearningRate 0.1090  ProxyLR: 5.4520  Epoch: 6  Global Step: 37190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:31,317-Speed 3899.08 samples/sec  Loss 1.2369  LearningRate 0.1090  ProxyLR: 5.4509  Epoch: 6  Global Step: 37200   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:10:33,931-Speed 3918.28 samples/sec  Loss 1.2090  LearningRate 0.1090  ProxyLR: 5.4499  Epoch: 6  Global Step: 37210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:36,555-Speed 3902.09 samples/sec  Loss 1.0992  LearningRate 0.1090  ProxyLR: 5.4489  Epoch: 6  Global Step: 37220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:39,182-Speed 3899.92 samples/sec  Loss 1.1177  LearningRate 0.1090  ProxyLR: 5.4478  Epoch: 6  Global Step: 37230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:41,814-Speed 3891.22 samples/sec  Loss 1.2044  LearningRate 0.1089  ProxyLR: 5.4468  Epoch: 6  Global Step: 37240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:44,446-Speed 3891.17 samples/sec  Loss 1.0860  LearningRate 0.1089  ProxyLR: 5.4457  Epoch: 6  Global Step: 37250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:47,078-Speed 3892.05 samples/sec  Loss 1.2539  LearningRate 0.1089  ProxyLR: 5.4447  Epoch: 6  Global Step: 37260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:49,708-Speed 3894.40 samples/sec  Loss 1.2603  LearningRate 0.1089  ProxyLR: 5.4437  Epoch: 6  Global Step: 37270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:52,335-Speed 3898.11 samples/sec  Loss 1.0988  LearningRate 0.1089  ProxyLR: 5.4426  Epoch: 6  Global Step: 37280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:54,966-Speed 3893.56 samples/sec  Loss 1.1820  LearningRate 0.1088  ProxyLR: 5.4416  Epoch: 6  Global Step: 37290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:10:57,595-Speed 3896.43 samples/sec  Loss 1.0996  LearningRate 0.1088  ProxyLR: 5.4406  Epoch: 6  Global Step: 37300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:00,224-Speed 3895.48 samples/sec  Loss 1.0832  LearningRate 0.1088  ProxyLR: 5.4395  Epoch: 6  Global Step: 37310   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:11:02,840-Speed 3915.12 samples/sec  Loss 1.1603  LearningRate 0.1088  ProxyLR: 5.4385  Epoch: 6  Global Step: 37320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:05,469-Speed 3897.03 samples/sec  Loss 1.2213  LearningRate 0.1087  ProxyLR: 5.4374  Epoch: 6  Global Step: 37330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:08,096-Speed 3898.84 samples/sec  Loss 1.1303  LearningRate 0.1087  ProxyLR: 5.4364  Epoch: 6  Global Step: 37340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:10,722-Speed 3899.47 samples/sec  Loss 1.1820  LearningRate 0.1087  ProxyLR: 5.4354  Epoch: 6  Global Step: 37350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:13,350-Speed 3898.47 samples/sec  Loss 1.1331  LearningRate 0.1087  ProxyLR: 5.4343  Epoch: 6  Global Step: 37360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:15,977-Speed 3899.19 samples/sec  Loss 1.1505  LearningRate 0.1087  ProxyLR: 5.4333  Epoch: 6  Global Step: 37370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:18,602-Speed 3901.51 samples/sec  Loss 1.2327  LearningRate 0.1086  ProxyLR: 5.4323  Epoch: 6  Global Step: 37380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:21,229-Speed 3899.17 samples/sec  Loss 1.1343  LearningRate 0.1086  ProxyLR: 5.4312  Epoch: 6  Global Step: 37390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:23,856-Speed 3898.47 samples/sec  Loss 1.3100  LearningRate 0.1086  ProxyLR: 5.4302  Epoch: 6  Global Step: 37400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:26,470-Speed 3918.88 samples/sec  Loss 1.2438  LearningRate 0.1086  ProxyLR: 5.4291  Epoch: 6  Global Step: 37410   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:29,098-Speed 3897.28 samples/sec  Loss 1.1589  LearningRate 0.1086  ProxyLR: 5.4281  Epoch: 6  Global Step: 37420   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:31,725-Speed 3898.84 samples/sec  Loss 1.2427  LearningRate 0.1085  ProxyLR: 5.4271  Epoch: 6  Global Step: 37430   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:34,353-Speed 3897.73 samples/sec  Loss 1.0943  LearningRate 0.1085  ProxyLR: 5.4260  Epoch: 6  Global Step: 37440   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:36,980-Speed 3898.45 samples/sec  Loss 1.2274  LearningRate 0.1085  ProxyLR: 5.4250  Epoch: 6  Global Step: 37450   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:39,607-Speed 3899.45 samples/sec  Loss 1.1417  LearningRate 0.1085  ProxyLR: 5.4240  Epoch: 6  Global Step: 37460   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:42,233-Speed 3899.86 samples/sec  Loss 1.2218  LearningRate 0.1085  ProxyLR: 5.4229  Epoch: 6  Global Step: 37470   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:44,860-Speed 3899.52 samples/sec  Loss 1.2561  LearningRate 0.1084  ProxyLR: 5.4219  Epoch: 6  Global Step: 37480   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:47,487-Speed 3898.13 samples/sec  Loss 1.1810  LearningRate 0.1084  ProxyLR: 5.4209  Epoch: 6  Global Step: 37490   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:50,114-Speed 3899.56 samples/sec  Loss 1.1645  LearningRate 0.1084  ProxyLR: 5.4198  Epoch: 6  Global Step: 37500   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:11:52,742-Speed 3897.79 samples/sec  Loss 1.1543  LearningRate 0.1084  ProxyLR: 5.4188  Epoch: 6  Global Step: 37510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:55,367-Speed 3902.05 samples/sec  Loss 1.0842  LearningRate 0.1084  ProxyLR: 5.4177  Epoch: 6  Global Step: 37520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:11:57,992-Speed 3901.00 samples/sec  Loss 1.1960  LearningRate 0.1083  ProxyLR: 5.4167  Epoch: 6  Global Step: 37530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:00,605-Speed 3920.72 samples/sec  Loss 1.2249  LearningRate 0.1083  ProxyLR: 5.4157  Epoch: 6  Global Step: 37540   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:03,230-Speed 3901.81 samples/sec  Loss 1.1910  LearningRate 0.1083  ProxyLR: 5.4146  Epoch: 6  Global Step: 37550   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:05,855-Speed 3902.04 samples/sec  Loss 1.1647  LearningRate 0.1083  ProxyLR: 5.4136  Epoch: 6  Global Step: 37560   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:08,479-Speed 3902.60 samples/sec  Loss 1.1809  LearningRate 0.1083  ProxyLR: 5.4126  Epoch: 6  Global Step: 37570   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:11,107-Speed 3898.39 samples/sec  Loss 1.1451  LearningRate 0.1082  ProxyLR: 5.4115  Epoch: 6  Global Step: 37580   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:13,733-Speed 3900.40 samples/sec  Loss 1.2012  LearningRate 0.1082  ProxyLR: 5.4105  Epoch: 6  Global Step: 37590   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:16,357-Speed 3902.62 samples/sec  Loss 1.2502  LearningRate 0.1082  ProxyLR: 5.4095  Epoch: 6  Global Step: 37600   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:18,983-Speed 3900.50 samples/sec  Loss 1.1680  LearningRate 0.1082  ProxyLR: 5.4084  Epoch: 6  Global Step: 37610   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:21,609-Speed 3900.10 samples/sec  Loss 1.1833  LearningRate 0.1081  ProxyLR: 5.4074  Epoch: 6  Global Step: 37620   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:24,234-Speed 3901.83 samples/sec  Loss 1.2693  LearningRate 0.1081  ProxyLR: 5.4064  Epoch: 6  Global Step: 37630   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:12:26,860-Speed 3900.78 samples/sec  Loss 1.2088  LearningRate 0.1081  ProxyLR: 5.4053  Epoch: 6  Global Step: 37640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:29,487-Speed 3899.65 samples/sec  Loss 1.2131  LearningRate 0.1081  ProxyLR: 5.4043  Epoch: 6  Global Step: 37650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:32,113-Speed 3900.58 samples/sec  Loss 1.0822  LearningRate 0.1081  ProxyLR: 5.4033  Epoch: 6  Global Step: 37660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:34,738-Speed 3901.61 samples/sec  Loss 1.2495  LearningRate 0.1080  ProxyLR: 5.4022  Epoch: 6  Global Step: 37670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:37,365-Speed 3899.24 samples/sec  Loss 1.1647  LearningRate 0.1080  ProxyLR: 5.4012  Epoch: 6  Global Step: 37680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:39,989-Speed 3902.86 samples/sec  Loss 1.2363  LearningRate 0.1080  ProxyLR: 5.4002  Epoch: 6  Global Step: 37690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:42,615-Speed 3900.53 samples/sec  Loss 1.2583  LearningRate 0.1080  ProxyLR: 5.3991  Epoch: 6  Global Step: 37700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:45,241-Speed 3900.20 samples/sec  Loss 1.1537  LearningRate 0.1080  ProxyLR: 5.3981  Epoch: 6  Global Step: 37710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:47,865-Speed 3903.29 samples/sec  Loss 1.2208  LearningRate 0.1079  ProxyLR: 5.3971  Epoch: 6  Global Step: 37720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:50,491-Speed 3901.83 samples/sec  Loss 1.1933  LearningRate 0.1079  ProxyLR: 5.3960  Epoch: 6  Global Step: 37730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:53,104-Speed 3918.72 samples/sec  Loss 1.2221  LearningRate 0.1079  ProxyLR: 5.3950  Epoch: 6  Global Step: 37740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:55,731-Speed 3899.33 samples/sec  Loss 1.2043  LearningRate 0.1079  ProxyLR: 5.3940  Epoch: 6  Global Step: 37750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:12:58,356-Speed 3901.91 samples/sec  Loss 1.1611  LearningRate 0.1079  ProxyLR: 5.3929  Epoch: 6  Global Step: 37760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:00,980-Speed 3902.55 samples/sec  Loss 1.2342  LearningRate 0.1078  ProxyLR: 5.3919  Epoch: 6  Global Step: 37770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:03,607-Speed 3899.93 samples/sec  Loss 1.1960  LearningRate 0.1078  ProxyLR: 5.3909  Epoch: 6  Global Step: 37780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:06,232-Speed 3901.46 samples/sec  Loss 1.1861  LearningRate 0.1078  ProxyLR: 5.3898  Epoch: 6  Global Step: 37790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:08,858-Speed 3900.28 samples/sec  Loss 1.2237  LearningRate 0.1078  ProxyLR: 5.3888  Epoch: 6  Global Step: 37800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:11,484-Speed 3901.29 samples/sec  Loss 1.1480  LearningRate 0.1078  ProxyLR: 5.3878  Epoch: 6  Global Step: 37810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:14,109-Speed 3902.03 samples/sec  Loss 1.2389  LearningRate 0.1077  ProxyLR: 5.3867  Epoch: 6  Global Step: 37820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:16,734-Speed 3901.88 samples/sec  Loss 1.2313  LearningRate 0.1077  ProxyLR: 5.3857  Epoch: 6  Global Step: 37830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:19,360-Speed 3900.01 samples/sec  Loss 1.2047  LearningRate 0.1077  ProxyLR: 5.3847  Epoch: 6  Global Step: 37840   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:13:21,970-Speed 3923.65 samples/sec  Loss 1.2385  LearningRate 0.1077  ProxyLR: 5.3836  Epoch: 6  Global Step: 37850   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:24,597-Speed 3899.69 samples/sec  Loss 1.2574  LearningRate 0.1077  ProxyLR: 5.3826  Epoch: 6  Global Step: 37860   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:27,223-Speed 3899.80 samples/sec  Loss 1.1697  LearningRate 0.1076  ProxyLR: 5.3816  Epoch: 6  Global Step: 37870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:29,848-Speed 3901.90 samples/sec  Loss 1.2136  LearningRate 0.1076  ProxyLR: 5.3805  Epoch: 6  Global Step: 37880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:32,472-Speed 3903.92 samples/sec  Loss 1.2147  LearningRate 0.1076  ProxyLR: 5.3795  Epoch: 6  Global Step: 37890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:35,098-Speed 3899.84 samples/sec  Loss 1.1767  LearningRate 0.1076  ProxyLR: 5.3785  Epoch: 6  Global Step: 37900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:37,724-Speed 3901.60 samples/sec  Loss 1.2270  LearningRate 0.1075  ProxyLR: 5.3774  Epoch: 6  Global Step: 37910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:40,349-Speed 3901.08 samples/sec  Loss 1.1397  LearningRate 0.1075  ProxyLR: 5.3764  Epoch: 6  Global Step: 37920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:42,975-Speed 3900.66 samples/sec  Loss 1.1318  LearningRate 0.1075  ProxyLR: 5.3754  Epoch: 6  Global Step: 37930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:45,608-Speed 3890.07 samples/sec  Loss 1.2017  LearningRate 0.1075  ProxyLR: 5.3743  Epoch: 6  Global Step: 37940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:48,223-Speed 3916.84 samples/sec  Loss 1.2099  LearningRate 0.1075  ProxyLR: 5.3733  Epoch: 6  Global Step: 37950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:50,847-Speed 3903.89 samples/sec  Loss 1.2126  LearningRate 0.1074  ProxyLR: 5.3723  Epoch: 6  Global Step: 37960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:53,471-Speed 3903.15 samples/sec  Loss 1.1425  LearningRate 0.1074  ProxyLR: 5.3712  Epoch: 6  Global Step: 37970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:56,095-Speed 3903.03 samples/sec  Loss 1.1382  LearningRate 0.1074  ProxyLR: 5.3702  Epoch: 6  Global Step: 37980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:13:58,720-Speed 3901.91 samples/sec  Loss 1.2622  LearningRate 0.1074  ProxyLR: 5.3692  Epoch: 6  Global Step: 37990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:01,344-Speed 3902.54 samples/sec  Loss 1.2501  LearningRate 0.1074  ProxyLR: 5.3682  Epoch: 6  Global Step: 38000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:03,971-Speed 3899.79 samples/sec  Loss 1.2431  LearningRate 0.1073  ProxyLR: 5.3671  Epoch: 6  Global Step: 38010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:06,596-Speed 3901.94 samples/sec  Loss 1.1675  LearningRate 0.1073  ProxyLR: 5.3661  Epoch: 6  Global Step: 38020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:09,221-Speed 3902.75 samples/sec  Loss 1.2507  LearningRate 0.1073  ProxyLR: 5.3651  Epoch: 6  Global Step: 38030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:11,845-Speed 3902.46 samples/sec  Loss 1.1740  LearningRate 0.1073  ProxyLR: 5.3640  Epoch: 6  Global Step: 38040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:14,456-Speed 3924.02 samples/sec  Loss 1.1424  LearningRate 0.1073  ProxyLR: 5.3630  Epoch: 6  Global Step: 38050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:17,080-Speed 3903.10 samples/sec  Loss 1.2247  LearningRate 0.1072  ProxyLR: 5.3620  Epoch: 6  Global Step: 38060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:14:19,692-Speed 3920.30 samples/sec  Loss 1.1919  LearningRate 0.1072  ProxyLR: 5.3609  Epoch: 6  Global Step: 38070   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:14:22,318-Speed 3901.74 samples/sec  Loss 1.2970  LearningRate 0.1072  ProxyLR: 5.3599  Epoch: 6  Global Step: 38080   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:14:24,941-Speed 3903.88 samples/sec  Loss 1.2541  LearningRate 0.1072  ProxyLR: 5.3589  Epoch: 6  Global Step: 38090   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:14:27,554-Speed 3920.92 samples/sec  Loss 1.1840  LearningRate 0.1072  ProxyLR: 5.3578  Epoch: 6  Global Step: 38100   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:30,179-Speed 3901.03 samples/sec  Loss 1.1680  LearningRate 0.1071  ProxyLR: 5.3568  Epoch: 6  Global Step: 38110   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:32,804-Speed 3902.43 samples/sec  Loss 1.1685  LearningRate 0.1071  ProxyLR: 5.3558  Epoch: 6  Global Step: 38120   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:35,428-Speed 3903.16 samples/sec  Loss 1.2148  LearningRate 0.1071  ProxyLR: 5.3548  Epoch: 6  Global Step: 38130   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:38,053-Speed 3902.64 samples/sec  Loss 1.2102  LearningRate 0.1071  ProxyLR: 5.3537  Epoch: 6  Global Step: 38140   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:40,678-Speed 3900.55 samples/sec  Loss 1.2535  LearningRate 0.1071  ProxyLR: 5.3527  Epoch: 6  Global Step: 38150   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:43,304-Speed 3900.98 samples/sec  Loss 1.2162  LearningRate 0.1070  ProxyLR: 5.3517  Epoch: 6  Global Step: 38160   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:45,930-Speed 3900.83 samples/sec  Loss 1.2211  LearningRate 0.1070  ProxyLR: 5.3506  Epoch: 6  Global Step: 38170   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:48,555-Speed 3901.85 samples/sec  Loss 1.2666  LearningRate 0.1070  ProxyLR: 5.3496  Epoch: 6  Global Step: 38180   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:51,180-Speed 3901.89 samples/sec  Loss 1.1443  LearningRate 0.1070  ProxyLR: 5.3486  Epoch: 6  Global Step: 38190   Fp16 Grad Scale: 131072  Required: 8 hours
Training: 2023-05-04 17:14:53,806-Speed 3901.48 samples/sec  Loss 1.2389  LearningRate 0.1070  ProxyLR: 5.3476  Epoch: 6  Global Step: 38200   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:14:56,431-Speed 3901.08 samples/sec  Loss 1.2764  LearningRate 0.1069  ProxyLR: 5.3465  Epoch: 6  Global Step: 38210   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:14:59,055-Speed 3903.13 samples/sec  Loss 1.2996  LearningRate 0.1069  ProxyLR: 5.3455  Epoch: 6  Global Step: 38220   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:01,682-Speed 3898.86 samples/sec  Loss 1.2355  LearningRate 0.1069  ProxyLR: 5.3445  Epoch: 6  Global Step: 38230   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:04,307-Speed 3902.69 samples/sec  Loss 1.1875  LearningRate 0.1069  ProxyLR: 5.3434  Epoch: 6  Global Step: 38240   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:06,931-Speed 3903.51 samples/sec  Loss 1.2837  LearningRate 0.1068  ProxyLR: 5.3424  Epoch: 6  Global Step: 38250   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:09,557-Speed 3900.47 samples/sec  Loss 1.2493  LearningRate 0.1068  ProxyLR: 5.3414  Epoch: 6  Global Step: 38260   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:12,182-Speed 3901.73 samples/sec  Loss 1.3241  LearningRate 0.1068  ProxyLR: 5.3404  Epoch: 6  Global Step: 38270   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:14,809-Speed 3899.34 samples/sec  Loss 1.1925  LearningRate 0.1068  ProxyLR: 5.3393  Epoch: 6  Global Step: 38280   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:17,435-Speed 3899.86 samples/sec  Loss 1.2338  LearningRate 0.1068  ProxyLR: 5.3383  Epoch: 6  Global Step: 38290   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:15:20,061-Speed 3899.96 samples/sec  Loss 1.2175  LearningRate 0.1067  ProxyLR: 5.3373  Epoch: 6  Global Step: 38300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:22,687-Speed 3900.37 samples/sec  Loss 1.2061  LearningRate 0.1067  ProxyLR: 5.3362  Epoch: 6  Global Step: 38310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:25,312-Speed 3901.99 samples/sec  Loss 1.1721  LearningRate 0.1067  ProxyLR: 5.3352  Epoch: 6  Global Step: 38320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:27,939-Speed 3900.12 samples/sec  Loss 1.1242  LearningRate 0.1067  ProxyLR: 5.3342  Epoch: 6  Global Step: 38330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:30,562-Speed 3903.84 samples/sec  Loss 1.1647  LearningRate 0.1067  ProxyLR: 5.3332  Epoch: 6  Global Step: 38340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:33,188-Speed 3901.14 samples/sec  Loss 1.1619  LearningRate 0.1066  ProxyLR: 5.3321  Epoch: 6  Global Step: 38350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:35,813-Speed 3901.25 samples/sec  Loss 1.1649  LearningRate 0.1066  ProxyLR: 5.3311  Epoch: 6  Global Step: 38360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:38,440-Speed 3899.49 samples/sec  Loss 1.2072  LearningRate 0.1066  ProxyLR: 5.3301  Epoch: 6  Global Step: 38370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:41,063-Speed 3904.21 samples/sec  Loss 1.3248  LearningRate 0.1066  ProxyLR: 5.3291  Epoch: 6  Global Step: 38380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:43,688-Speed 3903.15 samples/sec  Loss 1.2713  LearningRate 0.1066  ProxyLR: 5.3280  Epoch: 6  Global Step: 38390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:46,313-Speed 3901.44 samples/sec  Loss 1.2254  LearningRate 0.1065  ProxyLR: 5.3270  Epoch: 6  Global Step: 38400   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:15:48,924-Speed 3922.47 samples/sec  Loss 1.2790  LearningRate 0.1065  ProxyLR: 5.3260  Epoch: 6  Global Step: 38410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:51,550-Speed 3900.71 samples/sec  Loss 1.2444  LearningRate 0.1065  ProxyLR: 5.3249  Epoch: 6  Global Step: 38420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:54,175-Speed 3902.75 samples/sec  Loss 1.2392  LearningRate 0.1065  ProxyLR: 5.3239  Epoch: 6  Global Step: 38430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:56,800-Speed 3900.67 samples/sec  Loss 1.2645  LearningRate 0.1065  ProxyLR: 5.3229  Epoch: 6  Global Step: 38440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:15:59,431-Speed 3894.05 samples/sec  Loss 1.2837  LearningRate 0.1064  ProxyLR: 5.3219  Epoch: 6  Global Step: 38450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:02,058-Speed 3898.61 samples/sec  Loss 1.2872  LearningRate 0.1064  ProxyLR: 5.3208  Epoch: 6  Global Step: 38460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:04,688-Speed 3894.90 samples/sec  Loss 1.2520  LearningRate 0.1064  ProxyLR: 5.3198  Epoch: 6  Global Step: 38470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:07,319-Speed 3891.82 samples/sec  Loss 1.2355  LearningRate 0.1064  ProxyLR: 5.3188  Epoch: 6  Global Step: 38480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:09,948-Speed 3896.86 samples/sec  Loss 1.1632  LearningRate 0.1064  ProxyLR: 5.3178  Epoch: 6  Global Step: 38490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:12,563-Speed 3917.62 samples/sec  Loss 1.1859  LearningRate 0.1063  ProxyLR: 5.3167  Epoch: 6  Global Step: 38500   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:15,190-Speed 3897.79 samples/sec  Loss 1.2448  LearningRate 0.1063  ProxyLR: 5.3157  Epoch: 6  Global Step: 38510   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:17,823-Speed 3889.77 samples/sec  Loss 1.1494  LearningRate 0.1063  ProxyLR: 5.3147  Epoch: 6  Global Step: 38520   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:20,455-Speed 3891.40 samples/sec  Loss 1.2544  LearningRate 0.1063  ProxyLR: 5.3137  Epoch: 6  Global Step: 38530   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:23,088-Speed 3891.31 samples/sec  Loss 1.1611  LearningRate 0.1063  ProxyLR: 5.3126  Epoch: 6  Global Step: 38540   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:25,722-Speed 3887.62 samples/sec  Loss 1.2126  LearningRate 0.1062  ProxyLR: 5.3116  Epoch: 6  Global Step: 38550   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:28,351-Speed 3896.91 samples/sec  Loss 1.2114  LearningRate 0.1062  ProxyLR: 5.3106  Epoch: 6  Global Step: 38560   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:30,981-Speed 3894.05 samples/sec  Loss 1.2376  LearningRate 0.1062  ProxyLR: 5.3096  Epoch: 6  Global Step: 38570   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:33,610-Speed 3895.35 samples/sec  Loss 1.3818  LearningRate 0.1062  ProxyLR: 5.3085  Epoch: 6  Global Step: 38580   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:36,240-Speed 3895.21 samples/sec  Loss 1.1684  LearningRate 0.1062  ProxyLR: 5.3075  Epoch: 6  Global Step: 38590   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:16:38,871-Speed 3892.34 samples/sec  Loss 1.1790  LearningRate 0.1061  ProxyLR: 5.3065  Epoch: 6  Global Step: 38600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:41,503-Speed 3892.18 samples/sec  Loss 1.3601  LearningRate 0.1061  ProxyLR: 5.3055  Epoch: 6  Global Step: 38610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:44,134-Speed 3893.06 samples/sec  Loss 1.2232  LearningRate 0.1061  ProxyLR: 5.3044  Epoch: 6  Global Step: 38620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:46,764-Speed 3893.83 samples/sec  Loss 1.1963  LearningRate 0.1061  ProxyLR: 5.3034  Epoch: 6  Global Step: 38630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:49,395-Speed 3893.51 samples/sec  Loss 1.2292  LearningRate 0.1060  ProxyLR: 5.3024  Epoch: 6  Global Step: 38640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:52,027-Speed 3892.06 samples/sec  Loss 1.3027  LearningRate 0.1060  ProxyLR: 5.3014  Epoch: 6  Global Step: 38650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:54,658-Speed 3893.31 samples/sec  Loss 1.2284  LearningRate 0.1060  ProxyLR: 5.3003  Epoch: 6  Global Step: 38660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:57,287-Speed 3896.05 samples/sec  Loss 1.3248  LearningRate 0.1060  ProxyLR: 5.2993  Epoch: 6  Global Step: 38670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:16:59,916-Speed 3894.87 samples/sec  Loss 1.1490  LearningRate 0.1060  ProxyLR: 5.2983  Epoch: 6  Global Step: 38680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:02,544-Speed 3898.09 samples/sec  Loss 1.2210  LearningRate 0.1059  ProxyLR: 5.2973  Epoch: 6  Global Step: 38690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:05,163-Speed 3910.44 samples/sec  Loss 1.0849  LearningRate 0.1059  ProxyLR: 5.2962  Epoch: 6  Global Step: 38700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:07,794-Speed 3893.77 samples/sec  Loss 1.2867  LearningRate 0.1059  ProxyLR: 5.2952  Epoch: 6  Global Step: 38710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:10,425-Speed 3892.65 samples/sec  Loss 1.1842  LearningRate 0.1059  ProxyLR: 5.2942  Epoch: 6  Global Step: 38720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:13,055-Speed 3894.46 samples/sec  Loss 1.2067  LearningRate 0.1059  ProxyLR: 5.2932  Epoch: 6  Global Step: 38730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:15,685-Speed 3893.93 samples/sec  Loss 1.3522  LearningRate 0.1058  ProxyLR: 5.2921  Epoch: 6  Global Step: 38740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:18,314-Speed 3895.58 samples/sec  Loss 1.2672  LearningRate 0.1058  ProxyLR: 5.2911  Epoch: 6  Global Step: 38750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:20,942-Speed 3897.91 samples/sec  Loss 1.1492  LearningRate 0.1058  ProxyLR: 5.2901  Epoch: 6  Global Step: 38760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:23,557-Speed 3917.71 samples/sec  Loss 1.3247  LearningRate 0.1058  ProxyLR: 5.2891  Epoch: 6  Global Step: 38770   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:26,186-Speed 3895.31 samples/sec  Loss 1.2852  LearningRate 0.1058  ProxyLR: 5.2880  Epoch: 6  Global Step: 38780   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:28,815-Speed 3895.75 samples/sec  Loss 1.2320  LearningRate 0.1057  ProxyLR: 5.2870  Epoch: 6  Global Step: 38790   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:31,444-Speed 3895.55 samples/sec  Loss 1.3158  LearningRate 0.1057  ProxyLR: 5.2860  Epoch: 6  Global Step: 38800   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:34,073-Speed 3896.23 samples/sec  Loss 1.3585  LearningRate 0.1057  ProxyLR: 5.2850  Epoch: 6  Global Step: 38810   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:36,705-Speed 3892.39 samples/sec  Loss 1.1597  LearningRate 0.1057  ProxyLR: 5.2840  Epoch: 6  Global Step: 38820   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:39,335-Speed 3894.62 samples/sec  Loss 1.2567  LearningRate 0.1057  ProxyLR: 5.2829  Epoch: 6  Global Step: 38830   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:41,964-Speed 3895.97 samples/sec  Loss 1.1940  LearningRate 0.1056  ProxyLR: 5.2819  Epoch: 6  Global Step: 38840   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:44,593-Speed 3895.61 samples/sec  Loss 1.2251  LearningRate 0.1056  ProxyLR: 5.2809  Epoch: 6  Global Step: 38850   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:47,222-Speed 3894.89 samples/sec  Loss 1.1966  LearningRate 0.1056  ProxyLR: 5.2799  Epoch: 6  Global Step: 38860   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:17:49,852-Speed 3895.11 samples/sec  Loss 1.3103  LearningRate 0.1056  ProxyLR: 5.2788  Epoch: 6  Global Step: 38870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:52,482-Speed 3894.69 samples/sec  Loss 1.2487  LearningRate 0.1056  ProxyLR: 5.2778  Epoch: 6  Global Step: 38880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:55,112-Speed 3893.67 samples/sec  Loss 1.3004  LearningRate 0.1055  ProxyLR: 5.2768  Epoch: 6  Global Step: 38890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:17:57,745-Speed 3890.85 samples/sec  Loss 1.2456  LearningRate 0.1055  ProxyLR: 5.2758  Epoch: 6  Global Step: 38900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:00,375-Speed 3894.45 samples/sec  Loss 1.1811  LearningRate 0.1055  ProxyLR: 5.2748  Epoch: 6  Global Step: 38910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:03,005-Speed 3894.06 samples/sec  Loss 1.1894  LearningRate 0.1055  ProxyLR: 5.2737  Epoch: 6  Global Step: 38920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:05,636-Speed 3892.38 samples/sec  Loss 1.2446  LearningRate 0.1055  ProxyLR: 5.2727  Epoch: 6  Global Step: 38930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:08,269-Speed 3891.20 samples/sec  Loss 1.1846  LearningRate 0.1054  ProxyLR: 5.2717  Epoch: 6  Global Step: 38940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:10,901-Speed 3890.89 samples/sec  Loss 1.2030  LearningRate 0.1054  ProxyLR: 5.2707  Epoch: 6  Global Step: 38950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:13,535-Speed 3888.98 samples/sec  Loss 1.2536  LearningRate 0.1054  ProxyLR: 5.2696  Epoch: 6  Global Step: 38960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:16,154-Speed 3909.89 samples/sec  Loss 1.2582  LearningRate 0.1054  ProxyLR: 5.2686  Epoch: 6  Global Step: 38970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:18,787-Speed 3890.65 samples/sec  Loss 1.2061  LearningRate 0.1054  ProxyLR: 5.2676  Epoch: 6  Global Step: 38980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:21,417-Speed 3894.72 samples/sec  Loss 1.2627  LearningRate 0.1053  ProxyLR: 5.2666  Epoch: 6  Global Step: 38990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:24,034-Speed 3913.30 samples/sec  Loss 1.2106  LearningRate 0.1053  ProxyLR: 5.2656  Epoch: 6  Global Step: 39000   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:26,667-Speed 3890.23 samples/sec  Loss 1.2353  LearningRate 0.1053  ProxyLR: 5.2645  Epoch: 6  Global Step: 39010   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:29,299-Speed 3891.73 samples/sec  Loss 1.2286  LearningRate 0.1053  ProxyLR: 5.2635  Epoch: 6  Global Step: 39020   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:31,931-Speed 3891.59 samples/sec  Loss 1.2066  LearningRate 0.1052  ProxyLR: 5.2625  Epoch: 6  Global Step: 39030   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:34,564-Speed 3889.54 samples/sec  Loss 1.1948  LearningRate 0.1052  ProxyLR: 5.2615  Epoch: 6  Global Step: 39040   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:37,197-Speed 3891.02 samples/sec  Loss 1.2025  LearningRate 0.1052  ProxyLR: 5.2605  Epoch: 6  Global Step: 39050   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:39,831-Speed 3888.54 samples/sec  Loss 1.2822  LearningRate 0.1052  ProxyLR: 5.2594  Epoch: 6  Global Step: 39060   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:42,463-Speed 3890.47 samples/sec  Loss 1.1901  LearningRate 0.1052  ProxyLR: 5.2584  Epoch: 6  Global Step: 39070   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:45,097-Speed 3888.58 samples/sec  Loss 1.2082  LearningRate 0.1051  ProxyLR: 5.2574  Epoch: 6  Global Step: 39080   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:47,730-Speed 3890.54 samples/sec  Loss 1.2339  LearningRate 0.1051  ProxyLR: 5.2564  Epoch: 6  Global Step: 39090   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:18:50,361-Speed 3892.94 samples/sec  Loss 1.2495  LearningRate 0.1051  ProxyLR: 5.2554  Epoch: 6  Global Step: 39100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:52,993-Speed 3891.12 samples/sec  Loss 1.1181  LearningRate 0.1051  ProxyLR: 5.2543  Epoch: 6  Global Step: 39110   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:55,627-Speed 3888.88 samples/sec  Loss 1.3265  LearningRate 0.1051  ProxyLR: 5.2533  Epoch: 6  Global Step: 39120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:18:58,261-Speed 3888.05 samples/sec  Loss 1.2819  LearningRate 0.1050  ProxyLR: 5.2523  Epoch: 6  Global Step: 39130   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:00,896-Speed 3887.08 samples/sec  Loss 1.2016  LearningRate 0.1050  ProxyLR: 5.2513  Epoch: 6  Global Step: 39140   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:03,531-Speed 3887.32 samples/sec  Loss 1.2727  LearningRate 0.1050  ProxyLR: 5.2503  Epoch: 6  Global Step: 39150   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:06,166-Speed 3887.14 samples/sec  Loss 1.2546  LearningRate 0.1050  ProxyLR: 5.2492  Epoch: 6  Global Step: 39160   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:08,803-Speed 3884.42 samples/sec  Loss 1.2277  LearningRate 0.1050  ProxyLR: 5.2482  Epoch: 6  Global Step: 39170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:11,438-Speed 3886.80 samples/sec  Loss 1.2472  LearningRate 0.1049  ProxyLR: 5.2472  Epoch: 6  Global Step: 39180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:14,069-Speed 3893.23 samples/sec  Loss 1.1523  LearningRate 0.1049  ProxyLR: 5.2462  Epoch: 6  Global Step: 39190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:16,686-Speed 3913.25 samples/sec  Loss 1.2793  LearningRate 0.1049  ProxyLR: 5.2452  Epoch: 6  Global Step: 39200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:19,316-Speed 3894.09 samples/sec  Loss 1.2215  LearningRate 0.1049  ProxyLR: 5.2441  Epoch: 6  Global Step: 39210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:21,945-Speed 3895.78 samples/sec  Loss 1.2502  LearningRate 0.1049  ProxyLR: 5.2431  Epoch: 6  Global Step: 39220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:24,576-Speed 3894.01 samples/sec  Loss 1.3258  LearningRate 0.1048  ProxyLR: 5.2421  Epoch: 6  Global Step: 39230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:27,206-Speed 3894.09 samples/sec  Loss 1.2860  LearningRate 0.1048  ProxyLR: 5.2411  Epoch: 6  Global Step: 39240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:29,835-Speed 3895.90 samples/sec  Loss 1.3362  LearningRate 0.1048  ProxyLR: 5.2401  Epoch: 6  Global Step: 39250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:32,465-Speed 3895.12 samples/sec  Loss 1.2373  LearningRate 0.1048  ProxyLR: 5.2391  Epoch: 6  Global Step: 39260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:35,094-Speed 3894.94 samples/sec  Loss 1.2740  LearningRate 0.1048  ProxyLR: 5.2380  Epoch: 6  Global Step: 39270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:37,725-Speed 3893.66 samples/sec  Loss 1.2503  LearningRate 0.1047  ProxyLR: 5.2370  Epoch: 6  Global Step: 39280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:40,353-Speed 3896.29 samples/sec  Loss 1.3334  LearningRate 0.1047  ProxyLR: 5.2360  Epoch: 6  Global Step: 39290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:42,971-Speed 3913.39 samples/sec  Loss 1.2697  LearningRate 0.1047  ProxyLR: 5.2350  Epoch: 6  Global Step: 39300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:45,602-Speed 3891.80 samples/sec  Loss 1.3170  LearningRate 0.1047  ProxyLR: 5.2340  Epoch: 6  Global Step: 39310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:48,233-Speed 3893.50 samples/sec  Loss 1.2433  LearningRate 0.1047  ProxyLR: 5.2329  Epoch: 6  Global Step: 39320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:50,862-Speed 3895.73 samples/sec  Loss 1.3015  LearningRate 0.1046  ProxyLR: 5.2319  Epoch: 6  Global Step: 39330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:19:53,479-Speed 3915.05 samples/sec  Loss 1.2134  LearningRate 0.1046  ProxyLR: 5.2309  Epoch: 6  Global Step: 39340   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:19:56,108-Speed 3895.81 samples/sec  Loss 1.3270  LearningRate 0.1046  ProxyLR: 5.2299  Epoch: 6  Global Step: 39350   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:19:58,738-Speed 3894.08 samples/sec  Loss 1.3338  LearningRate 0.1046  ProxyLR: 5.2289  Epoch: 6  Global Step: 39360   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:01,371-Speed 3890.24 samples/sec  Loss 1.3114  LearningRate 0.1046  ProxyLR: 5.2279  Epoch: 6  Global Step: 39370   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:04,003-Speed 3890.70 samples/sec  Loss 1.2562  LearningRate 0.1045  ProxyLR: 5.2268  Epoch: 6  Global Step: 39380   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:06,637-Speed 3889.83 samples/sec  Loss 1.3622  LearningRate 0.1045  ProxyLR: 5.2258  Epoch: 6  Global Step: 39390   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:09,271-Speed 3888.10 samples/sec  Loss 1.2646  LearningRate 0.1045  ProxyLR: 5.2248  Epoch: 6  Global Step: 39400   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:11,904-Speed 3888.92 samples/sec  Loss 1.2879  LearningRate 0.1045  ProxyLR: 5.2238  Epoch: 6  Global Step: 39410   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:14,535-Speed 3894.32 samples/sec  Loss 1.2613  LearningRate 0.1045  ProxyLR: 5.2228  Epoch: 6  Global Step: 39420   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:17,164-Speed 3895.67 samples/sec  Loss 1.2905  LearningRate 0.1044  ProxyLR: 5.2218  Epoch: 6  Global Step: 39430   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:20:19,794-Speed 3894.46 samples/sec  Loss 1.1282  LearningRate 0.1044  ProxyLR: 5.2207  Epoch: 6  Global Step: 39440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:22,423-Speed 3895.11 samples/sec  Loss 1.3177  LearningRate 0.1044  ProxyLR: 5.2197  Epoch: 6  Global Step: 39450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:25,052-Speed 3895.53 samples/sec  Loss 1.3648  LearningRate 0.1044  ProxyLR: 5.2187  Epoch: 6  Global Step: 39460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:27,682-Speed 3894.60 samples/sec  Loss 1.2033  LearningRate 0.1044  ProxyLR: 5.2177  Epoch: 6  Global Step: 39470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:30,312-Speed 3894.89 samples/sec  Loss 1.2702  LearningRate 0.1043  ProxyLR: 5.2167  Epoch: 6  Global Step: 39480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:32,942-Speed 3894.04 samples/sec  Loss 1.2714  LearningRate 0.1043  ProxyLR: 5.2157  Epoch: 6  Global Step: 39490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:35,572-Speed 3894.67 samples/sec  Loss 1.2769  LearningRate 0.1043  ProxyLR: 5.2146  Epoch: 6  Global Step: 39500   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:38,204-Speed 3891.88 samples/sec  Loss 1.2761  LearningRate 0.1043  ProxyLR: 5.2136  Epoch: 6  Global Step: 39510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:40,834-Speed 3895.33 samples/sec  Loss 1.2937  LearningRate 0.1043  ProxyLR: 5.2126  Epoch: 6  Global Step: 39520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:43,463-Speed 3894.83 samples/sec  Loss 1.3128  LearningRate 0.1042  ProxyLR: 5.2116  Epoch: 6  Global Step: 39530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:46,080-Speed 3914.33 samples/sec  Loss 1.2291  LearningRate 0.1042  ProxyLR: 5.2106  Epoch: 6  Global Step: 39540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:48,709-Speed 3895.97 samples/sec  Loss 1.2941  LearningRate 0.1042  ProxyLR: 5.2096  Epoch: 6  Global Step: 39550   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:51,339-Speed 3895.15 samples/sec  Loss 1.3116  LearningRate 0.1042  ProxyLR: 5.2085  Epoch: 6  Global Step: 39560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:53,969-Speed 3894.52 samples/sec  Loss 1.2160  LearningRate 0.1042  ProxyLR: 5.2075  Epoch: 6  Global Step: 39570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:56,598-Speed 3895.12 samples/sec  Loss 1.3108  LearningRate 0.1041  ProxyLR: 5.2065  Epoch: 6  Global Step: 39580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:20:59,229-Speed 3893.54 samples/sec  Loss 1.1844  LearningRate 0.1041  ProxyLR: 5.2055  Epoch: 6  Global Step: 39590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:01,859-Speed 3894.36 samples/sec  Loss 1.3975  LearningRate 0.1041  ProxyLR: 5.2045  Epoch: 6  Global Step: 39600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:04,487-Speed 3896.76 samples/sec  Loss 1.1708  LearningRate 0.1041  ProxyLR: 5.2035  Epoch: 6  Global Step: 39610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:07,117-Speed 3894.75 samples/sec  Loss 1.2472  LearningRate 0.1040  ProxyLR: 5.2025  Epoch: 6  Global Step: 39620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:09,745-Speed 3896.62 samples/sec  Loss 1.2097  LearningRate 0.1040  ProxyLR: 5.2014  Epoch: 6  Global Step: 39630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:12,375-Speed 3894.49 samples/sec  Loss 1.2657  LearningRate 0.1040  ProxyLR: 5.2004  Epoch: 6  Global Step: 39640   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:21:14,992-Speed 3914.38 samples/sec  Loss 1.1830  LearningRate 0.1040  ProxyLR: 5.1994  Epoch: 6  Global Step: 39650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:17,622-Speed 3894.03 samples/sec  Loss 1.2766  LearningRate 0.1040  ProxyLR: 5.1984  Epoch: 6  Global Step: 39660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:20,252-Speed 3894.91 samples/sec  Loss 1.3211  LearningRate 0.1039  ProxyLR: 5.1974  Epoch: 6  Global Step: 39670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:22,882-Speed 3894.87 samples/sec  Loss 1.2672  LearningRate 0.1039  ProxyLR: 5.1964  Epoch: 6  Global Step: 39680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:25,512-Speed 3894.61 samples/sec  Loss 1.2943  LearningRate 0.1039  ProxyLR: 5.1954  Epoch: 6  Global Step: 39690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:28,141-Speed 3894.97 samples/sec  Loss 1.2279  LearningRate 0.1039  ProxyLR: 5.1943  Epoch: 6  Global Step: 39700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:30,772-Speed 3893.36 samples/sec  Loss 1.2520  LearningRate 0.1039  ProxyLR: 5.1933  Epoch: 6  Global Step: 39710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:33,401-Speed 3895.62 samples/sec  Loss 1.1944  LearningRate 0.1038  ProxyLR: 5.1923  Epoch: 6  Global Step: 39720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:36,030-Speed 3895.77 samples/sec  Loss 1.2936  LearningRate 0.1038  ProxyLR: 5.1913  Epoch: 6  Global Step: 39730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:38,661-Speed 3893.58 samples/sec  Loss 1.2547  LearningRate 0.1038  ProxyLR: 5.1903  Epoch: 6  Global Step: 39740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:41,290-Speed 3895.44 samples/sec  Loss 1.2799  LearningRate 0.1038  ProxyLR: 5.1893  Epoch: 6  Global Step: 39750   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:21:43,906-Speed 3916.59 samples/sec  Loss 1.2950  LearningRate 0.1038  ProxyLR: 5.1883  Epoch: 6  Global Step: 39760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:46,534-Speed 3897.27 samples/sec  Loss 1.2905  LearningRate 0.1037  ProxyLR: 5.1872  Epoch: 6  Global Step: 39770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:49,164-Speed 3894.25 samples/sec  Loss 1.2439  LearningRate 0.1037  ProxyLR: 5.1862  Epoch: 6  Global Step: 39780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:51,797-Speed 3889.74 samples/sec  Loss 1.2809  LearningRate 0.1037  ProxyLR: 5.1852  Epoch: 6  Global Step: 39790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:21:54,471-Speed 3830.04 samples/sec  Loss 1.3158  LearningRate 0.1037  ProxyLR: 5.1842  Epoch: 6  Global Step: 39800   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:03,910-Speed 1085.04 samples/sec  Loss 1.8378  LearningRate 0.1037  ProxyLR: 5.1832  Epoch: 7  Global Step: 39810   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:06,587-Speed 3825.14 samples/sec  Loss 1.7275  LearningRate 0.1036  ProxyLR: 5.1822  Epoch: 7  Global Step: 39820   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:09,243-Speed 3856.28 samples/sec  Loss 1.6899  LearningRate 0.1036  ProxyLR: 5.1812  Epoch: 7  Global Step: 39830   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:11,873-Speed 3895.03 samples/sec  Loss 1.5527  LearningRate 0.1036  ProxyLR: 5.1802  Epoch: 7  Global Step: 39840   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:14,504-Speed 3892.69 samples/sec  Loss 1.6231  LearningRate 0.1036  ProxyLR: 5.1791  Epoch: 7  Global Step: 39850   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:17,137-Speed 3890.94 samples/sec  Loss 1.6560  LearningRate 0.1036  ProxyLR: 5.1781  Epoch: 7  Global Step: 39860   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:19,771-Speed 3888.70 samples/sec  Loss 1.5955  LearningRate 0.1035  ProxyLR: 5.1771  Epoch: 7  Global Step: 39870   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:22,404-Speed 3889.06 samples/sec  Loss 1.5715  LearningRate 0.1035  ProxyLR: 5.1761  Epoch: 7  Global Step: 39880   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:25,036-Speed 3892.17 samples/sec  Loss 1.6423  LearningRate 0.1035  ProxyLR: 5.1751  Epoch: 7  Global Step: 39890   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:22:27,669-Speed 3890.40 samples/sec  Loss 1.5462  LearningRate 0.1035  ProxyLR: 5.1741  Epoch: 7  Global Step: 39900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:30,323-Speed 3858.54 samples/sec  Loss 1.5553  LearningRate 0.1035  ProxyLR: 5.1731  Epoch: 7  Global Step: 39910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:32,957-Speed 3888.99 samples/sec  Loss 1.5669  LearningRate 0.1034  ProxyLR: 5.1721  Epoch: 7  Global Step: 39920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:35,590-Speed 3890.32 samples/sec  Loss 1.5065  LearningRate 0.1034  ProxyLR: 5.1710  Epoch: 7  Global Step: 39930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:38,221-Speed 3892.37 samples/sec  Loss 1.5321  LearningRate 0.1034  ProxyLR: 5.1700  Epoch: 7  Global Step: 39940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:40,853-Speed 3892.20 samples/sec  Loss 1.3989  LearningRate 0.1034  ProxyLR: 5.1690  Epoch: 7  Global Step: 39950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:43,485-Speed 3891.87 samples/sec  Loss 1.4532  LearningRate 0.1034  ProxyLR: 5.1680  Epoch: 7  Global Step: 39960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:46,169-Speed 3815.99 samples/sec  Loss 1.4032  LearningRate 0.1033  ProxyLR: 5.1670  Epoch: 7  Global Step: 39970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:48,797-Speed 3897.48 samples/sec  Loss 1.4750  LearningRate 0.1033  ProxyLR: 5.1660  Epoch: 7  Global Step: 39980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:51,475-Speed 3824.63 samples/sec  Loss 1.5189  LearningRate 0.1033  ProxyLR: 5.1650  Epoch: 7  Global Step: 39990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:22:54,102-Speed 3898.66 samples/sec  Loss 1.4605  LearningRate 0.1033  ProxyLR: 5.1640  Epoch: 7  Global Step: 40000   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:23:43,793-[lfw][40000]XNorm: 21.292016
Training: 2023-05-04 17:23:43,793-[lfw][40000]Accuracy-Flip: 0.99600+-0.00359
Training: 2023-05-04 17:23:43,793-[lfw][40000]Accuracy-Highest: 0.99633
Training: 2023-05-04 17:23:43,793-[lfw][40000]TPR@1stNon-Zero-FPR of 0.00033: 0.98300
Training: 2023-05-04 17:23:43,793-[lfw][40000]Highest TPR@FPR: 0.98300
Training: 2023-05-04 17:24:40,861-[cfp_fp][40000]XNorm: 20.993948
Training: 2023-05-04 17:24:40,861-[cfp_fp][40000]Accuracy-Flip: 0.95314+-0.01139
Training: 2023-05-04 17:24:40,861-[cfp_fp][40000]Accuracy-Highest: 0.95314
Training: 2023-05-04 17:24:40,861-[cfp_fp][40000]TPR@1stNon-Zero-FPR of 0.00029: 0.63486
Training: 2023-05-04 17:24:40,862-[cfp_fp][40000]Highest TPR@FPR: 0.63486
Training: 2023-05-04 17:25:30,559-[agedb_30][40000]XNorm: 21.372548
Training: 2023-05-04 17:25:30,559-[agedb_30][40000]Accuracy-Flip: 0.95517+-0.01029
Training: 2023-05-04 17:25:30,559-[agedb_30][40000]Accuracy-Highest: 0.95517
Training: 2023-05-04 17:25:30,560-[agedb_30][40000]TPR@1stNon-Zero-FPR of 0.00033: 0.56467
Training: 2023-05-04 17:25:30,560-[agedb_30][40000]Highest TPR@FPR: 0.56467
Training: 2023-05-04 17:26:21,684-[calfw][40000]XNorm: 21.453182
Training: 2023-05-04 17:26:21,684-[calfw][40000]Accuracy-Flip: 0.94733+-0.00940
Training: 2023-05-04 17:26:21,684-[calfw][40000]Accuracy-Highest: 0.94733
Training: 2023-05-04 17:26:21,685-[calfw][40000]TPR@1stNon-Zero-FPR of 0.00033: 0.79900
Training: 2023-05-04 17:26:21,685-[calfw][40000]Highest TPR@FPR: 0.79900
Training: 2023-05-04 17:27:12,769-[cplfw][40000]XNorm: 20.513547
Training: 2023-05-04 17:27:12,769-[cplfw][40000]Accuracy-Flip: 0.89867+-0.01928
Training: 2023-05-04 17:27:12,770-[cplfw][40000]Accuracy-Highest: 0.89867
Training: 2023-05-04 17:27:12,770-[cplfw][40000]TPR@1stNon-Zero-FPR of 0.00033: 0.00300
Training: 2023-05-04 17:27:12,770-[cplfw][40000]Highest TPR@FPR: 0.00300
Training: 2023-05-04 17:27:15,397-Speed 39.19 samples/sec  Loss 1.4492  LearningRate 0.1033  ProxyLR: 5.1630  Epoch: 7  Global Step: 40010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:18,014-Speed 3915.09 samples/sec  Loss 1.4695  LearningRate 0.1032  ProxyLR: 5.1619  Epoch: 7  Global Step: 40020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:20,629-Speed 3916.25 samples/sec  Loss 1.4138  LearningRate 0.1032  ProxyLR: 5.1609  Epoch: 7  Global Step: 40030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:23,245-Speed 3915.52 samples/sec  Loss 1.4807  LearningRate 0.1032  ProxyLR: 5.1599  Epoch: 7  Global Step: 40040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:25,862-Speed 3913.42 samples/sec  Loss 1.3937  LearningRate 0.1032  ProxyLR: 5.1589  Epoch: 7  Global Step: 40050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:28,479-Speed 3914.09 samples/sec  Loss 1.3481  LearningRate 0.1032  ProxyLR: 5.1579  Epoch: 7  Global Step: 40060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:31,097-Speed 3912.26 samples/sec  Loss 1.2996  LearningRate 0.1031  ProxyLR: 5.1569  Epoch: 7  Global Step: 40070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:33,717-Speed 3909.52 samples/sec  Loss 1.3395  LearningRate 0.1031  ProxyLR: 5.1559  Epoch: 7  Global Step: 40080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:36,336-Speed 3911.56 samples/sec  Loss 1.4431  LearningRate 0.1031  ProxyLR: 5.1549  Epoch: 7  Global Step: 40090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:38,956-Speed 3908.27 samples/sec  Loss 1.3265  LearningRate 0.1031  ProxyLR: 5.1539  Epoch: 7  Global Step: 40100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:41,563-Speed 3929.32 samples/sec  Loss 1.3445  LearningRate 0.1031  ProxyLR: 5.1528  Epoch: 7  Global Step: 40110   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:44,182-Speed 3910.26 samples/sec  Loss 1.4070  LearningRate 0.1030  ProxyLR: 5.1518  Epoch: 7  Global Step: 40120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:46,806-Speed 3903.97 samples/sec  Loss 1.3591  LearningRate 0.1030  ProxyLR: 5.1508  Epoch: 7  Global Step: 40130   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:49,430-Speed 3904.10 samples/sec  Loss 1.3784  LearningRate 0.1030  ProxyLR: 5.1498  Epoch: 7  Global Step: 40140   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:52,052-Speed 3906.27 samples/sec  Loss 1.3189  LearningRate 0.1030  ProxyLR: 5.1488  Epoch: 7  Global Step: 40150   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:54,676-Speed 3903.02 samples/sec  Loss 1.2569  LearningRate 0.1030  ProxyLR: 5.1478  Epoch: 7  Global Step: 40160   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:57,300-Speed 3904.51 samples/sec  Loss 1.4432  LearningRate 0.1029  ProxyLR: 5.1468  Epoch: 7  Global Step: 40170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:27:59,924-Speed 3903.56 samples/sec  Loss 1.3553  LearningRate 0.1029  ProxyLR: 5.1458  Epoch: 7  Global Step: 40180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:02,547-Speed 3904.18 samples/sec  Loss 1.3240  LearningRate 0.1029  ProxyLR: 5.1448  Epoch: 7  Global Step: 40190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:05,172-Speed 3902.90 samples/sec  Loss 1.3586  LearningRate 0.1029  ProxyLR: 5.1438  Epoch: 7  Global Step: 40200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:07,801-Speed 3895.22 samples/sec  Loss 1.3678  LearningRate 0.1029  ProxyLR: 5.1428  Epoch: 7  Global Step: 40210   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:28:10,432-Speed 3893.57 samples/sec  Loss 1.3400  LearningRate 0.1028  ProxyLR: 5.1417  Epoch: 7  Global Step: 40220   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:28:13,047-Speed 3916.19 samples/sec  Loss 1.3214  LearningRate 0.1028  ProxyLR: 5.1407  Epoch: 7  Global Step: 40230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:15,675-Speed 3897.60 samples/sec  Loss 1.2751  LearningRate 0.1028  ProxyLR: 5.1397  Epoch: 7  Global Step: 40240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:18,304-Speed 3895.80 samples/sec  Loss 1.3607  LearningRate 0.1028  ProxyLR: 5.1387  Epoch: 7  Global Step: 40250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:20,933-Speed 3896.65 samples/sec  Loss 1.2859  LearningRate 0.1028  ProxyLR: 5.1377  Epoch: 7  Global Step: 40260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:23,564-Speed 3894.28 samples/sec  Loss 1.3584  LearningRate 0.1027  ProxyLR: 5.1367  Epoch: 7  Global Step: 40270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:26,192-Speed 3897.21 samples/sec  Loss 1.3760  LearningRate 0.1027  ProxyLR: 5.1357  Epoch: 7  Global Step: 40280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:28,823-Speed 3893.32 samples/sec  Loss 1.3165  LearningRate 0.1027  ProxyLR: 5.1347  Epoch: 7  Global Step: 40290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:31,452-Speed 3895.68 samples/sec  Loss 1.3279  LearningRate 0.1027  ProxyLR: 5.1337  Epoch: 7  Global Step: 40300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:34,081-Speed 3896.16 samples/sec  Loss 1.2733  LearningRate 0.1027  ProxyLR: 5.1327  Epoch: 7  Global Step: 40310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:36,710-Speed 3895.18 samples/sec  Loss 1.3676  LearningRate 0.1026  ProxyLR: 5.1317  Epoch: 7  Global Step: 40320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:39,326-Speed 3915.50 samples/sec  Loss 1.2513  LearningRate 0.1026  ProxyLR: 5.1307  Epoch: 7  Global Step: 40330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:41,954-Speed 3897.64 samples/sec  Loss 1.3482  LearningRate 0.1026  ProxyLR: 5.1296  Epoch: 7  Global Step: 40340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:44,580-Speed 3900.43 samples/sec  Loss 1.3141  LearningRate 0.1026  ProxyLR: 5.1286  Epoch: 7  Global Step: 40350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:47,204-Speed 3903.55 samples/sec  Loss 1.2715  LearningRate 0.1026  ProxyLR: 5.1276  Epoch: 7  Global Step: 40360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:49,829-Speed 3901.46 samples/sec  Loss 1.3135  LearningRate 0.1025  ProxyLR: 5.1266  Epoch: 7  Global Step: 40370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:52,455-Speed 3900.41 samples/sec  Loss 1.3861  LearningRate 0.1025  ProxyLR: 5.1256  Epoch: 7  Global Step: 40380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:55,080-Speed 3901.76 samples/sec  Loss 1.2980  LearningRate 0.1025  ProxyLR: 5.1246  Epoch: 7  Global Step: 40390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:28:57,705-Speed 3902.58 samples/sec  Loss 1.3076  LearningRate 0.1025  ProxyLR: 5.1236  Epoch: 7  Global Step: 40400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:00,331-Speed 3900.48 samples/sec  Loss 1.3188  LearningRate 0.1025  ProxyLR: 5.1226  Epoch: 7  Global Step: 40410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:02,956-Speed 3901.29 samples/sec  Loss 1.3212  LearningRate 0.1024  ProxyLR: 5.1216  Epoch: 7  Global Step: 40420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:05,568-Speed 3921.26 samples/sec  Loss 1.2626  LearningRate 0.1024  ProxyLR: 5.1206  Epoch: 7  Global Step: 40430   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:08,193-Speed 3902.84 samples/sec  Loss 1.3472  LearningRate 0.1024  ProxyLR: 5.1196  Epoch: 7  Global Step: 40440   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:10,817-Speed 3902.54 samples/sec  Loss 1.3130  LearningRate 0.1024  ProxyLR: 5.1186  Epoch: 7  Global Step: 40450   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:13,443-Speed 3899.72 samples/sec  Loss 1.2881  LearningRate 0.1024  ProxyLR: 5.1176  Epoch: 7  Global Step: 40460   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:16,067-Speed 3904.78 samples/sec  Loss 1.2510  LearningRate 0.1023  ProxyLR: 5.1166  Epoch: 7  Global Step: 40470   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:18,690-Speed 3903.74 samples/sec  Loss 1.3024  LearningRate 0.1023  ProxyLR: 5.1155  Epoch: 7  Global Step: 40480   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:21,315-Speed 3902.02 samples/sec  Loss 1.3041  LearningRate 0.1023  ProxyLR: 5.1145  Epoch: 7  Global Step: 40490   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:23,940-Speed 3901.60 samples/sec  Loss 1.1660  LearningRate 0.1023  ProxyLR: 5.1135  Epoch: 7  Global Step: 40500   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:26,565-Speed 3902.04 samples/sec  Loss 1.3073  LearningRate 0.1023  ProxyLR: 5.1125  Epoch: 7  Global Step: 40510   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:29,190-Speed 3902.16 samples/sec  Loss 1.3027  LearningRate 0.1022  ProxyLR: 5.1115  Epoch: 7  Global Step: 40520   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:29:31,816-Speed 3900.40 samples/sec  Loss 1.3099  LearningRate 0.1022  ProxyLR: 5.1105  Epoch: 7  Global Step: 40530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:34,441-Speed 3901.57 samples/sec  Loss 1.2298  LearningRate 0.1022  ProxyLR: 5.1095  Epoch: 7  Global Step: 40540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:37,067-Speed 3901.24 samples/sec  Loss 1.2467  LearningRate 0.1022  ProxyLR: 5.1085  Epoch: 7  Global Step: 40550   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:39,692-Speed 3900.83 samples/sec  Loss 1.2607  LearningRate 0.1021  ProxyLR: 5.1075  Epoch: 7  Global Step: 40560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:42,319-Speed 3900.17 samples/sec  Loss 1.3493  LearningRate 0.1021  ProxyLR: 5.1065  Epoch: 7  Global Step: 40570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:44,943-Speed 3901.97 samples/sec  Loss 1.2460  LearningRate 0.1021  ProxyLR: 5.1055  Epoch: 7  Global Step: 40580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:47,566-Speed 3904.93 samples/sec  Loss 1.2533  LearningRate 0.1021  ProxyLR: 5.1045  Epoch: 7  Global Step: 40590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:50,192-Speed 3901.40 samples/sec  Loss 1.2000  LearningRate 0.1021  ProxyLR: 5.1035  Epoch: 7  Global Step: 40600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:52,817-Speed 3902.17 samples/sec  Loss 1.2523  LearningRate 0.1020  ProxyLR: 5.1025  Epoch: 7  Global Step: 40610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:55,441-Speed 3902.66 samples/sec  Loss 1.2229  LearningRate 0.1020  ProxyLR: 5.1015  Epoch: 7  Global Step: 40620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:29:58,055-Speed 3918.30 samples/sec  Loss 1.3588  LearningRate 0.1020  ProxyLR: 5.1005  Epoch: 7  Global Step: 40630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:00,680-Speed 3901.44 samples/sec  Loss 1.3643  LearningRate 0.1020  ProxyLR: 5.0995  Epoch: 7  Global Step: 40640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:03,305-Speed 3902.25 samples/sec  Loss 1.2515  LearningRate 0.1020  ProxyLR: 5.0985  Epoch: 7  Global Step: 40650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:05,930-Speed 3902.21 samples/sec  Loss 1.3420  LearningRate 0.1019  ProxyLR: 5.0974  Epoch: 7  Global Step: 40660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:08,557-Speed 3898.85 samples/sec  Loss 1.2773  LearningRate 0.1019  ProxyLR: 5.0964  Epoch: 7  Global Step: 40670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:11,183-Speed 3900.83 samples/sec  Loss 1.3006  LearningRate 0.1019  ProxyLR: 5.0954  Epoch: 7  Global Step: 40680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:13,808-Speed 3900.55 samples/sec  Loss 1.2305  LearningRate 0.1019  ProxyLR: 5.0944  Epoch: 7  Global Step: 40690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:16,434-Speed 3901.31 samples/sec  Loss 1.2084  LearningRate 0.1019  ProxyLR: 5.0934  Epoch: 7  Global Step: 40700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:19,060-Speed 3900.12 samples/sec  Loss 1.2048  LearningRate 0.1018  ProxyLR: 5.0924  Epoch: 7  Global Step: 40710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:21,685-Speed 3901.86 samples/sec  Loss 1.2822  LearningRate 0.1018  ProxyLR: 5.0914  Epoch: 7  Global Step: 40720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:24,310-Speed 3901.37 samples/sec  Loss 1.2732  LearningRate 0.1018  ProxyLR: 5.0904  Epoch: 7  Global Step: 40730   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:30:26,936-Speed 3900.57 samples/sec  Loss 1.2186  LearningRate 0.1018  ProxyLR: 5.0894  Epoch: 7  Global Step: 40740   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:30:29,548-Speed 3921.19 samples/sec  Loss 1.2594  LearningRate 0.1018  ProxyLR: 5.0884  Epoch: 7  Global Step: 40750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:32,174-Speed 3901.28 samples/sec  Loss 1.2542  LearningRate 0.1017  ProxyLR: 5.0874  Epoch: 7  Global Step: 40760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:34,798-Speed 3903.19 samples/sec  Loss 1.2209  LearningRate 0.1017  ProxyLR: 5.0864  Epoch: 7  Global Step: 40770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:37,425-Speed 3899.07 samples/sec  Loss 1.3454  LearningRate 0.1017  ProxyLR: 5.0854  Epoch: 7  Global Step: 40780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:40,049-Speed 3902.29 samples/sec  Loss 1.2838  LearningRate 0.1017  ProxyLR: 5.0844  Epoch: 7  Global Step: 40790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:42,674-Speed 3902.14 samples/sec  Loss 1.2862  LearningRate 0.1017  ProxyLR: 5.0834  Epoch: 7  Global Step: 40800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:45,301-Speed 3900.20 samples/sec  Loss 1.1683  LearningRate 0.1016  ProxyLR: 5.0824  Epoch: 7  Global Step: 40810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:47,925-Speed 3902.58 samples/sec  Loss 1.3550  LearningRate 0.1016  ProxyLR: 5.0814  Epoch: 7  Global Step: 40820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:50,550-Speed 3902.54 samples/sec  Loss 1.2699  LearningRate 0.1016  ProxyLR: 5.0804  Epoch: 7  Global Step: 40830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:30:53,162-Speed 3921.20 samples/sec  Loss 1.2155  LearningRate 0.1016  ProxyLR: 5.0794  Epoch: 7  Global Step: 40840   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:30:55,786-Speed 3903.30 samples/sec  Loss 1.2872  LearningRate 0.1016  ProxyLR: 5.0784  Epoch: 7  Global Step: 40850   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:30:58,409-Speed 3904.02 samples/sec  Loss 1.2794  LearningRate 0.1015  ProxyLR: 5.0774  Epoch: 7  Global Step: 40860   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:01,034-Speed 3903.20 samples/sec  Loss 1.3121  LearningRate 0.1015  ProxyLR: 5.0764  Epoch: 7  Global Step: 40870   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:03,656-Speed 3904.99 samples/sec  Loss 1.2907  LearningRate 0.1015  ProxyLR: 5.0754  Epoch: 7  Global Step: 40880   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:06,279-Speed 3904.69 samples/sec  Loss 1.2142  LearningRate 0.1015  ProxyLR: 5.0744  Epoch: 7  Global Step: 40890   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:08,903-Speed 3903.85 samples/sec  Loss 1.2685  LearningRate 0.1015  ProxyLR: 5.0734  Epoch: 7  Global Step: 40900   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:11,528-Speed 3902.55 samples/sec  Loss 1.3188  LearningRate 0.1014  ProxyLR: 5.0724  Epoch: 7  Global Step: 40910   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:14,151-Speed 3905.13 samples/sec  Loss 1.2185  LearningRate 0.1014  ProxyLR: 5.0714  Epoch: 7  Global Step: 40920   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:16,775-Speed 3902.17 samples/sec  Loss 1.1930  LearningRate 0.1014  ProxyLR: 5.0704  Epoch: 7  Global Step: 40930   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:31:19,400-Speed 3903.01 samples/sec  Loss 1.2416  LearningRate 0.1014  ProxyLR: 5.0694  Epoch: 7  Global Step: 40940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:22,024-Speed 3902.47 samples/sec  Loss 1.1837  LearningRate 0.1014  ProxyLR: 5.0684  Epoch: 7  Global Step: 40950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:24,647-Speed 3904.26 samples/sec  Loss 1.2193  LearningRate 0.1013  ProxyLR: 5.0674  Epoch: 7  Global Step: 40960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:27,272-Speed 3902.76 samples/sec  Loss 1.2411  LearningRate 0.1013  ProxyLR: 5.0664  Epoch: 7  Global Step: 40970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:29,897-Speed 3901.09 samples/sec  Loss 1.1809  LearningRate 0.1013  ProxyLR: 5.0654  Epoch: 7  Global Step: 40980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:32,521-Speed 3903.14 samples/sec  Loss 1.2159  LearningRate 0.1013  ProxyLR: 5.0644  Epoch: 7  Global Step: 40990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:35,148-Speed 3900.33 samples/sec  Loss 1.2966  LearningRate 0.1013  ProxyLR: 5.0634  Epoch: 7  Global Step: 41000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:37,774-Speed 3899.99 samples/sec  Loss 1.2191  LearningRate 0.1012  ProxyLR: 5.0624  Epoch: 7  Global Step: 41010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:40,401-Speed 3899.05 samples/sec  Loss 1.1797  LearningRate 0.1012  ProxyLR: 5.0613  Epoch: 7  Global Step: 41020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:43,025-Speed 3903.87 samples/sec  Loss 1.2652  LearningRate 0.1012  ProxyLR: 5.0603  Epoch: 7  Global Step: 41030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:45,638-Speed 3919.10 samples/sec  Loss 1.1751  LearningRate 0.1012  ProxyLR: 5.0593  Epoch: 7  Global Step: 41040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:48,265-Speed 3899.35 samples/sec  Loss 1.2523  LearningRate 0.1012  ProxyLR: 5.0583  Epoch: 7  Global Step: 41050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:50,890-Speed 3900.61 samples/sec  Loss 1.1507  LearningRate 0.1011  ProxyLR: 5.0573  Epoch: 7  Global Step: 41060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:53,516-Speed 3901.11 samples/sec  Loss 1.2294  LearningRate 0.1011  ProxyLR: 5.0563  Epoch: 7  Global Step: 41070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:56,141-Speed 3901.39 samples/sec  Loss 1.3035  LearningRate 0.1011  ProxyLR: 5.0553  Epoch: 7  Global Step: 41080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:31:58,766-Speed 3901.78 samples/sec  Loss 1.1890  LearningRate 0.1011  ProxyLR: 5.0543  Epoch: 7  Global Step: 41090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:01,390-Speed 3903.88 samples/sec  Loss 1.3354  LearningRate 0.1011  ProxyLR: 5.0533  Epoch: 7  Global Step: 41100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:04,015-Speed 3901.70 samples/sec  Loss 1.2037  LearningRate 0.1010  ProxyLR: 5.0523  Epoch: 7  Global Step: 41110   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:06,641-Speed 3900.28 samples/sec  Loss 1.2934  LearningRate 0.1010  ProxyLR: 5.0513  Epoch: 7  Global Step: 41120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:09,266-Speed 3902.87 samples/sec  Loss 1.2315  LearningRate 0.1010  ProxyLR: 5.0503  Epoch: 7  Global Step: 41130   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:11,891-Speed 3901.07 samples/sec  Loss 1.2613  LearningRate 0.1010  ProxyLR: 5.0493  Epoch: 7  Global Step: 41140   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:32:14,516-Speed 3901.39 samples/sec  Loss 1.2333  LearningRate 0.1010  ProxyLR: 5.0483  Epoch: 7  Global Step: 41150   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:32:17,128-Speed 3921.75 samples/sec  Loss 1.3234  LearningRate 0.1009  ProxyLR: 5.0473  Epoch: 7  Global Step: 41160   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:19,753-Speed 3901.33 samples/sec  Loss 1.2676  LearningRate 0.1009  ProxyLR: 5.0463  Epoch: 7  Global Step: 41170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:22,382-Speed 3896.67 samples/sec  Loss 1.2376  LearningRate 0.1009  ProxyLR: 5.0453  Epoch: 7  Global Step: 41180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:25,008-Speed 3901.38 samples/sec  Loss 1.1797  LearningRate 0.1009  ProxyLR: 5.0443  Epoch: 7  Global Step: 41190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:27,634-Speed 3900.26 samples/sec  Loss 1.2294  LearningRate 0.1009  ProxyLR: 5.0433  Epoch: 7  Global Step: 41200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:30,258-Speed 3902.50 samples/sec  Loss 1.2347  LearningRate 0.1008  ProxyLR: 5.0423  Epoch: 7  Global Step: 41210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:32,884-Speed 3900.20 samples/sec  Loss 1.2568  LearningRate 0.1008  ProxyLR: 5.0413  Epoch: 7  Global Step: 41220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:35,509-Speed 3902.21 samples/sec  Loss 1.2101  LearningRate 0.1008  ProxyLR: 5.0404  Epoch: 7  Global Step: 41230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:32:38,120-Speed 3922.49 samples/sec  Loss 1.3304  LearningRate 0.1008  ProxyLR: 5.0394  Epoch: 7  Global Step: 41240   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:40,744-Speed 3904.06 samples/sec  Loss 1.2643  LearningRate 0.1008  ProxyLR: 5.0384  Epoch: 7  Global Step: 41250   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:43,368-Speed 3903.27 samples/sec  Loss 1.2806  LearningRate 0.1007  ProxyLR: 5.0374  Epoch: 7  Global Step: 41260   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:45,991-Speed 3905.00 samples/sec  Loss 1.2765  LearningRate 0.1007  ProxyLR: 5.0364  Epoch: 7  Global Step: 41270   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:48,616-Speed 3901.48 samples/sec  Loss 1.2470  LearningRate 0.1007  ProxyLR: 5.0354  Epoch: 7  Global Step: 41280   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:51,240-Speed 3902.61 samples/sec  Loss 1.2399  LearningRate 0.1007  ProxyLR: 5.0344  Epoch: 7  Global Step: 41290   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:53,865-Speed 3902.29 samples/sec  Loss 1.3111  LearningRate 0.1007  ProxyLR: 5.0334  Epoch: 7  Global Step: 41300   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:56,490-Speed 3901.54 samples/sec  Loss 1.2174  LearningRate 0.1006  ProxyLR: 5.0324  Epoch: 7  Global Step: 41310   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:32:59,117-Speed 3899.64 samples/sec  Loss 1.3234  LearningRate 0.1006  ProxyLR: 5.0314  Epoch: 7  Global Step: 41320   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:33:01,741-Speed 3902.58 samples/sec  Loss 1.2563  LearningRate 0.1006  ProxyLR: 5.0304  Epoch: 7  Global Step: 41330   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:33:04,366-Speed 3902.79 samples/sec  Loss 1.2516  LearningRate 0.1006  ProxyLR: 5.0294  Epoch: 7  Global Step: 41340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:06,994-Speed 3897.10 samples/sec  Loss 1.2440  LearningRate 0.1006  ProxyLR: 5.0284  Epoch: 7  Global Step: 41350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:09,617-Speed 3903.99 samples/sec  Loss 1.1969  LearningRate 0.1005  ProxyLR: 5.0274  Epoch: 7  Global Step: 41360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:12,243-Speed 3901.40 samples/sec  Loss 1.2499  LearningRate 0.1005  ProxyLR: 5.0264  Epoch: 7  Global Step: 41370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:14,868-Speed 3902.25 samples/sec  Loss 1.1884  LearningRate 0.1005  ProxyLR: 5.0254  Epoch: 7  Global Step: 41380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:17,492-Speed 3902.92 samples/sec  Loss 1.2386  LearningRate 0.1005  ProxyLR: 5.0244  Epoch: 7  Global Step: 41390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:20,117-Speed 3901.36 samples/sec  Loss 1.2293  LearningRate 0.1005  ProxyLR: 5.0234  Epoch: 7  Global Step: 41400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:22,743-Speed 3900.86 samples/sec  Loss 1.2242  LearningRate 0.1004  ProxyLR: 5.0224  Epoch: 7  Global Step: 41410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:25,368-Speed 3902.23 samples/sec  Loss 1.2036  LearningRate 0.1004  ProxyLR: 5.0214  Epoch: 7  Global Step: 41420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:27,994-Speed 3899.17 samples/sec  Loss 1.1730  LearningRate 0.1004  ProxyLR: 5.0204  Epoch: 7  Global Step: 41430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:30,619-Speed 3902.79 samples/sec  Loss 1.1798  LearningRate 0.1004  ProxyLR: 5.0194  Epoch: 7  Global Step: 41440   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:33:33,230-Speed 3921.95 samples/sec  Loss 1.1564  LearningRate 0.1004  ProxyLR: 5.0184  Epoch: 7  Global Step: 41450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:35,857-Speed 3898.96 samples/sec  Loss 1.2692  LearningRate 0.1003  ProxyLR: 5.0174  Epoch: 7  Global Step: 41460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:38,484-Speed 3899.37 samples/sec  Loss 1.1896  LearningRate 0.1003  ProxyLR: 5.0164  Epoch: 7  Global Step: 41470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:41,111-Speed 3899.49 samples/sec  Loss 1.2119  LearningRate 0.1003  ProxyLR: 5.0154  Epoch: 7  Global Step: 41480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:43,739-Speed 3897.07 samples/sec  Loss 1.2879  LearningRate 0.1003  ProxyLR: 5.0144  Epoch: 7  Global Step: 41490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:46,368-Speed 3895.46 samples/sec  Loss 1.2360  LearningRate 0.1003  ProxyLR: 5.0134  Epoch: 7  Global Step: 41500   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:48,999-Speed 3893.80 samples/sec  Loss 1.2621  LearningRate 0.1002  ProxyLR: 5.0124  Epoch: 7  Global Step: 41510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:51,629-Speed 3893.83 samples/sec  Loss 1.1946  LearningRate 0.1002  ProxyLR: 5.0114  Epoch: 7  Global Step: 41520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:54,259-Speed 3893.98 samples/sec  Loss 1.2226  LearningRate 0.1002  ProxyLR: 5.0104  Epoch: 7  Global Step: 41530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:56,890-Speed 3893.71 samples/sec  Loss 1.3094  LearningRate 0.1002  ProxyLR: 5.0094  Epoch: 7  Global Step: 41540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:33:59,521-Speed 3891.88 samples/sec  Loss 1.2752  LearningRate 0.1002  ProxyLR: 5.0084  Epoch: 7  Global Step: 41550   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:34:02,140-Speed 3911.23 samples/sec  Loss 1.2572  LearningRate 0.1001  ProxyLR: 5.0074  Epoch: 7  Global Step: 41560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:04,770-Speed 3894.65 samples/sec  Loss 1.2480  LearningRate 0.1001  ProxyLR: 5.0064  Epoch: 7  Global Step: 41570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:07,399-Speed 3896.41 samples/sec  Loss 1.3100  LearningRate 0.1001  ProxyLR: 5.0055  Epoch: 7  Global Step: 41580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:10,029-Speed 3893.93 samples/sec  Loss 1.3153  LearningRate 0.1001  ProxyLR: 5.0045  Epoch: 7  Global Step: 41590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:12,659-Speed 3894.78 samples/sec  Loss 1.3005  LearningRate 0.1001  ProxyLR: 5.0035  Epoch: 7  Global Step: 41600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:15,287-Speed 3897.50 samples/sec  Loss 1.2547  LearningRate 0.1000  ProxyLR: 5.0025  Epoch: 7  Global Step: 41610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:17,900-Speed 3919.87 samples/sec  Loss 1.2046  LearningRate 0.1000  ProxyLR: 5.0015  Epoch: 7  Global Step: 41620   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:20,524-Speed 3902.65 samples/sec  Loss 1.1822  LearningRate 0.1000  ProxyLR: 5.0005  Epoch: 7  Global Step: 41630   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:23,147-Speed 3905.38 samples/sec  Loss 1.2027  LearningRate 0.1000  ProxyLR: 4.9995  Epoch: 7  Global Step: 41640   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:25,771-Speed 3902.35 samples/sec  Loss 1.2593  LearningRate 0.1000  ProxyLR: 4.9985  Epoch: 7  Global Step: 41650   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:28,395-Speed 3904.13 samples/sec  Loss 1.2446  LearningRate 0.0999  ProxyLR: 4.9975  Epoch: 7  Global Step: 41660   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:31,019-Speed 3903.90 samples/sec  Loss 1.2692  LearningRate 0.0999  ProxyLR: 4.9965  Epoch: 7  Global Step: 41670   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:33,643-Speed 3902.98 samples/sec  Loss 1.2260  LearningRate 0.0999  ProxyLR: 4.9955  Epoch: 7  Global Step: 41680   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:36,266-Speed 3904.50 samples/sec  Loss 1.2190  LearningRate 0.0999  ProxyLR: 4.9945  Epoch: 7  Global Step: 41690   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:38,890-Speed 3904.18 samples/sec  Loss 1.2498  LearningRate 0.0999  ProxyLR: 4.9935  Epoch: 7  Global Step: 41700   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:41,513-Speed 3904.05 samples/sec  Loss 1.1917  LearningRate 0.0999  ProxyLR: 4.9925  Epoch: 7  Global Step: 41710   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:34:44,138-Speed 3902.49 samples/sec  Loss 1.2812  LearningRate 0.0998  ProxyLR: 4.9915  Epoch: 7  Global Step: 41720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:46,761-Speed 3904.31 samples/sec  Loss 1.2843  LearningRate 0.0998  ProxyLR: 4.9905  Epoch: 7  Global Step: 41730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:49,385-Speed 3903.51 samples/sec  Loss 1.2623  LearningRate 0.0998  ProxyLR: 4.9895  Epoch: 7  Global Step: 41740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:52,011-Speed 3900.55 samples/sec  Loss 1.2255  LearningRate 0.0998  ProxyLR: 4.9885  Epoch: 7  Global Step: 41750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:54,637-Speed 3901.15 samples/sec  Loss 1.1913  LearningRate 0.0998  ProxyLR: 4.9875  Epoch: 7  Global Step: 41760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:57,260-Speed 3903.82 samples/sec  Loss 1.1871  LearningRate 0.0997  ProxyLR: 4.9866  Epoch: 7  Global Step: 41770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:34:59,884-Speed 3903.61 samples/sec  Loss 1.2323  LearningRate 0.0997  ProxyLR: 4.9856  Epoch: 7  Global Step: 41780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:02,508-Speed 3903.75 samples/sec  Loss 1.2689  LearningRate 0.0997  ProxyLR: 4.9846  Epoch: 7  Global Step: 41790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:05,130-Speed 3905.57 samples/sec  Loss 1.2691  LearningRate 0.0997  ProxyLR: 4.9836  Epoch: 7  Global Step: 41800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:07,753-Speed 3905.53 samples/sec  Loss 1.3130  LearningRate 0.0997  ProxyLR: 4.9826  Epoch: 7  Global Step: 41810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:10,364-Speed 3922.08 samples/sec  Loss 1.1999  LearningRate 0.0996  ProxyLR: 4.9816  Epoch: 7  Global Step: 41820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:12,989-Speed 3902.79 samples/sec  Loss 1.2156  LearningRate 0.0996  ProxyLR: 4.9806  Epoch: 7  Global Step: 41830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:15,612-Speed 3904.60 samples/sec  Loss 1.2703  LearningRate 0.0996  ProxyLR: 4.9796  Epoch: 7  Global Step: 41840   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:18,237-Speed 3901.30 samples/sec  Loss 1.2675  LearningRate 0.0996  ProxyLR: 4.9786  Epoch: 7  Global Step: 41850   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:20,863-Speed 3900.23 samples/sec  Loss 1.2339  LearningRate 0.0996  ProxyLR: 4.9776  Epoch: 7  Global Step: 41860   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:23,487-Speed 3904.33 samples/sec  Loss 1.1465  LearningRate 0.0995  ProxyLR: 4.9766  Epoch: 7  Global Step: 41870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:26,111-Speed 3903.40 samples/sec  Loss 1.3099  LearningRate 0.0995  ProxyLR: 4.9756  Epoch: 7  Global Step: 41880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:28,734-Speed 3904.12 samples/sec  Loss 1.3284  LearningRate 0.0995  ProxyLR: 4.9746  Epoch: 7  Global Step: 41890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:31,359-Speed 3901.84 samples/sec  Loss 1.2504  LearningRate 0.0995  ProxyLR: 4.9736  Epoch: 7  Global Step: 41900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:33,984-Speed 3902.01 samples/sec  Loss 1.2130  LearningRate 0.0995  ProxyLR: 4.9727  Epoch: 7  Global Step: 41910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:36,594-Speed 3923.80 samples/sec  Loss 1.2503  LearningRate 0.0994  ProxyLR: 4.9717  Epoch: 7  Global Step: 41920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:39,218-Speed 3903.68 samples/sec  Loss 1.2670  LearningRate 0.0994  ProxyLR: 4.9707  Epoch: 7  Global Step: 41930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:41,847-Speed 3895.82 samples/sec  Loss 1.2667  LearningRate 0.0994  ProxyLR: 4.9697  Epoch: 7  Global Step: 41940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:44,476-Speed 3895.64 samples/sec  Loss 1.1640  LearningRate 0.0994  ProxyLR: 4.9687  Epoch: 7  Global Step: 41950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:47,102-Speed 3900.77 samples/sec  Loss 1.2200  LearningRate 0.0994  ProxyLR: 4.9677  Epoch: 7  Global Step: 41960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:49,729-Speed 3899.06 samples/sec  Loss 1.2231  LearningRate 0.0993  ProxyLR: 4.9667  Epoch: 7  Global Step: 41970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:52,357-Speed 3897.75 samples/sec  Loss 1.2324  LearningRate 0.0993  ProxyLR: 4.9657  Epoch: 7  Global Step: 41980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:54,983-Speed 3900.28 samples/sec  Loss 1.2581  LearningRate 0.0993  ProxyLR: 4.9647  Epoch: 7  Global Step: 41990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:35:57,612-Speed 3895.85 samples/sec  Loss 1.2545  LearningRate 0.0993  ProxyLR: 4.9637  Epoch: 7  Global Step: 42000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:00,242-Speed 3894.63 samples/sec  Loss 1.2536  LearningRate 0.0993  ProxyLR: 4.9627  Epoch: 7  Global Step: 42010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:02,857-Speed 3917.02 samples/sec  Loss 1.1934  LearningRate 0.0992  ProxyLR: 4.9617  Epoch: 7  Global Step: 42020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:05,485-Speed 3896.57 samples/sec  Loss 1.2309  LearningRate 0.0992  ProxyLR: 4.9608  Epoch: 7  Global Step: 42030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:08,115-Speed 3895.33 samples/sec  Loss 1.2329  LearningRate 0.0992  ProxyLR: 4.9598  Epoch: 7  Global Step: 42040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:10,744-Speed 3896.04 samples/sec  Loss 1.2750  LearningRate 0.0992  ProxyLR: 4.9588  Epoch: 7  Global Step: 42050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:13,371-Speed 3898.24 samples/sec  Loss 1.2717  LearningRate 0.0992  ProxyLR: 4.9578  Epoch: 7  Global Step: 42060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:16,001-Speed 3895.51 samples/sec  Loss 1.2429  LearningRate 0.0991  ProxyLR: 4.9568  Epoch: 7  Global Step: 42070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:18,630-Speed 3894.88 samples/sec  Loss 1.2827  LearningRate 0.0991  ProxyLR: 4.9558  Epoch: 7  Global Step: 42080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:21,248-Speed 3912.49 samples/sec  Loss 1.2803  LearningRate 0.0991  ProxyLR: 4.9548  Epoch: 7  Global Step: 42090   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:23,877-Speed 3897.09 samples/sec  Loss 1.2642  LearningRate 0.0991  ProxyLR: 4.9538  Epoch: 7  Global Step: 42100   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:26,504-Speed 3898.18 samples/sec  Loss 1.2201  LearningRate 0.0991  ProxyLR: 4.9528  Epoch: 7  Global Step: 42110   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:29,132-Speed 3898.22 samples/sec  Loss 1.2260  LearningRate 0.0990  ProxyLR: 4.9518  Epoch: 7  Global Step: 42120   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:31,759-Speed 3898.42 samples/sec  Loss 1.3335  LearningRate 0.0990  ProxyLR: 4.9509  Epoch: 7  Global Step: 42130   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:34,387-Speed 3898.02 samples/sec  Loss 1.2895  LearningRate 0.0990  ProxyLR: 4.9499  Epoch: 7  Global Step: 42140   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:37,013-Speed 3899.63 samples/sec  Loss 1.2855  LearningRate 0.0990  ProxyLR: 4.9489  Epoch: 7  Global Step: 42150   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:39,641-Speed 3898.34 samples/sec  Loss 1.2487  LearningRate 0.0990  ProxyLR: 4.9479  Epoch: 7  Global Step: 42160   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:42,269-Speed 3897.08 samples/sec  Loss 1.3539  LearningRate 0.0989  ProxyLR: 4.9469  Epoch: 7  Global Step: 42170   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:44,896-Speed 3899.65 samples/sec  Loss 1.2407  LearningRate 0.0989  ProxyLR: 4.9459  Epoch: 7  Global Step: 42180   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:36:47,522-Speed 3900.42 samples/sec  Loss 1.2622  LearningRate 0.0989  ProxyLR: 4.9449  Epoch: 7  Global Step: 42190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:50,149-Speed 3898.79 samples/sec  Loss 1.2330  LearningRate 0.0989  ProxyLR: 4.9439  Epoch: 7  Global Step: 42200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:52,777-Speed 3897.88 samples/sec  Loss 1.2307  LearningRate 0.0989  ProxyLR: 4.9429  Epoch: 7  Global Step: 42210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:55,403-Speed 3899.47 samples/sec  Loss 1.2925  LearningRate 0.0988  ProxyLR: 4.9419  Epoch: 7  Global Step: 42220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:36:58,030-Speed 3898.98 samples/sec  Loss 1.2352  LearningRate 0.0988  ProxyLR: 4.9410  Epoch: 7  Global Step: 42230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:00,658-Speed 3898.57 samples/sec  Loss 1.2155  LearningRate 0.0988  ProxyLR: 4.9400  Epoch: 7  Global Step: 42240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:03,286-Speed 3897.22 samples/sec  Loss 1.3183  LearningRate 0.0988  ProxyLR: 4.9390  Epoch: 7  Global Step: 42250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:05,913-Speed 3898.06 samples/sec  Loss 1.3146  LearningRate 0.0988  ProxyLR: 4.9380  Epoch: 7  Global Step: 42260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:08,542-Speed 3895.79 samples/sec  Loss 1.2520  LearningRate 0.0987  ProxyLR: 4.9370  Epoch: 7  Global Step: 42270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:11,170-Speed 3898.43 samples/sec  Loss 1.1665  LearningRate 0.0987  ProxyLR: 4.9360  Epoch: 7  Global Step: 42280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:13,797-Speed 3898.61 samples/sec  Loss 1.2790  LearningRate 0.0987  ProxyLR: 4.9350  Epoch: 7  Global Step: 42290   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:37:16,408-Speed 3921.93 samples/sec  Loss 1.2220  LearningRate 0.0987  ProxyLR: 4.9340  Epoch: 7  Global Step: 42300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:19,037-Speed 3896.81 samples/sec  Loss 1.3347  LearningRate 0.0987  ProxyLR: 4.9330  Epoch: 7  Global Step: 42310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:21,667-Speed 3894.07 samples/sec  Loss 1.2753  LearningRate 0.0986  ProxyLR: 4.9321  Epoch: 7  Global Step: 42320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:24,297-Speed 3894.23 samples/sec  Loss 1.2318  LearningRate 0.0986  ProxyLR: 4.9311  Epoch: 7  Global Step: 42330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:26,926-Speed 3896.57 samples/sec  Loss 1.3400  LearningRate 0.0986  ProxyLR: 4.9301  Epoch: 7  Global Step: 42340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:29,557-Speed 3893.09 samples/sec  Loss 1.3863  LearningRate 0.0986  ProxyLR: 4.9291  Epoch: 7  Global Step: 42350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:32,188-Speed 3893.54 samples/sec  Loss 1.2695  LearningRate 0.0986  ProxyLR: 4.9281  Epoch: 7  Global Step: 42360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:34,821-Speed 3890.05 samples/sec  Loss 1.2315  LearningRate 0.0985  ProxyLR: 4.9271  Epoch: 7  Global Step: 42370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:37,457-Speed 3885.17 samples/sec  Loss 1.2978  LearningRate 0.0985  ProxyLR: 4.9261  Epoch: 7  Global Step: 42380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:40,088-Speed 3892.71 samples/sec  Loss 1.3211  LearningRate 0.0985  ProxyLR: 4.9251  Epoch: 7  Global Step: 42390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:42,718-Speed 3894.50 samples/sec  Loss 1.2746  LearningRate 0.0985  ProxyLR: 4.9242  Epoch: 7  Global Step: 42400   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:37:45,336-Speed 3912.32 samples/sec  Loss 1.3594  LearningRate 0.0985  ProxyLR: 4.9232  Epoch: 7  Global Step: 42410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:47,966-Speed 3894.73 samples/sec  Loss 1.2956  LearningRate 0.0984  ProxyLR: 4.9222  Epoch: 7  Global Step: 42420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:50,596-Speed 3894.68 samples/sec  Loss 1.2207  LearningRate 0.0984  ProxyLR: 4.9212  Epoch: 7  Global Step: 42430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:53,226-Speed 3895.18 samples/sec  Loss 1.2538  LearningRate 0.0984  ProxyLR: 4.9202  Epoch: 7  Global Step: 42440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:55,856-Speed 3894.92 samples/sec  Loss 1.2956  LearningRate 0.0984  ProxyLR: 4.9192  Epoch: 7  Global Step: 42450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:37:58,473-Speed 3913.81 samples/sec  Loss 1.2097  LearningRate 0.0984  ProxyLR: 4.9182  Epoch: 7  Global Step: 42460   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:01,100-Speed 3898.59 samples/sec  Loss 1.2433  LearningRate 0.0983  ProxyLR: 4.9173  Epoch: 7  Global Step: 42470   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:03,727-Speed 3899.35 samples/sec  Loss 1.2377  LearningRate 0.0983  ProxyLR: 4.9163  Epoch: 7  Global Step: 42480   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:06,353-Speed 3900.14 samples/sec  Loss 1.3537  LearningRate 0.0983  ProxyLR: 4.9153  Epoch: 7  Global Step: 42490   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:08,980-Speed 3898.17 samples/sec  Loss 1.2404  LearningRate 0.0983  ProxyLR: 4.9143  Epoch: 7  Global Step: 42500   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:11,607-Speed 3899.85 samples/sec  Loss 1.2419  LearningRate 0.0983  ProxyLR: 4.9133  Epoch: 7  Global Step: 42510   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:14,233-Speed 3900.66 samples/sec  Loss 1.2784  LearningRate 0.0982  ProxyLR: 4.9123  Epoch: 7  Global Step: 42520   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:16,860-Speed 3898.62 samples/sec  Loss 1.2232  LearningRate 0.0982  ProxyLR: 4.9113  Epoch: 7  Global Step: 42530   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:19,487-Speed 3899.18 samples/sec  Loss 1.3348  LearningRate 0.0982  ProxyLR: 4.9103  Epoch: 7  Global Step: 42540   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:22,114-Speed 3899.30 samples/sec  Loss 1.2349  LearningRate 0.0982  ProxyLR: 4.9094  Epoch: 7  Global Step: 42550   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:38:24,741-Speed 3898.07 samples/sec  Loss 1.2776  LearningRate 0.0982  ProxyLR: 4.9084  Epoch: 7  Global Step: 42560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:27,367-Speed 3901.54 samples/sec  Loss 1.2120  LearningRate 0.0981  ProxyLR: 4.9074  Epoch: 7  Global Step: 42570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:29,992-Speed 3900.74 samples/sec  Loss 1.2402  LearningRate 0.0981  ProxyLR: 4.9064  Epoch: 7  Global Step: 42580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:32,619-Speed 3899.49 samples/sec  Loss 1.2057  LearningRate 0.0981  ProxyLR: 4.9054  Epoch: 7  Global Step: 42590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:35,246-Speed 3898.65 samples/sec  Loss 1.2292  LearningRate 0.0981  ProxyLR: 4.9044  Epoch: 7  Global Step: 42600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:37,873-Speed 3899.48 samples/sec  Loss 1.2862  LearningRate 0.0981  ProxyLR: 4.9034  Epoch: 7  Global Step: 42610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:40,501-Speed 3897.17 samples/sec  Loss 1.3316  LearningRate 0.0980  ProxyLR: 4.9025  Epoch: 7  Global Step: 42620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:43,129-Speed 3898.28 samples/sec  Loss 1.2420  LearningRate 0.0980  ProxyLR: 4.9015  Epoch: 7  Global Step: 42630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:45,757-Speed 3896.89 samples/sec  Loss 1.2274  LearningRate 0.0980  ProxyLR: 4.9005  Epoch: 7  Global Step: 42640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:48,386-Speed 3896.11 samples/sec  Loss 1.2488  LearningRate 0.0980  ProxyLR: 4.8995  Epoch: 7  Global Step: 42650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:51,013-Speed 3900.24 samples/sec  Loss 1.2769  LearningRate 0.0980  ProxyLR: 4.8985  Epoch: 7  Global Step: 42660   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:38:53,625-Speed 3920.31 samples/sec  Loss 1.2916  LearningRate 0.0980  ProxyLR: 4.8975  Epoch: 7  Global Step: 42670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:56,253-Speed 3897.98 samples/sec  Loss 1.3695  LearningRate 0.0979  ProxyLR: 4.8966  Epoch: 7  Global Step: 42680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:38:58,882-Speed 3896.60 samples/sec  Loss 1.2497  LearningRate 0.0979  ProxyLR: 4.8956  Epoch: 7  Global Step: 42690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:01,509-Speed 3898.67 samples/sec  Loss 1.2652  LearningRate 0.0979  ProxyLR: 4.8946  Epoch: 7  Global Step: 42700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:04,135-Speed 3899.55 samples/sec  Loss 1.2470  LearningRate 0.0979  ProxyLR: 4.8936  Epoch: 7  Global Step: 42710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:06,765-Speed 3895.38 samples/sec  Loss 1.2728  LearningRate 0.0979  ProxyLR: 4.8926  Epoch: 7  Global Step: 42720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:09,393-Speed 3897.69 samples/sec  Loss 1.2402  LearningRate 0.0978  ProxyLR: 4.8916  Epoch: 7  Global Step: 42730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:12,020-Speed 3898.56 samples/sec  Loss 1.2284  LearningRate 0.0978  ProxyLR: 4.8906  Epoch: 7  Global Step: 42740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:14,646-Speed 3899.95 samples/sec  Loss 1.2869  LearningRate 0.0978  ProxyLR: 4.8897  Epoch: 7  Global Step: 42750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:17,274-Speed 3898.31 samples/sec  Loss 1.2479  LearningRate 0.0978  ProxyLR: 4.8887  Epoch: 7  Global Step: 42760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:19,890-Speed 3915.77 samples/sec  Loss 1.2077  LearningRate 0.0978  ProxyLR: 4.8877  Epoch: 7  Global Step: 42770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:22,515-Speed 3901.00 samples/sec  Loss 1.2784  LearningRate 0.0977  ProxyLR: 4.8867  Epoch: 7  Global Step: 42780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:25,142-Speed 3899.23 samples/sec  Loss 1.2191  LearningRate 0.0977  ProxyLR: 4.8857  Epoch: 7  Global Step: 42790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:27,770-Speed 3897.34 samples/sec  Loss 1.2303  LearningRate 0.0977  ProxyLR: 4.8847  Epoch: 7  Global Step: 42800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:30,399-Speed 3896.96 samples/sec  Loss 1.2973  LearningRate 0.0977  ProxyLR: 4.8838  Epoch: 7  Global Step: 42810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:33,026-Speed 3898.18 samples/sec  Loss 1.3092  LearningRate 0.0977  ProxyLR: 4.8828  Epoch: 7  Global Step: 42820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:35,655-Speed 3897.26 samples/sec  Loss 1.2802  LearningRate 0.0976  ProxyLR: 4.8818  Epoch: 7  Global Step: 42830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:38,284-Speed 3895.37 samples/sec  Loss 1.3193  LearningRate 0.0976  ProxyLR: 4.8808  Epoch: 7  Global Step: 42840   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:40,911-Speed 3898.95 samples/sec  Loss 1.2452  LearningRate 0.0976  ProxyLR: 4.8798  Epoch: 7  Global Step: 42850   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:43,536-Speed 3902.01 samples/sec  Loss 1.3238  LearningRate 0.0976  ProxyLR: 4.8788  Epoch: 7  Global Step: 42860   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:46,150-Speed 3918.04 samples/sec  Loss 1.2734  LearningRate 0.0976  ProxyLR: 4.8779  Epoch: 7  Global Step: 42870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:48,780-Speed 3894.50 samples/sec  Loss 1.2478  LearningRate 0.0975  ProxyLR: 4.8769  Epoch: 7  Global Step: 42880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:51,409-Speed 3896.45 samples/sec  Loss 1.2694  LearningRate 0.0975  ProxyLR: 4.8759  Epoch: 7  Global Step: 42890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:54,038-Speed 3896.67 samples/sec  Loss 1.3212  LearningRate 0.0975  ProxyLR: 4.8749  Epoch: 7  Global Step: 42900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:56,666-Speed 3897.49 samples/sec  Loss 1.3345  LearningRate 0.0975  ProxyLR: 4.8739  Epoch: 7  Global Step: 42910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:39:59,293-Speed 3897.73 samples/sec  Loss 1.2276  LearningRate 0.0975  ProxyLR: 4.8730  Epoch: 7  Global Step: 42920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:01,924-Speed 3894.20 samples/sec  Loss 1.2801  LearningRate 0.0974  ProxyLR: 4.8720  Epoch: 7  Global Step: 42930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:04,552-Speed 3897.38 samples/sec  Loss 1.1977  LearningRate 0.0974  ProxyLR: 4.8710  Epoch: 7  Global Step: 42940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:07,177-Speed 3901.22 samples/sec  Loss 1.2501  LearningRate 0.0974  ProxyLR: 4.8700  Epoch: 7  Global Step: 42950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:09,805-Speed 3897.74 samples/sec  Loss 1.2990  LearningRate 0.0974  ProxyLR: 4.8690  Epoch: 7  Global Step: 42960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:12,432-Speed 3899.10 samples/sec  Loss 1.2000  LearningRate 0.0974  ProxyLR: 4.8680  Epoch: 7  Global Step: 42970   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:40:15,047-Speed 3917.70 samples/sec  Loss 1.1973  LearningRate 0.0973  ProxyLR: 4.8671  Epoch: 7  Global Step: 42980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:17,677-Speed 3894.26 samples/sec  Loss 1.2304  LearningRate 0.0973  ProxyLR: 4.8661  Epoch: 7  Global Step: 42990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:20,304-Speed 3899.36 samples/sec  Loss 1.3142  LearningRate 0.0973  ProxyLR: 4.8651  Epoch: 7  Global Step: 43000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:22,933-Speed 3895.41 samples/sec  Loss 1.2434  LearningRate 0.0973  ProxyLR: 4.8641  Epoch: 7  Global Step: 43010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:25,561-Speed 3897.66 samples/sec  Loss 1.2264  LearningRate 0.0973  ProxyLR: 4.8631  Epoch: 7  Global Step: 43020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:28,187-Speed 3900.25 samples/sec  Loss 1.2617  LearningRate 0.0972  ProxyLR: 4.8622  Epoch: 7  Global Step: 43030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:30,815-Speed 3897.62 samples/sec  Loss 1.2586  LearningRate 0.0972  ProxyLR: 4.8612  Epoch: 7  Global Step: 43040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:33,444-Speed 3896.77 samples/sec  Loss 1.1918  LearningRate 0.0972  ProxyLR: 4.8602  Epoch: 7  Global Step: 43050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:36,071-Speed 3898.56 samples/sec  Loss 1.3133  LearningRate 0.0972  ProxyLR: 4.8592  Epoch: 7  Global Step: 43060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:38,697-Speed 3900.04 samples/sec  Loss 1.3112  LearningRate 0.0972  ProxyLR: 4.8582  Epoch: 7  Global Step: 43070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:41,326-Speed 3896.05 samples/sec  Loss 1.4127  LearningRate 0.0971  ProxyLR: 4.8573  Epoch: 7  Global Step: 43080   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:40:43,941-Speed 3917.64 samples/sec  Loss 1.2294  LearningRate 0.0971  ProxyLR: 4.8563  Epoch: 7  Global Step: 43090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:46,568-Speed 3899.62 samples/sec  Loss 1.2541  LearningRate 0.0971  ProxyLR: 4.8553  Epoch: 7  Global Step: 43100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:49,196-Speed 3896.99 samples/sec  Loss 1.2120  LearningRate 0.0971  ProxyLR: 4.8543  Epoch: 7  Global Step: 43110   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:51,824-Speed 3898.08 samples/sec  Loss 1.3010  LearningRate 0.0971  ProxyLR: 4.8533  Epoch: 7  Global Step: 43120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:54,454-Speed 3894.08 samples/sec  Loss 1.2405  LearningRate 0.0970  ProxyLR: 4.8523  Epoch: 7  Global Step: 43130   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:57,079-Speed 3901.29 samples/sec  Loss 1.3078  LearningRate 0.0970  ProxyLR: 4.8514  Epoch: 7  Global Step: 43140   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:40:59,707-Speed 3897.31 samples/sec  Loss 1.1736  LearningRate 0.0970  ProxyLR: 4.8504  Epoch: 7  Global Step: 43150   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:02,335-Speed 3898.04 samples/sec  Loss 1.3867  LearningRate 0.0970  ProxyLR: 4.8494  Epoch: 7  Global Step: 43160   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:04,960-Speed 3901.48 samples/sec  Loss 1.2677  LearningRate 0.0970  ProxyLR: 4.8484  Epoch: 7  Global Step: 43170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:07,586-Speed 3900.64 samples/sec  Loss 1.3081  LearningRate 0.0969  ProxyLR: 4.8475  Epoch: 7  Global Step: 43180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:10,213-Speed 3899.36 samples/sec  Loss 1.3100  LearningRate 0.0969  ProxyLR: 4.8465  Epoch: 7  Global Step: 43190   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:41:12,843-Speed 3895.14 samples/sec  Loss 1.2852  LearningRate 0.0969  ProxyLR: 4.8455  Epoch: 7  Global Step: 43200   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:41:15,458-Speed 3916.48 samples/sec  Loss 1.3089  LearningRate 0.0969  ProxyLR: 4.8445  Epoch: 7  Global Step: 43210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:18,085-Speed 3899.16 samples/sec  Loss 1.2544  LearningRate 0.0969  ProxyLR: 4.8435  Epoch: 7  Global Step: 43220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:20,712-Speed 3898.98 samples/sec  Loss 1.4275  LearningRate 0.0969  ProxyLR: 4.8426  Epoch: 7  Global Step: 43230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:23,339-Speed 3898.94 samples/sec  Loss 1.2884  LearningRate 0.0968  ProxyLR: 4.8416  Epoch: 7  Global Step: 43240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:25,966-Speed 3899.46 samples/sec  Loss 1.2716  LearningRate 0.0968  ProxyLR: 4.8406  Epoch: 7  Global Step: 43250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:28,593-Speed 3898.31 samples/sec  Loss 1.2654  LearningRate 0.0968  ProxyLR: 4.8396  Epoch: 7  Global Step: 43260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:31,221-Speed 3898.40 samples/sec  Loss 1.3303  LearningRate 0.0968  ProxyLR: 4.8386  Epoch: 7  Global Step: 43270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:33,849-Speed 3896.59 samples/sec  Loss 1.3415  LearningRate 0.0968  ProxyLR: 4.8377  Epoch: 7  Global Step: 43280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:36,477-Speed 3898.05 samples/sec  Loss 1.3536  LearningRate 0.0967  ProxyLR: 4.8367  Epoch: 7  Global Step: 43290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:39,104-Speed 3898.33 samples/sec  Loss 1.3040  LearningRate 0.0967  ProxyLR: 4.8357  Epoch: 7  Global Step: 43300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:41,720-Speed 3915.55 samples/sec  Loss 1.3223  LearningRate 0.0967  ProxyLR: 4.8347  Epoch: 7  Global Step: 43310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:44,350-Speed 3895.06 samples/sec  Loss 1.3675  LearningRate 0.0967  ProxyLR: 4.8337  Epoch: 7  Global Step: 43320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:46,979-Speed 3896.54 samples/sec  Loss 1.2644  LearningRate 0.0967  ProxyLR: 4.8328  Epoch: 7  Global Step: 43330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:49,606-Speed 3899.37 samples/sec  Loss 1.2825  LearningRate 0.0966  ProxyLR: 4.8318  Epoch: 7  Global Step: 43340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:52,233-Speed 3899.09 samples/sec  Loss 1.2185  LearningRate 0.0966  ProxyLR: 4.8308  Epoch: 7  Global Step: 43350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:54,863-Speed 3894.38 samples/sec  Loss 1.2376  LearningRate 0.0966  ProxyLR: 4.8298  Epoch: 7  Global Step: 43360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:41:57,491-Speed 3896.19 samples/sec  Loss 1.2775  LearningRate 0.0966  ProxyLR: 4.8289  Epoch: 7  Global Step: 43370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:00,106-Speed 3917.53 samples/sec  Loss 1.2688  LearningRate 0.0966  ProxyLR: 4.8279  Epoch: 7  Global Step: 43380   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:02,740-Speed 3889.46 samples/sec  Loss 1.2254  LearningRate 0.0965  ProxyLR: 4.8269  Epoch: 7  Global Step: 43390   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:05,368-Speed 3896.88 samples/sec  Loss 1.2135  LearningRate 0.0965  ProxyLR: 4.8259  Epoch: 7  Global Step: 43400   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:07,994-Speed 3900.72 samples/sec  Loss 1.3292  LearningRate 0.0965  ProxyLR: 4.8249  Epoch: 7  Global Step: 43410   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:10,622-Speed 3897.18 samples/sec  Loss 1.3349  LearningRate 0.0965  ProxyLR: 4.8240  Epoch: 7  Global Step: 43420   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:13,251-Speed 3896.05 samples/sec  Loss 1.2685  LearningRate 0.0965  ProxyLR: 4.8230  Epoch: 7  Global Step: 43430   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:15,881-Speed 3894.15 samples/sec  Loss 1.1872  LearningRate 0.0964  ProxyLR: 4.8220  Epoch: 7  Global Step: 43440   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:18,509-Speed 3897.42 samples/sec  Loss 1.2977  LearningRate 0.0964  ProxyLR: 4.8210  Epoch: 7  Global Step: 43450   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:21,137-Speed 3898.48 samples/sec  Loss 1.3065  LearningRate 0.0964  ProxyLR: 4.8201  Epoch: 7  Global Step: 43460   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:23,764-Speed 3898.08 samples/sec  Loss 1.2806  LearningRate 0.0964  ProxyLR: 4.8191  Epoch: 7  Global Step: 43470   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:26,393-Speed 3896.85 samples/sec  Loss 1.2374  LearningRate 0.0964  ProxyLR: 4.8181  Epoch: 7  Global Step: 43480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:29,019-Speed 3900.23 samples/sec  Loss 1.2913  LearningRate 0.0963  ProxyLR: 4.8171  Epoch: 7  Global Step: 43490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:31,648-Speed 3896.65 samples/sec  Loss 1.2253  LearningRate 0.0963  ProxyLR: 4.8162  Epoch: 7  Global Step: 43500   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:34,274-Speed 3899.36 samples/sec  Loss 1.2978  LearningRate 0.0963  ProxyLR: 4.8152  Epoch: 7  Global Step: 43510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:36,900-Speed 3900.66 samples/sec  Loss 1.2397  LearningRate 0.0963  ProxyLR: 4.8142  Epoch: 7  Global Step: 43520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:39,525-Speed 3901.66 samples/sec  Loss 1.2883  LearningRate 0.0963  ProxyLR: 4.8132  Epoch: 7  Global Step: 43530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:42,150-Speed 3901.87 samples/sec  Loss 1.2051  LearningRate 0.0962  ProxyLR: 4.8123  Epoch: 7  Global Step: 43540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:44,776-Speed 3901.23 samples/sec  Loss 1.2811  LearningRate 0.0962  ProxyLR: 4.8113  Epoch: 7  Global Step: 43550   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:47,400-Speed 3903.21 samples/sec  Loss 1.2793  LearningRate 0.0962  ProxyLR: 4.8103  Epoch: 7  Global Step: 43560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:50,025-Speed 3902.02 samples/sec  Loss 1.2850  LearningRate 0.0962  ProxyLR: 4.8093  Epoch: 7  Global Step: 43570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:42:52,638-Speed 3920.56 samples/sec  Loss 1.2682  LearningRate 0.0962  ProxyLR: 4.8083  Epoch: 7  Global Step: 43580   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:55,263-Speed 3902.06 samples/sec  Loss 1.2174  LearningRate 0.0961  ProxyLR: 4.8074  Epoch: 7  Global Step: 43590   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:42:57,889-Speed 3899.80 samples/sec  Loss 1.2715  LearningRate 0.0961  ProxyLR: 4.8064  Epoch: 7  Global Step: 43600   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:00,517-Speed 3897.94 samples/sec  Loss 1.2767  LearningRate 0.0961  ProxyLR: 4.8054  Epoch: 7  Global Step: 43610   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:03,142-Speed 3902.32 samples/sec  Loss 1.2339  LearningRate 0.0961  ProxyLR: 4.8044  Epoch: 7  Global Step: 43620   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:05,769-Speed 3899.34 samples/sec  Loss 1.2553  LearningRate 0.0961  ProxyLR: 4.8035  Epoch: 7  Global Step: 43630   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:08,400-Speed 3892.48 samples/sec  Loss 1.2923  LearningRate 0.0960  ProxyLR: 4.8025  Epoch: 7  Global Step: 43640   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:11,031-Speed 3893.85 samples/sec  Loss 1.2488  LearningRate 0.0960  ProxyLR: 4.8015  Epoch: 7  Global Step: 43650   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:13,660-Speed 3894.96 samples/sec  Loss 1.3119  LearningRate 0.0960  ProxyLR: 4.8005  Epoch: 7  Global Step: 43660   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:16,292-Speed 3891.98 samples/sec  Loss 1.2766  LearningRate 0.0960  ProxyLR: 4.7996  Epoch: 7  Global Step: 43670   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:43:18,926-Speed 3888.29 samples/sec  Loss 1.2543  LearningRate 0.0960  ProxyLR: 4.7986  Epoch: 7  Global Step: 43680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:21,557-Speed 3893.22 samples/sec  Loss 1.2177  LearningRate 0.0960  ProxyLR: 4.7976  Epoch: 7  Global Step: 43690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:24,189-Speed 3891.20 samples/sec  Loss 1.2864  LearningRate 0.0959  ProxyLR: 4.7966  Epoch: 7  Global Step: 43700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:26,821-Speed 3891.60 samples/sec  Loss 1.2666  LearningRate 0.0959  ProxyLR: 4.7957  Epoch: 7  Global Step: 43710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:29,453-Speed 3892.24 samples/sec  Loss 1.3457  LearningRate 0.0959  ProxyLR: 4.7947  Epoch: 7  Global Step: 43720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:32,084-Speed 3892.48 samples/sec  Loss 1.2923  LearningRate 0.0959  ProxyLR: 4.7937  Epoch: 7  Global Step: 43730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:34,715-Speed 3892.24 samples/sec  Loss 1.3231  LearningRate 0.0959  ProxyLR: 4.7927  Epoch: 7  Global Step: 43740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:37,351-Speed 3885.97 samples/sec  Loss 1.2899  LearningRate 0.0958  ProxyLR: 4.7918  Epoch: 7  Global Step: 43750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:39,985-Speed 3888.18 samples/sec  Loss 1.2319  LearningRate 0.0958  ProxyLR: 4.7908  Epoch: 7  Global Step: 43760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:42,618-Speed 3889.90 samples/sec  Loss 1.2994  LearningRate 0.0958  ProxyLR: 4.7898  Epoch: 7  Global Step: 43770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:45,240-Speed 3907.56 samples/sec  Loss 1.2973  LearningRate 0.0958  ProxyLR: 4.7889  Epoch: 7  Global Step: 43780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:47,873-Speed 3888.90 samples/sec  Loss 1.2887  LearningRate 0.0958  ProxyLR: 4.7879  Epoch: 7  Global Step: 43790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:50,499-Speed 3900.79 samples/sec  Loss 1.3674  LearningRate 0.0957  ProxyLR: 4.7869  Epoch: 7  Global Step: 43800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:53,124-Speed 3901.59 samples/sec  Loss 1.2541  LearningRate 0.0957  ProxyLR: 4.7859  Epoch: 7  Global Step: 43810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:55,749-Speed 3901.90 samples/sec  Loss 1.2234  LearningRate 0.0957  ProxyLR: 4.7850  Epoch: 7  Global Step: 43820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:43:58,372-Speed 3904.55 samples/sec  Loss 1.2561  LearningRate 0.0957  ProxyLR: 4.7840  Epoch: 7  Global Step: 43830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:00,997-Speed 3902.10 samples/sec  Loss 1.1493  LearningRate 0.0957  ProxyLR: 4.7830  Epoch: 7  Global Step: 43840   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:03,622-Speed 3902.53 samples/sec  Loss 1.2184  LearningRate 0.0956  ProxyLR: 4.7820  Epoch: 7  Global Step: 43850   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:06,248-Speed 3900.40 samples/sec  Loss 1.2700  LearningRate 0.0956  ProxyLR: 4.7811  Epoch: 7  Global Step: 43860   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:08,872-Speed 3903.33 samples/sec  Loss 1.2881  LearningRate 0.0956  ProxyLR: 4.7801  Epoch: 7  Global Step: 43870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:11,497-Speed 3901.79 samples/sec  Loss 1.3455  LearningRate 0.0956  ProxyLR: 4.7791  Epoch: 7  Global Step: 43880   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:44:14,109-Speed 3921.12 samples/sec  Loss 1.2428  LearningRate 0.0956  ProxyLR: 4.7782  Epoch: 7  Global Step: 43890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:16,734-Speed 3902.62 samples/sec  Loss 1.2067  LearningRate 0.0955  ProxyLR: 4.7772  Epoch: 7  Global Step: 43900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:19,357-Speed 3904.33 samples/sec  Loss 1.2046  LearningRate 0.0955  ProxyLR: 4.7762  Epoch: 7  Global Step: 43910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:21,981-Speed 3902.77 samples/sec  Loss 1.2724  LearningRate 0.0955  ProxyLR: 4.7752  Epoch: 7  Global Step: 43920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:24,605-Speed 3904.18 samples/sec  Loss 1.2817  LearningRate 0.0955  ProxyLR: 4.7743  Epoch: 7  Global Step: 43930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:27,229-Speed 3903.46 samples/sec  Loss 1.3012  LearningRate 0.0955  ProxyLR: 4.7733  Epoch: 7  Global Step: 43940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:29,852-Speed 3904.11 samples/sec  Loss 1.2757  LearningRate 0.0954  ProxyLR: 4.7723  Epoch: 7  Global Step: 43950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:32,477-Speed 3902.24 samples/sec  Loss 1.2878  LearningRate 0.0954  ProxyLR: 4.7713  Epoch: 7  Global Step: 43960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:35,099-Speed 3905.80 samples/sec  Loss 1.2711  LearningRate 0.0954  ProxyLR: 4.7704  Epoch: 7  Global Step: 43970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:37,722-Speed 3904.46 samples/sec  Loss 1.2902  LearningRate 0.0954  ProxyLR: 4.7694  Epoch: 7  Global Step: 43980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:40,346-Speed 3903.92 samples/sec  Loss 1.2634  LearningRate 0.0954  ProxyLR: 4.7684  Epoch: 7  Global Step: 43990   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:44:42,968-Speed 3906.28 samples/sec  Loss 1.2524  LearningRate 0.0953  ProxyLR: 4.7675  Epoch: 7  Global Step: 44000   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:44:45,579-Speed 3923.55 samples/sec  Loss 1.2661  LearningRate 0.0953  ProxyLR: 4.7665  Epoch: 7  Global Step: 44010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:48,203-Speed 3902.98 samples/sec  Loss 1.2725  LearningRate 0.0953  ProxyLR: 4.7655  Epoch: 7  Global Step: 44020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:50,827-Speed 3903.52 samples/sec  Loss 1.3260  LearningRate 0.0953  ProxyLR: 4.7645  Epoch: 7  Global Step: 44030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:53,450-Speed 3904.43 samples/sec  Loss 1.3407  LearningRate 0.0953  ProxyLR: 4.7636  Epoch: 7  Global Step: 44040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:44:56,060-Speed 3924.79 samples/sec  Loss 1.2979  LearningRate 0.0953  ProxyLR: 4.7626  Epoch: 7  Global Step: 44050   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:44:58,682-Speed 3905.66 samples/sec  Loss 1.2885  LearningRate 0.0952  ProxyLR: 4.7616  Epoch: 7  Global Step: 44060   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:01,306-Speed 3903.88 samples/sec  Loss 1.3410  LearningRate 0.0952  ProxyLR: 4.7607  Epoch: 7  Global Step: 44070   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:03,930-Speed 3902.82 samples/sec  Loss 1.2457  LearningRate 0.0952  ProxyLR: 4.7597  Epoch: 7  Global Step: 44080   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:06,555-Speed 3902.52 samples/sec  Loss 1.2152  LearningRate 0.0952  ProxyLR: 4.7587  Epoch: 7  Global Step: 44090   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:09,180-Speed 3900.99 samples/sec  Loss 1.2399  LearningRate 0.0952  ProxyLR: 4.7577  Epoch: 7  Global Step: 44100   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:11,804-Speed 3903.67 samples/sec  Loss 1.2768  LearningRate 0.0951  ProxyLR: 4.7568  Epoch: 7  Global Step: 44110   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:14,428-Speed 3904.06 samples/sec  Loss 1.3112  LearningRate 0.0951  ProxyLR: 4.7558  Epoch: 7  Global Step: 44120   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:17,052-Speed 3902.76 samples/sec  Loss 1.3164  LearningRate 0.0951  ProxyLR: 4.7548  Epoch: 7  Global Step: 44130   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:19,676-Speed 3903.59 samples/sec  Loss 1.2921  LearningRate 0.0951  ProxyLR: 4.7539  Epoch: 7  Global Step: 44140   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:22,299-Speed 3905.31 samples/sec  Loss 1.3208  LearningRate 0.0951  ProxyLR: 4.7529  Epoch: 7  Global Step: 44150   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:45:24,910-Speed 3921.64 samples/sec  Loss 1.2782  LearningRate 0.0950  ProxyLR: 4.7519  Epoch: 7  Global Step: 44160   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:27,539-Speed 3897.15 samples/sec  Loss 1.2697  LearningRate 0.0950  ProxyLR: 4.7510  Epoch: 7  Global Step: 44170   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:30,167-Speed 3895.97 samples/sec  Loss 1.3527  LearningRate 0.0950  ProxyLR: 4.7500  Epoch: 7  Global Step: 44180   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:32,795-Speed 3898.32 samples/sec  Loss 1.2556  LearningRate 0.0950  ProxyLR: 4.7490  Epoch: 7  Global Step: 44190   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:35,425-Speed 3894.63 samples/sec  Loss 1.2133  LearningRate 0.0950  ProxyLR: 4.7480  Epoch: 7  Global Step: 44200   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:38,054-Speed 3895.17 samples/sec  Loss 1.2458  LearningRate 0.0949  ProxyLR: 4.7471  Epoch: 7  Global Step: 44210   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:40,687-Speed 3889.77 samples/sec  Loss 1.2669  LearningRate 0.0949  ProxyLR: 4.7461  Epoch: 7  Global Step: 44220   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:43,319-Speed 3891.62 samples/sec  Loss 1.2348  LearningRate 0.0949  ProxyLR: 4.7451  Epoch: 7  Global Step: 44230   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:45,950-Speed 3894.09 samples/sec  Loss 1.1645  LearningRate 0.0949  ProxyLR: 4.7442  Epoch: 7  Global Step: 44240   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:48,579-Speed 3894.80 samples/sec  Loss 1.1391  LearningRate 0.0949  ProxyLR: 4.7432  Epoch: 7  Global Step: 44250   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:45:51,212-Speed 3890.88 samples/sec  Loss 1.1986  LearningRate 0.0948  ProxyLR: 4.7422  Epoch: 7  Global Step: 44260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:45:53,841-Speed 3894.98 samples/sec  Loss 1.2982  LearningRate 0.0948  ProxyLR: 4.7413  Epoch: 7  Global Step: 44270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:45:56,471-Speed 3894.38 samples/sec  Loss 1.2669  LearningRate 0.0948  ProxyLR: 4.7403  Epoch: 7  Global Step: 44280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:45:59,102-Speed 3893.19 samples/sec  Loss 1.3113  LearningRate 0.0948  ProxyLR: 4.7393  Epoch: 7  Global Step: 44290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:01,732-Speed 3894.53 samples/sec  Loss 1.2979  LearningRate 0.0948  ProxyLR: 4.7384  Epoch: 7  Global Step: 44300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:04,364-Speed 3892.24 samples/sec  Loss 1.3625  LearningRate 0.0947  ProxyLR: 4.7374  Epoch: 7  Global Step: 44310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:06,994-Speed 3893.92 samples/sec  Loss 1.3393  LearningRate 0.0947  ProxyLR: 4.7364  Epoch: 7  Global Step: 44320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:09,624-Speed 3894.67 samples/sec  Loss 1.2609  LearningRate 0.0947  ProxyLR: 4.7355  Epoch: 7  Global Step: 44330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:12,254-Speed 3895.26 samples/sec  Loss 1.3071  LearningRate 0.0947  ProxyLR: 4.7345  Epoch: 7  Global Step: 44340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:14,887-Speed 3889.59 samples/sec  Loss 1.3414  LearningRate 0.0947  ProxyLR: 4.7335  Epoch: 7  Global Step: 44350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:17,499-Speed 3920.54 samples/sec  Loss 1.3377  LearningRate 0.0947  ProxyLR: 4.7325  Epoch: 7  Global Step: 44360   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:20,124-Speed 3902.38 samples/sec  Loss 1.1873  LearningRate 0.0946  ProxyLR: 4.7316  Epoch: 7  Global Step: 44370   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:22,748-Speed 3902.78 samples/sec  Loss 1.3042  LearningRate 0.0946  ProxyLR: 4.7306  Epoch: 7  Global Step: 44380   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:25,373-Speed 3901.84 samples/sec  Loss 1.2811  LearningRate 0.0946  ProxyLR: 4.7296  Epoch: 7  Global Step: 44390   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:27,999-Speed 3900.56 samples/sec  Loss 1.2990  LearningRate 0.0946  ProxyLR: 4.7287  Epoch: 7  Global Step: 44400   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:30,624-Speed 3902.42 samples/sec  Loss 1.2668  LearningRate 0.0946  ProxyLR: 4.7277  Epoch: 7  Global Step: 44410   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:33,248-Speed 3902.87 samples/sec  Loss 1.3458  LearningRate 0.0945  ProxyLR: 4.7267  Epoch: 7  Global Step: 44420   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:35,873-Speed 3901.33 samples/sec  Loss 1.2479  LearningRate 0.0945  ProxyLR: 4.7258  Epoch: 7  Global Step: 44430   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:38,497-Speed 3904.18 samples/sec  Loss 1.3398  LearningRate 0.0945  ProxyLR: 4.7248  Epoch: 7  Global Step: 44440   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:41,122-Speed 3901.74 samples/sec  Loss 1.3021  LearningRate 0.0945  ProxyLR: 4.7238  Epoch: 7  Global Step: 44450   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:43,746-Speed 3902.72 samples/sec  Loss 1.2398  LearningRate 0.0945  ProxyLR: 4.7229  Epoch: 7  Global Step: 44460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:46,370-Speed 3903.53 samples/sec  Loss 1.2059  LearningRate 0.0944  ProxyLR: 4.7219  Epoch: 7  Global Step: 44470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:48,994-Speed 3903.33 samples/sec  Loss 1.2986  LearningRate 0.0944  ProxyLR: 4.7209  Epoch: 7  Global Step: 44480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:51,620-Speed 3901.09 samples/sec  Loss 1.3763  LearningRate 0.0944  ProxyLR: 4.7200  Epoch: 7  Global Step: 44490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:46:54,231-Speed 3922.69 samples/sec  Loss 1.2184  LearningRate 0.0944  ProxyLR: 4.7190  Epoch: 7  Global Step: 44500   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:56,853-Speed 3905.90 samples/sec  Loss 1.3129  LearningRate 0.0944  ProxyLR: 4.7180  Epoch: 7  Global Step: 44510   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:46:59,477-Speed 3903.87 samples/sec  Loss 1.3122  LearningRate 0.0943  ProxyLR: 4.7171  Epoch: 7  Global Step: 44520   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:02,099-Speed 3905.77 samples/sec  Loss 1.3045  LearningRate 0.0943  ProxyLR: 4.7161  Epoch: 7  Global Step: 44530   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:04,724-Speed 3902.18 samples/sec  Loss 1.2711  LearningRate 0.0943  ProxyLR: 4.7151  Epoch: 7  Global Step: 44540   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:07,347-Speed 3904.68 samples/sec  Loss 1.2818  LearningRate 0.0943  ProxyLR: 4.7142  Epoch: 7  Global Step: 44550   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:09,975-Speed 3897.35 samples/sec  Loss 1.3423  LearningRate 0.0943  ProxyLR: 4.7132  Epoch: 7  Global Step: 44560   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:12,602-Speed 3899.59 samples/sec  Loss 1.3134  LearningRate 0.0942  ProxyLR: 4.7122  Epoch: 7  Global Step: 44570   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:15,229-Speed 3898.40 samples/sec  Loss 1.2865  LearningRate 0.0942  ProxyLR: 4.7113  Epoch: 7  Global Step: 44580   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:17,856-Speed 3898.62 samples/sec  Loss 1.4070  LearningRate 0.0942  ProxyLR: 4.7103  Epoch: 7  Global Step: 44590   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:47:20,482-Speed 3900.70 samples/sec  Loss 1.2850  LearningRate 0.0942  ProxyLR: 4.7093  Epoch: 7  Global Step: 44600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:23,109-Speed 3899.63 samples/sec  Loss 1.2281  LearningRate 0.0942  ProxyLR: 4.7084  Epoch: 7  Global Step: 44610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:25,735-Speed 3899.62 samples/sec  Loss 1.2186  LearningRate 0.0941  ProxyLR: 4.7074  Epoch: 7  Global Step: 44620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:28,359-Speed 3904.11 samples/sec  Loss 1.3486  LearningRate 0.0941  ProxyLR: 4.7065  Epoch: 7  Global Step: 44630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:30,985-Speed 3900.29 samples/sec  Loss 1.2909  LearningRate 0.0941  ProxyLR: 4.7055  Epoch: 7  Global Step: 44640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:33,612-Speed 3899.05 samples/sec  Loss 1.2329  LearningRate 0.0941  ProxyLR: 4.7045  Epoch: 7  Global Step: 44650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:36,237-Speed 3901.28 samples/sec  Loss 1.3239  LearningRate 0.0941  ProxyLR: 4.7036  Epoch: 7  Global Step: 44660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:38,863-Speed 3901.01 samples/sec  Loss 1.2964  LearningRate 0.0941  ProxyLR: 4.7026  Epoch: 7  Global Step: 44670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:41,490-Speed 3898.52 samples/sec  Loss 1.2592  LearningRate 0.0940  ProxyLR: 4.7016  Epoch: 7  Global Step: 44680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:44,117-Speed 3899.68 samples/sec  Loss 1.3543  LearningRate 0.0940  ProxyLR: 4.7007  Epoch: 7  Global Step: 44690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:46,730-Speed 3919.76 samples/sec  Loss 1.3394  LearningRate 0.0940  ProxyLR: 4.6997  Epoch: 7  Global Step: 44700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:49,354-Speed 3903.12 samples/sec  Loss 1.3063  LearningRate 0.0940  ProxyLR: 4.6987  Epoch: 7  Global Step: 44710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:51,979-Speed 3901.11 samples/sec  Loss 1.4162  LearningRate 0.0940  ProxyLR: 4.6978  Epoch: 7  Global Step: 44720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:54,606-Speed 3899.86 samples/sec  Loss 1.3067  LearningRate 0.0939  ProxyLR: 4.6968  Epoch: 7  Global Step: 44730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:57,232-Speed 3899.69 samples/sec  Loss 1.3377  LearningRate 0.0939  ProxyLR: 4.6958  Epoch: 7  Global Step: 44740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:47:59,859-Speed 3899.41 samples/sec  Loss 1.2717  LearningRate 0.0939  ProxyLR: 4.6949  Epoch: 7  Global Step: 44750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:02,484-Speed 3902.24 samples/sec  Loss 1.1747  LearningRate 0.0939  ProxyLR: 4.6939  Epoch: 7  Global Step: 44760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:05,095-Speed 3922.59 samples/sec  Loss 1.2328  LearningRate 0.0939  ProxyLR: 4.6929  Epoch: 7  Global Step: 44770   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:07,719-Speed 3903.73 samples/sec  Loss 1.2959  LearningRate 0.0938  ProxyLR: 4.6920  Epoch: 7  Global Step: 44780   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:10,345-Speed 3901.44 samples/sec  Loss 1.3549  LearningRate 0.0938  ProxyLR: 4.6910  Epoch: 7  Global Step: 44790   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:12,970-Speed 3901.25 samples/sec  Loss 1.2956  LearningRate 0.0938  ProxyLR: 4.6901  Epoch: 7  Global Step: 44800   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:15,594-Speed 3902.95 samples/sec  Loss 1.1602  LearningRate 0.0938  ProxyLR: 4.6891  Epoch: 7  Global Step: 44810   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:18,220-Speed 3901.67 samples/sec  Loss 1.2823  LearningRate 0.0938  ProxyLR: 4.6881  Epoch: 7  Global Step: 44820   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:20,847-Speed 3898.81 samples/sec  Loss 1.3052  LearningRate 0.0937  ProxyLR: 4.6872  Epoch: 7  Global Step: 44830   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:23,474-Speed 3897.60 samples/sec  Loss 1.2724  LearningRate 0.0937  ProxyLR: 4.6862  Epoch: 7  Global Step: 44840   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:26,101-Speed 3900.03 samples/sec  Loss 1.3539  LearningRate 0.0937  ProxyLR: 4.6852  Epoch: 7  Global Step: 44850   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:28,727-Speed 3899.21 samples/sec  Loss 1.2924  LearningRate 0.0937  ProxyLR: 4.6843  Epoch: 7  Global Step: 44860   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:48:31,355-Speed 3897.83 samples/sec  Loss 1.3782  LearningRate 0.0937  ProxyLR: 4.6833  Epoch: 7  Global Step: 44870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:33,985-Speed 3895.38 samples/sec  Loss 1.2668  LearningRate 0.0936  ProxyLR: 4.6824  Epoch: 7  Global Step: 44880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:36,611-Speed 3899.58 samples/sec  Loss 1.2947  LearningRate 0.0936  ProxyLR: 4.6814  Epoch: 7  Global Step: 44890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:39,239-Speed 3897.06 samples/sec  Loss 1.2921  LearningRate 0.0936  ProxyLR: 4.6804  Epoch: 7  Global Step: 44900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:41,867-Speed 3897.84 samples/sec  Loss 1.3193  LearningRate 0.0936  ProxyLR: 4.6795  Epoch: 7  Global Step: 44910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:44,494-Speed 3898.89 samples/sec  Loss 1.2385  LearningRate 0.0936  ProxyLR: 4.6785  Epoch: 7  Global Step: 44920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:47,122-Speed 3897.85 samples/sec  Loss 1.2456  LearningRate 0.0936  ProxyLR: 4.6775  Epoch: 7  Global Step: 44930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:49,748-Speed 3900.33 samples/sec  Loss 1.2951  LearningRate 0.0935  ProxyLR: 4.6766  Epoch: 7  Global Step: 44940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:52,375-Speed 3898.94 samples/sec  Loss 1.2671  LearningRate 0.0935  ProxyLR: 4.6756  Epoch: 7  Global Step: 44950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:55,002-Speed 3898.43 samples/sec  Loss 1.2959  LearningRate 0.0935  ProxyLR: 4.6747  Epoch: 7  Global Step: 44960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:48:57,613-Speed 3923.01 samples/sec  Loss 1.2140  LearningRate 0.0935  ProxyLR: 4.6737  Epoch: 7  Global Step: 44970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:00,240-Speed 3899.38 samples/sec  Loss 1.3006  LearningRate 0.0935  ProxyLR: 4.6727  Epoch: 7  Global Step: 44980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:02,866-Speed 3900.31 samples/sec  Loss 1.3586  LearningRate 0.0934  ProxyLR: 4.6718  Epoch: 7  Global Step: 44990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:05,490-Speed 3903.63 samples/sec  Loss 1.3358  LearningRate 0.0934  ProxyLR: 4.6708  Epoch: 7  Global Step: 45000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:08,116-Speed 3900.69 samples/sec  Loss 1.2865  LearningRate 0.0934  ProxyLR: 4.6698  Epoch: 7  Global Step: 45010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:10,729-Speed 3918.72 samples/sec  Loss 1.2253  LearningRate 0.0934  ProxyLR: 4.6689  Epoch: 7  Global Step: 45020   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:13,356-Speed 3899.39 samples/sec  Loss 1.2863  LearningRate 0.0934  ProxyLR: 4.6679  Epoch: 7  Global Step: 45030   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:15,982-Speed 3899.77 samples/sec  Loss 1.2821  LearningRate 0.0933  ProxyLR: 4.6670  Epoch: 7  Global Step: 45040   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:18,608-Speed 3900.42 samples/sec  Loss 1.2706  LearningRate 0.0933  ProxyLR: 4.6660  Epoch: 7  Global Step: 45050   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:21,234-Speed 3899.90 samples/sec  Loss 1.2868  LearningRate 0.0933  ProxyLR: 4.6650  Epoch: 7  Global Step: 45060   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:23,859-Speed 3902.14 samples/sec  Loss 1.3085  LearningRate 0.0933  ProxyLR: 4.6641  Epoch: 7  Global Step: 45070   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:26,484-Speed 3902.62 samples/sec  Loss 1.4930  LearningRate 0.0933  ProxyLR: 4.6631  Epoch: 7  Global Step: 45080   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:29,108-Speed 3903.18 samples/sec  Loss 1.3317  LearningRate 0.0932  ProxyLR: 4.6622  Epoch: 7  Global Step: 45090   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:31,734-Speed 3900.83 samples/sec  Loss 1.2156  LearningRate 0.0932  ProxyLR: 4.6612  Epoch: 7  Global Step: 45100   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:34,358-Speed 3902.95 samples/sec  Loss 1.2948  LearningRate 0.0932  ProxyLR: 4.6602  Epoch: 7  Global Step: 45110   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:36,983-Speed 3902.85 samples/sec  Loss 1.2888  LearningRate 0.0932  ProxyLR: 4.6593  Epoch: 7  Global Step: 45120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:39,607-Speed 3903.20 samples/sec  Loss 1.2741  LearningRate 0.0932  ProxyLR: 4.6583  Epoch: 7  Global Step: 45130   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:49:42,218-Speed 3922.71 samples/sec  Loss 1.2952  LearningRate 0.0931  ProxyLR: 4.6574  Epoch: 7  Global Step: 45140   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:44,843-Speed 3901.92 samples/sec  Loss 1.1959  LearningRate 0.0931  ProxyLR: 4.6564  Epoch: 7  Global Step: 45150   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:47,467-Speed 3902.23 samples/sec  Loss 1.3089  LearningRate 0.0931  ProxyLR: 4.6554  Epoch: 7  Global Step: 45160   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:50,092-Speed 3901.93 samples/sec  Loss 1.3041  LearningRate 0.0931  ProxyLR: 4.6545  Epoch: 7  Global Step: 45170   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:52,718-Speed 3901.56 samples/sec  Loss 1.2731  LearningRate 0.0931  ProxyLR: 4.6535  Epoch: 7  Global Step: 45180   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:55,344-Speed 3900.96 samples/sec  Loss 1.2469  LearningRate 0.0931  ProxyLR: 4.6526  Epoch: 7  Global Step: 45190   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:49:57,968-Speed 3903.35 samples/sec  Loss 1.3163  LearningRate 0.0930  ProxyLR: 4.6516  Epoch: 7  Global Step: 45200   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:00,593-Speed 3902.07 samples/sec  Loss 1.2526  LearningRate 0.0930  ProxyLR: 4.6506  Epoch: 7  Global Step: 45210   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:03,219-Speed 3900.31 samples/sec  Loss 1.3255  LearningRate 0.0930  ProxyLR: 4.6497  Epoch: 7  Global Step: 45220   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:05,842-Speed 3904.15 samples/sec  Loss 1.3193  LearningRate 0.0930  ProxyLR: 4.6487  Epoch: 7  Global Step: 45230   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:08,466-Speed 3904.64 samples/sec  Loss 1.1949  LearningRate 0.0930  ProxyLR: 4.6478  Epoch: 7  Global Step: 45240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:11,092-Speed 3900.39 samples/sec  Loss 1.3491  LearningRate 0.0929  ProxyLR: 4.6468  Epoch: 7  Global Step: 45250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:13,717-Speed 3901.60 samples/sec  Loss 1.1886  LearningRate 0.0929  ProxyLR: 4.6458  Epoch: 7  Global Step: 45260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:16,344-Speed 3898.91 samples/sec  Loss 1.2720  LearningRate 0.0929  ProxyLR: 4.6449  Epoch: 7  Global Step: 45270   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:18,967-Speed 3904.60 samples/sec  Loss 1.3113  LearningRate 0.0929  ProxyLR: 4.6439  Epoch: 7  Global Step: 45280   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:21,591-Speed 3903.16 samples/sec  Loss 1.1961  LearningRate 0.0929  ProxyLR: 4.6430  Epoch: 7  Global Step: 45290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:24,202-Speed 3923.18 samples/sec  Loss 1.2391  LearningRate 0.0928  ProxyLR: 4.6420  Epoch: 7  Global Step: 45300   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:26,826-Speed 3903.12 samples/sec  Loss 1.2881  LearningRate 0.0928  ProxyLR: 4.6410  Epoch: 7  Global Step: 45310   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:29,449-Speed 3905.31 samples/sec  Loss 1.3487  LearningRate 0.0928  ProxyLR: 4.6401  Epoch: 7  Global Step: 45320   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:32,073-Speed 3902.90 samples/sec  Loss 1.2644  LearningRate 0.0928  ProxyLR: 4.6391  Epoch: 7  Global Step: 45330   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:34,698-Speed 3902.04 samples/sec  Loss 1.3377  LearningRate 0.0928  ProxyLR: 4.6382  Epoch: 7  Global Step: 45340   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:37,321-Speed 3904.78 samples/sec  Loss 1.3046  LearningRate 0.0927  ProxyLR: 4.6372  Epoch: 7  Global Step: 45350   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:39,945-Speed 3903.60 samples/sec  Loss 1.2848  LearningRate 0.0927  ProxyLR: 4.6363  Epoch: 7  Global Step: 45360   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:42,568-Speed 3904.68 samples/sec  Loss 1.3571  LearningRate 0.0927  ProxyLR: 4.6353  Epoch: 7  Global Step: 45370   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:45,193-Speed 3903.33 samples/sec  Loss 1.3308  LearningRate 0.0927  ProxyLR: 4.6343  Epoch: 7  Global Step: 45380   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:47,818-Speed 3901.31 samples/sec  Loss 1.2444  LearningRate 0.0927  ProxyLR: 4.6334  Epoch: 7  Global Step: 45390   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:50:50,442-Speed 3902.89 samples/sec  Loss 1.2356  LearningRate 0.0926  ProxyLR: 4.6324  Epoch: 7  Global Step: 45400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:53,068-Speed 3901.70 samples/sec  Loss 1.2738  LearningRate 0.0926  ProxyLR: 4.6315  Epoch: 7  Global Step: 45410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:55,692-Speed 3902.47 samples/sec  Loss 1.2843  LearningRate 0.0926  ProxyLR: 4.6305  Epoch: 7  Global Step: 45420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:50:58,318-Speed 3901.79 samples/sec  Loss 1.2479  LearningRate 0.0926  ProxyLR: 4.6296  Epoch: 7  Global Step: 45430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:00,940-Speed 3905.63 samples/sec  Loss 1.2649  LearningRate 0.0926  ProxyLR: 4.6286  Epoch: 7  Global Step: 45440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:03,564-Speed 3903.94 samples/sec  Loss 1.2745  LearningRate 0.0926  ProxyLR: 4.6276  Epoch: 7  Global Step: 45450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:06,191-Speed 3899.18 samples/sec  Loss 1.2994  LearningRate 0.0925  ProxyLR: 4.6267  Epoch: 7  Global Step: 45460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:08,815-Speed 3902.71 samples/sec  Loss 1.3224  LearningRate 0.0925  ProxyLR: 4.6257  Epoch: 7  Global Step: 45470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:11,499-Speed 3816.90 samples/sec  Loss 1.2694  LearningRate 0.0925  ProxyLR: 4.6248  Epoch: 7  Global Step: 45480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:20,630-Speed 1121.48 samples/sec  Loss 1.2627  LearningRate 0.0925  ProxyLR: 4.6238  Epoch: 8  Global Step: 45490   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:23,342-Speed 3778.01 samples/sec  Loss 0.8854  LearningRate 0.0925  ProxyLR: 4.6229  Epoch: 8  Global Step: 45500   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:51:25,961-Speed 3909.34 samples/sec  Loss 0.8120  LearningRate 0.0924  ProxyLR: 4.6219  Epoch: 8  Global Step: 45510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:28,618-Speed 3855.22 samples/sec  Loss 0.7979  LearningRate 0.0924  ProxyLR: 4.6209  Epoch: 8  Global Step: 45520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:31,249-Speed 3893.23 samples/sec  Loss 0.8124  LearningRate 0.0924  ProxyLR: 4.6200  Epoch: 8  Global Step: 45530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:33,876-Speed 3898.51 samples/sec  Loss 0.7300  LearningRate 0.0924  ProxyLR: 4.6190  Epoch: 8  Global Step: 45540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:36,501-Speed 3901.99 samples/sec  Loss 0.7554  LearningRate 0.0924  ProxyLR: 4.6181  Epoch: 8  Global Step: 45550   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:39,128-Speed 3898.99 samples/sec  Loss 0.8196  LearningRate 0.0923  ProxyLR: 4.6171  Epoch: 8  Global Step: 45560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:41,754-Speed 3900.55 samples/sec  Loss 0.7569  LearningRate 0.0923  ProxyLR: 4.6162  Epoch: 8  Global Step: 45570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:44,381-Speed 3899.27 samples/sec  Loss 0.8163  LearningRate 0.0923  ProxyLR: 4.6152  Epoch: 8  Global Step: 45580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:47,009-Speed 3897.25 samples/sec  Loss 0.7649  LearningRate 0.0923  ProxyLR: 4.6142  Epoch: 8  Global Step: 45590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:49,638-Speed 3896.62 samples/sec  Loss 0.7798  LearningRate 0.0923  ProxyLR: 4.6133  Epoch: 8  Global Step: 45600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:51:52,291-Speed 3860.58 samples/sec  Loss 0.8097  LearningRate 0.0922  ProxyLR: 4.6123  Epoch: 8  Global Step: 45610   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:51:54,916-Speed 3900.60 samples/sec  Loss 0.7033  LearningRate 0.0922  ProxyLR: 4.6114  Epoch: 8  Global Step: 45620   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:51:57,528-Speed 3922.81 samples/sec  Loss 0.7407  LearningRate 0.0922  ProxyLR: 4.6104  Epoch: 8  Global Step: 45630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:00,150-Speed 3905.46 samples/sec  Loss 0.7708  LearningRate 0.0922  ProxyLR: 4.6095  Epoch: 8  Global Step: 45640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:02,817-Speed 3841.56 samples/sec  Loss 0.7525  LearningRate 0.0922  ProxyLR: 4.6085  Epoch: 8  Global Step: 45650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:05,441-Speed 3903.52 samples/sec  Loss 0.7485  LearningRate 0.0922  ProxyLR: 4.6076  Epoch: 8  Global Step: 45660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:08,067-Speed 3900.29 samples/sec  Loss 0.8158  LearningRate 0.0921  ProxyLR: 4.6066  Epoch: 8  Global Step: 45670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:10,697-Speed 3894.73 samples/sec  Loss 0.7455  LearningRate 0.0921  ProxyLR: 4.6056  Epoch: 8  Global Step: 45680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:13,322-Speed 3901.47 samples/sec  Loss 0.8127  LearningRate 0.0921  ProxyLR: 4.6047  Epoch: 8  Global Step: 45690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:15,947-Speed 3902.25 samples/sec  Loss 0.8006  LearningRate 0.0921  ProxyLR: 4.6037  Epoch: 8  Global Step: 45700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:18,570-Speed 3904.74 samples/sec  Loss 0.7733  LearningRate 0.0921  ProxyLR: 4.6028  Epoch: 8  Global Step: 45710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:21,194-Speed 3903.29 samples/sec  Loss 0.8079  LearningRate 0.0920  ProxyLR: 4.6018  Epoch: 8  Global Step: 45720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:23,819-Speed 3901.90 samples/sec  Loss 0.7969  LearningRate 0.0920  ProxyLR: 4.6009  Epoch: 8  Global Step: 45730   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:52:26,444-Speed 3901.71 samples/sec  Loss 0.8204  LearningRate 0.0920  ProxyLR: 4.5999  Epoch: 8  Global Step: 45740   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:52:29,133-Speed 3808.29 samples/sec  Loss 0.7725  LearningRate 0.0920  ProxyLR: 4.5990  Epoch: 8  Global Step: 45750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:31,758-Speed 3901.82 samples/sec  Loss 0.7832  LearningRate 0.0920  ProxyLR: 4.5980  Epoch: 8  Global Step: 45760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:34,383-Speed 3903.13 samples/sec  Loss 0.7857  LearningRate 0.0919  ProxyLR: 4.5971  Epoch: 8  Global Step: 45770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:37,007-Speed 3902.95 samples/sec  Loss 0.7930  LearningRate 0.0919  ProxyLR: 4.5961  Epoch: 8  Global Step: 45780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:39,631-Speed 3903.35 samples/sec  Loss 0.8105  LearningRate 0.0919  ProxyLR: 4.5952  Epoch: 8  Global Step: 45790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:42,255-Speed 3902.88 samples/sec  Loss 0.8313  LearningRate 0.0919  ProxyLR: 4.5942  Epoch: 8  Global Step: 45800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:44,879-Speed 3904.26 samples/sec  Loss 0.7586  LearningRate 0.0919  ProxyLR: 4.5932  Epoch: 8  Global Step: 45810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:47,504-Speed 3901.90 samples/sec  Loss 0.7760  LearningRate 0.0918  ProxyLR: 4.5923  Epoch: 8  Global Step: 45820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:50,127-Speed 3904.55 samples/sec  Loss 0.7904  LearningRate 0.0918  ProxyLR: 4.5913  Epoch: 8  Global Step: 45830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:52,750-Speed 3904.86 samples/sec  Loss 0.7748  LearningRate 0.0918  ProxyLR: 4.5904  Epoch: 8  Global Step: 45840   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:52:55,375-Speed 3902.60 samples/sec  Loss 0.8490  LearningRate 0.0918  ProxyLR: 4.5894  Epoch: 8  Global Step: 45850   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:52:58,000-Speed 3901.60 samples/sec  Loss 0.7655  LearningRate 0.0918  ProxyLR: 4.5885  Epoch: 8  Global Step: 45860   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:53:00,612-Speed 3920.47 samples/sec  Loss 0.8312  LearningRate 0.0918  ProxyLR: 4.5875  Epoch: 8  Global Step: 45870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:03,239-Speed 3899.98 samples/sec  Loss 0.8189  LearningRate 0.0917  ProxyLR: 4.5866  Epoch: 8  Global Step: 45880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:05,864-Speed 3902.16 samples/sec  Loss 0.8316  LearningRate 0.0917  ProxyLR: 4.5856  Epoch: 8  Global Step: 45890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:08,491-Speed 3898.27 samples/sec  Loss 0.8067  LearningRate 0.0917  ProxyLR: 4.5847  Epoch: 8  Global Step: 45900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:11,121-Speed 3895.29 samples/sec  Loss 0.7782  LearningRate 0.0917  ProxyLR: 4.5837  Epoch: 8  Global Step: 45910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:13,747-Speed 3900.12 samples/sec  Loss 0.8672  LearningRate 0.0917  ProxyLR: 4.5828  Epoch: 8  Global Step: 45920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:16,376-Speed 3896.80 samples/sec  Loss 0.8006  LearningRate 0.0916  ProxyLR: 4.5818  Epoch: 8  Global Step: 45930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:19,002-Speed 3899.87 samples/sec  Loss 0.8445  LearningRate 0.0916  ProxyLR: 4.5809  Epoch: 8  Global Step: 45940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:21,628-Speed 3900.95 samples/sec  Loss 0.7664  LearningRate 0.0916  ProxyLR: 4.5799  Epoch: 8  Global Step: 45950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:24,255-Speed 3898.94 samples/sec  Loss 0.8755  LearningRate 0.0916  ProxyLR: 4.5790  Epoch: 8  Global Step: 45960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:26,884-Speed 3895.59 samples/sec  Loss 0.8270  LearningRate 0.0916  ProxyLR: 4.5780  Epoch: 8  Global Step: 45970   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:53:29,513-Speed 3896.64 samples/sec  Loss 0.7921  LearningRate 0.0915  ProxyLR: 4.5770  Epoch: 8  Global Step: 45980   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:53:32,139-Speed 3900.54 samples/sec  Loss 0.8382  LearningRate 0.0915  ProxyLR: 4.5761  Epoch: 8  Global Step: 45990   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:53:34,767-Speed 3897.11 samples/sec  Loss 0.7693  LearningRate 0.0915  ProxyLR: 4.5751  Epoch: 8  Global Step: 46000   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:53:37,380-Speed 3920.18 samples/sec  Loss 0.7921  LearningRate 0.0915  ProxyLR: 4.5742  Epoch: 8  Global Step: 46010   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:40,006-Speed 3899.48 samples/sec  Loss 0.8424  LearningRate 0.0915  ProxyLR: 4.5732  Epoch: 8  Global Step: 46020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:42,637-Speed 3893.43 samples/sec  Loss 0.7705  LearningRate 0.0914  ProxyLR: 4.5723  Epoch: 8  Global Step: 46030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:45,264-Speed 3899.21 samples/sec  Loss 0.7750  LearningRate 0.0914  ProxyLR: 4.5713  Epoch: 8  Global Step: 46040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:47,891-Speed 3899.19 samples/sec  Loss 0.8369  LearningRate 0.0914  ProxyLR: 4.5704  Epoch: 8  Global Step: 46050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:50,519-Speed 3897.13 samples/sec  Loss 0.8068  LearningRate 0.0914  ProxyLR: 4.5694  Epoch: 8  Global Step: 46060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:53,145-Speed 3900.49 samples/sec  Loss 0.7901  LearningRate 0.0914  ProxyLR: 4.5685  Epoch: 8  Global Step: 46070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:55,772-Speed 3899.10 samples/sec  Loss 0.8434  LearningRate 0.0914  ProxyLR: 4.5675  Epoch: 8  Global Step: 46080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:53:58,397-Speed 3902.00 samples/sec  Loss 0.7734  LearningRate 0.0913  ProxyLR: 4.5666  Epoch: 8  Global Step: 46090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:01,030-Speed 3888.93 samples/sec  Loss 0.8376  LearningRate 0.0913  ProxyLR: 4.5656  Epoch: 8  Global Step: 46100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:03,667-Speed 3884.79 samples/sec  Loss 0.7815  LearningRate 0.0913  ProxyLR: 4.5647  Epoch: 8  Global Step: 46110   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:06,308-Speed 3878.03 samples/sec  Loss 0.7708  LearningRate 0.0913  ProxyLR: 4.5637  Epoch: 8  Global Step: 46120   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:08,947-Speed 3880.64 samples/sec  Loss 0.7317  LearningRate 0.0913  ProxyLR: 4.5628  Epoch: 8  Global Step: 46130   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:11,588-Speed 3878.58 samples/sec  Loss 0.8248  LearningRate 0.0912  ProxyLR: 4.5618  Epoch: 8  Global Step: 46140   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:14,227-Speed 3880.87 samples/sec  Loss 0.7809  LearningRate 0.0912  ProxyLR: 4.5609  Epoch: 8  Global Step: 46150   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:16,867-Speed 3881.30 samples/sec  Loss 0.8642  LearningRate 0.0912  ProxyLR: 4.5599  Epoch: 8  Global Step: 46160   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:19,492-Speed 3901.25 samples/sec  Loss 0.8657  LearningRate 0.0912  ProxyLR: 4.5590  Epoch: 8  Global Step: 46170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:22,130-Speed 3882.05 samples/sec  Loss 0.7738  LearningRate 0.0912  ProxyLR: 4.5580  Epoch: 8  Global Step: 46180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:24,770-Speed 3880.70 samples/sec  Loss 0.8085  LearningRate 0.0911  ProxyLR: 4.5571  Epoch: 8  Global Step: 46190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:27,408-Speed 3882.51 samples/sec  Loss 0.7848  LearningRate 0.0911  ProxyLR: 4.5561  Epoch: 8  Global Step: 46200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:30,046-Speed 3881.99 samples/sec  Loss 0.8089  LearningRate 0.0911  ProxyLR: 4.5552  Epoch: 8  Global Step: 46210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:32,686-Speed 3880.60 samples/sec  Loss 0.8522  LearningRate 0.0911  ProxyLR: 4.5542  Epoch: 8  Global Step: 46220   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:35,324-Speed 3882.72 samples/sec  Loss 0.8244  LearningRate 0.0911  ProxyLR: 4.5533  Epoch: 8  Global Step: 46230   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:37,963-Speed 3881.01 samples/sec  Loss 0.8354  LearningRate 0.0910  ProxyLR: 4.5523  Epoch: 8  Global Step: 46240   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:40,603-Speed 3878.98 samples/sec  Loss 0.8545  LearningRate 0.0910  ProxyLR: 4.5514  Epoch: 8  Global Step: 46250   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:43,241-Speed 3882.71 samples/sec  Loss 0.8236  LearningRate 0.0910  ProxyLR: 4.5504  Epoch: 8  Global Step: 46260   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:45,878-Speed 3884.40 samples/sec  Loss 0.7562  LearningRate 0.0910  ProxyLR: 4.5495  Epoch: 8  Global Step: 46270   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:48,518-Speed 3879.35 samples/sec  Loss 0.8523  LearningRate 0.0910  ProxyLR: 4.5485  Epoch: 8  Global Step: 46280   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:54:51,143-Speed 3901.72 samples/sec  Loss 0.7823  LearningRate 0.0910  ProxyLR: 4.5476  Epoch: 8  Global Step: 46290   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:53,781-Speed 3883.16 samples/sec  Loss 0.8452  LearningRate 0.0909  ProxyLR: 4.5466  Epoch: 8  Global Step: 46300   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:56,417-Speed 3885.69 samples/sec  Loss 0.8615  LearningRate 0.0909  ProxyLR: 4.5457  Epoch: 8  Global Step: 46310   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:54:59,053-Speed 3885.46 samples/sec  Loss 0.8274  LearningRate 0.0909  ProxyLR: 4.5447  Epoch: 8  Global Step: 46320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:01,693-Speed 3880.09 samples/sec  Loss 0.7798  LearningRate 0.0909  ProxyLR: 4.5438  Epoch: 8  Global Step: 46330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:04,332-Speed 3881.77 samples/sec  Loss 0.8286  LearningRate 0.0909  ProxyLR: 4.5428  Epoch: 8  Global Step: 46340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:06,969-Speed 3883.65 samples/sec  Loss 0.8349  LearningRate 0.0908  ProxyLR: 4.5419  Epoch: 8  Global Step: 46350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:09,609-Speed 3879.88 samples/sec  Loss 0.8333  LearningRate 0.0908  ProxyLR: 4.5409  Epoch: 8  Global Step: 46360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:12,245-Speed 3885.32 samples/sec  Loss 0.8502  LearningRate 0.0908  ProxyLR: 4.5400  Epoch: 8  Global Step: 46370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:14,883-Speed 3882.63 samples/sec  Loss 0.8957  LearningRate 0.0908  ProxyLR: 4.5391  Epoch: 8  Global Step: 46380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:17,498-Speed 3917.00 samples/sec  Loss 0.8661  LearningRate 0.0908  ProxyLR: 4.5381  Epoch: 8  Global Step: 46390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:20,124-Speed 3900.14 samples/sec  Loss 0.8021  LearningRate 0.0907  ProxyLR: 4.5372  Epoch: 8  Global Step: 46400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:22,751-Speed 3898.50 samples/sec  Loss 0.8477  LearningRate 0.0907  ProxyLR: 4.5362  Epoch: 8  Global Step: 46410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:25,378-Speed 3899.95 samples/sec  Loss 0.8608  LearningRate 0.0907  ProxyLR: 4.5353  Epoch: 8  Global Step: 46420   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:28,006-Speed 3897.57 samples/sec  Loss 0.8412  LearningRate 0.0907  ProxyLR: 4.5343  Epoch: 8  Global Step: 46430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:30,631-Speed 3900.94 samples/sec  Loss 0.8407  LearningRate 0.0907  ProxyLR: 4.5334  Epoch: 8  Global Step: 46440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:33,261-Speed 3895.40 samples/sec  Loss 0.8499  LearningRate 0.0906  ProxyLR: 4.5324  Epoch: 8  Global Step: 46450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:35,888-Speed 3898.62 samples/sec  Loss 0.8047  LearningRate 0.0906  ProxyLR: 4.5315  Epoch: 8  Global Step: 46460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:38,513-Speed 3901.34 samples/sec  Loss 0.7827  LearningRate 0.0906  ProxyLR: 4.5305  Epoch: 8  Global Step: 46470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:41,140-Speed 3899.83 samples/sec  Loss 0.8799  LearningRate 0.0906  ProxyLR: 4.5296  Epoch: 8  Global Step: 46480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:43,767-Speed 3898.08 samples/sec  Loss 0.8941  LearningRate 0.0906  ProxyLR: 4.5286  Epoch: 8  Global Step: 46490   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:55:46,381-Speed 3918.73 samples/sec  Loss 0.8928  LearningRate 0.0906  ProxyLR: 4.5277  Epoch: 8  Global Step: 46500   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:49,006-Speed 3901.05 samples/sec  Loss 0.8609  LearningRate 0.0905  ProxyLR: 4.5267  Epoch: 8  Global Step: 46510   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:51,633-Speed 3900.10 samples/sec  Loss 0.8556  LearningRate 0.0905  ProxyLR: 4.5258  Epoch: 8  Global Step: 46520   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:54,258-Speed 3900.58 samples/sec  Loss 0.8597  LearningRate 0.0905  ProxyLR: 4.5248  Epoch: 8  Global Step: 46530   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:56,885-Speed 3900.28 samples/sec  Loss 0.8739  LearningRate 0.0905  ProxyLR: 4.5239  Epoch: 8  Global Step: 46540   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:55:59,511-Speed 3899.04 samples/sec  Loss 0.8333  LearningRate 0.0905  ProxyLR: 4.5230  Epoch: 8  Global Step: 46550   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:02,138-Speed 3898.96 samples/sec  Loss 0.8311  LearningRate 0.0904  ProxyLR: 4.5220  Epoch: 8  Global Step: 46560   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:04,766-Speed 3898.61 samples/sec  Loss 0.8562  LearningRate 0.0904  ProxyLR: 4.5211  Epoch: 8  Global Step: 46570   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:07,392-Speed 3899.11 samples/sec  Loss 0.7997  LearningRate 0.0904  ProxyLR: 4.5201  Epoch: 8  Global Step: 46580   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:10,019-Speed 3900.14 samples/sec  Loss 0.8772  LearningRate 0.0904  ProxyLR: 4.5192  Epoch: 8  Global Step: 46590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:12,645-Speed 3900.24 samples/sec  Loss 0.9041  LearningRate 0.0904  ProxyLR: 4.5182  Epoch: 8  Global Step: 46600   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:56:15,271-Speed 3900.20 samples/sec  Loss 0.9055  LearningRate 0.0903  ProxyLR: 4.5173  Epoch: 8  Global Step: 46610   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:56:17,900-Speed 3895.80 samples/sec  Loss 0.8300  LearningRate 0.0903  ProxyLR: 4.5163  Epoch: 8  Global Step: 46620   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:56:20,527-Speed 3899.58 samples/sec  Loss 0.8765  LearningRate 0.0903  ProxyLR: 4.5154  Epoch: 8  Global Step: 46630   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:56:23,142-Speed 3916.35 samples/sec  Loss 0.8514  LearningRate 0.0903  ProxyLR: 4.5144  Epoch: 8  Global Step: 46640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:25,768-Speed 3900.85 samples/sec  Loss 0.8563  LearningRate 0.0903  ProxyLR: 4.5135  Epoch: 8  Global Step: 46650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:28,396-Speed 3897.09 samples/sec  Loss 0.9132  LearningRate 0.0903  ProxyLR: 4.5126  Epoch: 8  Global Step: 46660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:31,021-Speed 3901.80 samples/sec  Loss 0.8316  LearningRate 0.0902  ProxyLR: 4.5116  Epoch: 8  Global Step: 46670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:33,649-Speed 3897.40 samples/sec  Loss 0.8468  LearningRate 0.0902  ProxyLR: 4.5107  Epoch: 8  Global Step: 46680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:36,275-Speed 3899.75 samples/sec  Loss 0.8471  LearningRate 0.0902  ProxyLR: 4.5097  Epoch: 8  Global Step: 46690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:38,903-Speed 3898.05 samples/sec  Loss 0.8763  LearningRate 0.0902  ProxyLR: 4.5088  Epoch: 8  Global Step: 46700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:41,532-Speed 3895.64 samples/sec  Loss 0.8293  LearningRate 0.0902  ProxyLR: 4.5078  Epoch: 8  Global Step: 46710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:44,159-Speed 3899.19 samples/sec  Loss 0.8401  LearningRate 0.0901  ProxyLR: 4.5069  Epoch: 8  Global Step: 46720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:46,786-Speed 3898.57 samples/sec  Loss 0.8940  LearningRate 0.0901  ProxyLR: 4.5059  Epoch: 8  Global Step: 46730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:49,413-Speed 3899.72 samples/sec  Loss 0.8587  LearningRate 0.0901  ProxyLR: 4.5050  Epoch: 8  Global Step: 46740   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:56:52,026-Speed 3919.91 samples/sec  Loss 0.9195  LearningRate 0.0901  ProxyLR: 4.5040  Epoch: 8  Global Step: 46750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:54,653-Speed 3898.40 samples/sec  Loss 0.8801  LearningRate 0.0901  ProxyLR: 4.5031  Epoch: 8  Global Step: 46760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:57,279-Speed 3900.06 samples/sec  Loss 0.8558  LearningRate 0.0900  ProxyLR: 4.5022  Epoch: 8  Global Step: 46770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:56:59,908-Speed 3896.78 samples/sec  Loss 0.8929  LearningRate 0.0900  ProxyLR: 4.5012  Epoch: 8  Global Step: 46780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:02,534-Speed 3900.44 samples/sec  Loss 0.9157  LearningRate 0.0900  ProxyLR: 4.5003  Epoch: 8  Global Step: 46790   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:05,149-Speed 3916.99 samples/sec  Loss 0.9190  LearningRate 0.0900  ProxyLR: 4.4993  Epoch: 8  Global Step: 46800   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:07,776-Speed 3898.09 samples/sec  Loss 0.8649  LearningRate 0.0900  ProxyLR: 4.4984  Epoch: 8  Global Step: 46810   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:10,402-Speed 3900.27 samples/sec  Loss 0.8723  LearningRate 0.0899  ProxyLR: 4.4974  Epoch: 8  Global Step: 46820   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:13,029-Speed 3899.21 samples/sec  Loss 0.8465  LearningRate 0.0899  ProxyLR: 4.4965  Epoch: 8  Global Step: 46830   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:15,656-Speed 3899.90 samples/sec  Loss 0.8913  LearningRate 0.0899  ProxyLR: 4.4956  Epoch: 8  Global Step: 46840   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:18,281-Speed 3900.51 samples/sec  Loss 0.8720  LearningRate 0.0899  ProxyLR: 4.4946  Epoch: 8  Global Step: 46850   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:20,909-Speed 3898.38 samples/sec  Loss 0.9421  LearningRate 0.0899  ProxyLR: 4.4937  Epoch: 8  Global Step: 46860   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:23,536-Speed 3899.39 samples/sec  Loss 0.9218  LearningRate 0.0899  ProxyLR: 4.4927  Epoch: 8  Global Step: 46870   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:26,162-Speed 3899.32 samples/sec  Loss 0.9175  LearningRate 0.0898  ProxyLR: 4.4918  Epoch: 8  Global Step: 46880   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:28,788-Speed 3900.60 samples/sec  Loss 0.8803  LearningRate 0.0898  ProxyLR: 4.4908  Epoch: 8  Global Step: 46890   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:57:31,415-Speed 3898.64 samples/sec  Loss 0.8599  LearningRate 0.0898  ProxyLR: 4.4899  Epoch: 8  Global Step: 46900   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:34,042-Speed 3899.10 samples/sec  Loss 0.8930  LearningRate 0.0898  ProxyLR: 4.4890  Epoch: 8  Global Step: 46910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:36,667-Speed 3901.98 samples/sec  Loss 0.9015  LearningRate 0.0898  ProxyLR: 4.4880  Epoch: 8  Global Step: 46920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:39,293-Speed 3900.63 samples/sec  Loss 0.9167  LearningRate 0.0897  ProxyLR: 4.4871  Epoch: 8  Global Step: 46930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:41,917-Speed 3903.97 samples/sec  Loss 0.8760  LearningRate 0.0897  ProxyLR: 4.4861  Epoch: 8  Global Step: 46940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:44,543-Speed 3900.15 samples/sec  Loss 0.9646  LearningRate 0.0897  ProxyLR: 4.4852  Epoch: 8  Global Step: 46950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:47,169-Speed 3900.14 samples/sec  Loss 0.9697  LearningRate 0.0897  ProxyLR: 4.4842  Epoch: 8  Global Step: 46960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:49,794-Speed 3901.18 samples/sec  Loss 0.8601  LearningRate 0.0897  ProxyLR: 4.4833  Epoch: 8  Global Step: 46970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:52,420-Speed 3900.65 samples/sec  Loss 0.9152  LearningRate 0.0896  ProxyLR: 4.4824  Epoch: 8  Global Step: 46980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:55,049-Speed 3896.91 samples/sec  Loss 0.9846  LearningRate 0.0896  ProxyLR: 4.4814  Epoch: 8  Global Step: 46990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:57:57,676-Speed 3899.05 samples/sec  Loss 0.9099  LearningRate 0.0896  ProxyLR: 4.4805  Epoch: 8  Global Step: 47000   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:58:00,304-Speed 3897.05 samples/sec  Loss 0.9315  LearningRate 0.0896  ProxyLR: 4.4795  Epoch: 8  Global Step: 47010   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:58:02,932-Speed 3897.90 samples/sec  Loss 0.8920  LearningRate 0.0896  ProxyLR: 4.4786  Epoch: 8  Global Step: 47020   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:58:05,544-Speed 3920.46 samples/sec  Loss 0.9344  LearningRate 0.0896  ProxyLR: 4.4776  Epoch: 8  Global Step: 47030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:08,170-Speed 3901.14 samples/sec  Loss 0.9207  LearningRate 0.0895  ProxyLR: 4.4767  Epoch: 8  Global Step: 47040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:10,796-Speed 3900.20 samples/sec  Loss 0.9520  LearningRate 0.0895  ProxyLR: 4.4758  Epoch: 8  Global Step: 47050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:13,423-Speed 3897.98 samples/sec  Loss 1.0032  LearningRate 0.0895  ProxyLR: 4.4748  Epoch: 8  Global Step: 47060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:16,051-Speed 3898.17 samples/sec  Loss 0.8927  LearningRate 0.0895  ProxyLR: 4.4739  Epoch: 8  Global Step: 47070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:18,677-Speed 3900.17 samples/sec  Loss 1.0019  LearningRate 0.0895  ProxyLR: 4.4729  Epoch: 8  Global Step: 47080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:21,305-Speed 3896.88 samples/sec  Loss 0.9701  LearningRate 0.0894  ProxyLR: 4.4720  Epoch: 8  Global Step: 47090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:23,933-Speed 3898.04 samples/sec  Loss 0.9431  LearningRate 0.0894  ProxyLR: 4.4711  Epoch: 8  Global Step: 47100   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:26,561-Speed 3898.07 samples/sec  Loss 0.9741  LearningRate 0.0894  ProxyLR: 4.4701  Epoch: 8  Global Step: 47110   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:29,188-Speed 3898.31 samples/sec  Loss 0.9548  LearningRate 0.0894  ProxyLR: 4.4692  Epoch: 8  Global Step: 47120   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:31,817-Speed 3896.72 samples/sec  Loss 0.9286  LearningRate 0.0894  ProxyLR: 4.4682  Epoch: 8  Global Step: 47130   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:58:34,431-Speed 3918.27 samples/sec  Loss 0.9512  LearningRate 0.0893  ProxyLR: 4.4673  Epoch: 8  Global Step: 47140   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:37,058-Speed 3897.99 samples/sec  Loss 0.9317  LearningRate 0.0893  ProxyLR: 4.4664  Epoch: 8  Global Step: 47150   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:39,686-Speed 3897.62 samples/sec  Loss 0.9778  LearningRate 0.0893  ProxyLR: 4.4654  Epoch: 8  Global Step: 47160   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:42,314-Speed 3898.31 samples/sec  Loss 0.9311  LearningRate 0.0893  ProxyLR: 4.4645  Epoch: 8  Global Step: 47170   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:44,940-Speed 3899.15 samples/sec  Loss 0.8823  LearningRate 0.0893  ProxyLR: 4.4635  Epoch: 8  Global Step: 47180   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:47,567-Speed 3899.61 samples/sec  Loss 0.8920  LearningRate 0.0893  ProxyLR: 4.4626  Epoch: 8  Global Step: 47190   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:50,194-Speed 3898.83 samples/sec  Loss 0.9470  LearningRate 0.0892  ProxyLR: 4.4617  Epoch: 8  Global Step: 47200   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:52,821-Speed 3899.07 samples/sec  Loss 0.9291  LearningRate 0.0892  ProxyLR: 4.4607  Epoch: 8  Global Step: 47210   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:58:55,433-Speed 3921.22 samples/sec  Loss 0.9146  LearningRate 0.0892  ProxyLR: 4.4598  Epoch: 8  Global Step: 47220   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:58:58,061-Speed 3897.76 samples/sec  Loss 0.9351  LearningRate 0.0892  ProxyLR: 4.4588  Epoch: 8  Global Step: 47230   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:00,686-Speed 3901.84 samples/sec  Loss 0.8958  LearningRate 0.0892  ProxyLR: 4.4579  Epoch: 8  Global Step: 47240   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:03,313-Speed 3899.04 samples/sec  Loss 0.9704  LearningRate 0.0891  ProxyLR: 4.4570  Epoch: 8  Global Step: 47250   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:05,941-Speed 3897.74 samples/sec  Loss 0.9002  LearningRate 0.0891  ProxyLR: 4.4560  Epoch: 8  Global Step: 47260   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:08,567-Speed 3899.47 samples/sec  Loss 0.9109  LearningRate 0.0891  ProxyLR: 4.4551  Epoch: 8  Global Step: 47270   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:11,195-Speed 3898.31 samples/sec  Loss 0.9281  LearningRate 0.0891  ProxyLR: 4.4541  Epoch: 8  Global Step: 47280   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:13,819-Speed 3903.18 samples/sec  Loss 0.9507  LearningRate 0.0891  ProxyLR: 4.4532  Epoch: 8  Global Step: 47290   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:16,447-Speed 3897.70 samples/sec  Loss 1.0224  LearningRate 0.0890  ProxyLR: 4.4523  Epoch: 8  Global Step: 47300   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:19,072-Speed 3901.98 samples/sec  Loss 0.9412  LearningRate 0.0890  ProxyLR: 4.4513  Epoch: 8  Global Step: 47310   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 17:59:21,698-Speed 3899.56 samples/sec  Loss 0.9623  LearningRate 0.0890  ProxyLR: 4.4504  Epoch: 8  Global Step: 47320   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:24,325-Speed 3899.48 samples/sec  Loss 0.9828  LearningRate 0.0890  ProxyLR: 4.4494  Epoch: 8  Global Step: 47330   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:26,952-Speed 3898.75 samples/sec  Loss 0.9521  LearningRate 0.0890  ProxyLR: 4.4485  Epoch: 8  Global Step: 47340   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:29,578-Speed 3899.54 samples/sec  Loss 0.9720  LearningRate 0.0890  ProxyLR: 4.4476  Epoch: 8  Global Step: 47350   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:32,205-Speed 3899.05 samples/sec  Loss 1.0107  LearningRate 0.0889  ProxyLR: 4.4466  Epoch: 8  Global Step: 47360   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:34,834-Speed 3897.23 samples/sec  Loss 0.9773  LearningRate 0.0889  ProxyLR: 4.4457  Epoch: 8  Global Step: 47370   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:37,462-Speed 3897.38 samples/sec  Loss 0.9639  LearningRate 0.0889  ProxyLR: 4.4448  Epoch: 8  Global Step: 47380   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:40,089-Speed 3898.55 samples/sec  Loss 1.0017  LearningRate 0.0889  ProxyLR: 4.4438  Epoch: 8  Global Step: 47390   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:42,714-Speed 3901.11 samples/sec  Loss 1.0575  LearningRate 0.0889  ProxyLR: 4.4429  Epoch: 8  Global Step: 47400   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:45,341-Speed 3899.67 samples/sec  Loss 0.9736  LearningRate 0.0888  ProxyLR: 4.4419  Epoch: 8  Global Step: 47410   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:47,969-Speed 3897.28 samples/sec  Loss 0.9555  LearningRate 0.0888  ProxyLR: 4.4410  Epoch: 8  Global Step: 47420   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 17:59:50,584-Speed 3916.25 samples/sec  Loss 0.9530  LearningRate 0.0888  ProxyLR: 4.4401  Epoch: 8  Global Step: 47430   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:53,212-Speed 3897.44 samples/sec  Loss 0.9888  LearningRate 0.0888  ProxyLR: 4.4391  Epoch: 8  Global Step: 47440   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:55,838-Speed 3901.11 samples/sec  Loss 0.9641  LearningRate 0.0888  ProxyLR: 4.4382  Epoch: 8  Global Step: 47450   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 17:59:58,468-Speed 3893.91 samples/sec  Loss 0.9666  LearningRate 0.0887  ProxyLR: 4.4373  Epoch: 8  Global Step: 47460   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:01,097-Speed 3895.96 samples/sec  Loss 0.9680  LearningRate 0.0887  ProxyLR: 4.4363  Epoch: 8  Global Step: 47470   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:03,727-Speed 3895.04 samples/sec  Loss 1.0185  LearningRate 0.0887  ProxyLR: 4.4354  Epoch: 8  Global Step: 47480   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:06,341-Speed 3918.31 samples/sec  Loss 1.0169  LearningRate 0.0887  ProxyLR: 4.4344  Epoch: 8  Global Step: 47490   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:08,968-Speed 3898.76 samples/sec  Loss 0.9835  LearningRate 0.0887  ProxyLR: 4.4335  Epoch: 8  Global Step: 47500   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:11,595-Speed 3898.12 samples/sec  Loss 1.0056  LearningRate 0.0887  ProxyLR: 4.4326  Epoch: 8  Global Step: 47510   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:14,222-Speed 3899.12 samples/sec  Loss 0.9593  LearningRate 0.0886  ProxyLR: 4.4316  Epoch: 8  Global Step: 47520   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:16,850-Speed 3897.27 samples/sec  Loss 0.9861  LearningRate 0.0886  ProxyLR: 4.4307  Epoch: 8  Global Step: 47530   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:19,478-Speed 3897.81 samples/sec  Loss 0.9626  LearningRate 0.0886  ProxyLR: 4.4298  Epoch: 8  Global Step: 47540   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:22,105-Speed 3899.22 samples/sec  Loss 0.9886  LearningRate 0.0886  ProxyLR: 4.4288  Epoch: 8  Global Step: 47550   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:24,732-Speed 3898.39 samples/sec  Loss 1.0498  LearningRate 0.0886  ProxyLR: 4.4279  Epoch: 8  Global Step: 47560   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:27,360-Speed 3897.35 samples/sec  Loss 1.0014  LearningRate 0.0885  ProxyLR: 4.4270  Epoch: 8  Global Step: 47570   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:29,988-Speed 3898.46 samples/sec  Loss 0.9754  LearningRate 0.0885  ProxyLR: 4.4260  Epoch: 8  Global Step: 47580   Fp16 Grad Scale: 262144  Required: 8 hours
Training: 2023-05-04 18:00:32,617-Speed 3896.42 samples/sec  Loss 1.0403  LearningRate 0.0885  ProxyLR: 4.4251  Epoch: 8  Global Step: 47590   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:35,245-Speed 3896.54 samples/sec  Loss 0.9362  LearningRate 0.0885  ProxyLR: 4.4241  Epoch: 8  Global Step: 47600   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:37,874-Speed 3896.59 samples/sec  Loss 1.0623  LearningRate 0.0885  ProxyLR: 4.4232  Epoch: 8  Global Step: 47610   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:40,502-Speed 3896.57 samples/sec  Loss 1.0621  LearningRate 0.0884  ProxyLR: 4.4223  Epoch: 8  Global Step: 47620   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:43,131-Speed 3897.27 samples/sec  Loss 1.0271  LearningRate 0.0884  ProxyLR: 4.4213  Epoch: 8  Global Step: 47630   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:45,759-Speed 3896.90 samples/sec  Loss 1.0205  LearningRate 0.0884  ProxyLR: 4.4204  Epoch: 8  Global Step: 47640   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:48,387-Speed 3896.46 samples/sec  Loss 1.0129  LearningRate 0.0884  ProxyLR: 4.4195  Epoch: 8  Global Step: 47650   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:51,016-Speed 3896.54 samples/sec  Loss 1.0149  LearningRate 0.0884  ProxyLR: 4.4185  Epoch: 8  Global Step: 47660   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:53,646-Speed 3894.81 samples/sec  Loss 0.9912  LearningRate 0.0884  ProxyLR: 4.4176  Epoch: 8  Global Step: 47670   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:56,274-Speed 3896.86 samples/sec  Loss 0.9283  LearningRate 0.0883  ProxyLR: 4.4167  Epoch: 8  Global Step: 47680   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:00:59,125-Speed 3591.98 samples/sec  Loss 0.9991  LearningRate 0.0883  ProxyLR: 4.4157  Epoch: 8  Global Step: 47690   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:01,753-Speed 3897.33 samples/sec  Loss 1.0278  LearningRate 0.0883  ProxyLR: 4.4148  Epoch: 8  Global Step: 47700   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:04,382-Speed 3896.37 samples/sec  Loss 1.0789  LearningRate 0.0883  ProxyLR: 4.4139  Epoch: 8  Global Step: 47710   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:07,012-Speed 3895.23 samples/sec  Loss 0.9599  LearningRate 0.0883  ProxyLR: 4.4129  Epoch: 8  Global Step: 47720   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:09,639-Speed 3897.46 samples/sec  Loss 0.9740  LearningRate 0.0882  ProxyLR: 4.4120  Epoch: 8  Global Step: 47730   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:12,268-Speed 3896.06 samples/sec  Loss 1.0112  LearningRate 0.0882  ProxyLR: 4.4111  Epoch: 8  Global Step: 47740   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:14,897-Speed 3896.00 samples/sec  Loss 1.0233  LearningRate 0.0882  ProxyLR: 4.4101  Epoch: 8  Global Step: 47750   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:17,526-Speed 3896.63 samples/sec  Loss 1.0170  LearningRate 0.0882  ProxyLR: 4.4092  Epoch: 8  Global Step: 47760   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:20,152-Speed 3899.60 samples/sec  Loss 1.0323  LearningRate 0.0882  ProxyLR: 4.4083  Epoch: 8  Global Step: 47770   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:22,781-Speed 3896.37 samples/sec  Loss 1.0900  LearningRate 0.0881  ProxyLR: 4.4073  Epoch: 8  Global Step: 47780   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:25,408-Speed 3899.12 samples/sec  Loss 0.9786  LearningRate 0.0881  ProxyLR: 4.4064  Epoch: 8  Global Step: 47790   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 18:01:28,021-Speed 3920.72 samples/sec  Loss 0.9705  LearningRate 0.0881  ProxyLR: 4.4054  Epoch: 8  Global Step: 47800   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:30,646-Speed 3901.57 samples/sec  Loss 1.0084  LearningRate 0.0881  ProxyLR: 4.4045  Epoch: 8  Global Step: 47810   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:33,271-Speed 3901.38 samples/sec  Loss 0.9429  LearningRate 0.0881  ProxyLR: 4.4036  Epoch: 8  Global Step: 47820   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:35,897-Speed 3900.78 samples/sec  Loss 0.9476  LearningRate 0.0881  ProxyLR: 4.4026  Epoch: 8  Global Step: 47830   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:38,523-Speed 3900.17 samples/sec  Loss 1.0480  LearningRate 0.0880  ProxyLR: 4.4017  Epoch: 8  Global Step: 47840   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:41,150-Speed 3899.54 samples/sec  Loss 1.0264  LearningRate 0.0880  ProxyLR: 4.4008  Epoch: 8  Global Step: 47850   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:43,775-Speed 3901.40 samples/sec  Loss 1.0577  LearningRate 0.0880  ProxyLR: 4.3998  Epoch: 8  Global Step: 47860   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:46,401-Speed 3900.19 samples/sec  Loss 0.9902  LearningRate 0.0880  ProxyLR: 4.3989  Epoch: 8  Global Step: 47870   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:49,027-Speed 3900.72 samples/sec  Loss 0.9777  LearningRate 0.0880  ProxyLR: 4.3980  Epoch: 8  Global Step: 47880   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:51,653-Speed 3901.07 samples/sec  Loss 0.9975  LearningRate 0.0879  ProxyLR: 4.3970  Epoch: 8  Global Step: 47890   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:54,280-Speed 3898.21 samples/sec  Loss 0.9901  LearningRate 0.0879  ProxyLR: 4.3961  Epoch: 8  Global Step: 47900   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 18:01:57,132-Speed 3591.64 samples/sec  Loss 1.0590  LearningRate 0.0879  ProxyLR: 4.3952  Epoch: 8  Global Step: 47910   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:01:59,756-Speed 3903.70 samples/sec  Loss 1.0347  LearningRate 0.0879  ProxyLR: 4.3942  Epoch: 8  Global Step: 47920   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:02,381-Speed 3902.46 samples/sec  Loss 1.0035  LearningRate 0.0879  ProxyLR: 4.3933  Epoch: 8  Global Step: 47930   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:05,006-Speed 3901.24 samples/sec  Loss 1.0378  LearningRate 0.0878  ProxyLR: 4.3924  Epoch: 8  Global Step: 47940   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:07,631-Speed 3902.08 samples/sec  Loss 0.9875  LearningRate 0.0878  ProxyLR: 4.3915  Epoch: 8  Global Step: 47950   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:10,256-Speed 3901.69 samples/sec  Loss 1.0240  LearningRate 0.0878  ProxyLR: 4.3905  Epoch: 8  Global Step: 47960   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:12,880-Speed 3902.63 samples/sec  Loss 0.9697  LearningRate 0.0878  ProxyLR: 4.3896  Epoch: 8  Global Step: 47970   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:15,507-Speed 3899.43 samples/sec  Loss 0.9933  LearningRate 0.0878  ProxyLR: 4.3887  Epoch: 8  Global Step: 47980   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:18,132-Speed 3902.03 samples/sec  Loss 1.0236  LearningRate 0.0878  ProxyLR: 4.3877  Epoch: 8  Global Step: 47990   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:20,758-Speed 3900.80 samples/sec  Loss 1.0215  LearningRate 0.0877  ProxyLR: 4.3868  Epoch: 8  Global Step: 48000   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:23,382-Speed 3903.49 samples/sec  Loss 1.0332  LearningRate 0.0877  ProxyLR: 4.3859  Epoch: 8  Global Step: 48010   Fp16 Grad Scale: 1048576  Required: 8 hours
Training: 2023-05-04 18:02:25,994-Speed 3922.08 samples/sec  Loss 1.1050  LearningRate 0.0877  ProxyLR: 4.3849  Epoch: 8  Global Step: 48020   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:28,618-Speed 3902.61 samples/sec  Loss 1.0045  LearningRate 0.0877  ProxyLR: 4.3840  Epoch: 8  Global Step: 48030   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:31,243-Speed 3902.72 samples/sec  Loss 0.9568  LearningRate 0.0877  ProxyLR: 4.3831  Epoch: 8  Global Step: 48040   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:33,867-Speed 3902.42 samples/sec  Loss 0.9963  LearningRate 0.0876  ProxyLR: 4.3821  Epoch: 8  Global Step: 48050   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:36,499-Speed 3891.75 samples/sec  Loss 1.0608  LearningRate 0.0876  ProxyLR: 4.3812  Epoch: 8  Global Step: 48060   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:39,131-Speed 3891.62 samples/sec  Loss 1.0504  LearningRate 0.0876  ProxyLR: 4.3803  Epoch: 8  Global Step: 48070   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:41,761-Speed 3894.13 samples/sec  Loss 1.0823  LearningRate 0.0876  ProxyLR: 4.3793  Epoch: 8  Global Step: 48080   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:44,392-Speed 3893.13 samples/sec  Loss 0.9811  LearningRate 0.0876  ProxyLR: 4.3784  Epoch: 8  Global Step: 48090   Fp16 Grad Scale: 524288  Required: 8 hours
Training: 2023-05-04 18:02:47,008-Speed 3915.91 samples/sec  Loss 1.0255  LearningRate 0.0875  ProxyLR: 4.3775  Epoch: 8  Global Step: 48100   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:02:49,637-Speed 3895.64 samples/sec  Loss 1.0716  LearningRate 0.0875  ProxyLR: 4.3765  Epoch: 8  Global Step: 48110   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:02:52,266-Speed 3896.34 samples/sec  Loss 1.0676  LearningRate 0.0875  ProxyLR: 4.3756  Epoch: 8  Global Step: 48120   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:02:54,894-Speed 3897.75 samples/sec  Loss 1.0983  LearningRate 0.0875  ProxyLR: 4.3747  Epoch: 8  Global Step: 48130   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:02:57,523-Speed 3894.90 samples/sec  Loss 1.1153  LearningRate 0.0875  ProxyLR: 4.3738  Epoch: 8  Global Step: 48140   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:00,154-Speed 3894.18 samples/sec  Loss 1.0291  LearningRate 0.0875  ProxyLR: 4.3728  Epoch: 8  Global Step: 48150   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:02,782-Speed 3896.28 samples/sec  Loss 1.0595  LearningRate 0.0874  ProxyLR: 4.3719  Epoch: 8  Global Step: 48160   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:05,410-Speed 3897.24 samples/sec  Loss 0.9811  LearningRate 0.0874  ProxyLR: 4.3710  Epoch: 8  Global Step: 48170   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:08,040-Speed 3896.04 samples/sec  Loss 1.0971  LearningRate 0.0874  ProxyLR: 4.3700  Epoch: 8  Global Step: 48180   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:10,671-Speed 3893.33 samples/sec  Loss 1.0916  LearningRate 0.0874  ProxyLR: 4.3691  Epoch: 8  Global Step: 48190   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:13,299-Speed 3897.36 samples/sec  Loss 1.0292  LearningRate 0.0874  ProxyLR: 4.3682  Epoch: 8  Global Step: 48200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:15,929-Speed 3894.30 samples/sec  Loss 1.0670  LearningRate 0.0873  ProxyLR: 4.3672  Epoch: 8  Global Step: 48210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:18,558-Speed 3894.98 samples/sec  Loss 1.0939  LearningRate 0.0873  ProxyLR: 4.3663  Epoch: 8  Global Step: 48220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:21,185-Speed 3898.84 samples/sec  Loss 1.0603  LearningRate 0.0873  ProxyLR: 4.3654  Epoch: 8  Global Step: 48230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:23,799-Speed 3919.01 samples/sec  Loss 1.0408  LearningRate 0.0873  ProxyLR: 4.3645  Epoch: 8  Global Step: 48240   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:26,426-Speed 3899.53 samples/sec  Loss 1.0468  LearningRate 0.0873  ProxyLR: 4.3635  Epoch: 8  Global Step: 48250   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:29,053-Speed 3898.75 samples/sec  Loss 1.0370  LearningRate 0.0873  ProxyLR: 4.3626  Epoch: 8  Global Step: 48260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:31,680-Speed 3899.13 samples/sec  Loss 1.0375  LearningRate 0.0872  ProxyLR: 4.3617  Epoch: 8  Global Step: 48270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:34,306-Speed 3900.68 samples/sec  Loss 0.9928  LearningRate 0.0872  ProxyLR: 4.3607  Epoch: 8  Global Step: 48280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:36,932-Speed 3900.27 samples/sec  Loss 1.0373  LearningRate 0.0872  ProxyLR: 4.3598  Epoch: 8  Global Step: 48290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:39,559-Speed 3898.59 samples/sec  Loss 1.0436  LearningRate 0.0872  ProxyLR: 4.3589  Epoch: 8  Global Step: 48300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:42,190-Speed 3892.99 samples/sec  Loss 1.0236  LearningRate 0.0872  ProxyLR: 4.3580  Epoch: 8  Global Step: 48310   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:44,823-Speed 3889.33 samples/sec  Loss 1.0585  LearningRate 0.0871  ProxyLR: 4.3570  Epoch: 8  Global Step: 48320   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:47,457-Speed 3889.52 samples/sec  Loss 1.1272  LearningRate 0.0871  ProxyLR: 4.3561  Epoch: 8  Global Step: 48330   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:03:50,091-Speed 3888.83 samples/sec  Loss 1.0210  LearningRate 0.0871  ProxyLR: 4.3552  Epoch: 8  Global Step: 48340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:52,726-Speed 3886.84 samples/sec  Loss 1.0752  LearningRate 0.0871  ProxyLR: 4.3542  Epoch: 8  Global Step: 48350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:55,356-Speed 3893.56 samples/sec  Loss 1.0030  LearningRate 0.0871  ProxyLR: 4.3533  Epoch: 8  Global Step: 48360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:03:57,986-Speed 3895.70 samples/sec  Loss 1.1259  LearningRate 0.0870  ProxyLR: 4.3524  Epoch: 8  Global Step: 48370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:00,615-Speed 3895.40 samples/sec  Loss 1.0786  LearningRate 0.0870  ProxyLR: 4.3515  Epoch: 8  Global Step: 48380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:03,247-Speed 3891.76 samples/sec  Loss 1.0786  LearningRate 0.0870  ProxyLR: 4.3505  Epoch: 8  Global Step: 48390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:05,876-Speed 3895.44 samples/sec  Loss 1.1055  LearningRate 0.0870  ProxyLR: 4.3496  Epoch: 8  Global Step: 48400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:08,508-Speed 3892.78 samples/sec  Loss 1.1319  LearningRate 0.0870  ProxyLR: 4.3487  Epoch: 8  Global Step: 48410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:11,384-Speed 3560.42 samples/sec  Loss 0.9968  LearningRate 0.0870  ProxyLR: 4.3477  Epoch: 8  Global Step: 48420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:14,018-Speed 3888.25 samples/sec  Loss 1.0820  LearningRate 0.0869  ProxyLR: 4.3468  Epoch: 8  Global Step: 48430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:16,639-Speed 3908.92 samples/sec  Loss 1.0589  LearningRate 0.0869  ProxyLR: 4.3459  Epoch: 8  Global Step: 48440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:19,272-Speed 3889.42 samples/sec  Loss 1.1049  LearningRate 0.0869  ProxyLR: 4.3450  Epoch: 8  Global Step: 48450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:21,905-Speed 3890.05 samples/sec  Loss 1.0552  LearningRate 0.0869  ProxyLR: 4.3440  Epoch: 8  Global Step: 48460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:24,537-Speed 3890.89 samples/sec  Loss 1.0562  LearningRate 0.0869  ProxyLR: 4.3431  Epoch: 8  Global Step: 48470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:27,171-Speed 3888.50 samples/sec  Loss 1.0827  LearningRate 0.0868  ProxyLR: 4.3422  Epoch: 8  Global Step: 48480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:29,806-Speed 3887.59 samples/sec  Loss 1.0625  LearningRate 0.0868  ProxyLR: 4.3412  Epoch: 8  Global Step: 48490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:32,439-Speed 3889.77 samples/sec  Loss 1.0892  LearningRate 0.0868  ProxyLR: 4.3403  Epoch: 8  Global Step: 48500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:35,075-Speed 3886.16 samples/sec  Loss 1.0857  LearningRate 0.0868  ProxyLR: 4.3394  Epoch: 8  Global Step: 48510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:37,709-Speed 3888.04 samples/sec  Loss 1.0708  LearningRate 0.0868  ProxyLR: 4.3385  Epoch: 8  Global Step: 48520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:40,344-Speed 3886.98 samples/sec  Loss 1.0558  LearningRate 0.0868  ProxyLR: 4.3375  Epoch: 8  Global Step: 48530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:42,968-Speed 3904.63 samples/sec  Loss 1.0839  LearningRate 0.0867  ProxyLR: 4.3366  Epoch: 8  Global Step: 48540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:45,600-Speed 3891.61 samples/sec  Loss 1.0593  LearningRate 0.0867  ProxyLR: 4.3357  Epoch: 8  Global Step: 48550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:48,234-Speed 3887.89 samples/sec  Loss 1.0485  LearningRate 0.0867  ProxyLR: 4.3348  Epoch: 8  Global Step: 48560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:50,868-Speed 3888.26 samples/sec  Loss 1.0977  LearningRate 0.0867  ProxyLR: 4.3338  Epoch: 8  Global Step: 48570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:53,505-Speed 3883.74 samples/sec  Loss 1.0950  LearningRate 0.0867  ProxyLR: 4.3329  Epoch: 8  Global Step: 48580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:56,141-Speed 3886.33 samples/sec  Loss 1.1273  LearningRate 0.0866  ProxyLR: 4.3320  Epoch: 8  Global Step: 48590   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:04:58,777-Speed 3886.42 samples/sec  Loss 1.0394  LearningRate 0.0866  ProxyLR: 4.3311  Epoch: 8  Global Step: 48600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:01,407-Speed 3894.06 samples/sec  Loss 1.0261  LearningRate 0.0866  ProxyLR: 4.3301  Epoch: 8  Global Step: 48610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:04,039-Speed 3892.14 samples/sec  Loss 1.0833  LearningRate 0.0866  ProxyLR: 4.3292  Epoch: 8  Global Step: 48620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:06,674-Speed 3886.93 samples/sec  Loss 1.0507  LearningRate 0.0866  ProxyLR: 4.3283  Epoch: 8  Global Step: 48630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:09,309-Speed 3886.42 samples/sec  Loss 1.0929  LearningRate 0.0865  ProxyLR: 4.3274  Epoch: 8  Global Step: 48640   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:05:11,930-Speed 3908.98 samples/sec  Loss 1.1195  LearningRate 0.0865  ProxyLR: 4.3264  Epoch: 8  Global Step: 48650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:14,565-Speed 3886.54 samples/sec  Loss 1.1104  LearningRate 0.0865  ProxyLR: 4.3255  Epoch: 8  Global Step: 48660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:17,198-Speed 3890.35 samples/sec  Loss 1.0785  LearningRate 0.0865  ProxyLR: 4.3246  Epoch: 8  Global Step: 48670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:19,833-Speed 3886.22 samples/sec  Loss 1.1477  LearningRate 0.0865  ProxyLR: 4.3237  Epoch: 8  Global Step: 48680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:22,468-Speed 3886.91 samples/sec  Loss 1.1364  LearningRate 0.0865  ProxyLR: 4.3227  Epoch: 8  Global Step: 48690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:25,103-Speed 3887.17 samples/sec  Loss 0.9866  LearningRate 0.0864  ProxyLR: 4.3218  Epoch: 8  Global Step: 48700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:27,738-Speed 3887.68 samples/sec  Loss 1.1082  LearningRate 0.0864  ProxyLR: 4.3209  Epoch: 8  Global Step: 48710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:30,374-Speed 3885.67 samples/sec  Loss 1.0568  LearningRate 0.0864  ProxyLR: 4.3200  Epoch: 8  Global Step: 48720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:33,008-Speed 3888.41 samples/sec  Loss 1.0916  LearningRate 0.0864  ProxyLR: 4.3190  Epoch: 8  Global Step: 48730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:35,643-Speed 3887.58 samples/sec  Loss 1.0420  LearningRate 0.0864  ProxyLR: 4.3181  Epoch: 8  Global Step: 48740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:38,279-Speed 3884.86 samples/sec  Loss 1.0005  LearningRate 0.0863  ProxyLR: 4.3172  Epoch: 8  Global Step: 48750   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:05:40,901-Speed 3907.34 samples/sec  Loss 1.0263  LearningRate 0.0863  ProxyLR: 4.3163  Epoch: 8  Global Step: 48760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:43,534-Speed 3889.34 samples/sec  Loss 1.0337  LearningRate 0.0863  ProxyLR: 4.3153  Epoch: 8  Global Step: 48770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:46,168-Speed 3888.95 samples/sec  Loss 1.1230  LearningRate 0.0863  ProxyLR: 4.3144  Epoch: 8  Global Step: 48780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:48,803-Speed 3886.83 samples/sec  Loss 1.1043  LearningRate 0.0863  ProxyLR: 4.3135  Epoch: 8  Global Step: 48790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:51,438-Speed 3888.16 samples/sec  Loss 1.1115  LearningRate 0.0863  ProxyLR: 4.3126  Epoch: 8  Global Step: 48800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:54,072-Speed 3888.04 samples/sec  Loss 1.0685  LearningRate 0.0862  ProxyLR: 4.3116  Epoch: 8  Global Step: 48810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:56,708-Speed 3885.23 samples/sec  Loss 1.0917  LearningRate 0.0862  ProxyLR: 4.3107  Epoch: 8  Global Step: 48820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:05:59,346-Speed 3883.29 samples/sec  Loss 1.0489  LearningRate 0.0862  ProxyLR: 4.3098  Epoch: 8  Global Step: 48830   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:01,982-Speed 3885.43 samples/sec  Loss 1.1059  LearningRate 0.0862  ProxyLR: 4.3089  Epoch: 8  Global Step: 48840   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:04,617-Speed 3886.80 samples/sec  Loss 1.0981  LearningRate 0.0862  ProxyLR: 4.3079  Epoch: 8  Global Step: 48850   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:07,237-Speed 3910.08 samples/sec  Loss 1.0718  LearningRate 0.0861  ProxyLR: 4.3070  Epoch: 8  Global Step: 48860   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:09,869-Speed 3891.33 samples/sec  Loss 1.0559  LearningRate 0.0861  ProxyLR: 4.3061  Epoch: 8  Global Step: 48870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:12,507-Speed 3881.86 samples/sec  Loss 1.0916  LearningRate 0.0861  ProxyLR: 4.3052  Epoch: 8  Global Step: 48880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:15,140-Speed 3890.70 samples/sec  Loss 1.1089  LearningRate 0.0861  ProxyLR: 4.3042  Epoch: 8  Global Step: 48890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:17,760-Speed 3909.25 samples/sec  Loss 1.0967  LearningRate 0.0861  ProxyLR: 4.3033  Epoch: 8  Global Step: 48900   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:20,393-Speed 3890.74 samples/sec  Loss 1.1852  LearningRate 0.0860  ProxyLR: 4.3024  Epoch: 8  Global Step: 48910   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:23,026-Speed 3889.98 samples/sec  Loss 1.1637  LearningRate 0.0860  ProxyLR: 4.3015  Epoch: 8  Global Step: 48920   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:25,658-Speed 3890.58 samples/sec  Loss 1.0845  LearningRate 0.0860  ProxyLR: 4.3006  Epoch: 8  Global Step: 48930   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:28,289-Speed 3893.26 samples/sec  Loss 1.0375  LearningRate 0.0860  ProxyLR: 4.2996  Epoch: 8  Global Step: 48940   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:30,922-Speed 3890.29 samples/sec  Loss 1.1123  LearningRate 0.0860  ProxyLR: 4.2987  Epoch: 8  Global Step: 48950   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:33,552-Speed 3894.32 samples/sec  Loss 1.1305  LearningRate 0.0860  ProxyLR: 4.2978  Epoch: 8  Global Step: 48960   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:36,182-Speed 3894.99 samples/sec  Loss 1.1146  LearningRate 0.0859  ProxyLR: 4.2969  Epoch: 8  Global Step: 48970   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:38,813-Speed 3893.35 samples/sec  Loss 1.1874  LearningRate 0.0859  ProxyLR: 4.2959  Epoch: 8  Global Step: 48980   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:41,445-Speed 3890.87 samples/sec  Loss 0.9959  LearningRate 0.0859  ProxyLR: 4.2950  Epoch: 8  Global Step: 48990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:06:44,074-Speed 3896.86 samples/sec  Loss 1.0348  LearningRate 0.0859  ProxyLR: 4.2941  Epoch: 8  Global Step: 49000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:46,700-Speed 3900.02 samples/sec  Loss 1.1599  LearningRate 0.0859  ProxyLR: 4.2932  Epoch: 8  Global Step: 49010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:49,328-Speed 3898.23 samples/sec  Loss 1.1452  LearningRate 0.0858  ProxyLR: 4.2923  Epoch: 8  Global Step: 49020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:51,958-Speed 3894.50 samples/sec  Loss 1.1024  LearningRate 0.0858  ProxyLR: 4.2913  Epoch: 8  Global Step: 49030   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:54,587-Speed 3894.92 samples/sec  Loss 1.0501  LearningRate 0.0858  ProxyLR: 4.2904  Epoch: 8  Global Step: 49040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:57,216-Speed 3895.78 samples/sec  Loss 1.1250  LearningRate 0.0858  ProxyLR: 4.2895  Epoch: 8  Global Step: 49050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:06:59,845-Speed 3896.68 samples/sec  Loss 1.0743  LearningRate 0.0858  ProxyLR: 4.2886  Epoch: 8  Global Step: 49060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:02,475-Speed 3893.95 samples/sec  Loss 1.1218  LearningRate 0.0858  ProxyLR: 4.2876  Epoch: 8  Global Step: 49070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:05,106-Speed 3894.14 samples/sec  Loss 1.1109  LearningRate 0.0857  ProxyLR: 4.2867  Epoch: 8  Global Step: 49080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:07,735-Speed 3894.92 samples/sec  Loss 1.1551  LearningRate 0.0857  ProxyLR: 4.2858  Epoch: 8  Global Step: 49090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:10,366-Speed 3893.50 samples/sec  Loss 1.1390  LearningRate 0.0857  ProxyLR: 4.2849  Epoch: 8  Global Step: 49100   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:07:12,998-Speed 3891.22 samples/sec  Loss 1.0632  LearningRate 0.0857  ProxyLR: 4.2840  Epoch: 8  Global Step: 49110   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:07:15,617-Speed 3910.36 samples/sec  Loss 1.1023  LearningRate 0.0857  ProxyLR: 4.2830  Epoch: 8  Global Step: 49120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:18,247-Speed 3894.40 samples/sec  Loss 1.1629  LearningRate 0.0856  ProxyLR: 4.2821  Epoch: 8  Global Step: 49130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:20,876-Speed 3896.78 samples/sec  Loss 1.0993  LearningRate 0.0856  ProxyLR: 4.2812  Epoch: 8  Global Step: 49140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:23,504-Speed 3896.53 samples/sec  Loss 1.1323  LearningRate 0.0856  ProxyLR: 4.2803  Epoch: 8  Global Step: 49150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:26,136-Speed 3892.42 samples/sec  Loss 1.1232  LearningRate 0.0856  ProxyLR: 4.2794  Epoch: 8  Global Step: 49160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:28,767-Speed 3892.80 samples/sec  Loss 1.0859  LearningRate 0.0856  ProxyLR: 4.2784  Epoch: 8  Global Step: 49170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:31,395-Speed 3896.75 samples/sec  Loss 1.1396  LearningRate 0.0856  ProxyLR: 4.2775  Epoch: 8  Global Step: 49180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:34,027-Speed 3892.89 samples/sec  Loss 1.2116  LearningRate 0.0855  ProxyLR: 4.2766  Epoch: 8  Global Step: 49190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:36,657-Speed 3894.52 samples/sec  Loss 1.1234  LearningRate 0.0855  ProxyLR: 4.2757  Epoch: 8  Global Step: 49200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:39,287-Speed 3894.13 samples/sec  Loss 1.1516  LearningRate 0.0855  ProxyLR: 4.2748  Epoch: 8  Global Step: 49210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:41,917-Speed 3895.32 samples/sec  Loss 1.1270  LearningRate 0.0855  ProxyLR: 4.2738  Epoch: 8  Global Step: 49220   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:07:44,533-Speed 3914.77 samples/sec  Loss 1.1250  LearningRate 0.0855  ProxyLR: 4.2729  Epoch: 8  Global Step: 49230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:47,160-Speed 3898.72 samples/sec  Loss 1.1572  LearningRate 0.0854  ProxyLR: 4.2720  Epoch: 8  Global Step: 49240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:49,790-Speed 3894.40 samples/sec  Loss 1.1106  LearningRate 0.0854  ProxyLR: 4.2711  Epoch: 8  Global Step: 49250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:52,420-Speed 3895.13 samples/sec  Loss 1.0764  LearningRate 0.0854  ProxyLR: 4.2702  Epoch: 8  Global Step: 49260   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:55,050-Speed 3893.96 samples/sec  Loss 1.2013  LearningRate 0.0854  ProxyLR: 4.2692  Epoch: 8  Global Step: 49270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:07:57,678-Speed 3897.96 samples/sec  Loss 1.1730  LearningRate 0.0854  ProxyLR: 4.2683  Epoch: 8  Global Step: 49280   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:00,309-Speed 3893.26 samples/sec  Loss 1.1146  LearningRate 0.0853  ProxyLR: 4.2674  Epoch: 8  Global Step: 49290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:02,938-Speed 3895.45 samples/sec  Loss 1.1338  LearningRate 0.0853  ProxyLR: 4.2665  Epoch: 8  Global Step: 49300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:05,568-Speed 3894.17 samples/sec  Loss 1.1286  LearningRate 0.0853  ProxyLR: 4.2656  Epoch: 8  Global Step: 49310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:08,199-Speed 3894.16 samples/sec  Loss 1.1323  LearningRate 0.0853  ProxyLR: 4.2646  Epoch: 8  Global Step: 49320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:10,827-Speed 3896.37 samples/sec  Loss 1.1781  LearningRate 0.0853  ProxyLR: 4.2637  Epoch: 8  Global Step: 49330   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:08:13,441-Speed 3918.31 samples/sec  Loss 1.1053  LearningRate 0.0853  ProxyLR: 4.2628  Epoch: 8  Global Step: 49340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:16,071-Speed 3895.28 samples/sec  Loss 1.0857  LearningRate 0.0852  ProxyLR: 4.2619  Epoch: 8  Global Step: 49350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:18,701-Speed 3894.55 samples/sec  Loss 1.1598  LearningRate 0.0852  ProxyLR: 4.2610  Epoch: 8  Global Step: 49360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:21,331-Speed 3894.18 samples/sec  Loss 1.0823  LearningRate 0.0852  ProxyLR: 4.2601  Epoch: 8  Global Step: 49370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:23,961-Speed 3894.37 samples/sec  Loss 1.0883  LearningRate 0.0852  ProxyLR: 4.2591  Epoch: 8  Global Step: 49380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:26,593-Speed 3891.72 samples/sec  Loss 1.1346  LearningRate 0.0852  ProxyLR: 4.2582  Epoch: 8  Global Step: 49390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:29,228-Speed 3886.79 samples/sec  Loss 1.0731  LearningRate 0.0851  ProxyLR: 4.2573  Epoch: 8  Global Step: 49400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:31,864-Speed 3886.41 samples/sec  Loss 1.0602  LearningRate 0.0851  ProxyLR: 4.2564  Epoch: 8  Global Step: 49410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:34,499-Speed 3886.85 samples/sec  Loss 1.0906  LearningRate 0.0851  ProxyLR: 4.2555  Epoch: 8  Global Step: 49420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:37,132-Speed 3890.77 samples/sec  Loss 1.1434  LearningRate 0.0851  ProxyLR: 4.2545  Epoch: 8  Global Step: 49430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:39,751-Speed 3909.55 samples/sec  Loss 1.0970  LearningRate 0.0851  ProxyLR: 4.2536  Epoch: 8  Global Step: 49440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:42,383-Speed 3891.78 samples/sec  Loss 1.1449  LearningRate 0.0851  ProxyLR: 4.2527  Epoch: 8  Global Step: 49450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:45,016-Speed 3891.07 samples/sec  Loss 1.1166  LearningRate 0.0850  ProxyLR: 4.2518  Epoch: 8  Global Step: 49460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:47,648-Speed 3890.80 samples/sec  Loss 1.1037  LearningRate 0.0850  ProxyLR: 4.2509  Epoch: 8  Global Step: 49470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:50,278-Speed 3894.39 samples/sec  Loss 1.1659  LearningRate 0.0850  ProxyLR: 4.2500  Epoch: 8  Global Step: 49480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:52,908-Speed 3894.02 samples/sec  Loss 1.1077  LearningRate 0.0850  ProxyLR: 4.2490  Epoch: 8  Global Step: 49490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:55,540-Speed 3891.86 samples/sec  Loss 1.1510  LearningRate 0.0850  ProxyLR: 4.2481  Epoch: 8  Global Step: 49500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:08:58,170-Speed 3893.95 samples/sec  Loss 1.1149  LearningRate 0.0849  ProxyLR: 4.2472  Epoch: 8  Global Step: 49510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:00,802-Speed 3891.77 samples/sec  Loss 1.0374  LearningRate 0.0849  ProxyLR: 4.2463  Epoch: 8  Global Step: 49520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:03,434-Speed 3891.45 samples/sec  Loss 1.0796  LearningRate 0.0849  ProxyLR: 4.2454  Epoch: 8  Global Step: 49530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:06,064-Speed 3895.08 samples/sec  Loss 1.0609  LearningRate 0.0849  ProxyLR: 4.2445  Epoch: 8  Global Step: 49540   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:09:08,680-Speed 3915.30 samples/sec  Loss 1.1538  LearningRate 0.0849  ProxyLR: 4.2435  Epoch: 8  Global Step: 49550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:11,310-Speed 3895.04 samples/sec  Loss 1.1871  LearningRate 0.0849  ProxyLR: 4.2426  Epoch: 8  Global Step: 49560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:13,926-Speed 3915.00 samples/sec  Loss 1.1032  LearningRate 0.0848  ProxyLR: 4.2417  Epoch: 8  Global Step: 49570   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:16,555-Speed 3895.21 samples/sec  Loss 1.1227  LearningRate 0.0848  ProxyLR: 4.2408  Epoch: 8  Global Step: 49580   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:19,185-Speed 3894.79 samples/sec  Loss 1.1597  LearningRate 0.0848  ProxyLR: 4.2399  Epoch: 8  Global Step: 49590   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:21,815-Speed 3894.96 samples/sec  Loss 1.1714  LearningRate 0.0848  ProxyLR: 4.2390  Epoch: 8  Global Step: 49600   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:24,449-Speed 3888.91 samples/sec  Loss 1.1567  LearningRate 0.0848  ProxyLR: 4.2380  Epoch: 8  Global Step: 49610   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:27,081-Speed 3891.98 samples/sec  Loss 1.0863  LearningRate 0.0847  ProxyLR: 4.2371  Epoch: 8  Global Step: 49620   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:29,712-Speed 3892.35 samples/sec  Loss 1.1141  LearningRate 0.0847  ProxyLR: 4.2362  Epoch: 8  Global Step: 49630   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:32,344-Speed 3891.26 samples/sec  Loss 1.1293  LearningRate 0.0847  ProxyLR: 4.2353  Epoch: 8  Global Step: 49640   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:34,978-Speed 3889.65 samples/sec  Loss 1.1692  LearningRate 0.0847  ProxyLR: 4.2344  Epoch: 8  Global Step: 49650   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:37,607-Speed 3895.25 samples/sec  Loss 1.1863  LearningRate 0.0847  ProxyLR: 4.2335  Epoch: 8  Global Step: 49660   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:09:40,240-Speed 3890.03 samples/sec  Loss 1.1064  LearningRate 0.0847  ProxyLR: 4.2325  Epoch: 8  Global Step: 49670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:42,874-Speed 3888.09 samples/sec  Loss 1.1048  LearningRate 0.0846  ProxyLR: 4.2316  Epoch: 8  Global Step: 49680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:45,507-Speed 3890.76 samples/sec  Loss 1.0574  LearningRate 0.0846  ProxyLR: 4.2307  Epoch: 8  Global Step: 49690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:48,138-Speed 3892.60 samples/sec  Loss 1.1543  LearningRate 0.0846  ProxyLR: 4.2298  Epoch: 8  Global Step: 49700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:50,774-Speed 3885.71 samples/sec  Loss 1.1461  LearningRate 0.0846  ProxyLR: 4.2289  Epoch: 8  Global Step: 49710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:53,409-Speed 3887.38 samples/sec  Loss 1.1128  LearningRate 0.0846  ProxyLR: 4.2280  Epoch: 8  Global Step: 49720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:56,040-Speed 3893.28 samples/sec  Loss 1.1656  LearningRate 0.0845  ProxyLR: 4.2271  Epoch: 8  Global Step: 49730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:09:58,671-Speed 3893.02 samples/sec  Loss 1.1420  LearningRate 0.0845  ProxyLR: 4.2261  Epoch: 8  Global Step: 49740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:01,304-Speed 3890.09 samples/sec  Loss 1.1069  LearningRate 0.0845  ProxyLR: 4.2252  Epoch: 8  Global Step: 49750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:03,938-Speed 3888.60 samples/sec  Loss 1.0529  LearningRate 0.0845  ProxyLR: 4.2243  Epoch: 8  Global Step: 49760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:06,569-Speed 3892.93 samples/sec  Loss 1.1552  LearningRate 0.0845  ProxyLR: 4.2234  Epoch: 8  Global Step: 49770   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:10:09,187-Speed 3912.35 samples/sec  Loss 1.1511  LearningRate 0.0844  ProxyLR: 4.2225  Epoch: 8  Global Step: 49780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:11,819-Speed 3891.68 samples/sec  Loss 1.0889  LearningRate 0.0844  ProxyLR: 4.2216  Epoch: 8  Global Step: 49790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:14,451-Speed 3891.30 samples/sec  Loss 1.1266  LearningRate 0.0844  ProxyLR: 4.2207  Epoch: 8  Global Step: 49800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:17,082-Speed 3892.64 samples/sec  Loss 1.1063  LearningRate 0.0844  ProxyLR: 4.2197  Epoch: 8  Global Step: 49810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:19,713-Speed 3893.95 samples/sec  Loss 1.2088  LearningRate 0.0844  ProxyLR: 4.2188  Epoch: 8  Global Step: 49820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:22,346-Speed 3889.74 samples/sec  Loss 1.1646  LearningRate 0.0844  ProxyLR: 4.2179  Epoch: 8  Global Step: 49830   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:24,976-Speed 3894.33 samples/sec  Loss 1.1926  LearningRate 0.0843  ProxyLR: 4.2170  Epoch: 8  Global Step: 49840   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:27,607-Speed 3893.50 samples/sec  Loss 1.1278  LearningRate 0.0843  ProxyLR: 4.2161  Epoch: 8  Global Step: 49850   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:30,239-Speed 3890.72 samples/sec  Loss 1.1859  LearningRate 0.0843  ProxyLR: 4.2152  Epoch: 8  Global Step: 49860   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:32,871-Speed 3892.20 samples/sec  Loss 1.1262  LearningRate 0.0843  ProxyLR: 4.2143  Epoch: 8  Global Step: 49870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:35,489-Speed 3911.61 samples/sec  Loss 1.0984  LearningRate 0.0843  ProxyLR: 4.2133  Epoch: 8  Global Step: 49880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:38,119-Speed 3895.30 samples/sec  Loss 1.1630  LearningRate 0.0842  ProxyLR: 4.2124  Epoch: 8  Global Step: 49890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:40,750-Speed 3892.96 samples/sec  Loss 1.1077  LearningRate 0.0842  ProxyLR: 4.2115  Epoch: 8  Global Step: 49900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:43,384-Speed 3888.86 samples/sec  Loss 1.1721  LearningRate 0.0842  ProxyLR: 4.2106  Epoch: 8  Global Step: 49910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:46,014-Speed 3893.32 samples/sec  Loss 1.1303  LearningRate 0.0842  ProxyLR: 4.2097  Epoch: 8  Global Step: 49920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:48,644-Speed 3895.32 samples/sec  Loss 1.1291  LearningRate 0.0842  ProxyLR: 4.2088  Epoch: 8  Global Step: 49930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:51,273-Speed 3896.36 samples/sec  Loss 1.0722  LearningRate 0.0842  ProxyLR: 4.2079  Epoch: 8  Global Step: 49940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:53,902-Speed 3894.85 samples/sec  Loss 1.1838  LearningRate 0.0841  ProxyLR: 4.2070  Epoch: 8  Global Step: 49950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:56,534-Speed 3892.72 samples/sec  Loss 1.1499  LearningRate 0.0841  ProxyLR: 4.2060  Epoch: 8  Global Step: 49960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:10:59,162-Speed 3896.59 samples/sec  Loss 1.1008  LearningRate 0.0841  ProxyLR: 4.2051  Epoch: 8  Global Step: 49970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:11:01,791-Speed 3896.84 samples/sec  Loss 1.0840  LearningRate 0.0841  ProxyLR: 4.2042  Epoch: 8  Global Step: 49980   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:11:04,418-Speed 3898.14 samples/sec  Loss 1.0988  LearningRate 0.0841  ProxyLR: 4.2033  Epoch: 8  Global Step: 49990   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:11:07,032-Speed 3918.13 samples/sec  Loss 1.1816  LearningRate 0.0840  ProxyLR: 4.2024  Epoch: 8  Global Step: 50000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:11:56,791-[lfw][50000]XNorm: 20.240887
Training: 2023-05-04 18:11:56,791-[lfw][50000]Accuracy-Flip: 0.99683+-0.00311
Training: 2023-05-04 18:11:56,792-[lfw][50000]Accuracy-Highest: 0.99683
Training: 2023-05-04 18:11:56,792-[lfw][50000]TPR@1stNon-Zero-FPR of 0.00033: 0.99433
Training: 2023-05-04 18:11:56,792-[lfw][50000]Highest TPR@FPR: 0.99433
Training: 2023-05-04 18:12:53,870-[cfp_fp][50000]XNorm: 19.836477
Training: 2023-05-04 18:12:53,870-[cfp_fp][50000]Accuracy-Flip: 0.93714+-0.01427
Training: 2023-05-04 18:12:53,870-[cfp_fp][50000]Accuracy-Highest: 0.95314
Training: 2023-05-04 18:12:53,870-[cfp_fp][50000]TPR@1stNon-Zero-FPR of 0.00029: 0.53143
Training: 2023-05-04 18:12:53,870-[cfp_fp][50000]Highest TPR@FPR: 0.63486
Training: 2023-05-04 18:13:43,595-[agedb_30][50000]XNorm: 20.390789
Training: 2023-05-04 18:13:43,596-[agedb_30][50000]Accuracy-Flip: 0.94833+-0.01249
Training: 2023-05-04 18:13:43,596-[agedb_30][50000]Accuracy-Highest: 0.95517
Training: 2023-05-04 18:13:43,596-[agedb_30][50000]TPR@1stNon-Zero-FPR of 0.00033: 0.54533
Training: 2023-05-04 18:13:43,596-[agedb_30][50000]Highest TPR@FPR: 0.56467
Training: 2023-05-04 18:14:34,802-[calfw][50000]XNorm: 20.542484
Training: 2023-05-04 18:14:34,802-[calfw][50000]Accuracy-Flip: 0.94733+-0.01057
Training: 2023-05-04 18:14:34,803-[calfw][50000]Accuracy-Highest: 0.94733
Training: 2023-05-04 18:14:34,803-[calfw][50000]TPR@1stNon-Zero-FPR of 0.00033: 0.79800
Training: 2023-05-04 18:14:34,803-[calfw][50000]Highest TPR@FPR: 0.79900
Training: 2023-05-04 18:15:25,971-[cplfw][50000]XNorm: 19.227982
Training: 2023-05-04 18:15:25,971-[cplfw][50000]Accuracy-Flip: 0.89383+-0.01800
Training: 2023-05-04 18:15:25,971-[cplfw][50000]Accuracy-Highest: 0.89867
Training: 2023-05-04 18:15:25,972-[cplfw][50000]TPR@1stNon-Zero-FPR of 0.00033: 0.00067
Training: 2023-05-04 18:15:25,972-[cplfw][50000]Highest TPR@FPR: 0.00300
Training: 2023-05-04 18:15:28,634-Speed 39.14 samples/sec  Loss 1.1773  LearningRate 0.0840  ProxyLR: 4.2015  Epoch: 8  Global Step: 50010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:31,252-Speed 3912.81 samples/sec  Loss 1.0911  LearningRate 0.0840  ProxyLR: 4.2006  Epoch: 8  Global Step: 50020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:33,870-Speed 3911.88 samples/sec  Loss 1.1178  LearningRate 0.0840  ProxyLR: 4.1997  Epoch: 8  Global Step: 50030   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:36,489-Speed 3910.47 samples/sec  Loss 1.1556  LearningRate 0.0840  ProxyLR: 4.1987  Epoch: 8  Global Step: 50040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:39,110-Speed 3908.09 samples/sec  Loss 1.1496  LearningRate 0.0840  ProxyLR: 4.1978  Epoch: 8  Global Step: 50050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:41,730-Speed 3909.51 samples/sec  Loss 1.1628  LearningRate 0.0839  ProxyLR: 4.1969  Epoch: 8  Global Step: 50060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:44,351-Speed 3909.00 samples/sec  Loss 1.1641  LearningRate 0.0839  ProxyLR: 4.1960  Epoch: 8  Global Step: 50070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:46,972-Speed 3908.05 samples/sec  Loss 1.1082  LearningRate 0.0839  ProxyLR: 4.1951  Epoch: 8  Global Step: 50080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:49,594-Speed 3906.10 samples/sec  Loss 1.1644  LearningRate 0.0839  ProxyLR: 4.1942  Epoch: 8  Global Step: 50090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:52,217-Speed 3904.85 samples/sec  Loss 1.1289  LearningRate 0.0839  ProxyLR: 4.1933  Epoch: 8  Global Step: 50100   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:15:54,827-Speed 3924.34 samples/sec  Loss 1.1689  LearningRate 0.0838  ProxyLR: 4.1924  Epoch: 8  Global Step: 50110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:15:57,451-Speed 3902.29 samples/sec  Loss 1.1050  LearningRate 0.0838  ProxyLR: 4.1915  Epoch: 8  Global Step: 50120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:00,076-Speed 3902.67 samples/sec  Loss 1.1163  LearningRate 0.0838  ProxyLR: 4.1905  Epoch: 8  Global Step: 50130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:02,701-Speed 3902.25 samples/sec  Loss 1.1856  LearningRate 0.0838  ProxyLR: 4.1896  Epoch: 8  Global Step: 50140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:05,327-Speed 3900.07 samples/sec  Loss 1.1657  LearningRate 0.0838  ProxyLR: 4.1887  Epoch: 8  Global Step: 50150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:07,951-Speed 3902.81 samples/sec  Loss 1.1410  LearningRate 0.0838  ProxyLR: 4.1878  Epoch: 8  Global Step: 50160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:10,578-Speed 3900.38 samples/sec  Loss 1.0900  LearningRate 0.0837  ProxyLR: 4.1869  Epoch: 8  Global Step: 50170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:13,203-Speed 3900.66 samples/sec  Loss 1.1348  LearningRate 0.0837  ProxyLR: 4.1860  Epoch: 8  Global Step: 50180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:15,834-Speed 3893.34 samples/sec  Loss 1.1554  LearningRate 0.0837  ProxyLR: 4.1851  Epoch: 8  Global Step: 50190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:18,460-Speed 3901.35 samples/sec  Loss 1.2012  LearningRate 0.0837  ProxyLR: 4.1842  Epoch: 8  Global Step: 50200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:21,070-Speed 3923.71 samples/sec  Loss 1.1870  LearningRate 0.0837  ProxyLR: 4.1833  Epoch: 8  Global Step: 50210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:23,693-Speed 3905.30 samples/sec  Loss 1.1382  LearningRate 0.0836  ProxyLR: 4.1824  Epoch: 8  Global Step: 50220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:26,302-Speed 3925.73 samples/sec  Loss 1.1703  LearningRate 0.0836  ProxyLR: 4.1814  Epoch: 8  Global Step: 50230   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:28,926-Speed 3903.78 samples/sec  Loss 1.1530  LearningRate 0.0836  ProxyLR: 4.1805  Epoch: 8  Global Step: 50240   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:31,548-Speed 3906.07 samples/sec  Loss 1.1103  LearningRate 0.0836  ProxyLR: 4.1796  Epoch: 8  Global Step: 50250   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:34,172-Speed 3903.32 samples/sec  Loss 1.1353  LearningRate 0.0836  ProxyLR: 4.1787  Epoch: 8  Global Step: 50260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:36,799-Speed 3898.14 samples/sec  Loss 1.0965  LearningRate 0.0836  ProxyLR: 4.1778  Epoch: 8  Global Step: 50270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:39,424-Speed 3902.88 samples/sec  Loss 1.1375  LearningRate 0.0835  ProxyLR: 4.1769  Epoch: 8  Global Step: 50280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:42,050-Speed 3900.37 samples/sec  Loss 1.1619  LearningRate 0.0835  ProxyLR: 4.1760  Epoch: 8  Global Step: 50290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:44,677-Speed 3899.68 samples/sec  Loss 1.1433  LearningRate 0.0835  ProxyLR: 4.1751  Epoch: 8  Global Step: 50300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:47,302-Speed 3900.93 samples/sec  Loss 1.2533  LearningRate 0.0835  ProxyLR: 4.1742  Epoch: 8  Global Step: 50310   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:49,928-Speed 3901.22 samples/sec  Loss 1.1309  LearningRate 0.0835  ProxyLR: 4.1733  Epoch: 8  Global Step: 50320   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:16:52,551-Speed 3904.21 samples/sec  Loss 1.0619  LearningRate 0.0834  ProxyLR: 4.1724  Epoch: 8  Global Step: 50330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:55,177-Speed 3900.32 samples/sec  Loss 1.1074  LearningRate 0.0834  ProxyLR: 4.1714  Epoch: 8  Global Step: 50340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:16:57,804-Speed 3899.06 samples/sec  Loss 1.1518  LearningRate 0.0834  ProxyLR: 4.1705  Epoch: 8  Global Step: 50350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:00,427-Speed 3905.57 samples/sec  Loss 1.0996  LearningRate 0.0834  ProxyLR: 4.1696  Epoch: 8  Global Step: 50360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:03,051-Speed 3903.45 samples/sec  Loss 1.1747  LearningRate 0.0834  ProxyLR: 4.1687  Epoch: 8  Global Step: 50370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:05,674-Speed 3904.89 samples/sec  Loss 1.1307  LearningRate 0.0834  ProxyLR: 4.1678  Epoch: 8  Global Step: 50380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:08,297-Speed 3904.07 samples/sec  Loss 1.1048  LearningRate 0.0833  ProxyLR: 4.1669  Epoch: 8  Global Step: 50390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:10,921-Speed 3904.68 samples/sec  Loss 1.0941  LearningRate 0.0833  ProxyLR: 4.1660  Epoch: 8  Global Step: 50400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:13,547-Speed 3900.15 samples/sec  Loss 1.1168  LearningRate 0.0833  ProxyLR: 4.1651  Epoch: 8  Global Step: 50410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:16,170-Speed 3904.66 samples/sec  Loss 1.1931  LearningRate 0.0833  ProxyLR: 4.1642  Epoch: 8  Global Step: 50420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:18,794-Speed 3904.31 samples/sec  Loss 1.1031  LearningRate 0.0833  ProxyLR: 4.1633  Epoch: 8  Global Step: 50430   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:17:21,403-Speed 3925.37 samples/sec  Loss 1.1344  LearningRate 0.0832  ProxyLR: 4.1624  Epoch: 8  Global Step: 50440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:24,027-Speed 3903.62 samples/sec  Loss 1.2038  LearningRate 0.0832  ProxyLR: 4.1615  Epoch: 8  Global Step: 50450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:26,651-Speed 3903.59 samples/sec  Loss 1.1594  LearningRate 0.0832  ProxyLR: 4.1605  Epoch: 8  Global Step: 50460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:29,274-Speed 3903.84 samples/sec  Loss 1.1541  LearningRate 0.0832  ProxyLR: 4.1596  Epoch: 8  Global Step: 50470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:31,904-Speed 3894.23 samples/sec  Loss 1.1496  LearningRate 0.0832  ProxyLR: 4.1587  Epoch: 8  Global Step: 50480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:34,533-Speed 3897.12 samples/sec  Loss 1.1444  LearningRate 0.0832  ProxyLR: 4.1578  Epoch: 8  Global Step: 50490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:37,162-Speed 3895.78 samples/sec  Loss 1.1455  LearningRate 0.0831  ProxyLR: 4.1569  Epoch: 8  Global Step: 50500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:39,795-Speed 3890.59 samples/sec  Loss 1.1225  LearningRate 0.0831  ProxyLR: 4.1560  Epoch: 8  Global Step: 50510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:42,426-Speed 3892.95 samples/sec  Loss 1.1345  LearningRate 0.0831  ProxyLR: 4.1551  Epoch: 8  Global Step: 50520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:17:45,043-Speed 3913.38 samples/sec  Loss 1.1390  LearningRate 0.0831  ProxyLR: 4.1542  Epoch: 8  Global Step: 50530   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:17:47,675-Speed 3892.14 samples/sec  Loss 1.2691  LearningRate 0.0831  ProxyLR: 4.1533  Epoch: 8  Global Step: 50540   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:17:50,307-Speed 3890.43 samples/sec  Loss 1.2184  LearningRate 0.0830  ProxyLR: 4.1524  Epoch: 8  Global Step: 50550   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:17:52,940-Speed 3891.28 samples/sec  Loss 1.1643  LearningRate 0.0830  ProxyLR: 4.1515  Epoch: 8  Global Step: 50560   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:17:55,572-Speed 3890.41 samples/sec  Loss 1.1862  LearningRate 0.0830  ProxyLR: 4.1506  Epoch: 8  Global Step: 50570   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:17:58,204-Speed 3891.56 samples/sec  Loss 1.1928  LearningRate 0.0830  ProxyLR: 4.1497  Epoch: 8  Global Step: 50580   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:00,838-Speed 3889.16 samples/sec  Loss 1.1205  LearningRate 0.0830  ProxyLR: 4.1488  Epoch: 8  Global Step: 50590   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:03,471-Speed 3889.53 samples/sec  Loss 1.1661  LearningRate 0.0830  ProxyLR: 4.1478  Epoch: 8  Global Step: 50600   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:06,105-Speed 3889.48 samples/sec  Loss 1.1408  LearningRate 0.0829  ProxyLR: 4.1469  Epoch: 8  Global Step: 50610   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:08,737-Speed 3891.13 samples/sec  Loss 1.2465  LearningRate 0.0829  ProxyLR: 4.1460  Epoch: 8  Global Step: 50620   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:11,355-Speed 3912.13 samples/sec  Loss 1.0877  LearningRate 0.0829  ProxyLR: 4.1451  Epoch: 8  Global Step: 50630   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:13,987-Speed 3891.08 samples/sec  Loss 1.1879  LearningRate 0.0829  ProxyLR: 4.1442  Epoch: 8  Global Step: 50640   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:16,621-Speed 3889.12 samples/sec  Loss 1.1764  LearningRate 0.0829  ProxyLR: 4.1433  Epoch: 8  Global Step: 50650   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:19,254-Speed 3890.81 samples/sec  Loss 1.2113  LearningRate 0.0828  ProxyLR: 4.1424  Epoch: 8  Global Step: 50660   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:21,885-Speed 3892.52 samples/sec  Loss 1.1435  LearningRate 0.0828  ProxyLR: 4.1415  Epoch: 8  Global Step: 50670   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:24,521-Speed 3885.49 samples/sec  Loss 1.2783  LearningRate 0.0828  ProxyLR: 4.1406  Epoch: 8  Global Step: 50680   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:27,156-Speed 3886.94 samples/sec  Loss 1.2264  LearningRate 0.0828  ProxyLR: 4.1397  Epoch: 8  Global Step: 50690   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:29,790-Speed 3888.26 samples/sec  Loss 1.2522  LearningRate 0.0828  ProxyLR: 4.1388  Epoch: 8  Global Step: 50700   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:32,425-Speed 3887.21 samples/sec  Loss 1.2193  LearningRate 0.0828  ProxyLR: 4.1379  Epoch: 8  Global Step: 50710   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:35,061-Speed 3885.59 samples/sec  Loss 1.2376  LearningRate 0.0827  ProxyLR: 4.1370  Epoch: 8  Global Step: 50720   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:18:37,706-Speed 3873.04 samples/sec  Loss 1.1662  LearningRate 0.0827  ProxyLR: 4.1361  Epoch: 8  Global Step: 50730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:40,344-Speed 3882.56 samples/sec  Loss 1.1970  LearningRate 0.0827  ProxyLR: 4.1352  Epoch: 8  Global Step: 50740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:42,979-Speed 3887.18 samples/sec  Loss 1.1258  LearningRate 0.0827  ProxyLR: 4.1343  Epoch: 8  Global Step: 50750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:45,616-Speed 3885.01 samples/sec  Loss 1.0998  LearningRate 0.0827  ProxyLR: 4.1334  Epoch: 8  Global Step: 50760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:48,253-Speed 3882.89 samples/sec  Loss 1.2030  LearningRate 0.0826  ProxyLR: 4.1325  Epoch: 8  Global Step: 50770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:50,892-Speed 3881.84 samples/sec  Loss 1.1726  LearningRate 0.0826  ProxyLR: 4.1316  Epoch: 8  Global Step: 50780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:53,530-Speed 3883.27 samples/sec  Loss 1.2373  LearningRate 0.0826  ProxyLR: 4.1307  Epoch: 8  Global Step: 50790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:56,168-Speed 3882.67 samples/sec  Loss 1.1807  LearningRate 0.0826  ProxyLR: 4.1297  Epoch: 8  Global Step: 50800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:18:58,806-Speed 3881.95 samples/sec  Loss 1.1041  LearningRate 0.0826  ProxyLR: 4.1288  Epoch: 8  Global Step: 50810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:01,445-Speed 3882.07 samples/sec  Loss 1.1527  LearningRate 0.0826  ProxyLR: 4.1279  Epoch: 8  Global Step: 50820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:04,086-Speed 3877.92 samples/sec  Loss 1.1455  LearningRate 0.0825  ProxyLR: 4.1270  Epoch: 8  Global Step: 50830   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:19:06,724-Speed 3882.58 samples/sec  Loss 1.1714  LearningRate 0.0825  ProxyLR: 4.1261  Epoch: 8  Global Step: 50840   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:19:09,348-Speed 3902.94 samples/sec  Loss 1.1774  LearningRate 0.0825  ProxyLR: 4.1252  Epoch: 8  Global Step: 50850   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:11,986-Speed 3883.73 samples/sec  Loss 1.1879  LearningRate 0.0825  ProxyLR: 4.1243  Epoch: 8  Global Step: 50860   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:14,624-Speed 3881.86 samples/sec  Loss 1.0943  LearningRate 0.0825  ProxyLR: 4.1234  Epoch: 8  Global Step: 50870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:17,262-Speed 3883.46 samples/sec  Loss 1.1476  LearningRate 0.0825  ProxyLR: 4.1225  Epoch: 8  Global Step: 50880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:19,898-Speed 3885.82 samples/sec  Loss 1.1735  LearningRate 0.0824  ProxyLR: 4.1216  Epoch: 8  Global Step: 50890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:22,536-Speed 3882.01 samples/sec  Loss 1.1544  LearningRate 0.0824  ProxyLR: 4.1207  Epoch: 8  Global Step: 50900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:25,173-Speed 3884.85 samples/sec  Loss 1.2077  LearningRate 0.0824  ProxyLR: 4.1198  Epoch: 8  Global Step: 50910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:27,808-Speed 3886.14 samples/sec  Loss 1.1447  LearningRate 0.0824  ProxyLR: 4.1189  Epoch: 8  Global Step: 50920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:30,445-Speed 3885.17 samples/sec  Loss 1.1072  LearningRate 0.0824  ProxyLR: 4.1180  Epoch: 8  Global Step: 50930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:33,079-Speed 3888.04 samples/sec  Loss 1.1191  LearningRate 0.0823  ProxyLR: 4.1171  Epoch: 8  Global Step: 50940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:35,698-Speed 3911.19 samples/sec  Loss 1.1178  LearningRate 0.0823  ProxyLR: 4.1162  Epoch: 8  Global Step: 50950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:38,329-Speed 3893.42 samples/sec  Loss 1.1796  LearningRate 0.0823  ProxyLR: 4.1153  Epoch: 8  Global Step: 50960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:40,960-Speed 3893.39 samples/sec  Loss 1.1669  LearningRate 0.0823  ProxyLR: 4.1144  Epoch: 8  Global Step: 50970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:43,594-Speed 3887.93 samples/sec  Loss 1.2527  LearningRate 0.0823  ProxyLR: 4.1135  Epoch: 8  Global Step: 50980   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:46,227-Speed 3890.75 samples/sec  Loss 1.2089  LearningRate 0.0823  ProxyLR: 4.1126  Epoch: 8  Global Step: 50990   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:48,860-Speed 3889.73 samples/sec  Loss 1.1267  LearningRate 0.0822  ProxyLR: 4.1117  Epoch: 8  Global Step: 51000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:51,493-Speed 3889.37 samples/sec  Loss 1.1150  LearningRate 0.0822  ProxyLR: 4.1108  Epoch: 8  Global Step: 51010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:54,126-Speed 3891.42 samples/sec  Loss 1.1472  LearningRate 0.0822  ProxyLR: 4.1099  Epoch: 8  Global Step: 51020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:56,760-Speed 3888.00 samples/sec  Loss 1.1386  LearningRate 0.0822  ProxyLR: 4.1090  Epoch: 8  Global Step: 51030   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:19:59,393-Speed 3890.54 samples/sec  Loss 1.0741  LearningRate 0.0822  ProxyLR: 4.1081  Epoch: 8  Global Step: 51040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:02,025-Speed 3890.84 samples/sec  Loss 1.1277  LearningRate 0.0821  ProxyLR: 4.1072  Epoch: 8  Global Step: 51050   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:20:04,651-Speed 3900.30 samples/sec  Loss 1.1105  LearningRate 0.0821  ProxyLR: 4.1063  Epoch: 8  Global Step: 51060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:07,285-Speed 3889.57 samples/sec  Loss 1.2384  LearningRate 0.0821  ProxyLR: 4.1054  Epoch: 8  Global Step: 51070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:09,914-Speed 3895.16 samples/sec  Loss 1.1948  LearningRate 0.0821  ProxyLR: 4.1045  Epoch: 8  Global Step: 51080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:12,546-Speed 3891.86 samples/sec  Loss 1.1608  LearningRate 0.0821  ProxyLR: 4.1036  Epoch: 8  Global Step: 51090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:15,177-Speed 3893.30 samples/sec  Loss 1.2001  LearningRate 0.0821  ProxyLR: 4.1027  Epoch: 8  Global Step: 51100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:17,808-Speed 3892.63 samples/sec  Loss 1.1383  LearningRate 0.0820  ProxyLR: 4.1018  Epoch: 8  Global Step: 51110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:20,439-Speed 3892.29 samples/sec  Loss 1.2044  LearningRate 0.0820  ProxyLR: 4.1009  Epoch: 8  Global Step: 51120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:23,070-Speed 3892.79 samples/sec  Loss 1.1565  LearningRate 0.0820  ProxyLR: 4.1000  Epoch: 8  Global Step: 51130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:25,704-Speed 3889.86 samples/sec  Loss 1.2052  LearningRate 0.0820  ProxyLR: 4.0991  Epoch: 8  Global Step: 51140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:28,334-Speed 3893.44 samples/sec  Loss 1.1349  LearningRate 0.0820  ProxyLR: 4.0982  Epoch: 8  Global Step: 51150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:30,953-Speed 3911.47 samples/sec  Loss 1.1202  LearningRate 0.0819  ProxyLR: 4.0973  Epoch: 8  Global Step: 51160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:33,639-Speed 3813.37 samples/sec  Loss 1.1921  LearningRate 0.0819  ProxyLR: 4.0964  Epoch: 8  Global Step: 51170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:43,030-Speed 1090.48 samples/sec  Loss 1.0088  LearningRate 0.0819  ProxyLR: 4.0955  Epoch: 9  Global Step: 51180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:45,719-Speed 3809.16 samples/sec  Loss 0.8003  LearningRate 0.0819  ProxyLR: 4.0946  Epoch: 9  Global Step: 51190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:48,357-Speed 3882.35 samples/sec  Loss 0.7478  LearningRate 0.0819  ProxyLR: 4.0937  Epoch: 9  Global Step: 51200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:20:51,017-Speed 3850.60 samples/sec  Loss 0.7588  LearningRate 0.0819  ProxyLR: 4.0928  Epoch: 9  Global Step: 51210   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:20:53,655-Speed 3883.64 samples/sec  Loss 0.7910  LearningRate 0.0818  ProxyLR: 4.0919  Epoch: 9  Global Step: 51220   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:20:56,291-Speed 3885.79 samples/sec  Loss 0.7134  LearningRate 0.0818  ProxyLR: 4.0910  Epoch: 9  Global Step: 51230   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:20:58,926-Speed 3887.73 samples/sec  Loss 0.7600  LearningRate 0.0818  ProxyLR: 4.0901  Epoch: 9  Global Step: 51240   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:01,559-Speed 3889.65 samples/sec  Loss 0.7191  LearningRate 0.0818  ProxyLR: 4.0892  Epoch: 9  Global Step: 51250   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:04,191-Speed 3891.98 samples/sec  Loss 0.7227  LearningRate 0.0818  ProxyLR: 4.0883  Epoch: 9  Global Step: 51260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:06,826-Speed 3887.14 samples/sec  Loss 0.7736  LearningRate 0.0817  ProxyLR: 4.0874  Epoch: 9  Global Step: 51270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:09,536-Speed 3779.74 samples/sec  Loss 0.6921  LearningRate 0.0817  ProxyLR: 4.0865  Epoch: 9  Global Step: 51280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:12,169-Speed 3890.19 samples/sec  Loss 0.7083  LearningRate 0.0817  ProxyLR: 4.0856  Epoch: 9  Global Step: 51290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:14,804-Speed 3887.93 samples/sec  Loss 0.6743  LearningRate 0.0817  ProxyLR: 4.0847  Epoch: 9  Global Step: 51300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:17,434-Speed 3894.58 samples/sec  Loss 0.7413  LearningRate 0.0817  ProxyLR: 4.0838  Epoch: 9  Global Step: 51310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:20,063-Speed 3895.26 samples/sec  Loss 0.7334  LearningRate 0.0817  ProxyLR: 4.0829  Epoch: 9  Global Step: 51320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:22,694-Speed 3892.86 samples/sec  Loss 0.7157  LearningRate 0.0816  ProxyLR: 4.0820  Epoch: 9  Global Step: 51330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:25,378-Speed 3817.30 samples/sec  Loss 0.7076  LearningRate 0.0816  ProxyLR: 4.0811  Epoch: 9  Global Step: 51340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:28,009-Speed 3892.75 samples/sec  Loss 0.6840  LearningRate 0.0816  ProxyLR: 4.0802  Epoch: 9  Global Step: 51350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:30,642-Speed 3890.59 samples/sec  Loss 0.6973  LearningRate 0.0816  ProxyLR: 4.0793  Epoch: 9  Global Step: 51360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:33,276-Speed 3888.28 samples/sec  Loss 0.7388  LearningRate 0.0816  ProxyLR: 4.0784  Epoch: 9  Global Step: 51370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:35,908-Speed 3892.25 samples/sec  Loss 0.7085  LearningRate 0.0815  ProxyLR: 4.0775  Epoch: 9  Global Step: 51380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:21:38,525-Speed 3913.03 samples/sec  Loss 0.7251  LearningRate 0.0815  ProxyLR: 4.0766  Epoch: 9  Global Step: 51390   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:41,160-Speed 3888.10 samples/sec  Loss 0.6855  LearningRate 0.0815  ProxyLR: 4.0757  Epoch: 9  Global Step: 51400   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:43,796-Speed 3884.95 samples/sec  Loss 0.7253  LearningRate 0.0815  ProxyLR: 4.0748  Epoch: 9  Global Step: 51410   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:46,434-Speed 3883.07 samples/sec  Loss 0.7177  LearningRate 0.0815  ProxyLR: 4.0739  Epoch: 9  Global Step: 51420   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:49,071-Speed 3884.05 samples/sec  Loss 0.7159  LearningRate 0.0815  ProxyLR: 4.0730  Epoch: 9  Global Step: 51430   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:51,710-Speed 3881.45 samples/sec  Loss 0.7852  LearningRate 0.0814  ProxyLR: 4.0721  Epoch: 9  Global Step: 51440   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:54,383-Speed 3832.61 samples/sec  Loss 0.8148  LearningRate 0.0814  ProxyLR: 4.0712  Epoch: 9  Global Step: 51450   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:57,018-Speed 3886.70 samples/sec  Loss 0.7679  LearningRate 0.0814  ProxyLR: 4.0703  Epoch: 9  Global Step: 51460   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:21:59,653-Speed 3887.62 samples/sec  Loss 0.8136  LearningRate 0.0814  ProxyLR: 4.0694  Epoch: 9  Global Step: 51470   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:22:02,290-Speed 3884.51 samples/sec  Loss 0.7437  LearningRate 0.0814  ProxyLR: 4.0685  Epoch: 9  Global Step: 51480   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:22:04,926-Speed 3884.71 samples/sec  Loss 0.7522  LearningRate 0.0814  ProxyLR: 4.0676  Epoch: 9  Global Step: 51490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:07,563-Speed 3885.45 samples/sec  Loss 0.7439  LearningRate 0.0813  ProxyLR: 4.0667  Epoch: 9  Global Step: 51500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:10,201-Speed 3882.57 samples/sec  Loss 0.7474  LearningRate 0.0813  ProxyLR: 4.0658  Epoch: 9  Global Step: 51510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:12,837-Speed 3884.85 samples/sec  Loss 0.7339  LearningRate 0.0813  ProxyLR: 4.0649  Epoch: 9  Global Step: 51520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:15,474-Speed 3884.91 samples/sec  Loss 0.7428  LearningRate 0.0813  ProxyLR: 4.0640  Epoch: 9  Global Step: 51530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:18,113-Speed 3881.91 samples/sec  Loss 0.7329  LearningRate 0.0813  ProxyLR: 4.0631  Epoch: 9  Global Step: 51540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:20,748-Speed 3886.66 samples/sec  Loss 0.7590  LearningRate 0.0812  ProxyLR: 4.0622  Epoch: 9  Global Step: 51550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:23,384-Speed 3886.34 samples/sec  Loss 0.7575  LearningRate 0.0812  ProxyLR: 4.0613  Epoch: 9  Global Step: 51560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:26,020-Speed 3885.20 samples/sec  Loss 0.7400  LearningRate 0.0812  ProxyLR: 4.0604  Epoch: 9  Global Step: 51570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:28,658-Speed 3882.56 samples/sec  Loss 0.7302  LearningRate 0.0812  ProxyLR: 4.0595  Epoch: 9  Global Step: 51580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:31,292-Speed 3888.53 samples/sec  Loss 0.7350  LearningRate 0.0812  ProxyLR: 4.0586  Epoch: 9  Global Step: 51590   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:22:33,928-Speed 3886.34 samples/sec  Loss 0.7344  LearningRate 0.0812  ProxyLR: 4.0577  Epoch: 9  Global Step: 51600   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:22:36,563-Speed 3886.33 samples/sec  Loss 0.7007  LearningRate 0.0811  ProxyLR: 4.0568  Epoch: 9  Global Step: 51610   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:22:39,196-Speed 3890.91 samples/sec  Loss 0.7357  LearningRate 0.0811  ProxyLR: 4.0559  Epoch: 9  Global Step: 51620   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:22:41,816-Speed 3909.08 samples/sec  Loss 0.6838  LearningRate 0.0811  ProxyLR: 4.0550  Epoch: 9  Global Step: 51630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:44,449-Speed 3890.18 samples/sec  Loss 0.7393  LearningRate 0.0811  ProxyLR: 4.0541  Epoch: 9  Global Step: 51640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:47,084-Speed 3887.92 samples/sec  Loss 0.7925  LearningRate 0.0811  ProxyLR: 4.0533  Epoch: 9  Global Step: 51650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:49,717-Speed 3889.23 samples/sec  Loss 0.8170  LearningRate 0.0810  ProxyLR: 4.0524  Epoch: 9  Global Step: 51660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:52,350-Speed 3889.79 samples/sec  Loss 0.7367  LearningRate 0.0810  ProxyLR: 4.0515  Epoch: 9  Global Step: 51670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:54,979-Speed 3897.46 samples/sec  Loss 0.7477  LearningRate 0.0810  ProxyLR: 4.0506  Epoch: 9  Global Step: 51680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:22:57,611-Speed 3890.80 samples/sec  Loss 0.6994  LearningRate 0.0810  ProxyLR: 4.0497  Epoch: 9  Global Step: 51690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:00,226-Speed 3917.39 samples/sec  Loss 0.8361  LearningRate 0.0810  ProxyLR: 4.0488  Epoch: 9  Global Step: 51700   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:02,852-Speed 3900.77 samples/sec  Loss 0.8472  LearningRate 0.0810  ProxyLR: 4.0479  Epoch: 9  Global Step: 51710   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:05,479-Speed 3898.78 samples/sec  Loss 0.7630  LearningRate 0.0809  ProxyLR: 4.0470  Epoch: 9  Global Step: 51720   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:08,109-Speed 3894.38 samples/sec  Loss 0.7514  LearningRate 0.0809  ProxyLR: 4.0461  Epoch: 9  Global Step: 51730   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:10,736-Speed 3898.58 samples/sec  Loss 0.7576  LearningRate 0.0809  ProxyLR: 4.0452  Epoch: 9  Global Step: 51740   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:13,364-Speed 3898.59 samples/sec  Loss 0.7837  LearningRate 0.0809  ProxyLR: 4.0443  Epoch: 9  Global Step: 51750   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:15,992-Speed 3897.65 samples/sec  Loss 0.7981  LearningRate 0.0809  ProxyLR: 4.0434  Epoch: 9  Global Step: 51760   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:18,619-Speed 3898.12 samples/sec  Loss 0.7382  LearningRate 0.0809  ProxyLR: 4.0425  Epoch: 9  Global Step: 51770   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:21,247-Speed 3897.79 samples/sec  Loss 0.7520  LearningRate 0.0808  ProxyLR: 4.0416  Epoch: 9  Global Step: 51780   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:23,876-Speed 3895.55 samples/sec  Loss 0.7412  LearningRate 0.0808  ProxyLR: 4.0407  Epoch: 9  Global Step: 51790   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:23:26,503-Speed 3899.65 samples/sec  Loss 0.7766  LearningRate 0.0808  ProxyLR: 4.0398  Epoch: 9  Global Step: 51800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:29,131-Speed 3897.26 samples/sec  Loss 0.7211  LearningRate 0.0808  ProxyLR: 4.0389  Epoch: 9  Global Step: 51810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:31,758-Speed 3898.64 samples/sec  Loss 0.8000  LearningRate 0.0808  ProxyLR: 4.0380  Epoch: 9  Global Step: 51820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:34,386-Speed 3897.94 samples/sec  Loss 0.7594  LearningRate 0.0807  ProxyLR: 4.0371  Epoch: 9  Global Step: 51830   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:37,016-Speed 3894.19 samples/sec  Loss 0.7496  LearningRate 0.0807  ProxyLR: 4.0362  Epoch: 9  Global Step: 51840   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:39,651-Speed 3887.52 samples/sec  Loss 0.8098  LearningRate 0.0807  ProxyLR: 4.0354  Epoch: 9  Global Step: 51850   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:42,286-Speed 3887.71 samples/sec  Loss 0.7608  LearningRate 0.0807  ProxyLR: 4.0345  Epoch: 9  Global Step: 51860   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:44,918-Speed 3891.20 samples/sec  Loss 0.7594  LearningRate 0.0807  ProxyLR: 4.0336  Epoch: 9  Global Step: 51870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:47,554-Speed 3885.09 samples/sec  Loss 0.7527  LearningRate 0.0807  ProxyLR: 4.0327  Epoch: 9  Global Step: 51880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:50,188-Speed 3889.82 samples/sec  Loss 0.7672  LearningRate 0.0806  ProxyLR: 4.0318  Epoch: 9  Global Step: 51890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:23:52,823-Speed 3885.82 samples/sec  Loss 0.7349  LearningRate 0.0806  ProxyLR: 4.0309  Epoch: 9  Global Step: 51900   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:23:55,458-Speed 3887.77 samples/sec  Loss 0.8408  LearningRate 0.0806  ProxyLR: 4.0300  Epoch: 9  Global Step: 51910   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:23:58,078-Speed 3909.53 samples/sec  Loss 0.7846  LearningRate 0.0806  ProxyLR: 4.0291  Epoch: 9  Global Step: 51920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:00,712-Speed 3888.53 samples/sec  Loss 0.7648  LearningRate 0.0806  ProxyLR: 4.0282  Epoch: 9  Global Step: 51930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:03,348-Speed 3886.19 samples/sec  Loss 0.8099  LearningRate 0.0805  ProxyLR: 4.0273  Epoch: 9  Global Step: 51940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:05,980-Speed 3890.82 samples/sec  Loss 0.7998  LearningRate 0.0805  ProxyLR: 4.0264  Epoch: 9  Global Step: 51950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:08,613-Speed 3890.48 samples/sec  Loss 0.7675  LearningRate 0.0805  ProxyLR: 4.0255  Epoch: 9  Global Step: 51960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:11,245-Speed 3891.77 samples/sec  Loss 0.8291  LearningRate 0.0805  ProxyLR: 4.0246  Epoch: 9  Global Step: 51970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:13,876-Speed 3893.63 samples/sec  Loss 0.7701  LearningRate 0.0805  ProxyLR: 4.0237  Epoch: 9  Global Step: 51980   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:16,507-Speed 3892.01 samples/sec  Loss 0.7547  LearningRate 0.0805  ProxyLR: 4.0229  Epoch: 9  Global Step: 51990   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:19,136-Speed 3895.88 samples/sec  Loss 0.8208  LearningRate 0.0804  ProxyLR: 4.0220  Epoch: 9  Global Step: 52000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:21,767-Speed 3892.74 samples/sec  Loss 0.7584  LearningRate 0.0804  ProxyLR: 4.0211  Epoch: 9  Global Step: 52010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:24,386-Speed 3911.93 samples/sec  Loss 0.7738  LearningRate 0.0804  ProxyLR: 4.0202  Epoch: 9  Global Step: 52020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:27,016-Speed 3893.83 samples/sec  Loss 0.8128  LearningRate 0.0804  ProxyLR: 4.0193  Epoch: 9  Global Step: 52030   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:29,648-Speed 3892.76 samples/sec  Loss 0.7901  LearningRate 0.0804  ProxyLR: 4.0184  Epoch: 9  Global Step: 52040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:32,278-Speed 3894.41 samples/sec  Loss 0.7752  LearningRate 0.0803  ProxyLR: 4.0175  Epoch: 9  Global Step: 52050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:34,910-Speed 3891.65 samples/sec  Loss 0.7981  LearningRate 0.0803  ProxyLR: 4.0166  Epoch: 9  Global Step: 52060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:37,539-Speed 3894.70 samples/sec  Loss 0.7969  LearningRate 0.0803  ProxyLR: 4.0157  Epoch: 9  Global Step: 52070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:40,172-Speed 3891.21 samples/sec  Loss 0.8353  LearningRate 0.0803  ProxyLR: 4.0148  Epoch: 9  Global Step: 52080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:42,802-Speed 3894.27 samples/sec  Loss 0.8303  LearningRate 0.0803  ProxyLR: 4.0139  Epoch: 9  Global Step: 52090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:45,433-Speed 3892.28 samples/sec  Loss 0.8860  LearningRate 0.0803  ProxyLR: 4.0130  Epoch: 9  Global Step: 52100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:48,064-Speed 3893.36 samples/sec  Loss 0.7755  LearningRate 0.0802  ProxyLR: 4.0122  Epoch: 9  Global Step: 52110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:24:50,697-Speed 3890.94 samples/sec  Loss 0.8503  LearningRate 0.0802  ProxyLR: 4.0113  Epoch: 9  Global Step: 52120   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:24:53,326-Speed 3895.23 samples/sec  Loss 0.7766  LearningRate 0.0802  ProxyLR: 4.0104  Epoch: 9  Global Step: 52130   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:24:55,957-Speed 3893.02 samples/sec  Loss 0.7916  LearningRate 0.0802  ProxyLR: 4.0095  Epoch: 9  Global Step: 52140   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:24:58,590-Speed 3890.65 samples/sec  Loss 0.7821  LearningRate 0.0802  ProxyLR: 4.0086  Epoch: 9  Global Step: 52150   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:25:01,221-Speed 3892.18 samples/sec  Loss 0.7584  LearningRate 0.0802  ProxyLR: 4.0077  Epoch: 9  Global Step: 52160   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:25:03,851-Speed 3894.27 samples/sec  Loss 0.8527  LearningRate 0.0801  ProxyLR: 4.0068  Epoch: 9  Global Step: 52170   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:25:06,467-Speed 3915.91 samples/sec  Loss 0.7564  LearningRate 0.0801  ProxyLR: 4.0059  Epoch: 9  Global Step: 52180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:09,095-Speed 3896.65 samples/sec  Loss 0.8120  LearningRate 0.0801  ProxyLR: 4.0050  Epoch: 9  Global Step: 52190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:11,729-Speed 3889.56 samples/sec  Loss 0.7734  LearningRate 0.0801  ProxyLR: 4.0041  Epoch: 9  Global Step: 52200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:14,359-Speed 3894.84 samples/sec  Loss 0.7884  LearningRate 0.0801  ProxyLR: 4.0032  Epoch: 9  Global Step: 52210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:16,989-Speed 3894.20 samples/sec  Loss 0.7611  LearningRate 0.0800  ProxyLR: 4.0024  Epoch: 9  Global Step: 52220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:19,620-Speed 3892.96 samples/sec  Loss 0.7955  LearningRate 0.0800  ProxyLR: 4.0015  Epoch: 9  Global Step: 52230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:22,246-Speed 3899.75 samples/sec  Loss 0.7634  LearningRate 0.0800  ProxyLR: 4.0006  Epoch: 9  Global Step: 52240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:24,876-Speed 3895.58 samples/sec  Loss 0.8514  LearningRate 0.0800  ProxyLR: 3.9997  Epoch: 9  Global Step: 52250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:27,504-Speed 3896.97 samples/sec  Loss 0.8025  LearningRate 0.0800  ProxyLR: 3.9988  Epoch: 9  Global Step: 52260   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:30,130-Speed 3900.14 samples/sec  Loss 0.8484  LearningRate 0.0800  ProxyLR: 3.9979  Epoch: 9  Global Step: 52270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:32,755-Speed 3901.82 samples/sec  Loss 0.8582  LearningRate 0.0799  ProxyLR: 3.9970  Epoch: 9  Global Step: 52280   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:25:35,367-Speed 3922.10 samples/sec  Loss 0.8384  LearningRate 0.0799  ProxyLR: 3.9961  Epoch: 9  Global Step: 52290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:37,993-Speed 3899.94 samples/sec  Loss 0.8875  LearningRate 0.0799  ProxyLR: 3.9952  Epoch: 9  Global Step: 52300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:40,618-Speed 3901.53 samples/sec  Loss 0.8606  LearningRate 0.0799  ProxyLR: 3.9943  Epoch: 9  Global Step: 52310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:43,244-Speed 3901.13 samples/sec  Loss 0.9023  LearningRate 0.0799  ProxyLR: 3.9935  Epoch: 9  Global Step: 52320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:45,869-Speed 3901.98 samples/sec  Loss 0.8354  LearningRate 0.0799  ProxyLR: 3.9926  Epoch: 9  Global Step: 52330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:48,497-Speed 3896.84 samples/sec  Loss 0.8787  LearningRate 0.0798  ProxyLR: 3.9917  Epoch: 9  Global Step: 52340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:51,124-Speed 3899.44 samples/sec  Loss 0.8180  LearningRate 0.0798  ProxyLR: 3.9908  Epoch: 9  Global Step: 52350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:53,749-Speed 3900.91 samples/sec  Loss 0.8434  LearningRate 0.0798  ProxyLR: 3.9899  Epoch: 9  Global Step: 52360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:56,375-Speed 3901.50 samples/sec  Loss 0.8459  LearningRate 0.0798  ProxyLR: 3.9890  Epoch: 9  Global Step: 52370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:25:59,001-Speed 3900.34 samples/sec  Loss 0.8330  LearningRate 0.0798  ProxyLR: 3.9881  Epoch: 9  Global Step: 52380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:01,627-Speed 3900.32 samples/sec  Loss 0.8394  LearningRate 0.0797  ProxyLR: 3.9872  Epoch: 9  Global Step: 52390   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:26:04,254-Speed 3899.12 samples/sec  Loss 0.8080  LearningRate 0.0797  ProxyLR: 3.9863  Epoch: 9  Global Step: 52400   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:26:06,880-Speed 3899.49 samples/sec  Loss 0.8783  LearningRate 0.0797  ProxyLR: 3.9855  Epoch: 9  Global Step: 52410   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:26:09,493-Speed 3920.16 samples/sec  Loss 0.8602  LearningRate 0.0797  ProxyLR: 3.9846  Epoch: 9  Global Step: 52420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:12,119-Speed 3899.92 samples/sec  Loss 0.7904  LearningRate 0.0797  ProxyLR: 3.9837  Epoch: 9  Global Step: 52430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:14,745-Speed 3900.49 samples/sec  Loss 0.7957  LearningRate 0.0797  ProxyLR: 3.9828  Epoch: 9  Global Step: 52440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:17,374-Speed 3896.09 samples/sec  Loss 0.8760  LearningRate 0.0796  ProxyLR: 3.9819  Epoch: 9  Global Step: 52450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:20,001-Speed 3899.75 samples/sec  Loss 0.7842  LearningRate 0.0796  ProxyLR: 3.9810  Epoch: 9  Global Step: 52460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:22,628-Speed 3898.78 samples/sec  Loss 0.8682  LearningRate 0.0796  ProxyLR: 3.9801  Epoch: 9  Global Step: 52470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:25,257-Speed 3896.47 samples/sec  Loss 0.8847  LearningRate 0.0796  ProxyLR: 3.9792  Epoch: 9  Global Step: 52480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:27,885-Speed 3896.61 samples/sec  Loss 0.8274  LearningRate 0.0796  ProxyLR: 3.9784  Epoch: 9  Global Step: 52490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:30,512-Speed 3899.55 samples/sec  Loss 0.8604  LearningRate 0.0795  ProxyLR: 3.9775  Epoch: 9  Global Step: 52500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:33,138-Speed 3899.70 samples/sec  Loss 0.9035  LearningRate 0.0795  ProxyLR: 3.9766  Epoch: 9  Global Step: 52510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:35,765-Speed 3898.34 samples/sec  Loss 0.8557  LearningRate 0.0795  ProxyLR: 3.9757  Epoch: 9  Global Step: 52520   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:26:38,392-Speed 3899.61 samples/sec  Loss 0.8439  LearningRate 0.0795  ProxyLR: 3.9748  Epoch: 9  Global Step: 52530   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:26:41,006-Speed 3918.18 samples/sec  Loss 0.8457  LearningRate 0.0795  ProxyLR: 3.9739  Epoch: 9  Global Step: 52540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:43,633-Speed 3899.56 samples/sec  Loss 0.8364  LearningRate 0.0795  ProxyLR: 3.9730  Epoch: 9  Global Step: 52550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:46,259-Speed 3899.80 samples/sec  Loss 0.8503  LearningRate 0.0794  ProxyLR: 3.9721  Epoch: 9  Global Step: 52560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:48,885-Speed 3899.55 samples/sec  Loss 0.8813  LearningRate 0.0794  ProxyLR: 3.9713  Epoch: 9  Global Step: 52570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:51,513-Speed 3897.94 samples/sec  Loss 0.9054  LearningRate 0.0794  ProxyLR: 3.9704  Epoch: 9  Global Step: 52580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:54,140-Speed 3899.97 samples/sec  Loss 0.8249  LearningRate 0.0794  ProxyLR: 3.9695  Epoch: 9  Global Step: 52590   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:56,768-Speed 3897.14 samples/sec  Loss 0.8380  LearningRate 0.0794  ProxyLR: 3.9686  Epoch: 9  Global Step: 52600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:26:59,401-Speed 3890.20 samples/sec  Loss 0.9124  LearningRate 0.0794  ProxyLR: 3.9677  Epoch: 9  Global Step: 52610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:02,033-Speed 3892.33 samples/sec  Loss 0.8489  LearningRate 0.0793  ProxyLR: 3.9668  Epoch: 9  Global Step: 52620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:04,664-Speed 3892.66 samples/sec  Loss 0.8602  LearningRate 0.0793  ProxyLR: 3.9659  Epoch: 9  Global Step: 52630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:07,282-Speed 3911.41 samples/sec  Loss 0.8662  LearningRate 0.0793  ProxyLR: 3.9651  Epoch: 9  Global Step: 52640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:09,917-Speed 3888.05 samples/sec  Loss 0.8443  LearningRate 0.0793  ProxyLR: 3.9642  Epoch: 9  Global Step: 52650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:12,550-Speed 3889.95 samples/sec  Loss 0.8059  LearningRate 0.0793  ProxyLR: 3.9633  Epoch: 9  Global Step: 52660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:15,179-Speed 3894.93 samples/sec  Loss 0.8667  LearningRate 0.0792  ProxyLR: 3.9624  Epoch: 9  Global Step: 52670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:17,809-Speed 3894.54 samples/sec  Loss 0.8282  LearningRate 0.0792  ProxyLR: 3.9615  Epoch: 9  Global Step: 52680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:20,441-Speed 3892.76 samples/sec  Loss 0.8775  LearningRate 0.0792  ProxyLR: 3.9606  Epoch: 9  Global Step: 52690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:23,072-Speed 3892.02 samples/sec  Loss 0.8532  LearningRate 0.0792  ProxyLR: 3.9597  Epoch: 9  Global Step: 52700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:25,705-Speed 3890.99 samples/sec  Loss 0.8642  LearningRate 0.0792  ProxyLR: 3.9589  Epoch: 9  Global Step: 52710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:28,336-Speed 3892.14 samples/sec  Loss 0.8990  LearningRate 0.0792  ProxyLR: 3.9580  Epoch: 9  Global Step: 52720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:30,969-Speed 3891.07 samples/sec  Loss 0.8584  LearningRate 0.0791  ProxyLR: 3.9571  Epoch: 9  Global Step: 52730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:33,601-Speed 3890.48 samples/sec  Loss 0.8453  LearningRate 0.0791  ProxyLR: 3.9562  Epoch: 9  Global Step: 52740   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:27:36,221-Speed 3909.36 samples/sec  Loss 0.9120  LearningRate 0.0791  ProxyLR: 3.9553  Epoch: 9  Global Step: 52750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:38,854-Speed 3890.95 samples/sec  Loss 0.8481  LearningRate 0.0791  ProxyLR: 3.9544  Epoch: 9  Global Step: 52760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:41,487-Speed 3890.24 samples/sec  Loss 0.8146  LearningRate 0.0791  ProxyLR: 3.9535  Epoch: 9  Global Step: 52770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:44,118-Speed 3892.43 samples/sec  Loss 0.8980  LearningRate 0.0791  ProxyLR: 3.9527  Epoch: 9  Global Step: 52780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:46,747-Speed 3896.53 samples/sec  Loss 0.8920  LearningRate 0.0790  ProxyLR: 3.9518  Epoch: 9  Global Step: 52790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:49,376-Speed 3896.08 samples/sec  Loss 0.8755  LearningRate 0.0790  ProxyLR: 3.9509  Epoch: 9  Global Step: 52800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:52,004-Speed 3896.92 samples/sec  Loss 0.8974  LearningRate 0.0790  ProxyLR: 3.9500  Epoch: 9  Global Step: 52810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:54,635-Speed 3893.94 samples/sec  Loss 0.8201  LearningRate 0.0790  ProxyLR: 3.9491  Epoch: 9  Global Step: 52820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:57,263-Speed 3897.09 samples/sec  Loss 0.8860  LearningRate 0.0790  ProxyLR: 3.9482  Epoch: 9  Global Step: 52830   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:27:59,890-Speed 3898.89 samples/sec  Loss 0.8903  LearningRate 0.0789  ProxyLR: 3.9474  Epoch: 9  Global Step: 52840   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:02,503-Speed 3919.87 samples/sec  Loss 0.8456  LearningRate 0.0789  ProxyLR: 3.9465  Epoch: 9  Global Step: 52850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:05,130-Speed 3899.11 samples/sec  Loss 0.8275  LearningRate 0.0789  ProxyLR: 3.9456  Epoch: 9  Global Step: 52860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:07,754-Speed 3902.08 samples/sec  Loss 0.8860  LearningRate 0.0789  ProxyLR: 3.9447  Epoch: 9  Global Step: 52870   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:10,377-Speed 3905.88 samples/sec  Loss 0.8661  LearningRate 0.0789  ProxyLR: 3.9438  Epoch: 9  Global Step: 52880   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:13,002-Speed 3901.22 samples/sec  Loss 0.7761  LearningRate 0.0789  ProxyLR: 3.9429  Epoch: 9  Global Step: 52890   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:15,625-Speed 3904.67 samples/sec  Loss 0.8364  LearningRate 0.0788  ProxyLR: 3.9421  Epoch: 9  Global Step: 52900   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:18,250-Speed 3902.89 samples/sec  Loss 0.9389  LearningRate 0.0788  ProxyLR: 3.9412  Epoch: 9  Global Step: 52910   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:20,875-Speed 3902.15 samples/sec  Loss 0.9162  LearningRate 0.0788  ProxyLR: 3.9403  Epoch: 9  Global Step: 52920   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:23,501-Speed 3899.66 samples/sec  Loss 0.8431  LearningRate 0.0788  ProxyLR: 3.9394  Epoch: 9  Global Step: 52930   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:26,127-Speed 3900.45 samples/sec  Loss 0.8740  LearningRate 0.0788  ProxyLR: 3.9385  Epoch: 9  Global Step: 52940   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:28:28,754-Speed 3898.95 samples/sec  Loss 0.8945  LearningRate 0.0788  ProxyLR: 3.9376  Epoch: 9  Global Step: 52950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:31,379-Speed 3902.16 samples/sec  Loss 0.8644  LearningRate 0.0787  ProxyLR: 3.9368  Epoch: 9  Global Step: 52960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:34,004-Speed 3902.27 samples/sec  Loss 0.8623  LearningRate 0.0787  ProxyLR: 3.9359  Epoch: 9  Global Step: 52970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:36,629-Speed 3901.41 samples/sec  Loss 0.8942  LearningRate 0.0787  ProxyLR: 3.9350  Epoch: 9  Global Step: 52980   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:39,255-Speed 3900.73 samples/sec  Loss 0.8709  LearningRate 0.0787  ProxyLR: 3.9341  Epoch: 9  Global Step: 52990   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:41,882-Speed 3899.59 samples/sec  Loss 0.8414  LearningRate 0.0787  ProxyLR: 3.9332  Epoch: 9  Global Step: 53000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:44,506-Speed 3902.47 samples/sec  Loss 0.9134  LearningRate 0.0786  ProxyLR: 3.9323  Epoch: 9  Global Step: 53010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:47,132-Speed 3901.23 samples/sec  Loss 0.8580  LearningRate 0.0786  ProxyLR: 3.9315  Epoch: 9  Global Step: 53020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:49,758-Speed 3899.95 samples/sec  Loss 0.8676  LearningRate 0.0786  ProxyLR: 3.9306  Epoch: 9  Global Step: 53030   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:52,382-Speed 3903.93 samples/sec  Loss 0.8741  LearningRate 0.0786  ProxyLR: 3.9297  Epoch: 9  Global Step: 53040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:54,995-Speed 3919.02 samples/sec  Loss 0.9502  LearningRate 0.0786  ProxyLR: 3.9288  Epoch: 9  Global Step: 53050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:28:57,619-Speed 3903.87 samples/sec  Loss 0.9394  LearningRate 0.0786  ProxyLR: 3.9279  Epoch: 9  Global Step: 53060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:00,243-Speed 3902.60 samples/sec  Loss 0.8741  LearningRate 0.0785  ProxyLR: 3.9271  Epoch: 9  Global Step: 53070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:02,867-Speed 3903.41 samples/sec  Loss 0.8860  LearningRate 0.0785  ProxyLR: 3.9262  Epoch: 9  Global Step: 53080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:05,493-Speed 3901.08 samples/sec  Loss 0.9593  LearningRate 0.0785  ProxyLR: 3.9253  Epoch: 9  Global Step: 53090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:08,118-Speed 3902.11 samples/sec  Loss 0.9398  LearningRate 0.0785  ProxyLR: 3.9244  Epoch: 9  Global Step: 53100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:10,743-Speed 3901.87 samples/sec  Loss 0.9255  LearningRate 0.0785  ProxyLR: 3.9235  Epoch: 9  Global Step: 53110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:13,368-Speed 3902.05 samples/sec  Loss 0.8958  LearningRate 0.0785  ProxyLR: 3.9226  Epoch: 9  Global Step: 53120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:15,997-Speed 3896.02 samples/sec  Loss 0.8748  LearningRate 0.0784  ProxyLR: 3.9218  Epoch: 9  Global Step: 53130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:18,622-Speed 3901.75 samples/sec  Loss 0.9081  LearningRate 0.0784  ProxyLR: 3.9209  Epoch: 9  Global Step: 53140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:21,247-Speed 3901.98 samples/sec  Loss 0.8715  LearningRate 0.0784  ProxyLR: 3.9200  Epoch: 9  Global Step: 53150   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:29:23,859-Speed 3921.39 samples/sec  Loss 0.9343  LearningRate 0.0784  ProxyLR: 3.9191  Epoch: 9  Global Step: 53160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:26,483-Speed 3904.10 samples/sec  Loss 0.8985  LearningRate 0.0784  ProxyLR: 3.9182  Epoch: 9  Global Step: 53170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:29,107-Speed 3903.04 samples/sec  Loss 0.8741  LearningRate 0.0783  ProxyLR: 3.9174  Epoch: 9  Global Step: 53180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:31,730-Speed 3904.47 samples/sec  Loss 0.9506  LearningRate 0.0783  ProxyLR: 3.9165  Epoch: 9  Global Step: 53190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:34,356-Speed 3900.72 samples/sec  Loss 0.9187  LearningRate 0.0783  ProxyLR: 3.9156  Epoch: 9  Global Step: 53200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:36,980-Speed 3903.04 samples/sec  Loss 0.8888  LearningRate 0.0783  ProxyLR: 3.9147  Epoch: 9  Global Step: 53210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:39,605-Speed 3902.32 samples/sec  Loss 0.9049  LearningRate 0.0783  ProxyLR: 3.9138  Epoch: 9  Global Step: 53220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:42,229-Speed 3902.75 samples/sec  Loss 0.8774  LearningRate 0.0783  ProxyLR: 3.9130  Epoch: 9  Global Step: 53230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:44,853-Speed 3904.12 samples/sec  Loss 0.8814  LearningRate 0.0782  ProxyLR: 3.9121  Epoch: 9  Global Step: 53240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:47,475-Speed 3905.96 samples/sec  Loss 0.9113  LearningRate 0.0782  ProxyLR: 3.9112  Epoch: 9  Global Step: 53250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:50,099-Speed 3903.27 samples/sec  Loss 0.8569  LearningRate 0.0782  ProxyLR: 3.9103  Epoch: 9  Global Step: 53260   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:29:52,711-Speed 3921.95 samples/sec  Loss 0.9645  LearningRate 0.0782  ProxyLR: 3.9094  Epoch: 9  Global Step: 53270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:55,337-Speed 3899.67 samples/sec  Loss 0.9240  LearningRate 0.0782  ProxyLR: 3.9086  Epoch: 9  Global Step: 53280   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:29:57,961-Speed 3903.26 samples/sec  Loss 0.9269  LearningRate 0.0782  ProxyLR: 3.9077  Epoch: 9  Global Step: 53290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:00,588-Speed 3899.07 samples/sec  Loss 0.8937  LearningRate 0.0781  ProxyLR: 3.9068  Epoch: 9  Global Step: 53300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:03,212-Speed 3903.95 samples/sec  Loss 0.9321  LearningRate 0.0781  ProxyLR: 3.9059  Epoch: 9  Global Step: 53310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:05,836-Speed 3903.47 samples/sec  Loss 0.9153  LearningRate 0.0781  ProxyLR: 3.9050  Epoch: 9  Global Step: 53320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:08,459-Speed 3904.18 samples/sec  Loss 0.9350  LearningRate 0.0781  ProxyLR: 3.9042  Epoch: 9  Global Step: 53330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:11,084-Speed 3903.09 samples/sec  Loss 0.9760  LearningRate 0.0781  ProxyLR: 3.9033  Epoch: 9  Global Step: 53340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:13,708-Speed 3903.59 samples/sec  Loss 0.9290  LearningRate 0.0780  ProxyLR: 3.9024  Epoch: 9  Global Step: 53350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:16,332-Speed 3902.68 samples/sec  Loss 0.8973  LearningRate 0.0780  ProxyLR: 3.9015  Epoch: 9  Global Step: 53360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:18,943-Speed 3922.77 samples/sec  Loss 0.9548  LearningRate 0.0780  ProxyLR: 3.9006  Epoch: 9  Global Step: 53370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:21,570-Speed 3898.54 samples/sec  Loss 0.8971  LearningRate 0.0780  ProxyLR: 3.8998  Epoch: 9  Global Step: 53380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:24,196-Speed 3901.38 samples/sec  Loss 0.9099  LearningRate 0.0780  ProxyLR: 3.8989  Epoch: 9  Global Step: 53390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:26,821-Speed 3900.94 samples/sec  Loss 0.9152  LearningRate 0.0780  ProxyLR: 3.8980  Epoch: 9  Global Step: 53400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:29,445-Speed 3903.35 samples/sec  Loss 0.9384  LearningRate 0.0779  ProxyLR: 3.8971  Epoch: 9  Global Step: 53410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:32,072-Speed 3899.43 samples/sec  Loss 0.9717  LearningRate 0.0779  ProxyLR: 3.8963  Epoch: 9  Global Step: 53420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:34,697-Speed 3902.52 samples/sec  Loss 0.9251  LearningRate 0.0779  ProxyLR: 3.8954  Epoch: 9  Global Step: 53430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:37,321-Speed 3902.72 samples/sec  Loss 0.9339  LearningRate 0.0779  ProxyLR: 3.8945  Epoch: 9  Global Step: 53440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:39,946-Speed 3902.09 samples/sec  Loss 0.8920  LearningRate 0.0779  ProxyLR: 3.8936  Epoch: 9  Global Step: 53450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:42,572-Speed 3899.92 samples/sec  Loss 0.9076  LearningRate 0.0779  ProxyLR: 3.8927  Epoch: 9  Global Step: 53460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:45,198-Speed 3901.74 samples/sec  Loss 0.9549  LearningRate 0.0778  ProxyLR: 3.8919  Epoch: 9  Global Step: 53470   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:30:47,823-Speed 3901.73 samples/sec  Loss 0.9434  LearningRate 0.0778  ProxyLR: 3.8910  Epoch: 9  Global Step: 53480   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:30:50,437-Speed 3918.56 samples/sec  Loss 0.9335  LearningRate 0.0778  ProxyLR: 3.8901  Epoch: 9  Global Step: 53490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:53,063-Speed 3900.06 samples/sec  Loss 0.9364  LearningRate 0.0778  ProxyLR: 3.8892  Epoch: 9  Global Step: 53500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:55,688-Speed 3901.76 samples/sec  Loss 0.9101  LearningRate 0.0778  ProxyLR: 3.8884  Epoch: 9  Global Step: 53510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:30:58,314-Speed 3900.12 samples/sec  Loss 0.9769  LearningRate 0.0777  ProxyLR: 3.8875  Epoch: 9  Global Step: 53520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:00,941-Speed 3899.56 samples/sec  Loss 0.9371  LearningRate 0.0777  ProxyLR: 3.8866  Epoch: 9  Global Step: 53530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:03,566-Speed 3901.60 samples/sec  Loss 0.8964  LearningRate 0.0777  ProxyLR: 3.8857  Epoch: 9  Global Step: 53540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:06,190-Speed 3903.27 samples/sec  Loss 0.8987  LearningRate 0.0777  ProxyLR: 3.8848  Epoch: 9  Global Step: 53550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:08,814-Speed 3903.45 samples/sec  Loss 0.9033  LearningRate 0.0777  ProxyLR: 3.8840  Epoch: 9  Global Step: 53560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:11,439-Speed 3901.61 samples/sec  Loss 0.8776  LearningRate 0.0777  ProxyLR: 3.8831  Epoch: 9  Global Step: 53570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:14,064-Speed 3902.16 samples/sec  Loss 0.9431  LearningRate 0.0776  ProxyLR: 3.8822  Epoch: 9  Global Step: 53580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:16,690-Speed 3900.50 samples/sec  Loss 0.9098  LearningRate 0.0776  ProxyLR: 3.8813  Epoch: 9  Global Step: 53590   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:31:19,305-Speed 3917.36 samples/sec  Loss 0.9206  LearningRate 0.0776  ProxyLR: 3.8805  Epoch: 9  Global Step: 53600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:21,935-Speed 3893.56 samples/sec  Loss 0.9259  LearningRate 0.0776  ProxyLR: 3.8796  Epoch: 9  Global Step: 53610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:24,568-Speed 3891.30 samples/sec  Loss 0.9295  LearningRate 0.0776  ProxyLR: 3.8787  Epoch: 9  Global Step: 53620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:27,202-Speed 3888.42 samples/sec  Loss 0.9458  LearningRate 0.0776  ProxyLR: 3.8778  Epoch: 9  Global Step: 53630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:29,834-Speed 3891.41 samples/sec  Loss 0.9684  LearningRate 0.0775  ProxyLR: 3.8770  Epoch: 9  Global Step: 53640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:32,465-Speed 3891.91 samples/sec  Loss 0.9827  LearningRate 0.0775  ProxyLR: 3.8761  Epoch: 9  Global Step: 53650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:35,099-Speed 3889.11 samples/sec  Loss 0.9957  LearningRate 0.0775  ProxyLR: 3.8752  Epoch: 9  Global Step: 53660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:37,732-Speed 3889.93 samples/sec  Loss 0.9623  LearningRate 0.0775  ProxyLR: 3.8743  Epoch: 9  Global Step: 53670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:40,364-Speed 3891.26 samples/sec  Loss 0.9035  LearningRate 0.0775  ProxyLR: 3.8735  Epoch: 9  Global Step: 53680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:42,997-Speed 3890.58 samples/sec  Loss 0.9542  LearningRate 0.0775  ProxyLR: 3.8726  Epoch: 9  Global Step: 53690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:45,615-Speed 3911.75 samples/sec  Loss 0.9642  LearningRate 0.0774  ProxyLR: 3.8717  Epoch: 9  Global Step: 53700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:48,251-Speed 3886.74 samples/sec  Loss 0.9471  LearningRate 0.0774  ProxyLR: 3.8708  Epoch: 9  Global Step: 53710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:50,885-Speed 3887.89 samples/sec  Loss 1.0361  LearningRate 0.0774  ProxyLR: 3.8700  Epoch: 9  Global Step: 53720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:53,518-Speed 3891.24 samples/sec  Loss 0.9927  LearningRate 0.0774  ProxyLR: 3.8691  Epoch: 9  Global Step: 53730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:56,153-Speed 3886.95 samples/sec  Loss 0.9939  LearningRate 0.0774  ProxyLR: 3.8682  Epoch: 9  Global Step: 53740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:31:58,787-Speed 3887.53 samples/sec  Loss 0.9284  LearningRate 0.0773  ProxyLR: 3.8673  Epoch: 9  Global Step: 53750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:01,421-Speed 3889.42 samples/sec  Loss 0.9379  LearningRate 0.0773  ProxyLR: 3.8665  Epoch: 9  Global Step: 53760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:04,043-Speed 3906.11 samples/sec  Loss 0.9592  LearningRate 0.0773  ProxyLR: 3.8656  Epoch: 9  Global Step: 53770   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:06,679-Speed 3886.05 samples/sec  Loss 0.9677  LearningRate 0.0773  ProxyLR: 3.8647  Epoch: 9  Global Step: 53780   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:09,304-Speed 3901.92 samples/sec  Loss 0.9555  LearningRate 0.0773  ProxyLR: 3.8638  Epoch: 9  Global Step: 53790   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:11,928-Speed 3902.58 samples/sec  Loss 0.9718  LearningRate 0.0773  ProxyLR: 3.8630  Epoch: 9  Global Step: 53800   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:14,554-Speed 3901.36 samples/sec  Loss 0.9706  LearningRate 0.0772  ProxyLR: 3.8621  Epoch: 9  Global Step: 53810   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:17,179-Speed 3901.00 samples/sec  Loss 1.0242  LearningRate 0.0772  ProxyLR: 3.8612  Epoch: 9  Global Step: 53820   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:19,806-Speed 3900.20 samples/sec  Loss 0.9561  LearningRate 0.0772  ProxyLR: 3.8603  Epoch: 9  Global Step: 53830   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:22,430-Speed 3902.28 samples/sec  Loss 0.9452  LearningRate 0.0772  ProxyLR: 3.8595  Epoch: 9  Global Step: 53840   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:25,055-Speed 3902.23 samples/sec  Loss 0.9989  LearningRate 0.0772  ProxyLR: 3.8586  Epoch: 9  Global Step: 53850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:27,679-Speed 3902.87 samples/sec  Loss 0.9582  LearningRate 0.0772  ProxyLR: 3.8577  Epoch: 9  Global Step: 53860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:32:30,304-Speed 3902.38 samples/sec  Loss 0.9423  LearningRate 0.0771  ProxyLR: 3.8568  Epoch: 9  Global Step: 53870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:32,930-Speed 3901.29 samples/sec  Loss 0.9441  LearningRate 0.0771  ProxyLR: 3.8560  Epoch: 9  Global Step: 53880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:35,557-Speed 3897.68 samples/sec  Loss 0.9766  LearningRate 0.0771  ProxyLR: 3.8551  Epoch: 9  Global Step: 53890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:38,188-Speed 3893.95 samples/sec  Loss 0.9761  LearningRate 0.0771  ProxyLR: 3.8542  Epoch: 9  Global Step: 53900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:40,818-Speed 3894.69 samples/sec  Loss 0.9675  LearningRate 0.0771  ProxyLR: 3.8533  Epoch: 9  Global Step: 53910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:43,452-Speed 3888.94 samples/sec  Loss 0.9718  LearningRate 0.0770  ProxyLR: 3.8525  Epoch: 9  Global Step: 53920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:46,082-Speed 3894.26 samples/sec  Loss 0.9314  LearningRate 0.0770  ProxyLR: 3.8516  Epoch: 9  Global Step: 53930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:48,713-Speed 3893.21 samples/sec  Loss 0.9232  LearningRate 0.0770  ProxyLR: 3.8507  Epoch: 9  Global Step: 53940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:51,344-Speed 3892.64 samples/sec  Loss 0.9474  LearningRate 0.0770  ProxyLR: 3.8498  Epoch: 9  Global Step: 53950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:53,976-Speed 3890.85 samples/sec  Loss 0.9461  LearningRate 0.0770  ProxyLR: 3.8490  Epoch: 9  Global Step: 53960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:32:56,610-Speed 3888.71 samples/sec  Loss 0.9206  LearningRate 0.0770  ProxyLR: 3.8481  Epoch: 9  Global Step: 53970   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:32:59,228-Speed 3912.62 samples/sec  Loss 0.9303  LearningRate 0.0769  ProxyLR: 3.8472  Epoch: 9  Global Step: 53980   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:01,843-Speed 3916.15 samples/sec  Loss 1.0233  LearningRate 0.0769  ProxyLR: 3.8464  Epoch: 9  Global Step: 53990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:04,474-Speed 3893.30 samples/sec  Loss 0.8854  LearningRate 0.0769  ProxyLR: 3.8455  Epoch: 9  Global Step: 54000   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:07,106-Speed 3891.84 samples/sec  Loss 0.9369  LearningRate 0.0769  ProxyLR: 3.8446  Epoch: 9  Global Step: 54010   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:09,739-Speed 3890.43 samples/sec  Loss 0.9718  LearningRate 0.0769  ProxyLR: 3.8437  Epoch: 9  Global Step: 54020   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:12,370-Speed 3893.06 samples/sec  Loss 0.9599  LearningRate 0.0769  ProxyLR: 3.8429  Epoch: 9  Global Step: 54030   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:15,000-Speed 3894.42 samples/sec  Loss 0.9377  LearningRate 0.0768  ProxyLR: 3.8420  Epoch: 9  Global Step: 54040   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:17,630-Speed 3893.72 samples/sec  Loss 0.9905  LearningRate 0.0768  ProxyLR: 3.8411  Epoch: 9  Global Step: 54050   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:20,261-Speed 3893.83 samples/sec  Loss 1.0264  LearningRate 0.0768  ProxyLR: 3.8402  Epoch: 9  Global Step: 54060   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:22,893-Speed 3891.75 samples/sec  Loss 0.9726  LearningRate 0.0768  ProxyLR: 3.8394  Epoch: 9  Global Step: 54070   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:25,524-Speed 3891.74 samples/sec  Loss 0.9874  LearningRate 0.0768  ProxyLR: 3.8385  Epoch: 9  Global Step: 54080   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:33:28,156-Speed 3892.40 samples/sec  Loss 0.8922  LearningRate 0.0768  ProxyLR: 3.8376  Epoch: 9  Global Step: 54090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:30,788-Speed 3891.11 samples/sec  Loss 0.9442  LearningRate 0.0767  ProxyLR: 3.8368  Epoch: 9  Global Step: 54100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:33,423-Speed 3887.17 samples/sec  Loss 0.9584  LearningRate 0.0767  ProxyLR: 3.8359  Epoch: 9  Global Step: 54110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:36,057-Speed 3889.68 samples/sec  Loss 0.9551  LearningRate 0.0767  ProxyLR: 3.8350  Epoch: 9  Global Step: 54120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:38,691-Speed 3888.28 samples/sec  Loss 0.9436  LearningRate 0.0767  ProxyLR: 3.8341  Epoch: 9  Global Step: 54130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:41,327-Speed 3885.94 samples/sec  Loss 1.0084  LearningRate 0.0767  ProxyLR: 3.8333  Epoch: 9  Global Step: 54140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:43,963-Speed 3885.74 samples/sec  Loss 0.9268  LearningRate 0.0766  ProxyLR: 3.8324  Epoch: 9  Global Step: 54150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:46,601-Speed 3881.45 samples/sec  Loss 1.0358  LearningRate 0.0766  ProxyLR: 3.8315  Epoch: 9  Global Step: 54160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:49,239-Speed 3883.42 samples/sec  Loss 0.9488  LearningRate 0.0766  ProxyLR: 3.8307  Epoch: 9  Global Step: 54170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:51,878-Speed 3881.09 samples/sec  Loss 1.0268  LearningRate 0.0766  ProxyLR: 3.8298  Epoch: 9  Global Step: 54180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:54,504-Speed 3899.69 samples/sec  Loss 0.9914  LearningRate 0.0766  ProxyLR: 3.8289  Epoch: 9  Global Step: 54190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:57,141-Speed 3884.38 samples/sec  Loss 1.0233  LearningRate 0.0766  ProxyLR: 3.8281  Epoch: 9  Global Step: 54200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:33:59,782-Speed 3879.28 samples/sec  Loss 0.9779  LearningRate 0.0765  ProxyLR: 3.8272  Epoch: 9  Global Step: 54210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:02,421-Speed 3880.03 samples/sec  Loss 1.0463  LearningRate 0.0765  ProxyLR: 3.8263  Epoch: 9  Global Step: 54220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:05,060-Speed 3881.94 samples/sec  Loss 0.9840  LearningRate 0.0765  ProxyLR: 3.8254  Epoch: 9  Global Step: 54230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:07,699-Speed 3881.61 samples/sec  Loss 1.0031  LearningRate 0.0765  ProxyLR: 3.8246  Epoch: 9  Global Step: 54240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:10,337-Speed 3881.78 samples/sec  Loss 1.0044  LearningRate 0.0765  ProxyLR: 3.8237  Epoch: 9  Global Step: 54250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:12,978-Speed 3878.10 samples/sec  Loss 1.0151  LearningRate 0.0765  ProxyLR: 3.8228  Epoch: 9  Global Step: 54260   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:15,617-Speed 3880.94 samples/sec  Loss 1.0224  LearningRate 0.0764  ProxyLR: 3.8220  Epoch: 9  Global Step: 54270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:18,255-Speed 3883.71 samples/sec  Loss 0.9912  LearningRate 0.0764  ProxyLR: 3.8211  Epoch: 9  Global Step: 54280   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:20,878-Speed 3903.91 samples/sec  Loss 0.9904  LearningRate 0.0764  ProxyLR: 3.8202  Epoch: 9  Global Step: 54290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:23,512-Speed 3888.57 samples/sec  Loss 1.0041  LearningRate 0.0764  ProxyLR: 3.8194  Epoch: 9  Global Step: 54300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:26,146-Speed 3888.63 samples/sec  Loss 0.9692  LearningRate 0.0764  ProxyLR: 3.8185  Epoch: 9  Global Step: 54310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:28,782-Speed 3886.56 samples/sec  Loss 1.0215  LearningRate 0.0764  ProxyLR: 3.8176  Epoch: 9  Global Step: 54320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:31,416-Speed 3887.85 samples/sec  Loss 1.0057  LearningRate 0.0763  ProxyLR: 3.8167  Epoch: 9  Global Step: 54330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:34,052-Speed 3887.00 samples/sec  Loss 0.9812  LearningRate 0.0763  ProxyLR: 3.8159  Epoch: 9  Global Step: 54340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:36,686-Speed 3888.19 samples/sec  Loss 1.0507  LearningRate 0.0763  ProxyLR: 3.8150  Epoch: 9  Global Step: 54350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:39,320-Speed 3888.52 samples/sec  Loss 0.9639  LearningRate 0.0763  ProxyLR: 3.8141  Epoch: 9  Global Step: 54360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:41,952-Speed 3891.05 samples/sec  Loss 0.9864  LearningRate 0.0763  ProxyLR: 3.8133  Epoch: 9  Global Step: 54370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:44,585-Speed 3889.87 samples/sec  Loss 0.9735  LearningRate 0.0762  ProxyLR: 3.8124  Epoch: 9  Global Step: 54380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:47,221-Speed 3885.82 samples/sec  Loss 0.9603  LearningRate 0.0762  ProxyLR: 3.8115  Epoch: 9  Global Step: 54390   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:34:49,853-Speed 3892.35 samples/sec  Loss 0.9789  LearningRate 0.0762  ProxyLR: 3.8107  Epoch: 9  Global Step: 54400   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:34:52,485-Speed 3891.32 samples/sec  Loss 1.0466  LearningRate 0.0762  ProxyLR: 3.8098  Epoch: 9  Global Step: 54410   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:34:55,104-Speed 3910.74 samples/sec  Loss 0.9764  LearningRate 0.0762  ProxyLR: 3.8089  Epoch: 9  Global Step: 54420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:34:57,737-Speed 3889.98 samples/sec  Loss 0.9772  LearningRate 0.0762  ProxyLR: 3.8081  Epoch: 9  Global Step: 54430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:00,370-Speed 3889.44 samples/sec  Loss 1.0317  LearningRate 0.0761  ProxyLR: 3.8072  Epoch: 9  Global Step: 54440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:03,000-Speed 3894.20 samples/sec  Loss 1.0019  LearningRate 0.0761  ProxyLR: 3.8063  Epoch: 9  Global Step: 54450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:05,618-Speed 3912.35 samples/sec  Loss 1.0156  LearningRate 0.0761  ProxyLR: 3.8055  Epoch: 9  Global Step: 54460   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:08,248-Speed 3895.07 samples/sec  Loss 1.0378  LearningRate 0.0761  ProxyLR: 3.8046  Epoch: 9  Global Step: 54470   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:10,877-Speed 3895.66 samples/sec  Loss 1.0382  LearningRate 0.0761  ProxyLR: 3.8037  Epoch: 9  Global Step: 54480   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:13,508-Speed 3894.10 samples/sec  Loss 1.0684  LearningRate 0.0761  ProxyLR: 3.8029  Epoch: 9  Global Step: 54490   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:16,137-Speed 3895.35 samples/sec  Loss 1.0358  LearningRate 0.0760  ProxyLR: 3.8020  Epoch: 9  Global Step: 54500   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:18,768-Speed 3892.77 samples/sec  Loss 1.0231  LearningRate 0.0760  ProxyLR: 3.8011  Epoch: 9  Global Step: 54510   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:21,398-Speed 3894.57 samples/sec  Loss 1.0024  LearningRate 0.0760  ProxyLR: 3.8002  Epoch: 9  Global Step: 54520   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:24,029-Speed 3892.95 samples/sec  Loss 1.0115  LearningRate 0.0760  ProxyLR: 3.7994  Epoch: 9  Global Step: 54530   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:26,659-Speed 3895.07 samples/sec  Loss 1.0635  LearningRate 0.0760  ProxyLR: 3.7985  Epoch: 9  Global Step: 54540   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:29,290-Speed 3893.70 samples/sec  Loss 1.0034  LearningRate 0.0760  ProxyLR: 3.7976  Epoch: 9  Global Step: 54550   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:35:31,922-Speed 3891.33 samples/sec  Loss 1.0466  LearningRate 0.0759  ProxyLR: 3.7968  Epoch: 9  Global Step: 54560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:34,552-Speed 3893.75 samples/sec  Loss 0.9622  LearningRate 0.0759  ProxyLR: 3.7959  Epoch: 9  Global Step: 54570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:37,182-Speed 3894.91 samples/sec  Loss 0.9899  LearningRate 0.0759  ProxyLR: 3.7950  Epoch: 9  Global Step: 54580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:39,812-Speed 3894.63 samples/sec  Loss 0.9998  LearningRate 0.0759  ProxyLR: 3.7942  Epoch: 9  Global Step: 54590   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:42,444-Speed 3891.53 samples/sec  Loss 1.0366  LearningRate 0.0759  ProxyLR: 3.7933  Epoch: 9  Global Step: 54600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:45,076-Speed 3891.52 samples/sec  Loss 1.0063  LearningRate 0.0758  ProxyLR: 3.7924  Epoch: 9  Global Step: 54610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:47,706-Speed 3893.68 samples/sec  Loss 1.0217  LearningRate 0.0758  ProxyLR: 3.7916  Epoch: 9  Global Step: 54620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:50,338-Speed 3892.63 samples/sec  Loss 0.9758  LearningRate 0.0758  ProxyLR: 3.7907  Epoch: 9  Global Step: 54630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:52,968-Speed 3893.83 samples/sec  Loss 0.9653  LearningRate 0.0758  ProxyLR: 3.7898  Epoch: 9  Global Step: 54640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:55,597-Speed 3895.56 samples/sec  Loss 1.0210  LearningRate 0.0758  ProxyLR: 3.7890  Epoch: 9  Global Step: 54650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:35:58,227-Speed 3894.41 samples/sec  Loss 0.9954  LearningRate 0.0758  ProxyLR: 3.7881  Epoch: 9  Global Step: 54660   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:36:00,860-Speed 3890.53 samples/sec  Loss 0.9981  LearningRate 0.0757  ProxyLR: 3.7872  Epoch: 9  Global Step: 54670   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:36:03,476-Speed 3915.45 samples/sec  Loss 1.0201  LearningRate 0.0757  ProxyLR: 3.7864  Epoch: 9  Global Step: 54680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:06,107-Speed 3892.71 samples/sec  Loss 1.0091  LearningRate 0.0757  ProxyLR: 3.7855  Epoch: 9  Global Step: 54690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:08,739-Speed 3891.51 samples/sec  Loss 1.0461  LearningRate 0.0757  ProxyLR: 3.7847  Epoch: 9  Global Step: 54700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:11,371-Speed 3891.46 samples/sec  Loss 1.0168  LearningRate 0.0757  ProxyLR: 3.7838  Epoch: 9  Global Step: 54710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:14,002-Speed 3894.06 samples/sec  Loss 0.9503  LearningRate 0.0757  ProxyLR: 3.7829  Epoch: 9  Global Step: 54720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:16,632-Speed 3893.58 samples/sec  Loss 1.0179  LearningRate 0.0756  ProxyLR: 3.7821  Epoch: 9  Global Step: 54730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:19,264-Speed 3891.65 samples/sec  Loss 0.9839  LearningRate 0.0756  ProxyLR: 3.7812  Epoch: 9  Global Step: 54740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:21,896-Speed 3892.37 samples/sec  Loss 1.0668  LearningRate 0.0756  ProxyLR: 3.7803  Epoch: 9  Global Step: 54750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:24,527-Speed 3893.25 samples/sec  Loss 1.0050  LearningRate 0.0756  ProxyLR: 3.7795  Epoch: 9  Global Step: 54760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:27,158-Speed 3891.78 samples/sec  Loss 1.0266  LearningRate 0.0756  ProxyLR: 3.7786  Epoch: 9  Global Step: 54770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:29,777-Speed 3912.02 samples/sec  Loss 1.0014  LearningRate 0.0756  ProxyLR: 3.7777  Epoch: 9  Global Step: 54780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:32,407-Speed 3893.11 samples/sec  Loss 1.0114  LearningRate 0.0755  ProxyLR: 3.7769  Epoch: 9  Global Step: 54790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:35,038-Speed 3893.85 samples/sec  Loss 1.0439  LearningRate 0.0755  ProxyLR: 3.7760  Epoch: 9  Global Step: 54800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:37,669-Speed 3893.65 samples/sec  Loss 1.0055  LearningRate 0.0755  ProxyLR: 3.7751  Epoch: 9  Global Step: 54810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:40,300-Speed 3892.35 samples/sec  Loss 1.0439  LearningRate 0.0755  ProxyLR: 3.7743  Epoch: 9  Global Step: 54820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:36:42,919-Speed 3910.35 samples/sec  Loss 1.0029  LearningRate 0.0755  ProxyLR: 3.7734  Epoch: 9  Global Step: 54830   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:36:45,550-Speed 3892.68 samples/sec  Loss 0.9801  LearningRate 0.0755  ProxyLR: 3.7725  Epoch: 9  Global Step: 54840   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:36:48,180-Speed 3894.47 samples/sec  Loss 0.9504  LearningRate 0.0754  ProxyLR: 3.7717  Epoch: 9  Global Step: 54850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:36:50,812-Speed 3891.94 samples/sec  Loss 0.9688  LearningRate 0.0754  ProxyLR: 3.7708  Epoch: 9  Global Step: 54860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:36:53,444-Speed 3891.92 samples/sec  Loss 1.0278  LearningRate 0.0754  ProxyLR: 3.7700  Epoch: 9  Global Step: 54870   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:36:56,076-Speed 3892.29 samples/sec  Loss 1.0571  LearningRate 0.0754  ProxyLR: 3.7691  Epoch: 9  Global Step: 54880   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:36:58,708-Speed 3891.39 samples/sec  Loss 1.0204  LearningRate 0.0754  ProxyLR: 3.7682  Epoch: 9  Global Step: 54890   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:37:01,340-Speed 3890.97 samples/sec  Loss 1.0855  LearningRate 0.0753  ProxyLR: 3.7674  Epoch: 9  Global Step: 54900   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:37:03,974-Speed 3888.53 samples/sec  Loss 1.0855  LearningRate 0.0753  ProxyLR: 3.7665  Epoch: 9  Global Step: 54910   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:37:06,612-Speed 3882.39 samples/sec  Loss 1.0061  LearningRate 0.0753  ProxyLR: 3.7656  Epoch: 9  Global Step: 54920   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:37:09,249-Speed 3884.34 samples/sec  Loss 0.9840  LearningRate 0.0753  ProxyLR: 3.7648  Epoch: 9  Global Step: 54930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:11,886-Speed 3883.75 samples/sec  Loss 1.0219  LearningRate 0.0753  ProxyLR: 3.7639  Epoch: 9  Global Step: 54940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:14,524-Speed 3883.72 samples/sec  Loss 1.0465  LearningRate 0.0753  ProxyLR: 3.7630  Epoch: 9  Global Step: 54950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:17,161-Speed 3884.71 samples/sec  Loss 0.9898  LearningRate 0.0752  ProxyLR: 3.7622  Epoch: 9  Global Step: 54960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:19,797-Speed 3885.25 samples/sec  Loss 1.0452  LearningRate 0.0752  ProxyLR: 3.7613  Epoch: 9  Global Step: 54970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:22,433-Speed 3885.95 samples/sec  Loss 1.0440  LearningRate 0.0752  ProxyLR: 3.7605  Epoch: 9  Global Step: 54980   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:25,069-Speed 3885.56 samples/sec  Loss 1.0553  LearningRate 0.0752  ProxyLR: 3.7596  Epoch: 9  Global Step: 54990   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:27,706-Speed 3883.93 samples/sec  Loss 1.0743  LearningRate 0.0752  ProxyLR: 3.7587  Epoch: 9  Global Step: 55000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:30,341-Speed 3886.56 samples/sec  Loss 1.0207  LearningRate 0.0752  ProxyLR: 3.7579  Epoch: 9  Global Step: 55010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:32,978-Speed 3885.20 samples/sec  Loss 1.0312  LearningRate 0.0751  ProxyLR: 3.7570  Epoch: 9  Global Step: 55020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:35,615-Speed 3883.11 samples/sec  Loss 0.9523  LearningRate 0.0751  ProxyLR: 3.7561  Epoch: 9  Global Step: 55030   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:37:38,256-Speed 3879.07 samples/sec  Loss 1.0408  LearningRate 0.0751  ProxyLR: 3.7553  Epoch: 9  Global Step: 55040   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:37:40,882-Speed 3899.75 samples/sec  Loss 0.9712  LearningRate 0.0751  ProxyLR: 3.7544  Epoch: 9  Global Step: 55050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:43,520-Speed 3883.18 samples/sec  Loss 1.0109  LearningRate 0.0751  ProxyLR: 3.7536  Epoch: 9  Global Step: 55060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:46,159-Speed 3880.93 samples/sec  Loss 1.0441  LearningRate 0.0751  ProxyLR: 3.7527  Epoch: 9  Global Step: 55070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:48,796-Speed 3884.06 samples/sec  Loss 1.0785  LearningRate 0.0750  ProxyLR: 3.7518  Epoch: 9  Global Step: 55080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:51,435-Speed 3880.36 samples/sec  Loss 1.0405  LearningRate 0.0750  ProxyLR: 3.7510  Epoch: 9  Global Step: 55090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:54,073-Speed 3882.93 samples/sec  Loss 1.0128  LearningRate 0.0750  ProxyLR: 3.7501  Epoch: 9  Global Step: 55100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:56,711-Speed 3883.16 samples/sec  Loss 1.0168  LearningRate 0.0750  ProxyLR: 3.7492  Epoch: 9  Global Step: 55110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:37:59,349-Speed 3883.53 samples/sec  Loss 1.0689  LearningRate 0.0750  ProxyLR: 3.7484  Epoch: 9  Global Step: 55120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:01,986-Speed 3884.02 samples/sec  Loss 1.0472  LearningRate 0.0750  ProxyLR: 3.7475  Epoch: 9  Global Step: 55130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:04,621-Speed 3885.82 samples/sec  Loss 1.0778  LearningRate 0.0749  ProxyLR: 3.7467  Epoch: 9  Global Step: 55140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:07,245-Speed 3904.24 samples/sec  Loss 1.0885  LearningRate 0.0749  ProxyLR: 3.7458  Epoch: 9  Global Step: 55150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:09,881-Speed 3886.54 samples/sec  Loss 1.0842  LearningRate 0.0749  ProxyLR: 3.7449  Epoch: 9  Global Step: 55160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:12,514-Speed 3888.91 samples/sec  Loss 1.1406  LearningRate 0.0749  ProxyLR: 3.7441  Epoch: 9  Global Step: 55170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:15,149-Speed 3887.38 samples/sec  Loss 1.0290  LearningRate 0.0749  ProxyLR: 3.7432  Epoch: 9  Global Step: 55180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:17,786-Speed 3884.65 samples/sec  Loss 1.0263  LearningRate 0.0748  ProxyLR: 3.7424  Epoch: 9  Global Step: 55190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:20,424-Speed 3882.04 samples/sec  Loss 0.9398  LearningRate 0.0748  ProxyLR: 3.7415  Epoch: 9  Global Step: 55200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:23,060-Speed 3885.60 samples/sec  Loss 1.0069  LearningRate 0.0748  ProxyLR: 3.7406  Epoch: 9  Global Step: 55210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:25,696-Speed 3885.61 samples/sec  Loss 0.9836  LearningRate 0.0748  ProxyLR: 3.7398  Epoch: 9  Global Step: 55220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:28,333-Speed 3884.21 samples/sec  Loss 1.0485  LearningRate 0.0748  ProxyLR: 3.7389  Epoch: 9  Global Step: 55230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:30,970-Speed 3884.73 samples/sec  Loss 1.0242  LearningRate 0.0748  ProxyLR: 3.7381  Epoch: 9  Global Step: 55240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:38:33,591-Speed 3906.80 samples/sec  Loss 1.0166  LearningRate 0.0747  ProxyLR: 3.7372  Epoch: 9  Global Step: 55250   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:36,226-Speed 3887.72 samples/sec  Loss 0.9973  LearningRate 0.0747  ProxyLR: 3.7363  Epoch: 9  Global Step: 55260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:38,861-Speed 3885.85 samples/sec  Loss 1.0956  LearningRate 0.0747  ProxyLR: 3.7355  Epoch: 9  Global Step: 55270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:41,500-Speed 3881.32 samples/sec  Loss 1.0170  LearningRate 0.0747  ProxyLR: 3.7346  Epoch: 9  Global Step: 55280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:44,138-Speed 3883.70 samples/sec  Loss 0.9942  LearningRate 0.0747  ProxyLR: 3.7338  Epoch: 9  Global Step: 55290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:46,776-Speed 3882.33 samples/sec  Loss 1.0149  LearningRate 0.0747  ProxyLR: 3.7329  Epoch: 9  Global Step: 55300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:49,412-Speed 3885.31 samples/sec  Loss 1.0657  LearningRate 0.0746  ProxyLR: 3.7320  Epoch: 9  Global Step: 55310   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:52,049-Speed 3884.26 samples/sec  Loss 1.0079  LearningRate 0.0746  ProxyLR: 3.7312  Epoch: 9  Global Step: 55320   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:54,685-Speed 3885.00 samples/sec  Loss 1.0739  LearningRate 0.0746  ProxyLR: 3.7303  Epoch: 9  Global Step: 55330   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:57,323-Speed 3883.27 samples/sec  Loss 1.0538  LearningRate 0.0746  ProxyLR: 3.7295  Epoch: 9  Global Step: 55340   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:38:59,945-Speed 3905.67 samples/sec  Loss 1.1706  LearningRate 0.0746  ProxyLR: 3.7286  Epoch: 9  Global Step: 55350   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:02,583-Speed 3883.20 samples/sec  Loss 1.0741  LearningRate 0.0746  ProxyLR: 3.7277  Epoch: 9  Global Step: 55360   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:05,221-Speed 3882.54 samples/sec  Loss 1.0901  LearningRate 0.0745  ProxyLR: 3.7269  Epoch: 9  Global Step: 55370   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:07,852-Speed 3893.57 samples/sec  Loss 1.0537  LearningRate 0.0745  ProxyLR: 3.7260  Epoch: 9  Global Step: 55380   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:10,484-Speed 3891.47 samples/sec  Loss 1.1374  LearningRate 0.0745  ProxyLR: 3.7252  Epoch: 9  Global Step: 55390   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:13,116-Speed 3890.58 samples/sec  Loss 1.0982  LearningRate 0.0745  ProxyLR: 3.7243  Epoch: 9  Global Step: 55400   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:15,749-Speed 3890.82 samples/sec  Loss 1.0547  LearningRate 0.0745  ProxyLR: 3.7234  Epoch: 9  Global Step: 55410   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:18,381-Speed 3891.31 samples/sec  Loss 1.0727  LearningRate 0.0745  ProxyLR: 3.7226  Epoch: 9  Global Step: 55420   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:21,015-Speed 3888.05 samples/sec  Loss 1.0576  LearningRate 0.0744  ProxyLR: 3.7217  Epoch: 9  Global Step: 55430   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:23,647-Speed 3891.81 samples/sec  Loss 1.0951  LearningRate 0.0744  ProxyLR: 3.7209  Epoch: 9  Global Step: 55440   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:26,280-Speed 3889.54 samples/sec  Loss 1.0199  LearningRate 0.0744  ProxyLR: 3.7200  Epoch: 9  Global Step: 55450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:39:28,915-Speed 3887.83 samples/sec  Loss 1.0345  LearningRate 0.0744  ProxyLR: 3.7192  Epoch: 9  Global Step: 55460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:39:31,547-Speed 3890.69 samples/sec  Loss 1.0288  LearningRate 0.0744  ProxyLR: 3.7183  Epoch: 9  Global Step: 55470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:39:34,184-Speed 3884.29 samples/sec  Loss 1.0122  LearningRate 0.0743  ProxyLR: 3.7174  Epoch: 9  Global Step: 55480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:39:36,810-Speed 3901.00 samples/sec  Loss 1.1028  LearningRate 0.0743  ProxyLR: 3.7166  Epoch: 9  Global Step: 55490   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:39,446-Speed 3884.61 samples/sec  Loss 1.1151  LearningRate 0.0743  ProxyLR: 3.7157  Epoch: 9  Global Step: 55500   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:42,082-Speed 3885.68 samples/sec  Loss 1.1648  LearningRate 0.0743  ProxyLR: 3.7149  Epoch: 9  Global Step: 55510   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:44,717-Speed 3887.11 samples/sec  Loss 1.0697  LearningRate 0.0743  ProxyLR: 3.7140  Epoch: 9  Global Step: 55520   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:47,353-Speed 3886.43 samples/sec  Loss 1.0630  LearningRate 0.0743  ProxyLR: 3.7132  Epoch: 9  Global Step: 55530   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:49,987-Speed 3887.28 samples/sec  Loss 1.0618  LearningRate 0.0742  ProxyLR: 3.7123  Epoch: 9  Global Step: 55540   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:52,623-Speed 3886.49 samples/sec  Loss 0.9969  LearningRate 0.0742  ProxyLR: 3.7114  Epoch: 9  Global Step: 55550   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:55,255-Speed 3891.00 samples/sec  Loss 1.0754  LearningRate 0.0742  ProxyLR: 3.7106  Epoch: 9  Global Step: 55560   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:39:57,890-Speed 3887.91 samples/sec  Loss 1.0236  LearningRate 0.0742  ProxyLR: 3.7097  Epoch: 9  Global Step: 55570   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:00,524-Speed 3888.39 samples/sec  Loss 1.0210  LearningRate 0.0742  ProxyLR: 3.7089  Epoch: 9  Global Step: 55580   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:03,159-Speed 3887.46 samples/sec  Loss 1.0552  LearningRate 0.0742  ProxyLR: 3.7080  Epoch: 9  Global Step: 55590   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:05,793-Speed 3888.25 samples/sec  Loss 1.0008  LearningRate 0.0741  ProxyLR: 3.7072  Epoch: 9  Global Step: 55600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:08,426-Speed 3890.01 samples/sec  Loss 1.0387  LearningRate 0.0741  ProxyLR: 3.7063  Epoch: 9  Global Step: 55610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:11,061-Speed 3886.51 samples/sec  Loss 1.0650  LearningRate 0.0741  ProxyLR: 3.7054  Epoch: 9  Global Step: 55620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:13,697-Speed 3885.95 samples/sec  Loss 1.0715  LearningRate 0.0741  ProxyLR: 3.7046  Epoch: 9  Global Step: 55630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:16,339-Speed 3877.00 samples/sec  Loss 1.0781  LearningRate 0.0741  ProxyLR: 3.7037  Epoch: 9  Global Step: 55640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:18,977-Speed 3882.62 samples/sec  Loss 1.1337  LearningRate 0.0741  ProxyLR: 3.7029  Epoch: 9  Global Step: 55650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:21,615-Speed 3881.81 samples/sec  Loss 1.0183  LearningRate 0.0740  ProxyLR: 3.7020  Epoch: 9  Global Step: 55660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:24,255-Speed 3881.11 samples/sec  Loss 1.0255  LearningRate 0.0740  ProxyLR: 3.7012  Epoch: 9  Global Step: 55670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:26,892-Speed 3883.91 samples/sec  Loss 1.0640  LearningRate 0.0740  ProxyLR: 3.7003  Epoch: 9  Global Step: 55680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:40:29,529-Speed 3883.72 samples/sec  Loss 1.0718  LearningRate 0.0740  ProxyLR: 3.6994  Epoch: 9  Global Step: 55690   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:40:32,169-Speed 3880.30 samples/sec  Loss 1.0334  LearningRate 0.0740  ProxyLR: 3.6986  Epoch: 9  Global Step: 55700   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:40:34,806-Speed 3884.03 samples/sec  Loss 1.0480  LearningRate 0.0740  ProxyLR: 3.6977  Epoch: 9  Global Step: 55710   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:40:37,443-Speed 3883.27 samples/sec  Loss 1.0707  LearningRate 0.0739  ProxyLR: 3.6969  Epoch: 9  Global Step: 55720   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:40:40,053-Speed 3924.94 samples/sec  Loss 1.0295  LearningRate 0.0739  ProxyLR: 3.6960  Epoch: 9  Global Step: 55730   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:42,690-Speed 3883.44 samples/sec  Loss 1.0101  LearningRate 0.0739  ProxyLR: 3.6952  Epoch: 9  Global Step: 55740   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:45,327-Speed 3885.14 samples/sec  Loss 1.0346  LearningRate 0.0739  ProxyLR: 3.6943  Epoch: 9  Global Step: 55750   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:47,968-Speed 3877.89 samples/sec  Loss 0.9944  LearningRate 0.0739  ProxyLR: 3.6935  Epoch: 9  Global Step: 55760   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:50,607-Speed 3881.82 samples/sec  Loss 1.0453  LearningRate 0.0739  ProxyLR: 3.6926  Epoch: 9  Global Step: 55770   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:53,241-Speed 3888.27 samples/sec  Loss 1.0315  LearningRate 0.0738  ProxyLR: 3.6917  Epoch: 9  Global Step: 55780   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:55,877-Speed 3885.25 samples/sec  Loss 1.0233  LearningRate 0.0738  ProxyLR: 3.6909  Epoch: 9  Global Step: 55790   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:40:58,513-Speed 3885.84 samples/sec  Loss 1.0628  LearningRate 0.0738  ProxyLR: 3.6900  Epoch: 9  Global Step: 55800   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:41:01,146-Speed 3890.54 samples/sec  Loss 1.1494  LearningRate 0.0738  ProxyLR: 3.6892  Epoch: 9  Global Step: 55810   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:41:03,776-Speed 3893.31 samples/sec  Loss 1.1029  LearningRate 0.0738  ProxyLR: 3.6883  Epoch: 9  Global Step: 55820   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:41:06,405-Speed 3897.14 samples/sec  Loss 1.1420  LearningRate 0.0737  ProxyLR: 3.6875  Epoch: 9  Global Step: 55830   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:09,034-Speed 3894.62 samples/sec  Loss 1.1422  LearningRate 0.0737  ProxyLR: 3.6866  Epoch: 9  Global Step: 55840   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:11,662-Speed 3897.36 samples/sec  Loss 1.1307  LearningRate 0.0737  ProxyLR: 3.6858  Epoch: 9  Global Step: 55850   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:14,291-Speed 3896.12 samples/sec  Loss 1.1045  LearningRate 0.0737  ProxyLR: 3.6849  Epoch: 9  Global Step: 55860   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:16,921-Speed 3894.58 samples/sec  Loss 1.0952  LearningRate 0.0737  ProxyLR: 3.6841  Epoch: 9  Global Step: 55870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:19,551-Speed 3895.28 samples/sec  Loss 1.0621  LearningRate 0.0737  ProxyLR: 3.6832  Epoch: 9  Global Step: 55880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:22,179-Speed 3896.21 samples/sec  Loss 1.1040  LearningRate 0.0736  ProxyLR: 3.6824  Epoch: 9  Global Step: 55890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:24,808-Speed 3896.00 samples/sec  Loss 1.1000  LearningRate 0.0736  ProxyLR: 3.6815  Epoch: 9  Global Step: 55900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:27,436-Speed 3897.27 samples/sec  Loss 1.0331  LearningRate 0.0736  ProxyLR: 3.6806  Epoch: 9  Global Step: 55910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:30,066-Speed 3894.76 samples/sec  Loss 1.0544  LearningRate 0.0736  ProxyLR: 3.6798  Epoch: 9  Global Step: 55920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:32,693-Speed 3898.83 samples/sec  Loss 1.1428  LearningRate 0.0736  ProxyLR: 3.6789  Epoch: 9  Global Step: 55930   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:41:35,307-Speed 3918.73 samples/sec  Loss 1.0816  LearningRate 0.0736  ProxyLR: 3.6781  Epoch: 9  Global Step: 55940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:37,937-Speed 3894.02 samples/sec  Loss 1.1588  LearningRate 0.0735  ProxyLR: 3.6772  Epoch: 9  Global Step: 55950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:40,566-Speed 3896.31 samples/sec  Loss 1.1259  LearningRate 0.0735  ProxyLR: 3.6764  Epoch: 9  Global Step: 55960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:43,195-Speed 3896.00 samples/sec  Loss 1.0153  LearningRate 0.0735  ProxyLR: 3.6755  Epoch: 9  Global Step: 55970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:45,825-Speed 3893.64 samples/sec  Loss 1.0786  LearningRate 0.0735  ProxyLR: 3.6747  Epoch: 9  Global Step: 55980   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:48,454-Speed 3896.04 samples/sec  Loss 1.0757  LearningRate 0.0735  ProxyLR: 3.6738  Epoch: 9  Global Step: 55990   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:51,081-Speed 3899.20 samples/sec  Loss 1.0210  LearningRate 0.0735  ProxyLR: 3.6730  Epoch: 9  Global Step: 56000   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:41:53,697-Speed 3914.87 samples/sec  Loss 1.0788  LearningRate 0.0734  ProxyLR: 3.6721  Epoch: 9  Global Step: 56010   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:41:56,329-Speed 3892.39 samples/sec  Loss 1.1096  LearningRate 0.0734  ProxyLR: 3.6713  Epoch: 9  Global Step: 56020   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:41:58,957-Speed 3897.93 samples/sec  Loss 1.1534  LearningRate 0.0734  ProxyLR: 3.6704  Epoch: 9  Global Step: 56030   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:01,587-Speed 3894.27 samples/sec  Loss 1.0773  LearningRate 0.0734  ProxyLR: 3.6696  Epoch: 9  Global Step: 56040   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:04,219-Speed 3891.10 samples/sec  Loss 1.0901  LearningRate 0.0734  ProxyLR: 3.6687  Epoch: 9  Global Step: 56050   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:06,852-Speed 3890.59 samples/sec  Loss 1.0582  LearningRate 0.0734  ProxyLR: 3.6679  Epoch: 9  Global Step: 56060   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:09,485-Speed 3889.21 samples/sec  Loss 1.0570  LearningRate 0.0733  ProxyLR: 3.6670  Epoch: 9  Global Step: 56070   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:12,117-Speed 3892.27 samples/sec  Loss 1.1068  LearningRate 0.0733  ProxyLR: 3.6661  Epoch: 9  Global Step: 56080   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:14,748-Speed 3892.66 samples/sec  Loss 1.0168  LearningRate 0.0733  ProxyLR: 3.6653  Epoch: 9  Global Step: 56090   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:17,381-Speed 3889.41 samples/sec  Loss 1.1039  LearningRate 0.0733  ProxyLR: 3.6644  Epoch: 9  Global Step: 56100   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:42:20,013-Speed 3891.61 samples/sec  Loss 1.0740  LearningRate 0.0733  ProxyLR: 3.6636  Epoch: 9  Global Step: 56110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:22,646-Speed 3889.92 samples/sec  Loss 1.0770  LearningRate 0.0733  ProxyLR: 3.6627  Epoch: 9  Global Step: 56120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:25,281-Speed 3887.70 samples/sec  Loss 1.0402  LearningRate 0.0732  ProxyLR: 3.6619  Epoch: 9  Global Step: 56130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:27,912-Speed 3892.21 samples/sec  Loss 1.0765  LearningRate 0.0732  ProxyLR: 3.6610  Epoch: 9  Global Step: 56140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:30,544-Speed 3891.10 samples/sec  Loss 1.0751  LearningRate 0.0732  ProxyLR: 3.6602  Epoch: 9  Global Step: 56150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:33,179-Speed 3887.94 samples/sec  Loss 1.0467  LearningRate 0.0732  ProxyLR: 3.6593  Epoch: 9  Global Step: 56160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:35,814-Speed 3887.49 samples/sec  Loss 1.0003  LearningRate 0.0732  ProxyLR: 3.6585  Epoch: 9  Global Step: 56170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:38,447-Speed 3888.65 samples/sec  Loss 1.1191  LearningRate 0.0732  ProxyLR: 3.6576  Epoch: 9  Global Step: 56180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:41,087-Speed 3880.82 samples/sec  Loss 1.0431  LearningRate 0.0731  ProxyLR: 3.6568  Epoch: 9  Global Step: 56190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:43,727-Speed 3879.56 samples/sec  Loss 1.0926  LearningRate 0.0731  ProxyLR: 3.6559  Epoch: 9  Global Step: 56200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:46,354-Speed 3898.86 samples/sec  Loss 1.1009  LearningRate 0.0731  ProxyLR: 3.6551  Epoch: 9  Global Step: 56210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:48,994-Speed 3879.89 samples/sec  Loss 1.0987  LearningRate 0.0731  ProxyLR: 3.6542  Epoch: 9  Global Step: 56220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:51,635-Speed 3878.17 samples/sec  Loss 1.0835  LearningRate 0.0731  ProxyLR: 3.6534  Epoch: 9  Global Step: 56230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:54,274-Speed 3880.37 samples/sec  Loss 1.1165  LearningRate 0.0731  ProxyLR: 3.6525  Epoch: 9  Global Step: 56240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:56,911-Speed 3883.86 samples/sec  Loss 1.0594  LearningRate 0.0730  ProxyLR: 3.6517  Epoch: 9  Global Step: 56250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:42:59,548-Speed 3885.33 samples/sec  Loss 1.1094  LearningRate 0.0730  ProxyLR: 3.6508  Epoch: 9  Global Step: 56260   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:02,185-Speed 3884.50 samples/sec  Loss 1.0635  LearningRate 0.0730  ProxyLR: 3.6500  Epoch: 9  Global Step: 56270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:04,819-Speed 3887.82 samples/sec  Loss 1.1278  LearningRate 0.0730  ProxyLR: 3.6491  Epoch: 9  Global Step: 56280   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:07,456-Speed 3883.74 samples/sec  Loss 1.1040  LearningRate 0.0730  ProxyLR: 3.6483  Epoch: 9  Global Step: 56290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:10,088-Speed 3892.52 samples/sec  Loss 1.0409  LearningRate 0.0729  ProxyLR: 3.6474  Epoch: 9  Global Step: 56300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:12,718-Speed 3894.54 samples/sec  Loss 1.0225  LearningRate 0.0729  ProxyLR: 3.6466  Epoch: 9  Global Step: 56310   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:43:15,337-Speed 3910.83 samples/sec  Loss 1.1071  LearningRate 0.0729  ProxyLR: 3.6457  Epoch: 9  Global Step: 56320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:17,969-Speed 3890.82 samples/sec  Loss 1.0849  LearningRate 0.0729  ProxyLR: 3.6449  Epoch: 9  Global Step: 56330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:20,600-Speed 3893.39 samples/sec  Loss 1.1291  LearningRate 0.0729  ProxyLR: 3.6440  Epoch: 9  Global Step: 56340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:23,231-Speed 3892.35 samples/sec  Loss 1.0660  LearningRate 0.0729  ProxyLR: 3.6432  Epoch: 9  Global Step: 56350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:25,861-Speed 3894.77 samples/sec  Loss 1.0314  LearningRate 0.0728  ProxyLR: 3.6423  Epoch: 9  Global Step: 56360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:28,492-Speed 3892.12 samples/sec  Loss 1.0705  LearningRate 0.0728  ProxyLR: 3.6415  Epoch: 9  Global Step: 56370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:31,125-Speed 3890.78 samples/sec  Loss 1.1133  LearningRate 0.0728  ProxyLR: 3.6406  Epoch: 9  Global Step: 56380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:33,758-Speed 3890.57 samples/sec  Loss 1.1361  LearningRate 0.0728  ProxyLR: 3.6398  Epoch: 9  Global Step: 56390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:36,390-Speed 3890.46 samples/sec  Loss 1.0659  LearningRate 0.0728  ProxyLR: 3.6389  Epoch: 9  Global Step: 56400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:39,022-Speed 3892.38 samples/sec  Loss 1.0698  LearningRate 0.0728  ProxyLR: 3.6381  Epoch: 9  Global Step: 56410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:41,653-Speed 3891.90 samples/sec  Loss 1.0958  LearningRate 0.0727  ProxyLR: 3.6372  Epoch: 9  Global Step: 56420   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:43:44,274-Speed 3908.63 samples/sec  Loss 1.0834  LearningRate 0.0727  ProxyLR: 3.6364  Epoch: 9  Global Step: 56430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:46,906-Speed 3890.86 samples/sec  Loss 1.0587  LearningRate 0.0727  ProxyLR: 3.6355  Epoch: 9  Global Step: 56440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:49,538-Speed 3892.39 samples/sec  Loss 1.1195  LearningRate 0.0727  ProxyLR: 3.6347  Epoch: 9  Global Step: 56450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:52,169-Speed 3892.61 samples/sec  Loss 1.0489  LearningRate 0.0727  ProxyLR: 3.6338  Epoch: 9  Global Step: 56460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:54,801-Speed 3891.59 samples/sec  Loss 1.0699  LearningRate 0.0727  ProxyLR: 3.6330  Epoch: 9  Global Step: 56470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:43:57,432-Speed 3893.09 samples/sec  Loss 1.1027  LearningRate 0.0726  ProxyLR: 3.6322  Epoch: 9  Global Step: 56480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:44:00,064-Speed 3891.36 samples/sec  Loss 1.0372  LearningRate 0.0726  ProxyLR: 3.6313  Epoch: 9  Global Step: 56490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:44:02,699-Speed 3886.75 samples/sec  Loss 1.0583  LearningRate 0.0726  ProxyLR: 3.6305  Epoch: 9  Global Step: 56500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:44:05,322-Speed 3904.93 samples/sec  Loss 1.0753  LearningRate 0.0726  ProxyLR: 3.6296  Epoch: 9  Global Step: 56510   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:07,953-Speed 3893.13 samples/sec  Loss 1.0466  LearningRate 0.0726  ProxyLR: 3.6288  Epoch: 9  Global Step: 56520   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:10,584-Speed 3892.33 samples/sec  Loss 1.0579  LearningRate 0.0726  ProxyLR: 3.6279  Epoch: 9  Global Step: 56530   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:13,218-Speed 3889.23 samples/sec  Loss 1.2099  LearningRate 0.0725  ProxyLR: 3.6271  Epoch: 9  Global Step: 56540   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:15,857-Speed 3881.40 samples/sec  Loss 1.0877  LearningRate 0.0725  ProxyLR: 3.6262  Epoch: 9  Global Step: 56550   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:18,489-Speed 3891.26 samples/sec  Loss 1.1410  LearningRate 0.0725  ProxyLR: 3.6254  Epoch: 9  Global Step: 56560   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:21,121-Speed 3891.48 samples/sec  Loss 1.1232  LearningRate 0.0725  ProxyLR: 3.6245  Epoch: 9  Global Step: 56570   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:23,755-Speed 3888.86 samples/sec  Loss 1.1500  LearningRate 0.0725  ProxyLR: 3.6237  Epoch: 9  Global Step: 56580   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:26,392-Speed 3884.02 samples/sec  Loss 1.1361  LearningRate 0.0725  ProxyLR: 3.6228  Epoch: 9  Global Step: 56590   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:29,020-Speed 3896.34 samples/sec  Loss 1.2464  LearningRate 0.0724  ProxyLR: 3.6220  Epoch: 9  Global Step: 56600   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:31,647-Speed 3899.53 samples/sec  Loss 1.1512  LearningRate 0.0724  ProxyLR: 3.6211  Epoch: 9  Global Step: 56610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:44:34,276-Speed 3896.27 samples/sec  Loss 1.1401  LearningRate 0.0724  ProxyLR: 3.6203  Epoch: 9  Global Step: 56620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:44:36,893-Speed 3913.52 samples/sec  Loss 1.1275  LearningRate 0.0724  ProxyLR: 3.6194  Epoch: 9  Global Step: 56630   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:39,525-Speed 3891.25 samples/sec  Loss 1.1831  LearningRate 0.0724  ProxyLR: 3.6186  Epoch: 9  Global Step: 56640   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:42,157-Speed 3891.75 samples/sec  Loss 1.1063  LearningRate 0.0724  ProxyLR: 3.6177  Epoch: 9  Global Step: 56650   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:44,785-Speed 3896.93 samples/sec  Loss 1.1561  LearningRate 0.0723  ProxyLR: 3.6169  Epoch: 9  Global Step: 56660   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:47,420-Speed 3887.72 samples/sec  Loss 1.0964  LearningRate 0.0723  ProxyLR: 3.6161  Epoch: 9  Global Step: 56670   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:50,047-Speed 3898.23 samples/sec  Loss 1.0292  LearningRate 0.0723  ProxyLR: 3.6152  Epoch: 9  Global Step: 56680   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:52,673-Speed 3900.86 samples/sec  Loss 1.0329  LearningRate 0.0723  ProxyLR: 3.6144  Epoch: 9  Global Step: 56690   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:55,299-Speed 3899.57 samples/sec  Loss 0.9875  LearningRate 0.0723  ProxyLR: 3.6135  Epoch: 9  Global Step: 56700   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:44:57,926-Speed 3899.66 samples/sec  Loss 1.0697  LearningRate 0.0723  ProxyLR: 3.6127  Epoch: 9  Global Step: 56710   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:45:00,553-Speed 3898.19 samples/sec  Loss 1.1104  LearningRate 0.0722  ProxyLR: 3.6118  Epoch: 9  Global Step: 56720   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:45:03,183-Speed 3894.47 samples/sec  Loss 1.1048  LearningRate 0.0722  ProxyLR: 3.6110  Epoch: 9  Global Step: 56730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:05,825-Speed 3877.83 samples/sec  Loss 1.0228  LearningRate 0.0722  ProxyLR: 3.6101  Epoch: 9  Global Step: 56740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:08,457-Speed 3890.60 samples/sec  Loss 1.0987  LearningRate 0.0722  ProxyLR: 3.6093  Epoch: 9  Global Step: 56750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:11,091-Speed 3889.26 samples/sec  Loss 1.1220  LearningRate 0.0722  ProxyLR: 3.6084  Epoch: 9  Global Step: 56760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:13,723-Speed 3892.00 samples/sec  Loss 1.0631  LearningRate 0.0722  ProxyLR: 3.6076  Epoch: 9  Global Step: 56770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:16,353-Speed 3893.83 samples/sec  Loss 1.1310  LearningRate 0.0721  ProxyLR: 3.6068  Epoch: 9  Global Step: 56780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:18,981-Speed 3897.30 samples/sec  Loss 0.9528  LearningRate 0.0721  ProxyLR: 3.6059  Epoch: 9  Global Step: 56790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:21,609-Speed 3897.37 samples/sec  Loss 1.0777  LearningRate 0.0721  ProxyLR: 3.6051  Epoch: 9  Global Step: 56800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:24,243-Speed 3888.70 samples/sec  Loss 1.0768  LearningRate 0.0721  ProxyLR: 3.6042  Epoch: 9  Global Step: 56810   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:26,872-Speed 3896.23 samples/sec  Loss 1.0776  LearningRate 0.0721  ProxyLR: 3.6034  Epoch: 9  Global Step: 56820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:29,506-Speed 3887.62 samples/sec  Loss 1.0607  LearningRate 0.0721  ProxyLR: 3.6025  Epoch: 9  Global Step: 56830   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:45:32,133-Speed 3899.87 samples/sec  Loss 1.0630  LearningRate 0.0720  ProxyLR: 3.6017  Epoch: 9  Global Step: 56840   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:45:34,805-Speed 3832.46 samples/sec  Loss 1.0414  LearningRate 0.0720  ProxyLR: 3.6008  Epoch: 9  Global Step: 56850   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:37,435-Speed 3895.12 samples/sec  Loss 1.0125  LearningRate 0.0720  ProxyLR: 3.6000  Epoch: 9  Global Step: 56860   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:46,319-Speed 1152.69 samples/sec  Loss 3.0814  LearningRate 0.0720  ProxyLR: 3.5992  Epoch: 10  Global Step: 56870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:48,980-Speed 3848.90 samples/sec  Loss 2.9455  LearningRate 0.0720  ProxyLR: 3.5983  Epoch: 10  Global Step: 56880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:51,617-Speed 3884.61 samples/sec  Loss 2.8783  LearningRate 0.0719  ProxyLR: 3.5975  Epoch: 10  Global Step: 56890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:54,252-Speed 3887.87 samples/sec  Loss 2.9132  LearningRate 0.0719  ProxyLR: 3.5966  Epoch: 10  Global Step: 56900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:45:56,873-Speed 3907.51 samples/sec  Loss 2.8400  LearningRate 0.0719  ProxyLR: 3.5958  Epoch: 10  Global Step: 56910   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:45:59,507-Speed 3889.03 samples/sec  Loss 2.7762  LearningRate 0.0719  ProxyLR: 3.5949  Epoch: 10  Global Step: 56920   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:02,172-Speed 3842.96 samples/sec  Loss 2.7128  LearningRate 0.0719  ProxyLR: 3.5941  Epoch: 10  Global Step: 56930   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:04,805-Speed 3890.49 samples/sec  Loss 2.7099  LearningRate 0.0719  ProxyLR: 3.5932  Epoch: 10  Global Step: 56940   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:07,459-Speed 3858.20 samples/sec  Loss 2.6955  LearningRate 0.0718  ProxyLR: 3.5924  Epoch: 10  Global Step: 56950   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:10,086-Speed 3899.56 samples/sec  Loss 2.6155  LearningRate 0.0718  ProxyLR: 3.5916  Epoch: 10  Global Step: 56960   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:12,713-Speed 3899.18 samples/sec  Loss 2.6003  LearningRate 0.0718  ProxyLR: 3.5907  Epoch: 10  Global Step: 56970   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:15,340-Speed 3898.43 samples/sec  Loss 2.5581  LearningRate 0.0718  ProxyLR: 3.5899  Epoch: 10  Global Step: 56980   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:17,967-Speed 3899.36 samples/sec  Loss 2.5892  LearningRate 0.0718  ProxyLR: 3.5890  Epoch: 10  Global Step: 56990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:20,595-Speed 3898.48 samples/sec  Loss 2.6046  LearningRate 0.0718  ProxyLR: 3.5882  Epoch: 10  Global Step: 57000   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:23,220-Speed 3901.26 samples/sec  Loss 2.5007  LearningRate 0.0717  ProxyLR: 3.5873  Epoch: 10  Global Step: 57010   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:25,845-Speed 3902.07 samples/sec  Loss 2.5516  LearningRate 0.0717  ProxyLR: 3.5865  Epoch: 10  Global Step: 57020   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:28,472-Speed 3899.45 samples/sec  Loss 2.4791  LearningRate 0.0717  ProxyLR: 3.5857  Epoch: 10  Global Step: 57030   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:31,098-Speed 3898.89 samples/sec  Loss 2.4790  LearningRate 0.0717  ProxyLR: 3.5848  Epoch: 10  Global Step: 57040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:33,722-Speed 3904.13 samples/sec  Loss 2.3823  LearningRate 0.0717  ProxyLR: 3.5840  Epoch: 10  Global Step: 57050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:36,349-Speed 3899.60 samples/sec  Loss 2.4315  LearningRate 0.0717  ProxyLR: 3.5831  Epoch: 10  Global Step: 57060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:38,975-Speed 3899.39 samples/sec  Loss 2.3903  LearningRate 0.0716  ProxyLR: 3.5823  Epoch: 10  Global Step: 57070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:41,602-Speed 3899.45 samples/sec  Loss 2.3496  LearningRate 0.0716  ProxyLR: 3.5815  Epoch: 10  Global Step: 57080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:44,228-Speed 3899.81 samples/sec  Loss 2.4055  LearningRate 0.0716  ProxyLR: 3.5806  Epoch: 10  Global Step: 57090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:46:46,840-Speed 3920.93 samples/sec  Loss 2.4314  LearningRate 0.0716  ProxyLR: 3.5798  Epoch: 10  Global Step: 57100   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:49,465-Speed 3902.30 samples/sec  Loss 2.4099  LearningRate 0.0716  ProxyLR: 3.5789  Epoch: 10  Global Step: 57110   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:52,090-Speed 3902.28 samples/sec  Loss 2.3723  LearningRate 0.0716  ProxyLR: 3.5781  Epoch: 10  Global Step: 57120   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:54,763-Speed 3831.99 samples/sec  Loss 2.2859  LearningRate 0.0715  ProxyLR: 3.5772  Epoch: 10  Global Step: 57130   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:46:57,388-Speed 3901.58 samples/sec  Loss 2.3191  LearningRate 0.0715  ProxyLR: 3.5764  Epoch: 10  Global Step: 57140   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:00,013-Speed 3902.43 samples/sec  Loss 2.3362  LearningRate 0.0715  ProxyLR: 3.5756  Epoch: 10  Global Step: 57150   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:02,639-Speed 3900.67 samples/sec  Loss 2.2906  LearningRate 0.0715  ProxyLR: 3.5747  Epoch: 10  Global Step: 57160   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:05,264-Speed 3902.36 samples/sec  Loss 2.2245  LearningRate 0.0715  ProxyLR: 3.5739  Epoch: 10  Global Step: 57170   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:07,890-Speed 3899.34 samples/sec  Loss 2.2802  LearningRate 0.0715  ProxyLR: 3.5730  Epoch: 10  Global Step: 57180   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:10,517-Speed 3899.17 samples/sec  Loss 2.2085  LearningRate 0.0714  ProxyLR: 3.5722  Epoch: 10  Global Step: 57190   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:13,143-Speed 3900.11 samples/sec  Loss 2.1901  LearningRate 0.0714  ProxyLR: 3.5714  Epoch: 10  Global Step: 57200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:15,770-Speed 3899.57 samples/sec  Loss 2.2489  LearningRate 0.0714  ProxyLR: 3.5705  Epoch: 10  Global Step: 57210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:18,395-Speed 3901.76 samples/sec  Loss 2.2751  LearningRate 0.0714  ProxyLR: 3.5697  Epoch: 10  Global Step: 57220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:21,020-Speed 3901.52 samples/sec  Loss 2.1904  LearningRate 0.0714  ProxyLR: 3.5688  Epoch: 10  Global Step: 57230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:23,648-Speed 3898.81 samples/sec  Loss 2.2331  LearningRate 0.0714  ProxyLR: 3.5680  Epoch: 10  Global Step: 57240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:26,279-Speed 3892.44 samples/sec  Loss 2.1904  LearningRate 0.0713  ProxyLR: 3.5672  Epoch: 10  Global Step: 57250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:28,907-Speed 3897.62 samples/sec  Loss 2.2389  LearningRate 0.0713  ProxyLR: 3.5663  Epoch: 10  Global Step: 57260   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:31,534-Speed 3899.14 samples/sec  Loss 2.1744  LearningRate 0.0713  ProxyLR: 3.5655  Epoch: 10  Global Step: 57270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:34,161-Speed 3898.36 samples/sec  Loss 2.2028  LearningRate 0.0713  ProxyLR: 3.5646  Epoch: 10  Global Step: 57280   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:36,788-Speed 3899.60 samples/sec  Loss 2.1650  LearningRate 0.0713  ProxyLR: 3.5638  Epoch: 10  Global Step: 57290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:39,400-Speed 3920.41 samples/sec  Loss 2.1419  LearningRate 0.0713  ProxyLR: 3.5630  Epoch: 10  Global Step: 57300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:42,026-Speed 3901.03 samples/sec  Loss 2.0711  LearningRate 0.0712  ProxyLR: 3.5621  Epoch: 10  Global Step: 57310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:44,651-Speed 3901.18 samples/sec  Loss 2.1225  LearningRate 0.0712  ProxyLR: 3.5613  Epoch: 10  Global Step: 57320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:47,278-Speed 3899.71 samples/sec  Loss 2.1110  LearningRate 0.0712  ProxyLR: 3.5604  Epoch: 10  Global Step: 57330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:47:49,890-Speed 3920.95 samples/sec  Loss 2.0513  LearningRate 0.0712  ProxyLR: 3.5596  Epoch: 10  Global Step: 57340   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:52,517-Speed 3898.59 samples/sec  Loss 2.0782  LearningRate 0.0712  ProxyLR: 3.5588  Epoch: 10  Global Step: 57350   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:55,141-Speed 3903.51 samples/sec  Loss 1.9878  LearningRate 0.0712  ProxyLR: 3.5579  Epoch: 10  Global Step: 57360   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:47:57,768-Speed 3899.73 samples/sec  Loss 2.1092  LearningRate 0.0711  ProxyLR: 3.5571  Epoch: 10  Global Step: 57370   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:00,394-Speed 3900.28 samples/sec  Loss 2.0773  LearningRate 0.0711  ProxyLR: 3.5562  Epoch: 10  Global Step: 57380   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:03,019-Speed 3902.01 samples/sec  Loss 2.0174  LearningRate 0.0711  ProxyLR: 3.5554  Epoch: 10  Global Step: 57390   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:05,646-Speed 3898.97 samples/sec  Loss 2.0895  LearningRate 0.0711  ProxyLR: 3.5546  Epoch: 10  Global Step: 57400   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:08,271-Speed 3902.85 samples/sec  Loss 1.9791  LearningRate 0.0711  ProxyLR: 3.5537  Epoch: 10  Global Step: 57410   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:10,896-Speed 3900.91 samples/sec  Loss 2.0811  LearningRate 0.0711  ProxyLR: 3.5529  Epoch: 10  Global Step: 57420   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:13,522-Speed 3901.18 samples/sec  Loss 2.1081  LearningRate 0.0710  ProxyLR: 3.5520  Epoch: 10  Global Step: 57430   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:16,148-Speed 3900.10 samples/sec  Loss 2.0186  LearningRate 0.0710  ProxyLR: 3.5512  Epoch: 10  Global Step: 57440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:18,774-Speed 3900.10 samples/sec  Loss 2.0458  LearningRate 0.0710  ProxyLR: 3.5504  Epoch: 10  Global Step: 57450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:21,399-Speed 3902.51 samples/sec  Loss 2.0066  LearningRate 0.0710  ProxyLR: 3.5495  Epoch: 10  Global Step: 57460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:24,026-Speed 3898.24 samples/sec  Loss 1.9788  LearningRate 0.0710  ProxyLR: 3.5487  Epoch: 10  Global Step: 57470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:26,650-Speed 3903.08 samples/sec  Loss 2.0271  LearningRate 0.0710  ProxyLR: 3.5479  Epoch: 10  Global Step: 57480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:29,276-Speed 3901.46 samples/sec  Loss 1.9873  LearningRate 0.0709  ProxyLR: 3.5470  Epoch: 10  Global Step: 57490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:31,902-Speed 3899.75 samples/sec  Loss 1.9768  LearningRate 0.0709  ProxyLR: 3.5462  Epoch: 10  Global Step: 57500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:34,528-Speed 3900.78 samples/sec  Loss 1.9596  LearningRate 0.0709  ProxyLR: 3.5453  Epoch: 10  Global Step: 57510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:37,154-Speed 3899.94 samples/sec  Loss 1.9825  LearningRate 0.0709  ProxyLR: 3.5445  Epoch: 10  Global Step: 57520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:39,780-Speed 3900.58 samples/sec  Loss 1.9367  LearningRate 0.0709  ProxyLR: 3.5437  Epoch: 10  Global Step: 57530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:48:42,406-Speed 3900.30 samples/sec  Loss 1.8942  LearningRate 0.0709  ProxyLR: 3.5428  Epoch: 10  Global Step: 57540   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:48:45,005-Speed 3941.94 samples/sec  Loss 1.9315  LearningRate 0.0708  ProxyLR: 3.5420  Epoch: 10  Global Step: 57550   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:47,630-Speed 3901.51 samples/sec  Loss 1.9725  LearningRate 0.0708  ProxyLR: 3.5411  Epoch: 10  Global Step: 57560   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:50,256-Speed 3900.23 samples/sec  Loss 1.9665  LearningRate 0.0708  ProxyLR: 3.5403  Epoch: 10  Global Step: 57570   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:52,883-Speed 3900.06 samples/sec  Loss 1.9239  LearningRate 0.0708  ProxyLR: 3.5395  Epoch: 10  Global Step: 57580   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:55,508-Speed 3901.51 samples/sec  Loss 1.8972  LearningRate 0.0708  ProxyLR: 3.5386  Epoch: 10  Global Step: 57590   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:48:58,132-Speed 3902.52 samples/sec  Loss 1.8886  LearningRate 0.0708  ProxyLR: 3.5378  Epoch: 10  Global Step: 57600   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:00,759-Speed 3900.01 samples/sec  Loss 1.8430  LearningRate 0.0707  ProxyLR: 3.5370  Epoch: 10  Global Step: 57610   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:03,384-Speed 3902.03 samples/sec  Loss 1.8748  LearningRate 0.0707  ProxyLR: 3.5361  Epoch: 10  Global Step: 57620   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:06,009-Speed 3900.74 samples/sec  Loss 1.8614  LearningRate 0.0707  ProxyLR: 3.5353  Epoch: 10  Global Step: 57630   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:08,633-Speed 3903.33 samples/sec  Loss 1.9241  LearningRate 0.0707  ProxyLR: 3.5345  Epoch: 10  Global Step: 57640   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:11,259-Speed 3901.61 samples/sec  Loss 1.8470  LearningRate 0.0707  ProxyLR: 3.5336  Epoch: 10  Global Step: 57650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:49:13,884-Speed 3902.04 samples/sec  Loss 1.8365  LearningRate 0.0707  ProxyLR: 3.5328  Epoch: 10  Global Step: 57660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:49:16,498-Speed 3918.41 samples/sec  Loss 1.8345  LearningRate 0.0706  ProxyLR: 3.5319  Epoch: 10  Global Step: 57670   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:19,123-Speed 3901.33 samples/sec  Loss 1.8839  LearningRate 0.0706  ProxyLR: 3.5311  Epoch: 10  Global Step: 57680   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:21,749-Speed 3900.08 samples/sec  Loss 1.8648  LearningRate 0.0706  ProxyLR: 3.5303  Epoch: 10  Global Step: 57690   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:24,375-Speed 3900.77 samples/sec  Loss 1.7935  LearningRate 0.0706  ProxyLR: 3.5294  Epoch: 10  Global Step: 57700   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:27,001-Speed 3900.81 samples/sec  Loss 1.9291  LearningRate 0.0706  ProxyLR: 3.5286  Epoch: 10  Global Step: 57710   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:29,625-Speed 3903.17 samples/sec  Loss 1.8878  LearningRate 0.0706  ProxyLR: 3.5278  Epoch: 10  Global Step: 57720   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:32,251-Speed 3900.74 samples/sec  Loss 1.8561  LearningRate 0.0705  ProxyLR: 3.5269  Epoch: 10  Global Step: 57730   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:34,875-Speed 3903.21 samples/sec  Loss 1.8315  LearningRate 0.0705  ProxyLR: 3.5261  Epoch: 10  Global Step: 57740   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:37,498-Speed 3905.17 samples/sec  Loss 1.7963  LearningRate 0.0705  ProxyLR: 3.5253  Epoch: 10  Global Step: 57750   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:40,122-Speed 3902.21 samples/sec  Loss 1.8656  LearningRate 0.0705  ProxyLR: 3.5244  Epoch: 10  Global Step: 57760   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:42,749-Speed 3899.18 samples/sec  Loss 1.7793  LearningRate 0.0705  ProxyLR: 3.5236  Epoch: 10  Global Step: 57770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:49:45,384-Speed 3886.94 samples/sec  Loss 1.8342  LearningRate 0.0705  ProxyLR: 3.5228  Epoch: 10  Global Step: 57780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:49:48,008-Speed 3903.62 samples/sec  Loss 1.8219  LearningRate 0.0704  ProxyLR: 3.5219  Epoch: 10  Global Step: 57790   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:50,646-Speed 3882.67 samples/sec  Loss 1.8090  LearningRate 0.0704  ProxyLR: 3.5211  Epoch: 10  Global Step: 57800   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:53,281-Speed 3887.22 samples/sec  Loss 1.8357  LearningRate 0.0704  ProxyLR: 3.5202  Epoch: 10  Global Step: 57810   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:55,916-Speed 3888.10 samples/sec  Loss 1.7963  LearningRate 0.0704  ProxyLR: 3.5194  Epoch: 10  Global Step: 57820   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:49:58,553-Speed 3884.12 samples/sec  Loss 1.8209  LearningRate 0.0704  ProxyLR: 3.5186  Epoch: 10  Global Step: 57830   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:01,186-Speed 3889.17 samples/sec  Loss 1.8409  LearningRate 0.0704  ProxyLR: 3.5177  Epoch: 10  Global Step: 57840   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:03,821-Speed 3886.97 samples/sec  Loss 1.7672  LearningRate 0.0703  ProxyLR: 3.5169  Epoch: 10  Global Step: 57850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:06,453-Speed 3891.36 samples/sec  Loss 1.7219  LearningRate 0.0703  ProxyLR: 3.5161  Epoch: 10  Global Step: 57860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:09,086-Speed 3890.61 samples/sec  Loss 1.7839  LearningRate 0.0703  ProxyLR: 3.5152  Epoch: 10  Global Step: 57870   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:11,718-Speed 3892.03 samples/sec  Loss 1.7407  LearningRate 0.0703  ProxyLR: 3.5144  Epoch: 10  Global Step: 57880   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:14,352-Speed 3888.14 samples/sec  Loss 1.7333  LearningRate 0.0703  ProxyLR: 3.5136  Epoch: 10  Global Step: 57890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:16,986-Speed 3888.27 samples/sec  Loss 1.7511  LearningRate 0.0703  ProxyLR: 3.5127  Epoch: 10  Global Step: 57900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:19,617-Speed 3893.34 samples/sec  Loss 1.6934  LearningRate 0.0702  ProxyLR: 3.5119  Epoch: 10  Global Step: 57910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:22,245-Speed 3898.13 samples/sec  Loss 1.8116  LearningRate 0.0702  ProxyLR: 3.5111  Epoch: 10  Global Step: 57920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:24,871-Speed 3899.35 samples/sec  Loss 1.7632  LearningRate 0.0702  ProxyLR: 3.5102  Epoch: 10  Global Step: 57930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:27,502-Speed 3892.94 samples/sec  Loss 1.7001  LearningRate 0.0702  ProxyLR: 3.5094  Epoch: 10  Global Step: 57940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:30,134-Speed 3892.87 samples/sec  Loss 1.6822  LearningRate 0.0702  ProxyLR: 3.5086  Epoch: 10  Global Step: 57950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:32,762-Speed 3896.56 samples/sec  Loss 1.7315  LearningRate 0.0702  ProxyLR: 3.5077  Epoch: 10  Global Step: 57960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:35,390-Speed 3897.48 samples/sec  Loss 1.6520  LearningRate 0.0701  ProxyLR: 3.5069  Epoch: 10  Global Step: 57970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:50:38,006-Speed 3916.43 samples/sec  Loss 1.6629  LearningRate 0.0701  ProxyLR: 3.5061  Epoch: 10  Global Step: 57980   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:40,635-Speed 3895.26 samples/sec  Loss 1.7216  LearningRate 0.0701  ProxyLR: 3.5052  Epoch: 10  Global Step: 57990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:43,265-Speed 3894.72 samples/sec  Loss 1.7699  LearningRate 0.0701  ProxyLR: 3.5044  Epoch: 10  Global Step: 58000   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:45,891-Speed 3900.64 samples/sec  Loss 1.7234  LearningRate 0.0701  ProxyLR: 3.5036  Epoch: 10  Global Step: 58010   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:48,520-Speed 3896.43 samples/sec  Loss 1.7211  LearningRate 0.0701  ProxyLR: 3.5027  Epoch: 10  Global Step: 58020   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:51,147-Speed 3898.83 samples/sec  Loss 1.6975  LearningRate 0.0700  ProxyLR: 3.5019  Epoch: 10  Global Step: 58030   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:53,773-Speed 3899.34 samples/sec  Loss 1.6841  LearningRate 0.0700  ProxyLR: 3.5011  Epoch: 10  Global Step: 58040   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:56,399-Speed 3901.28 samples/sec  Loss 1.7183  LearningRate 0.0700  ProxyLR: 3.5002  Epoch: 10  Global Step: 58050   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:50:59,026-Speed 3898.12 samples/sec  Loss 1.6640  LearningRate 0.0700  ProxyLR: 3.4994  Epoch: 10  Global Step: 58060   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:01,652-Speed 3901.12 samples/sec  Loss 1.6183  LearningRate 0.0700  ProxyLR: 3.4986  Epoch: 10  Global Step: 58070   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:04,277-Speed 3901.52 samples/sec  Loss 1.7167  LearningRate 0.0700  ProxyLR: 3.4977  Epoch: 10  Global Step: 58080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:06,904-Speed 3899.63 samples/sec  Loss 1.6886  LearningRate 0.0699  ProxyLR: 3.4969  Epoch: 10  Global Step: 58090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:09,530-Speed 3900.78 samples/sec  Loss 1.6151  LearningRate 0.0699  ProxyLR: 3.4961  Epoch: 10  Global Step: 58100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:12,155-Speed 3901.21 samples/sec  Loss 1.6942  LearningRate 0.0699  ProxyLR: 3.4953  Epoch: 10  Global Step: 58110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:14,778-Speed 3904.44 samples/sec  Loss 1.6177  LearningRate 0.0699  ProxyLR: 3.4944  Epoch: 10  Global Step: 58120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:17,405-Speed 3899.62 samples/sec  Loss 1.6161  LearningRate 0.0699  ProxyLR: 3.4936  Epoch: 10  Global Step: 58130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:20,030-Speed 3901.53 samples/sec  Loss 1.7523  LearningRate 0.0699  ProxyLR: 3.4928  Epoch: 10  Global Step: 58140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:22,656-Speed 3900.88 samples/sec  Loss 1.6575  LearningRate 0.0698  ProxyLR: 3.4919  Epoch: 10  Global Step: 58150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:25,280-Speed 3903.80 samples/sec  Loss 1.6077  LearningRate 0.0698  ProxyLR: 3.4911  Epoch: 10  Global Step: 58160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:27,904-Speed 3902.55 samples/sec  Loss 1.6324  LearningRate 0.0698  ProxyLR: 3.4903  Epoch: 10  Global Step: 58170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:30,517-Speed 3920.59 samples/sec  Loss 1.6945  LearningRate 0.0698  ProxyLR: 3.4894  Epoch: 10  Global Step: 58180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:33,142-Speed 3901.04 samples/sec  Loss 1.6531  LearningRate 0.0698  ProxyLR: 3.4886  Epoch: 10  Global Step: 58190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:51:35,753-Speed 3922.74 samples/sec  Loss 1.6470  LearningRate 0.0698  ProxyLR: 3.4878  Epoch: 10  Global Step: 58200   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:38,380-Speed 3900.09 samples/sec  Loss 1.6811  LearningRate 0.0697  ProxyLR: 3.4869  Epoch: 10  Global Step: 58210   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:41,006-Speed 3900.45 samples/sec  Loss 1.6569  LearningRate 0.0697  ProxyLR: 3.4861  Epoch: 10  Global Step: 58220   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:43,645-Speed 3881.13 samples/sec  Loss 1.6484  LearningRate 0.0697  ProxyLR: 3.4853  Epoch: 10  Global Step: 58230   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:46,282-Speed 3884.32 samples/sec  Loss 1.6120  LearningRate 0.0697  ProxyLR: 3.4844  Epoch: 10  Global Step: 58240   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:48,918-Speed 3884.82 samples/sec  Loss 1.6637  LearningRate 0.0697  ProxyLR: 3.4836  Epoch: 10  Global Step: 58250   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:51,556-Speed 3883.53 samples/sec  Loss 1.6986  LearningRate 0.0697  ProxyLR: 3.4828  Epoch: 10  Global Step: 58260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:54,194-Speed 3882.73 samples/sec  Loss 1.7052  LearningRate 0.0696  ProxyLR: 3.4820  Epoch: 10  Global Step: 58270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:56,829-Speed 3887.29 samples/sec  Loss 1.6326  LearningRate 0.0696  ProxyLR: 3.4811  Epoch: 10  Global Step: 58280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:51:59,464-Speed 3886.92 samples/sec  Loss 1.6340  LearningRate 0.0696  ProxyLR: 3.4803  Epoch: 10  Global Step: 58290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:52:02,101-Speed 3884.28 samples/sec  Loss 1.5992  LearningRate 0.0696  ProxyLR: 3.4795  Epoch: 10  Global Step: 58300   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:04,739-Speed 3881.85 samples/sec  Loss 1.6169  LearningRate 0.0696  ProxyLR: 3.4786  Epoch: 10  Global Step: 58310   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:07,377-Speed 3882.56 samples/sec  Loss 1.6423  LearningRate 0.0696  ProxyLR: 3.4778  Epoch: 10  Global Step: 58320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:10,015-Speed 3883.68 samples/sec  Loss 1.6397  LearningRate 0.0695  ProxyLR: 3.4770  Epoch: 10  Global Step: 58330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:12,652-Speed 3884.15 samples/sec  Loss 1.6661  LearningRate 0.0695  ProxyLR: 3.4761  Epoch: 10  Global Step: 58340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:15,288-Speed 3885.52 samples/sec  Loss 1.7330  LearningRate 0.0695  ProxyLR: 3.4753  Epoch: 10  Global Step: 58350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:17,924-Speed 3885.63 samples/sec  Loss 1.6686  LearningRate 0.0695  ProxyLR: 3.4745  Epoch: 10  Global Step: 58360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:20,563-Speed 3880.77 samples/sec  Loss 1.6314  LearningRate 0.0695  ProxyLR: 3.4737  Epoch: 10  Global Step: 58370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:23,200-Speed 3884.67 samples/sec  Loss 1.7549  LearningRate 0.0695  ProxyLR: 3.4728  Epoch: 10  Global Step: 58380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:25,839-Speed 3881.51 samples/sec  Loss 1.5900  LearningRate 0.0694  ProxyLR: 3.4720  Epoch: 10  Global Step: 58390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:28,462-Speed 3904.47 samples/sec  Loss 1.6821  LearningRate 0.0694  ProxyLR: 3.4712  Epoch: 10  Global Step: 58400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:31,098-Speed 3885.26 samples/sec  Loss 1.6655  LearningRate 0.0694  ProxyLR: 3.4703  Epoch: 10  Global Step: 58410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:33,735-Speed 3884.25 samples/sec  Loss 1.6117  LearningRate 0.0694  ProxyLR: 3.4695  Epoch: 10  Global Step: 58420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:36,373-Speed 3883.45 samples/sec  Loss 1.5981  LearningRate 0.0694  ProxyLR: 3.4687  Epoch: 10  Global Step: 58430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:39,011-Speed 3882.40 samples/sec  Loss 1.5768  LearningRate 0.0694  ProxyLR: 3.4679  Epoch: 10  Global Step: 58440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:41,645-Speed 3888.34 samples/sec  Loss 1.6417  LearningRate 0.0693  ProxyLR: 3.4670  Epoch: 10  Global Step: 58450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:44,282-Speed 3883.88 samples/sec  Loss 1.6078  LearningRate 0.0693  ProxyLR: 3.4662  Epoch: 10  Global Step: 58460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:46,917-Speed 3887.20 samples/sec  Loss 1.5332  LearningRate 0.0693  ProxyLR: 3.4654  Epoch: 10  Global Step: 58470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:49,555-Speed 3883.31 samples/sec  Loss 1.6156  LearningRate 0.0693  ProxyLR: 3.4645  Epoch: 10  Global Step: 58480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:52,192-Speed 3884.79 samples/sec  Loss 1.5399  LearningRate 0.0693  ProxyLR: 3.4637  Epoch: 10  Global Step: 58490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:52:54,829-Speed 3883.81 samples/sec  Loss 1.5857  LearningRate 0.0693  ProxyLR: 3.4629  Epoch: 10  Global Step: 58500   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:52:57,450-Speed 3907.70 samples/sec  Loss 1.6671  LearningRate 0.0692  ProxyLR: 3.4621  Epoch: 10  Global Step: 58510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:00,083-Speed 3890.68 samples/sec  Loss 1.5912  LearningRate 0.0692  ProxyLR: 3.4612  Epoch: 10  Global Step: 58520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:02,708-Speed 3901.63 samples/sec  Loss 1.6041  LearningRate 0.0692  ProxyLR: 3.4604  Epoch: 10  Global Step: 58530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:05,335-Speed 3898.53 samples/sec  Loss 1.6174  LearningRate 0.0692  ProxyLR: 3.4596  Epoch: 10  Global Step: 58540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:07,960-Speed 3901.89 samples/sec  Loss 1.5202  LearningRate 0.0692  ProxyLR: 3.4587  Epoch: 10  Global Step: 58550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:10,587-Speed 3899.29 samples/sec  Loss 1.6148  LearningRate 0.0692  ProxyLR: 3.4579  Epoch: 10  Global Step: 58560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:13,211-Speed 3903.18 samples/sec  Loss 1.5816  LearningRate 0.0691  ProxyLR: 3.4571  Epoch: 10  Global Step: 58570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:15,838-Speed 3899.60 samples/sec  Loss 1.5714  LearningRate 0.0691  ProxyLR: 3.4563  Epoch: 10  Global Step: 58580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:18,466-Speed 3896.84 samples/sec  Loss 1.5832  LearningRate 0.0691  ProxyLR: 3.4554  Epoch: 10  Global Step: 58590   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:21,093-Speed 3899.84 samples/sec  Loss 1.5570  LearningRate 0.0691  ProxyLR: 3.4546  Epoch: 10  Global Step: 58600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:23,704-Speed 3921.73 samples/sec  Loss 1.6119  LearningRate 0.0691  ProxyLR: 3.4538  Epoch: 10  Global Step: 58610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:26,329-Speed 3902.48 samples/sec  Loss 1.4952  LearningRate 0.0691  ProxyLR: 3.4530  Epoch: 10  Global Step: 58620   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:28,954-Speed 3901.13 samples/sec  Loss 1.5271  LearningRate 0.0690  ProxyLR: 3.4521  Epoch: 10  Global Step: 58630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:31,580-Speed 3901.21 samples/sec  Loss 1.5609  LearningRate 0.0690  ProxyLR: 3.4513  Epoch: 10  Global Step: 58640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:34,205-Speed 3902.37 samples/sec  Loss 1.5468  LearningRate 0.0690  ProxyLR: 3.4505  Epoch: 10  Global Step: 58650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:36,831-Speed 3900.49 samples/sec  Loss 1.5309  LearningRate 0.0690  ProxyLR: 3.4497  Epoch: 10  Global Step: 58660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:39,456-Speed 3901.51 samples/sec  Loss 1.5557  LearningRate 0.0690  ProxyLR: 3.4488  Epoch: 10  Global Step: 58670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:42,081-Speed 3901.76 samples/sec  Loss 1.5702  LearningRate 0.0690  ProxyLR: 3.4480  Epoch: 10  Global Step: 58680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:44,705-Speed 3902.67 samples/sec  Loss 1.5124  LearningRate 0.0689  ProxyLR: 3.4472  Epoch: 10  Global Step: 58690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:47,333-Speed 3898.58 samples/sec  Loss 1.5923  LearningRate 0.0689  ProxyLR: 3.4463  Epoch: 10  Global Step: 58700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:49,947-Speed 3918.72 samples/sec  Loss 1.5703  LearningRate 0.0689  ProxyLR: 3.4455  Epoch: 10  Global Step: 58710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:52,574-Speed 3897.94 samples/sec  Loss 1.5695  LearningRate 0.0689  ProxyLR: 3.4447  Epoch: 10  Global Step: 58720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:55,201-Speed 3899.41 samples/sec  Loss 1.5689  LearningRate 0.0689  ProxyLR: 3.4439  Epoch: 10  Global Step: 58730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:53:57,827-Speed 3900.55 samples/sec  Loss 1.4863  LearningRate 0.0689  ProxyLR: 3.4430  Epoch: 10  Global Step: 58740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:00,452-Speed 3901.49 samples/sec  Loss 1.5590  LearningRate 0.0688  ProxyLR: 3.4422  Epoch: 10  Global Step: 58750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:03,078-Speed 3900.52 samples/sec  Loss 1.5115  LearningRate 0.0688  ProxyLR: 3.4414  Epoch: 10  Global Step: 58760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:05,703-Speed 3903.10 samples/sec  Loss 1.6020  LearningRate 0.0688  ProxyLR: 3.4406  Epoch: 10  Global Step: 58770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:08,330-Speed 3898.32 samples/sec  Loss 1.5254  LearningRate 0.0688  ProxyLR: 3.4397  Epoch: 10  Global Step: 58780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:10,959-Speed 3895.46 samples/sec  Loss 1.5040  LearningRate 0.0688  ProxyLR: 3.4389  Epoch: 10  Global Step: 58790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:13,587-Speed 3897.56 samples/sec  Loss 1.5857  LearningRate 0.0688  ProxyLR: 3.4381  Epoch: 10  Global Step: 58800   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:16,218-Speed 3893.26 samples/sec  Loss 1.5432  LearningRate 0.0687  ProxyLR: 3.4373  Epoch: 10  Global Step: 58810   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 18:54:18,834-Speed 3916.05 samples/sec  Loss 1.5229  LearningRate 0.0687  ProxyLR: 3.4364  Epoch: 10  Global Step: 58820   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:21,452-Speed 3912.98 samples/sec  Loss 1.5462  LearningRate 0.0687  ProxyLR: 3.4356  Epoch: 10  Global Step: 58830   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:24,079-Speed 3897.80 samples/sec  Loss 1.5896  LearningRate 0.0687  ProxyLR: 3.4348  Epoch: 10  Global Step: 58840   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:26,710-Speed 3893.83 samples/sec  Loss 1.5369  LearningRate 0.0687  ProxyLR: 3.4340  Epoch: 10  Global Step: 58850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:29,339-Speed 3895.12 samples/sec  Loss 1.5597  LearningRate 0.0687  ProxyLR: 3.4331  Epoch: 10  Global Step: 58860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:31,967-Speed 3897.86 samples/sec  Loss 1.5519  LearningRate 0.0686  ProxyLR: 3.4323  Epoch: 10  Global Step: 58870   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:34,595-Speed 3896.99 samples/sec  Loss 1.5488  LearningRate 0.0686  ProxyLR: 3.4315  Epoch: 10  Global Step: 58880   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:37,226-Speed 3894.16 samples/sec  Loss 1.6149  LearningRate 0.0686  ProxyLR: 3.4307  Epoch: 10  Global Step: 58890   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:39,854-Speed 3896.19 samples/sec  Loss 1.5570  LearningRate 0.0686  ProxyLR: 3.4298  Epoch: 10  Global Step: 58900   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:42,483-Speed 3897.22 samples/sec  Loss 1.6134  LearningRate 0.0686  ProxyLR: 3.4290  Epoch: 10  Global Step: 58910   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:45,114-Speed 3893.07 samples/sec  Loss 1.4982  LearningRate 0.0686  ProxyLR: 3.4282  Epoch: 10  Global Step: 58920   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:47,744-Speed 3893.84 samples/sec  Loss 1.5762  LearningRate 0.0685  ProxyLR: 3.4274  Epoch: 10  Global Step: 58930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:54:50,365-Speed 3907.79 samples/sec  Loss 1.5589  LearningRate 0.0685  ProxyLR: 3.4266  Epoch: 10  Global Step: 58940   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:53,001-Speed 3886.52 samples/sec  Loss 1.4930  LearningRate 0.0685  ProxyLR: 3.4257  Epoch: 10  Global Step: 58950   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:55,638-Speed 3884.58 samples/sec  Loss 1.5212  LearningRate 0.0685  ProxyLR: 3.4249  Epoch: 10  Global Step: 58960   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:54:58,276-Speed 3882.11 samples/sec  Loss 1.5054  LearningRate 0.0685  ProxyLR: 3.4241  Epoch: 10  Global Step: 58970   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:00,913-Speed 3884.49 samples/sec  Loss 1.4943  LearningRate 0.0685  ProxyLR: 3.4233  Epoch: 10  Global Step: 58980   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:03,551-Speed 3881.96 samples/sec  Loss 1.5120  LearningRate 0.0684  ProxyLR: 3.4224  Epoch: 10  Global Step: 58990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:06,189-Speed 3882.30 samples/sec  Loss 1.5424  LearningRate 0.0684  ProxyLR: 3.4216  Epoch: 10  Global Step: 59000   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:08,825-Speed 3885.78 samples/sec  Loss 1.4372  LearningRate 0.0684  ProxyLR: 3.4208  Epoch: 10  Global Step: 59010   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:11,460-Speed 3887.78 samples/sec  Loss 1.5443  LearningRate 0.0684  ProxyLR: 3.4200  Epoch: 10  Global Step: 59020   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:14,093-Speed 3890.44 samples/sec  Loss 1.4986  LearningRate 0.0684  ProxyLR: 3.4191  Epoch: 10  Global Step: 59030   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:16,729-Speed 3885.65 samples/sec  Loss 1.4715  LearningRate 0.0684  ProxyLR: 3.4183  Epoch: 10  Global Step: 59040   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:55:19,366-Speed 3884.10 samples/sec  Loss 1.4290  LearningRate 0.0683  ProxyLR: 3.4175  Epoch: 10  Global Step: 59050   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:55:22,001-Speed 3886.12 samples/sec  Loss 1.4867  LearningRate 0.0683  ProxyLR: 3.4167  Epoch: 10  Global Step: 59060   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:55:24,639-Speed 3882.94 samples/sec  Loss 1.5296  LearningRate 0.0683  ProxyLR: 3.4159  Epoch: 10  Global Step: 59070   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:55:27,271-Speed 3892.27 samples/sec  Loss 1.5357  LearningRate 0.0683  ProxyLR: 3.4150  Epoch: 10  Global Step: 59080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:55:29,904-Speed 3890.65 samples/sec  Loss 1.5096  LearningRate 0.0683  ProxyLR: 3.4142  Epoch: 10  Global Step: 59090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:55:32,523-Speed 3910.02 samples/sec  Loss 1.4670  LearningRate 0.0683  ProxyLR: 3.4134  Epoch: 10  Global Step: 59100   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:35,159-Speed 3886.47 samples/sec  Loss 1.5256  LearningRate 0.0683  ProxyLR: 3.4126  Epoch: 10  Global Step: 59110   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:37,795-Speed 3884.71 samples/sec  Loss 1.5665  LearningRate 0.0682  ProxyLR: 3.4117  Epoch: 10  Global Step: 59120   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:40,429-Speed 3889.02 samples/sec  Loss 1.4463  LearningRate 0.0682  ProxyLR: 3.4109  Epoch: 10  Global Step: 59130   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:43,063-Speed 3888.88 samples/sec  Loss 1.4406  LearningRate 0.0682  ProxyLR: 3.4101  Epoch: 10  Global Step: 59140   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:45,695-Speed 3892.27 samples/sec  Loss 1.5541  LearningRate 0.0682  ProxyLR: 3.4093  Epoch: 10  Global Step: 59150   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:48,327-Speed 3890.33 samples/sec  Loss 1.6092  LearningRate 0.0682  ProxyLR: 3.4085  Epoch: 10  Global Step: 59160   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:50,959-Speed 3891.59 samples/sec  Loss 1.5216  LearningRate 0.0682  ProxyLR: 3.4076  Epoch: 10  Global Step: 59170   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:53,593-Speed 3889.29 samples/sec  Loss 1.4598  LearningRate 0.0681  ProxyLR: 3.4068  Epoch: 10  Global Step: 59180   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:56,225-Speed 3891.15 samples/sec  Loss 1.5017  LearningRate 0.0681  ProxyLR: 3.4060  Epoch: 10  Global Step: 59190   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:55:58,859-Speed 3887.96 samples/sec  Loss 1.5375  LearningRate 0.0681  ProxyLR: 3.4052  Epoch: 10  Global Step: 59200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:01,495-Speed 3886.82 samples/sec  Loss 1.5199  LearningRate 0.0681  ProxyLR: 3.4044  Epoch: 10  Global Step: 59210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:04,129-Speed 3888.15 samples/sec  Loss 1.4653  LearningRate 0.0681  ProxyLR: 3.4035  Epoch: 10  Global Step: 59220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:06,763-Speed 3888.64 samples/sec  Loss 1.5193  LearningRate 0.0681  ProxyLR: 3.4027  Epoch: 10  Global Step: 59230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:09,396-Speed 3890.39 samples/sec  Loss 1.5117  LearningRate 0.0680  ProxyLR: 3.4019  Epoch: 10  Global Step: 59240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:12,028-Speed 3890.74 samples/sec  Loss 1.4728  LearningRate 0.0680  ProxyLR: 3.4011  Epoch: 10  Global Step: 59250   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:14,661-Speed 3890.24 samples/sec  Loss 1.5150  LearningRate 0.0680  ProxyLR: 3.4002  Epoch: 10  Global Step: 59260   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:17,294-Speed 3890.22 samples/sec  Loss 1.4424  LearningRate 0.0680  ProxyLR: 3.3994  Epoch: 10  Global Step: 59270   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:19,925-Speed 3893.11 samples/sec  Loss 1.5306  LearningRate 0.0680  ProxyLR: 3.3986  Epoch: 10  Global Step: 59280   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:22,557-Speed 3892.35 samples/sec  Loss 1.4625  LearningRate 0.0680  ProxyLR: 3.3978  Epoch: 10  Global Step: 59290   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:25,172-Speed 3915.58 samples/sec  Loss 1.5067  LearningRate 0.0679  ProxyLR: 3.3970  Epoch: 10  Global Step: 59300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:27,805-Speed 3890.82 samples/sec  Loss 1.5905  LearningRate 0.0679  ProxyLR: 3.3961  Epoch: 10  Global Step: 59310   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:30,436-Speed 3892.64 samples/sec  Loss 1.5063  LearningRate 0.0679  ProxyLR: 3.3953  Epoch: 10  Global Step: 59320   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:33,068-Speed 3891.39 samples/sec  Loss 1.5398  LearningRate 0.0679  ProxyLR: 3.3945  Epoch: 10  Global Step: 59330   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:35,699-Speed 3893.70 samples/sec  Loss 1.4586  LearningRate 0.0679  ProxyLR: 3.3937  Epoch: 10  Global Step: 59340   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:38,329-Speed 3893.61 samples/sec  Loss 1.4883  LearningRate 0.0679  ProxyLR: 3.3929  Epoch: 10  Global Step: 59350   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:40,964-Speed 3888.25 samples/sec  Loss 1.4681  LearningRate 0.0678  ProxyLR: 3.3920  Epoch: 10  Global Step: 59360   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:43,597-Speed 3889.94 samples/sec  Loss 1.4502  LearningRate 0.0678  ProxyLR: 3.3912  Epoch: 10  Global Step: 59370   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:46,229-Speed 3891.74 samples/sec  Loss 1.5782  LearningRate 0.0678  ProxyLR: 3.3904  Epoch: 10  Global Step: 59380   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:48,861-Speed 3891.72 samples/sec  Loss 1.4411  LearningRate 0.0678  ProxyLR: 3.3896  Epoch: 10  Global Step: 59390   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:56:51,492-Speed 3892.45 samples/sec  Loss 1.4411  LearningRate 0.0678  ProxyLR: 3.3888  Epoch: 10  Global Step: 59400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:54,122-Speed 3894.11 samples/sec  Loss 1.5355  LearningRate 0.0678  ProxyLR: 3.3880  Epoch: 10  Global Step: 59410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:56,756-Speed 3889.11 samples/sec  Loss 1.4843  LearningRate 0.0677  ProxyLR: 3.3871  Epoch: 10  Global Step: 59420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:56:59,389-Speed 3889.48 samples/sec  Loss 1.5112  LearningRate 0.0677  ProxyLR: 3.3863  Epoch: 10  Global Step: 59430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:02,020-Speed 3892.70 samples/sec  Loss 1.4578  LearningRate 0.0677  ProxyLR: 3.3855  Epoch: 10  Global Step: 59440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:04,652-Speed 3891.96 samples/sec  Loss 1.4645  LearningRate 0.0677  ProxyLR: 3.3847  Epoch: 10  Global Step: 59450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:07,282-Speed 3894.85 samples/sec  Loss 1.4582  LearningRate 0.0677  ProxyLR: 3.3839  Epoch: 10  Global Step: 59460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:09,914-Speed 3891.77 samples/sec  Loss 1.4571  LearningRate 0.0677  ProxyLR: 3.3830  Epoch: 10  Global Step: 59470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:12,548-Speed 3888.65 samples/sec  Loss 1.4358  LearningRate 0.0676  ProxyLR: 3.3822  Epoch: 10  Global Step: 59480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:15,180-Speed 3891.62 samples/sec  Loss 1.4410  LearningRate 0.0676  ProxyLR: 3.3814  Epoch: 10  Global Step: 59490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:17,799-Speed 3909.64 samples/sec  Loss 1.4358  LearningRate 0.0676  ProxyLR: 3.3806  Epoch: 10  Global Step: 59500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:20,431-Speed 3892.83 samples/sec  Loss 1.5102  LearningRate 0.0676  ProxyLR: 3.3798  Epoch: 10  Global Step: 59510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:23,061-Speed 3894.54 samples/sec  Loss 1.4749  LearningRate 0.0676  ProxyLR: 3.3790  Epoch: 10  Global Step: 59520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:25,693-Speed 3891.36 samples/sec  Loss 1.4725  LearningRate 0.0676  ProxyLR: 3.3781  Epoch: 10  Global Step: 59530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:28,325-Speed 3891.55 samples/sec  Loss 1.4792  LearningRate 0.0675  ProxyLR: 3.3773  Epoch: 10  Global Step: 59540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:30,957-Speed 3891.87 samples/sec  Loss 1.4688  LearningRate 0.0675  ProxyLR: 3.3765  Epoch: 10  Global Step: 59550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:33,588-Speed 3892.67 samples/sec  Loss 1.5272  LearningRate 0.0675  ProxyLR: 3.3757  Epoch: 10  Global Step: 59560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:57:36,205-Speed 3913.81 samples/sec  Loss 1.4868  LearningRate 0.0675  ProxyLR: 3.3749  Epoch: 10  Global Step: 59570   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:38,837-Speed 3891.96 samples/sec  Loss 1.4499  LearningRate 0.0675  ProxyLR: 3.3740  Epoch: 10  Global Step: 59580   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:41,468-Speed 3892.52 samples/sec  Loss 1.4696  LearningRate 0.0675  ProxyLR: 3.3732  Epoch: 10  Global Step: 59590   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:44,099-Speed 3893.27 samples/sec  Loss 1.4239  LearningRate 0.0674  ProxyLR: 3.3724  Epoch: 10  Global Step: 59600   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:46,731-Speed 3891.23 samples/sec  Loss 1.4287  LearningRate 0.0674  ProxyLR: 3.3716  Epoch: 10  Global Step: 59610   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:49,363-Speed 3891.63 samples/sec  Loss 1.4025  LearningRate 0.0674  ProxyLR: 3.3708  Epoch: 10  Global Step: 59620   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:51,993-Speed 3894.05 samples/sec  Loss 1.4158  LearningRate 0.0674  ProxyLR: 3.3700  Epoch: 10  Global Step: 59630   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:54,626-Speed 3890.38 samples/sec  Loss 1.4344  LearningRate 0.0674  ProxyLR: 3.3691  Epoch: 10  Global Step: 59640   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:57,259-Speed 3889.83 samples/sec  Loss 1.4826  LearningRate 0.0674  ProxyLR: 3.3683  Epoch: 10  Global Step: 59650   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:57:59,891-Speed 3892.08 samples/sec  Loss 1.4200  LearningRate 0.0674  ProxyLR: 3.3675  Epoch: 10  Global Step: 59660   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:02,523-Speed 3891.24 samples/sec  Loss 1.3965  LearningRate 0.0673  ProxyLR: 3.3667  Epoch: 10  Global Step: 59670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:05,156-Speed 3890.00 samples/sec  Loss 1.4464  LearningRate 0.0673  ProxyLR: 3.3659  Epoch: 10  Global Step: 59680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:07,788-Speed 3891.54 samples/sec  Loss 1.4102  LearningRate 0.0673  ProxyLR: 3.3651  Epoch: 10  Global Step: 59690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:10,421-Speed 3890.06 samples/sec  Loss 1.4302  LearningRate 0.0673  ProxyLR: 3.3642  Epoch: 10  Global Step: 59700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:13,052-Speed 3892.91 samples/sec  Loss 1.4826  LearningRate 0.0673  ProxyLR: 3.3634  Epoch: 10  Global Step: 59710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:15,684-Speed 3892.34 samples/sec  Loss 1.4477  LearningRate 0.0673  ProxyLR: 3.3626  Epoch: 10  Global Step: 59720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:18,317-Speed 3890.11 samples/sec  Loss 1.4452  LearningRate 0.0672  ProxyLR: 3.3618  Epoch: 10  Global Step: 59730   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:20,949-Speed 3890.46 samples/sec  Loss 1.4565  LearningRate 0.0672  ProxyLR: 3.3610  Epoch: 10  Global Step: 59740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:23,579-Speed 3894.98 samples/sec  Loss 1.4939  LearningRate 0.0672  ProxyLR: 3.3602  Epoch: 10  Global Step: 59750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:26,210-Speed 3892.54 samples/sec  Loss 1.5190  LearningRate 0.0672  ProxyLR: 3.3594  Epoch: 10  Global Step: 59760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:28,828-Speed 3912.71 samples/sec  Loss 1.4251  LearningRate 0.0672  ProxyLR: 3.3585  Epoch: 10  Global Step: 59770   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:31,461-Speed 3890.83 samples/sec  Loss 1.4552  LearningRate 0.0672  ProxyLR: 3.3577  Epoch: 10  Global Step: 59780   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:34,091-Speed 3894.20 samples/sec  Loss 1.4112  LearningRate 0.0671  ProxyLR: 3.3569  Epoch: 10  Global Step: 59790   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:36,723-Speed 3891.45 samples/sec  Loss 1.5066  LearningRate 0.0671  ProxyLR: 3.3561  Epoch: 10  Global Step: 59800   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:39,355-Speed 3892.01 samples/sec  Loss 1.4863  LearningRate 0.0671  ProxyLR: 3.3553  Epoch: 10  Global Step: 59810   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:41,986-Speed 3891.86 samples/sec  Loss 1.4586  LearningRate 0.0671  ProxyLR: 3.3545  Epoch: 10  Global Step: 59820   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:44,618-Speed 3891.57 samples/sec  Loss 1.3957  LearningRate 0.0671  ProxyLR: 3.3536  Epoch: 10  Global Step: 59830   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:47,252-Speed 3889.94 samples/sec  Loss 1.3947  LearningRate 0.0671  ProxyLR: 3.3528  Epoch: 10  Global Step: 59840   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:49,883-Speed 3892.82 samples/sec  Loss 1.3902  LearningRate 0.0670  ProxyLR: 3.3520  Epoch: 10  Global Step: 59850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:52,515-Speed 3890.74 samples/sec  Loss 1.4529  LearningRate 0.0670  ProxyLR: 3.3512  Epoch: 10  Global Step: 59860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:58:55,147-Speed 3892.25 samples/sec  Loss 1.4882  LearningRate 0.0670  ProxyLR: 3.3504  Epoch: 10  Global Step: 59870   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:58:57,777-Speed 3894.09 samples/sec  Loss 1.4159  LearningRate 0.0670  ProxyLR: 3.3496  Epoch: 10  Global Step: 59880   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:00,409-Speed 3891.29 samples/sec  Loss 1.4257  LearningRate 0.0670  ProxyLR: 3.3488  Epoch: 10  Global Step: 59890   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:03,040-Speed 3893.57 samples/sec  Loss 1.4735  LearningRate 0.0670  ProxyLR: 3.3479  Epoch: 10  Global Step: 59900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:05,671-Speed 3892.46 samples/sec  Loss 1.4458  LearningRate 0.0669  ProxyLR: 3.3471  Epoch: 10  Global Step: 59910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:08,305-Speed 3888.92 samples/sec  Loss 1.4427  LearningRate 0.0669  ProxyLR: 3.3463  Epoch: 10  Global Step: 59920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:10,937-Speed 3892.03 samples/sec  Loss 1.4003  LearningRate 0.0669  ProxyLR: 3.3455  Epoch: 10  Global Step: 59930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:13,567-Speed 3893.44 samples/sec  Loss 1.4235  LearningRate 0.0669  ProxyLR: 3.3447  Epoch: 10  Global Step: 59940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:16,198-Speed 3894.10 samples/sec  Loss 1.4048  LearningRate 0.0669  ProxyLR: 3.3439  Epoch: 10  Global Step: 59950   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:18,829-Speed 3892.41 samples/sec  Loss 1.4942  LearningRate 0.0669  ProxyLR: 3.3431  Epoch: 10  Global Step: 59960   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:21,448-Speed 3911.51 samples/sec  Loss 1.4794  LearningRate 0.0668  ProxyLR: 3.3422  Epoch: 10  Global Step: 59970   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 18:59:24,070-Speed 3906.54 samples/sec  Loss 1.3897  LearningRate 0.0668  ProxyLR: 3.3414  Epoch: 10  Global Step: 59980   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:59:26,708-Speed 3881.91 samples/sec  Loss 1.4180  LearningRate 0.0668  ProxyLR: 3.3406  Epoch: 10  Global Step: 59990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 18:59:29,344-Speed 3886.46 samples/sec  Loss 1.4666  LearningRate 0.0668  ProxyLR: 3.3398  Epoch: 10  Global Step: 60000   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:00:19,033-[lfw][60000]XNorm: 21.851888
Training: 2023-05-04 19:00:19,033-[lfw][60000]Accuracy-Flip: 0.99700+-0.00245
Training: 2023-05-04 19:00:19,034-[lfw][60000]Accuracy-Highest: 0.99700
Training: 2023-05-04 19:00:19,034-[lfw][60000]TPR@1stNon-Zero-FPR of 0.00033: 0.98433
Training: 2023-05-04 19:00:19,034-[lfw][60000]Highest TPR@FPR: 0.99433
Training: 2023-05-04 19:01:16,070-[cfp_fp][60000]XNorm: 21.552452
Training: 2023-05-04 19:01:16,070-[cfp_fp][60000]Accuracy-Flip: 0.96629+-0.01219
Training: 2023-05-04 19:01:16,071-[cfp_fp][60000]Accuracy-Highest: 0.96629
Training: 2023-05-04 19:01:16,071-[cfp_fp][60000]TPR@1stNon-Zero-FPR of 0.00029: 0.62029
Training: 2023-05-04 19:01:16,071-[cfp_fp][60000]Highest TPR@FPR: 0.63486
Training: 2023-05-04 19:02:05,760-[agedb_30][60000]XNorm: 22.094124
Training: 2023-05-04 19:02:05,760-[agedb_30][60000]Accuracy-Flip: 0.96550+-0.01108
Training: 2023-05-04 19:02:05,761-[agedb_30][60000]Accuracy-Highest: 0.96550
Training: 2023-05-04 19:02:05,761-[agedb_30][60000]TPR@1stNon-Zero-FPR of 0.00033: 0.66933
Training: 2023-05-04 19:02:05,761-[agedb_30][60000]Highest TPR@FPR: 0.66933
Training: 2023-05-04 19:02:56,831-[calfw][60000]XNorm: 22.117516
Training: 2023-05-04 19:02:56,831-[calfw][60000]Accuracy-Flip: 0.95467+-0.01157
Training: 2023-05-04 19:02:56,831-[calfw][60000]Accuracy-Highest: 0.95467
Training: 2023-05-04 19:02:56,832-[calfw][60000]TPR@1stNon-Zero-FPR of 0.00033: 0.73600
Training: 2023-05-04 19:02:56,832-[calfw][60000]Highest TPR@FPR: 0.79900
Training: 2023-05-04 19:03:47,866-[cplfw][60000]XNorm: 20.903426
Training: 2023-05-04 19:03:47,866-[cplfw][60000]Accuracy-Flip: 0.90800+-0.01919
Training: 2023-05-04 19:03:47,866-[cplfw][60000]Accuracy-Highest: 0.90800
Training: 2023-05-04 19:03:47,866-[cplfw][60000]TPR@1stNon-Zero-FPR of 0.00033: 0.00133
Training: 2023-05-04 19:03:47,866-[cplfw][60000]Highest TPR@FPR: 0.00300
Training: 2023-05-04 19:03:50,521-Speed 39.21 samples/sec  Loss 1.4061  LearningRate 0.0668  ProxyLR: 3.3390  Epoch: 10  Global Step: 60010   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:03:53,140-Speed 3911.88 samples/sec  Loss 1.4532  LearningRate 0.0668  ProxyLR: 3.3382  Epoch: 10  Global Step: 60020   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:03:55,757-Speed 3912.99 samples/sec  Loss 1.4873  LearningRate 0.0667  ProxyLR: 3.3374  Epoch: 10  Global Step: 60030   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:03:58,377-Speed 3909.56 samples/sec  Loss 1.3482  LearningRate 0.0667  ProxyLR: 3.3366  Epoch: 10  Global Step: 60040   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:00,999-Speed 3906.38 samples/sec  Loss 1.3858  LearningRate 0.0667  ProxyLR: 3.3357  Epoch: 10  Global Step: 60050   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:03,621-Speed 3905.87 samples/sec  Loss 1.4514  LearningRate 0.0667  ProxyLR: 3.3349  Epoch: 10  Global Step: 60060   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:06,244-Speed 3905.40 samples/sec  Loss 1.4062  LearningRate 0.0667  ProxyLR: 3.3341  Epoch: 10  Global Step: 60070   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:08,866-Speed 3906.79 samples/sec  Loss 1.4589  LearningRate 0.0667  ProxyLR: 3.3333  Epoch: 10  Global Step: 60080   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:11,491-Speed 3902.76 samples/sec  Loss 1.4290  LearningRate 0.0666  ProxyLR: 3.3325  Epoch: 10  Global Step: 60090   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:14,115-Speed 3902.48 samples/sec  Loss 1.4488  LearningRate 0.0666  ProxyLR: 3.3317  Epoch: 10  Global Step: 60100   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:16,740-Speed 3901.54 samples/sec  Loss 1.3919  LearningRate 0.0666  ProxyLR: 3.3309  Epoch: 10  Global Step: 60110   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:19,369-Speed 3896.72 samples/sec  Loss 1.4057  LearningRate 0.0666  ProxyLR: 3.3301  Epoch: 10  Global Step: 60120   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:21,994-Speed 3901.59 samples/sec  Loss 1.4235  LearningRate 0.0666  ProxyLR: 3.3292  Epoch: 10  Global Step: 60130   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:24,623-Speed 3896.77 samples/sec  Loss 1.4318  LearningRate 0.0666  ProxyLR: 3.3284  Epoch: 10  Global Step: 60140   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:27,251-Speed 3897.58 samples/sec  Loss 1.4186  LearningRate 0.0666  ProxyLR: 3.3276  Epoch: 10  Global Step: 60150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:29,879-Speed 3897.19 samples/sec  Loss 1.5242  LearningRate 0.0665  ProxyLR: 3.3268  Epoch: 10  Global Step: 60160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:32,505-Speed 3900.14 samples/sec  Loss 1.4677  LearningRate 0.0665  ProxyLR: 3.3260  Epoch: 10  Global Step: 60170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:35,121-Speed 3915.31 samples/sec  Loss 1.5040  LearningRate 0.0665  ProxyLR: 3.3252  Epoch: 10  Global Step: 60180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:37,750-Speed 3896.33 samples/sec  Loss 1.4840  LearningRate 0.0665  ProxyLR: 3.3244  Epoch: 10  Global Step: 60190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:40,378-Speed 3897.25 samples/sec  Loss 1.4939  LearningRate 0.0665  ProxyLR: 3.3236  Epoch: 10  Global Step: 60200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:43,007-Speed 3896.66 samples/sec  Loss 1.5086  LearningRate 0.0665  ProxyLR: 3.3228  Epoch: 10  Global Step: 60210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:04:45,622-Speed 3916.18 samples/sec  Loss 1.4602  LearningRate 0.0664  ProxyLR: 3.3219  Epoch: 10  Global Step: 60220   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:48,251-Speed 3896.94 samples/sec  Loss 1.4470  LearningRate 0.0664  ProxyLR: 3.3211  Epoch: 10  Global Step: 60230   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:50,880-Speed 3895.40 samples/sec  Loss 1.4506  LearningRate 0.0664  ProxyLR: 3.3203  Epoch: 10  Global Step: 60240   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:53,508-Speed 3897.22 samples/sec  Loss 1.3876  LearningRate 0.0664  ProxyLR: 3.3195  Epoch: 10  Global Step: 60250   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:56,136-Speed 3896.76 samples/sec  Loss 1.4192  LearningRate 0.0664  ProxyLR: 3.3187  Epoch: 10  Global Step: 60260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:04:58,765-Speed 3896.20 samples/sec  Loss 1.4205  LearningRate 0.0664  ProxyLR: 3.3179  Epoch: 10  Global Step: 60270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:05:01,392-Speed 3899.03 samples/sec  Loss 1.4074  LearningRate 0.0663  ProxyLR: 3.3171  Epoch: 10  Global Step: 60280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:05:04,022-Speed 3894.20 samples/sec  Loss 1.4391  LearningRate 0.0663  ProxyLR: 3.3163  Epoch: 10  Global Step: 60290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:05:06,652-Speed 3894.65 samples/sec  Loss 1.5086  LearningRate 0.0663  ProxyLR: 3.3155  Epoch: 10  Global Step: 60300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:05:09,280-Speed 3898.08 samples/sec  Loss 1.3843  LearningRate 0.0663  ProxyLR: 3.3146  Epoch: 10  Global Step: 60310   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:05:11,909-Speed 3896.09 samples/sec  Loss 1.4676  LearningRate 0.0663  ProxyLR: 3.3138  Epoch: 10  Global Step: 60320   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:14,537-Speed 3896.75 samples/sec  Loss 1.5030  LearningRate 0.0663  ProxyLR: 3.3130  Epoch: 10  Global Step: 60330   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:17,165-Speed 3898.58 samples/sec  Loss 1.4174  LearningRate 0.0662  ProxyLR: 3.3122  Epoch: 10  Global Step: 60340   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:19,794-Speed 3895.57 samples/sec  Loss 1.4839  LearningRate 0.0662  ProxyLR: 3.3114  Epoch: 10  Global Step: 60350   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:22,423-Speed 3895.29 samples/sec  Loss 1.4904  LearningRate 0.0662  ProxyLR: 3.3106  Epoch: 10  Global Step: 60360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:25,051-Speed 3897.63 samples/sec  Loss 1.4267  LearningRate 0.0662  ProxyLR: 3.3098  Epoch: 10  Global Step: 60370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:27,680-Speed 3896.99 samples/sec  Loss 1.4043  LearningRate 0.0662  ProxyLR: 3.3090  Epoch: 10  Global Step: 60380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:30,308-Speed 3897.40 samples/sec  Loss 1.4294  LearningRate 0.0662  ProxyLR: 3.3082  Epoch: 10  Global Step: 60390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:32,936-Speed 3897.28 samples/sec  Loss 1.4018  LearningRate 0.0661  ProxyLR: 3.3074  Epoch: 10  Global Step: 60400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:35,565-Speed 3894.82 samples/sec  Loss 1.3831  LearningRate 0.0661  ProxyLR: 3.3066  Epoch: 10  Global Step: 60410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:38,183-Speed 3913.65 samples/sec  Loss 1.4176  LearningRate 0.0661  ProxyLR: 3.3057  Epoch: 10  Global Step: 60420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:40,814-Speed 3892.88 samples/sec  Loss 1.4289  LearningRate 0.0661  ProxyLR: 3.3049  Epoch: 10  Global Step: 60430   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:43,446-Speed 3891.54 samples/sec  Loss 1.3726  LearningRate 0.0661  ProxyLR: 3.3041  Epoch: 10  Global Step: 60440   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:46,077-Speed 3892.28 samples/sec  Loss 1.4727  LearningRate 0.0661  ProxyLR: 3.3033  Epoch: 10  Global Step: 60450   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:48,709-Speed 3891.60 samples/sec  Loss 1.3561  LearningRate 0.0661  ProxyLR: 3.3025  Epoch: 10  Global Step: 60460   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:51,342-Speed 3889.91 samples/sec  Loss 1.5031  LearningRate 0.0660  ProxyLR: 3.3017  Epoch: 10  Global Step: 60470   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:53,974-Speed 3891.74 samples/sec  Loss 1.3647  LearningRate 0.0660  ProxyLR: 3.3009  Epoch: 10  Global Step: 60480   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:56,608-Speed 3889.24 samples/sec  Loss 1.4833  LearningRate 0.0660  ProxyLR: 3.3001  Epoch: 10  Global Step: 60490   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:05:59,241-Speed 3890.30 samples/sec  Loss 1.3970  LearningRate 0.0660  ProxyLR: 3.2993  Epoch: 10  Global Step: 60500   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:01,871-Speed 3894.18 samples/sec  Loss 1.3991  LearningRate 0.0660  ProxyLR: 3.2985  Epoch: 10  Global Step: 60510   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:04,489-Speed 3912.25 samples/sec  Loss 1.4130  LearningRate 0.0660  ProxyLR: 3.2977  Epoch: 10  Global Step: 60520   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:07,122-Speed 3889.32 samples/sec  Loss 1.4037  LearningRate 0.0659  ProxyLR: 3.2969  Epoch: 10  Global Step: 60530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:09,756-Speed 3889.45 samples/sec  Loss 1.4519  LearningRate 0.0659  ProxyLR: 3.2960  Epoch: 10  Global Step: 60540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:12,388-Speed 3891.97 samples/sec  Loss 1.4732  LearningRate 0.0659  ProxyLR: 3.2952  Epoch: 10  Global Step: 60550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:15,020-Speed 3891.90 samples/sec  Loss 1.3696  LearningRate 0.0659  ProxyLR: 3.2944  Epoch: 10  Global Step: 60560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:17,652-Speed 3890.28 samples/sec  Loss 1.3909  LearningRate 0.0659  ProxyLR: 3.2936  Epoch: 10  Global Step: 60570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:20,286-Speed 3888.82 samples/sec  Loss 1.4264  LearningRate 0.0659  ProxyLR: 3.2928  Epoch: 10  Global Step: 60580   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:22,919-Speed 3890.81 samples/sec  Loss 1.3820  LearningRate 0.0658  ProxyLR: 3.2920  Epoch: 10  Global Step: 60590   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:25,552-Speed 3889.55 samples/sec  Loss 1.4224  LearningRate 0.0658  ProxyLR: 3.2912  Epoch: 10  Global Step: 60600   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:28,184-Speed 3891.43 samples/sec  Loss 1.4221  LearningRate 0.0658  ProxyLR: 3.2904  Epoch: 10  Global Step: 60610   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:30,814-Speed 3893.70 samples/sec  Loss 1.4133  LearningRate 0.0658  ProxyLR: 3.2896  Epoch: 10  Global Step: 60620   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 19:06:33,432-Speed 3913.35 samples/sec  Loss 1.3990  LearningRate 0.0658  ProxyLR: 3.2888  Epoch: 10  Global Step: 60630   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:36,064-Speed 3891.67 samples/sec  Loss 1.4068  LearningRate 0.0658  ProxyLR: 3.2880  Epoch: 10  Global Step: 60640   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:38,693-Speed 3894.92 samples/sec  Loss 1.4670  LearningRate 0.0657  ProxyLR: 3.2872  Epoch: 10  Global Step: 60650   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:41,328-Speed 3887.77 samples/sec  Loss 1.4799  LearningRate 0.0657  ProxyLR: 3.2864  Epoch: 10  Global Step: 60660   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:43,962-Speed 3888.81 samples/sec  Loss 1.4125  LearningRate 0.0657  ProxyLR: 3.2856  Epoch: 10  Global Step: 60670   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:46,597-Speed 3887.51 samples/sec  Loss 1.4368  LearningRate 0.0657  ProxyLR: 3.2847  Epoch: 10  Global Step: 60680   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:49,232-Speed 3887.22 samples/sec  Loss 1.3665  LearningRate 0.0657  ProxyLR: 3.2839  Epoch: 10  Global Step: 60690   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:51,867-Speed 3887.24 samples/sec  Loss 1.4139  LearningRate 0.0657  ProxyLR: 3.2831  Epoch: 10  Global Step: 60700   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:54,501-Speed 3888.08 samples/sec  Loss 1.4121  LearningRate 0.0656  ProxyLR: 3.2823  Epoch: 10  Global Step: 60710   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:57,136-Speed 3886.33 samples/sec  Loss 1.4048  LearningRate 0.0656  ProxyLR: 3.2815  Epoch: 10  Global Step: 60720   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:06:59,773-Speed 3885.43 samples/sec  Loss 1.3744  LearningRate 0.0656  ProxyLR: 3.2807  Epoch: 10  Global Step: 60730   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 19:07:02,394-Speed 3906.70 samples/sec  Loss 1.3380  LearningRate 0.0656  ProxyLR: 3.2799  Epoch: 10  Global Step: 60740   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:05,032-Speed 3883.90 samples/sec  Loss 1.3911  LearningRate 0.0656  ProxyLR: 3.2791  Epoch: 10  Global Step: 60750   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:07,667-Speed 3886.42 samples/sec  Loss 1.3997  LearningRate 0.0656  ProxyLR: 3.2783  Epoch: 10  Global Step: 60760   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:10,302-Speed 3887.75 samples/sec  Loss 1.4244  LearningRate 0.0655  ProxyLR: 3.2775  Epoch: 10  Global Step: 60770   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:12,937-Speed 3887.03 samples/sec  Loss 1.4374  LearningRate 0.0655  ProxyLR: 3.2767  Epoch: 10  Global Step: 60780   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:15,571-Speed 3888.74 samples/sec  Loss 1.3712  LearningRate 0.0655  ProxyLR: 3.2759  Epoch: 10  Global Step: 60790   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:18,191-Speed 3908.31 samples/sec  Loss 1.4204  LearningRate 0.0655  ProxyLR: 3.2751  Epoch: 10  Global Step: 60800   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:20,825-Speed 3889.72 samples/sec  Loss 1.4014  LearningRate 0.0655  ProxyLR: 3.2743  Epoch: 10  Global Step: 60810   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:23,458-Speed 3888.96 samples/sec  Loss 1.3705  LearningRate 0.0655  ProxyLR: 3.2735  Epoch: 10  Global Step: 60820   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:26,091-Speed 3890.19 samples/sec  Loss 1.3821  LearningRate 0.0655  ProxyLR: 3.2727  Epoch: 10  Global Step: 60830   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:28,726-Speed 3887.57 samples/sec  Loss 1.4709  LearningRate 0.0654  ProxyLR: 3.2719  Epoch: 10  Global Step: 60840   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:31,358-Speed 3892.17 samples/sec  Loss 1.4184  LearningRate 0.0654  ProxyLR: 3.2711  Epoch: 10  Global Step: 60850   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:33,989-Speed 3892.56 samples/sec  Loss 1.3248  LearningRate 0.0654  ProxyLR: 3.2702  Epoch: 10  Global Step: 60860   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:36,620-Speed 3893.28 samples/sec  Loss 1.4092  LearningRate 0.0654  ProxyLR: 3.2694  Epoch: 10  Global Step: 60870   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:39,252-Speed 3891.08 samples/sec  Loss 1.4355  LearningRate 0.0654  ProxyLR: 3.2686  Epoch: 10  Global Step: 60880   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:41,888-Speed 3885.95 samples/sec  Loss 1.4148  LearningRate 0.0654  ProxyLR: 3.2678  Epoch: 10  Global Step: 60890   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:07:44,829-Speed 3482.40 samples/sec  Loss 1.4673  LearningRate 0.0653  ProxyLR: 3.2670  Epoch: 10  Global Step: 60900   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:47,465-Speed 3886.06 samples/sec  Loss 1.4631  LearningRate 0.0653  ProxyLR: 3.2662  Epoch: 10  Global Step: 60910   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:50,096-Speed 3892.22 samples/sec  Loss 1.3667  LearningRate 0.0653  ProxyLR: 3.2654  Epoch: 10  Global Step: 60920   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:52,728-Speed 3891.65 samples/sec  Loss 1.4140  LearningRate 0.0653  ProxyLR: 3.2646  Epoch: 10  Global Step: 60930   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:55,360-Speed 3892.26 samples/sec  Loss 1.3618  LearningRate 0.0653  ProxyLR: 3.2638  Epoch: 10  Global Step: 60940   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:07:57,978-Speed 3912.16 samples/sec  Loss 1.4281  LearningRate 0.0653  ProxyLR: 3.2630  Epoch: 10  Global Step: 60950   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:00,609-Speed 3893.33 samples/sec  Loss 1.4438  LearningRate 0.0652  ProxyLR: 3.2622  Epoch: 10  Global Step: 60960   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:03,241-Speed 3891.82 samples/sec  Loss 1.4348  LearningRate 0.0652  ProxyLR: 3.2614  Epoch: 10  Global Step: 60970   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:05,874-Speed 3889.74 samples/sec  Loss 1.4049  LearningRate 0.0652  ProxyLR: 3.2606  Epoch: 10  Global Step: 60980   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:08,504-Speed 3894.59 samples/sec  Loss 1.4371  LearningRate 0.0652  ProxyLR: 3.2598  Epoch: 10  Global Step: 60990   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:11,135-Speed 3892.15 samples/sec  Loss 1.4138  LearningRate 0.0652  ProxyLR: 3.2590  Epoch: 10  Global Step: 61000   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:13,768-Speed 3890.84 samples/sec  Loss 1.4533  LearningRate 0.0652  ProxyLR: 3.2582  Epoch: 10  Global Step: 61010   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:16,400-Speed 3890.92 samples/sec  Loss 1.3720  LearningRate 0.0651  ProxyLR: 3.2574  Epoch: 10  Global Step: 61020   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:19,034-Speed 3889.56 samples/sec  Loss 1.3595  LearningRate 0.0651  ProxyLR: 3.2566  Epoch: 10  Global Step: 61030   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:21,667-Speed 3890.17 samples/sec  Loss 1.4348  LearningRate 0.0651  ProxyLR: 3.2558  Epoch: 10  Global Step: 61040   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:24,287-Speed 3908.41 samples/sec  Loss 1.5291  LearningRate 0.0651  ProxyLR: 3.2550  Epoch: 10  Global Step: 61050   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:26,920-Speed 3890.63 samples/sec  Loss 1.3751  LearningRate 0.0651  ProxyLR: 3.2542  Epoch: 10  Global Step: 61060   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:29,553-Speed 3888.96 samples/sec  Loss 1.4213  LearningRate 0.0651  ProxyLR: 3.2534  Epoch: 10  Global Step: 61070   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:32,188-Speed 3887.34 samples/sec  Loss 1.4526  LearningRate 0.0651  ProxyLR: 3.2526  Epoch: 10  Global Step: 61080   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:34,822-Speed 3889.14 samples/sec  Loss 1.4182  LearningRate 0.0650  ProxyLR: 3.2518  Epoch: 10  Global Step: 61090   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:37,455-Speed 3889.65 samples/sec  Loss 1.3512  LearningRate 0.0650  ProxyLR: 3.2510  Epoch: 10  Global Step: 61100   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:40,088-Speed 3890.16 samples/sec  Loss 1.3624  LearningRate 0.0650  ProxyLR: 3.2502  Epoch: 10  Global Step: 61110   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:42,723-Speed 3886.99 samples/sec  Loss 1.3745  LearningRate 0.0650  ProxyLR: 3.2494  Epoch: 10  Global Step: 61120   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:45,354-Speed 3893.36 samples/sec  Loss 1.4761  LearningRate 0.0650  ProxyLR: 3.2486  Epoch: 10  Global Step: 61130   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:47,985-Speed 3893.20 samples/sec  Loss 1.4298  LearningRate 0.0650  ProxyLR: 3.2478  Epoch: 10  Global Step: 61140   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:08:50,614-Speed 3895.30 samples/sec  Loss 1.4111  LearningRate 0.0649  ProxyLR: 3.2470  Epoch: 10  Global Step: 61150   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:08:53,244-Speed 3894.74 samples/sec  Loss 1.4540  LearningRate 0.0649  ProxyLR: 3.2462  Epoch: 10  Global Step: 61160   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:08:55,875-Speed 3893.05 samples/sec  Loss 1.3325  LearningRate 0.0649  ProxyLR: 3.2454  Epoch: 10  Global Step: 61170   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:08:58,504-Speed 3895.93 samples/sec  Loss 1.4471  LearningRate 0.0649  ProxyLR: 3.2446  Epoch: 10  Global Step: 61180   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:01,134-Speed 3893.92 samples/sec  Loss 1.4310  LearningRate 0.0649  ProxyLR: 3.2437  Epoch: 10  Global Step: 61190   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:03,761-Speed 3898.81 samples/sec  Loss 1.3455  LearningRate 0.0649  ProxyLR: 3.2429  Epoch: 10  Global Step: 61200   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:06,391-Speed 3895.42 samples/sec  Loss 1.3848  LearningRate 0.0648  ProxyLR: 3.2421  Epoch: 10  Global Step: 61210   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:09,020-Speed 3895.90 samples/sec  Loss 1.3158  LearningRate 0.0648  ProxyLR: 3.2413  Epoch: 10  Global Step: 61220   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:11,649-Speed 3895.51 samples/sec  Loss 1.4153  LearningRate 0.0648  ProxyLR: 3.2405  Epoch: 10  Global Step: 61230   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:14,282-Speed 3890.88 samples/sec  Loss 1.4103  LearningRate 0.0648  ProxyLR: 3.2397  Epoch: 10  Global Step: 61240   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:16,910-Speed 3896.31 samples/sec  Loss 1.3325  LearningRate 0.0648  ProxyLR: 3.2389  Epoch: 10  Global Step: 61250   Fp16 Grad Scale: 1048576  Required: 7 hours
Training: 2023-05-04 19:09:19,514-Speed 3934.23 samples/sec  Loss 1.3561  LearningRate 0.0648  ProxyLR: 3.2381  Epoch: 10  Global Step: 61260   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:22,143-Speed 3895.51 samples/sec  Loss 1.4276  LearningRate 0.0647  ProxyLR: 3.2373  Epoch: 10  Global Step: 61270   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:24,772-Speed 3896.21 samples/sec  Loss 1.3772  LearningRate 0.0647  ProxyLR: 3.2365  Epoch: 10  Global Step: 61280   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:27,401-Speed 3896.38 samples/sec  Loss 1.5290  LearningRate 0.0647  ProxyLR: 3.2357  Epoch: 10  Global Step: 61290   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:30,030-Speed 3895.94 samples/sec  Loss 1.4835  LearningRate 0.0647  ProxyLR: 3.2349  Epoch: 10  Global Step: 61300   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:32,659-Speed 3895.15 samples/sec  Loss 1.3291  LearningRate 0.0647  ProxyLR: 3.2341  Epoch: 10  Global Step: 61310   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:35,290-Speed 3893.05 samples/sec  Loss 1.3575  LearningRate 0.0647  ProxyLR: 3.2333  Epoch: 10  Global Step: 61320   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:37,920-Speed 3895.03 samples/sec  Loss 1.4589  LearningRate 0.0647  ProxyLR: 3.2325  Epoch: 10  Global Step: 61330   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:40,549-Speed 3895.38 samples/sec  Loss 1.3512  LearningRate 0.0646  ProxyLR: 3.2317  Epoch: 10  Global Step: 61340   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:43,179-Speed 3894.97 samples/sec  Loss 1.3615  LearningRate 0.0646  ProxyLR: 3.2309  Epoch: 10  Global Step: 61350   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:09:45,808-Speed 3895.61 samples/sec  Loss 1.3562  LearningRate 0.0646  ProxyLR: 3.2301  Epoch: 10  Global Step: 61360   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:48,436-Speed 3896.27 samples/sec  Loss 1.3398  LearningRate 0.0646  ProxyLR: 3.2293  Epoch: 10  Global Step: 61370   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:51,067-Speed 3894.17 samples/sec  Loss 1.4205  LearningRate 0.0646  ProxyLR: 3.2285  Epoch: 10  Global Step: 61380   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:53,697-Speed 3894.42 samples/sec  Loss 1.4258  LearningRate 0.0646  ProxyLR: 3.2277  Epoch: 10  Global Step: 61390   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:56,326-Speed 3894.83 samples/sec  Loss 1.3505  LearningRate 0.0645  ProxyLR: 3.2269  Epoch: 10  Global Step: 61400   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:09:58,957-Speed 3894.06 samples/sec  Loss 1.3741  LearningRate 0.0645  ProxyLR: 3.2261  Epoch: 10  Global Step: 61410   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:01,590-Speed 3890.55 samples/sec  Loss 1.2584  LearningRate 0.0645  ProxyLR: 3.2253  Epoch: 10  Global Step: 61420   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:04,207-Speed 3913.73 samples/sec  Loss 1.4056  LearningRate 0.0645  ProxyLR: 3.2245  Epoch: 10  Global Step: 61430   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:06,836-Speed 3895.41 samples/sec  Loss 1.4106  LearningRate 0.0645  ProxyLR: 3.2237  Epoch: 10  Global Step: 61440   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:09,464-Speed 3896.71 samples/sec  Loss 1.3895  LearningRate 0.0645  ProxyLR: 3.2229  Epoch: 10  Global Step: 61450   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:12,094-Speed 3895.25 samples/sec  Loss 1.4611  LearningRate 0.0644  ProxyLR: 3.2221  Epoch: 10  Global Step: 61460   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:14,723-Speed 3895.91 samples/sec  Loss 1.4251  LearningRate 0.0644  ProxyLR: 3.2214  Epoch: 10  Global Step: 61470   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:17,354-Speed 3893.20 samples/sec  Loss 1.3441  LearningRate 0.0644  ProxyLR: 3.2206  Epoch: 10  Global Step: 61480   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:19,982-Speed 3896.82 samples/sec  Loss 1.4393  LearningRate 0.0644  ProxyLR: 3.2198  Epoch: 10  Global Step: 61490   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:22,611-Speed 3895.72 samples/sec  Loss 1.3602  LearningRate 0.0644  ProxyLR: 3.2190  Epoch: 10  Global Step: 61500   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:25,243-Speed 3891.89 samples/sec  Loss 1.4449  LearningRate 0.0644  ProxyLR: 3.2182  Epoch: 10  Global Step: 61510   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:27,872-Speed 3896.24 samples/sec  Loss 1.3835  LearningRate 0.0643  ProxyLR: 3.2174  Epoch: 10  Global Step: 61520   Fp16 Grad Scale: 262144  Required: 7 hours
Training: 2023-05-04 19:10:30,503-Speed 3893.38 samples/sec  Loss 1.3199  LearningRate 0.0643  ProxyLR: 3.2166  Epoch: 10  Global Step: 61530   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:33,132-Speed 3894.83 samples/sec  Loss 1.3796  LearningRate 0.0643  ProxyLR: 3.2158  Epoch: 10  Global Step: 61540   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:35,761-Speed 3896.03 samples/sec  Loss 1.3853  LearningRate 0.0643  ProxyLR: 3.2150  Epoch: 10  Global Step: 61550   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:38,391-Speed 3895.03 samples/sec  Loss 1.3461  LearningRate 0.0643  ProxyLR: 3.2142  Epoch: 10  Global Step: 61560   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:41,021-Speed 3893.67 samples/sec  Loss 1.4175  LearningRate 0.0643  ProxyLR: 3.2134  Epoch: 10  Global Step: 61570   Fp16 Grad Scale: 524288  Required: 7 hours
Training: 2023-05-04 19:10:43,652-Speed 3892.86 samples/sec  Loss 1.4730  LearningRate 0.0643  ProxyLR: 3.2126  Epoch: 10  Global Step: 61580   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:10:46,285-Speed 3890.68 samples/sec  Loss 1.4500  LearningRate 0.0642  ProxyLR: 3.2118  Epoch: 10  Global Step: 61590   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:10:48,914-Speed 3895.70 samples/sec  Loss 1.4405  LearningRate 0.0642  ProxyLR: 3.2110  Epoch: 10  Global Step: 61600   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:10:51,545-Speed 3894.07 samples/sec  Loss 1.3962  LearningRate 0.0642  ProxyLR: 3.2102  Epoch: 10  Global Step: 61610   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:10:54,176-Speed 3892.95 samples/sec  Loss 1.3641  LearningRate 0.0642  ProxyLR: 3.2094  Epoch: 10  Global Step: 61620   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:10:56,796-Speed 3908.78 samples/sec  Loss 1.3524  LearningRate 0.0642  ProxyLR: 3.2086  Epoch: 10  Global Step: 61630   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:10:59,426-Speed 3893.80 samples/sec  Loss 1.4220  LearningRate 0.0642  ProxyLR: 3.2078  Epoch: 10  Global Step: 61640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:02,059-Speed 3890.08 samples/sec  Loss 1.4172  LearningRate 0.0641  ProxyLR: 3.2070  Epoch: 10  Global Step: 61650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:04,691-Speed 3891.28 samples/sec  Loss 1.3437  LearningRate 0.0641  ProxyLR: 3.2062  Epoch: 10  Global Step: 61660   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:07,325-Speed 3888.61 samples/sec  Loss 1.3295  LearningRate 0.0641  ProxyLR: 3.2054  Epoch: 10  Global Step: 61670   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:09,961-Speed 3886.18 samples/sec  Loss 1.4010  LearningRate 0.0641  ProxyLR: 3.2046  Epoch: 10  Global Step: 61680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:12,597-Speed 3885.22 samples/sec  Loss 1.3584  LearningRate 0.0641  ProxyLR: 3.2038  Epoch: 10  Global Step: 61690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:15,230-Speed 3890.68 samples/sec  Loss 1.4259  LearningRate 0.0641  ProxyLR: 3.2030  Epoch: 10  Global Step: 61700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:17,861-Speed 3892.53 samples/sec  Loss 1.4061  LearningRate 0.0640  ProxyLR: 3.2022  Epoch: 10  Global Step: 61710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:20,490-Speed 3896.03 samples/sec  Loss 1.3615  LearningRate 0.0640  ProxyLR: 3.2014  Epoch: 10  Global Step: 61720   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:23,122-Speed 3891.20 samples/sec  Loss 1.3650  LearningRate 0.0640  ProxyLR: 3.2006  Epoch: 10  Global Step: 61730   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:11:25,740-Speed 3913.06 samples/sec  Loss 1.3269  LearningRate 0.0640  ProxyLR: 3.1998  Epoch: 10  Global Step: 61740   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:28,373-Speed 3889.91 samples/sec  Loss 1.3970  LearningRate 0.0640  ProxyLR: 3.1990  Epoch: 10  Global Step: 61750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:31,004-Speed 3892.09 samples/sec  Loss 1.3920  LearningRate 0.0640  ProxyLR: 3.1982  Epoch: 10  Global Step: 61760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:33,634-Speed 3894.24 samples/sec  Loss 1.4035  LearningRate 0.0639  ProxyLR: 3.1974  Epoch: 10  Global Step: 61770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:36,264-Speed 3894.90 samples/sec  Loss 1.3955  LearningRate 0.0639  ProxyLR: 3.1966  Epoch: 10  Global Step: 61780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:38,893-Speed 3896.60 samples/sec  Loss 1.2647  LearningRate 0.0639  ProxyLR: 3.1958  Epoch: 10  Global Step: 61790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:41,521-Speed 3896.67 samples/sec  Loss 1.4088  LearningRate 0.0639  ProxyLR: 3.1951  Epoch: 10  Global Step: 61800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:44,151-Speed 3894.73 samples/sec  Loss 1.4234  LearningRate 0.0639  ProxyLR: 3.1943  Epoch: 10  Global Step: 61810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:46,779-Speed 3897.47 samples/sec  Loss 1.3447  LearningRate 0.0639  ProxyLR: 3.1935  Epoch: 10  Global Step: 61820   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:49,404-Speed 3902.85 samples/sec  Loss 1.2991  LearningRate 0.0639  ProxyLR: 3.1927  Epoch: 10  Global Step: 61830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:52,029-Speed 3901.92 samples/sec  Loss 1.3857  LearningRate 0.0638  ProxyLR: 3.1919  Epoch: 10  Global Step: 61840   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:11:54,640-Speed 3921.97 samples/sec  Loss 1.3465  LearningRate 0.0638  ProxyLR: 3.1911  Epoch: 10  Global Step: 61850   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:57,266-Speed 3900.80 samples/sec  Loss 1.4257  LearningRate 0.0638  ProxyLR: 3.1903  Epoch: 10  Global Step: 61860   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:11:59,890-Speed 3903.32 samples/sec  Loss 1.3489  LearningRate 0.0638  ProxyLR: 3.1895  Epoch: 10  Global Step: 61870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:02,517-Speed 3898.98 samples/sec  Loss 1.3585  LearningRate 0.0638  ProxyLR: 3.1887  Epoch: 10  Global Step: 61880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:05,141-Speed 3902.45 samples/sec  Loss 1.4100  LearningRate 0.0638  ProxyLR: 3.1879  Epoch: 10  Global Step: 61890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:07,772-Speed 3892.90 samples/sec  Loss 1.3452  LearningRate 0.0637  ProxyLR: 3.1871  Epoch: 10  Global Step: 61900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:10,403-Speed 3893.48 samples/sec  Loss 1.3884  LearningRate 0.0637  ProxyLR: 3.1863  Epoch: 10  Global Step: 61910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:13,034-Speed 3893.55 samples/sec  Loss 1.3774  LearningRate 0.0637  ProxyLR: 3.1855  Epoch: 10  Global Step: 61920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:15,665-Speed 3892.01 samples/sec  Loss 1.3823  LearningRate 0.0637  ProxyLR: 3.1847  Epoch: 10  Global Step: 61930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:18,297-Speed 3892.05 samples/sec  Loss 1.3195  LearningRate 0.0637  ProxyLR: 3.1839  Epoch: 10  Global Step: 61940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:20,917-Speed 3908.97 samples/sec  Loss 1.4353  LearningRate 0.0637  ProxyLR: 3.1831  Epoch: 10  Global Step: 61950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:23,548-Speed 3892.70 samples/sec  Loss 1.4396  LearningRate 0.0636  ProxyLR: 3.1823  Epoch: 10  Global Step: 61960   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:26,165-Speed 3913.56 samples/sec  Loss 1.3649  LearningRate 0.0636  ProxyLR: 3.1815  Epoch: 10  Global Step: 61970   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:28,796-Speed 3893.91 samples/sec  Loss 1.3593  LearningRate 0.0636  ProxyLR: 3.1808  Epoch: 10  Global Step: 61980   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:31,427-Speed 3892.98 samples/sec  Loss 1.3777  LearningRate 0.0636  ProxyLR: 3.1800  Epoch: 10  Global Step: 61990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:34,059-Speed 3891.35 samples/sec  Loss 1.3482  LearningRate 0.0636  ProxyLR: 3.1792  Epoch: 10  Global Step: 62000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:36,692-Speed 3889.64 samples/sec  Loss 1.3404  LearningRate 0.0636  ProxyLR: 3.1784  Epoch: 10  Global Step: 62010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:39,320-Speed 3897.40 samples/sec  Loss 1.4417  LearningRate 0.0636  ProxyLR: 3.1776  Epoch: 10  Global Step: 62020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:41,949-Speed 3895.93 samples/sec  Loss 1.3403  LearningRate 0.0635  ProxyLR: 3.1768  Epoch: 10  Global Step: 62030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:44,578-Speed 3896.82 samples/sec  Loss 1.3423  LearningRate 0.0635  ProxyLR: 3.1760  Epoch: 10  Global Step: 62040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:47,208-Speed 3893.97 samples/sec  Loss 1.3763  LearningRate 0.0635  ProxyLR: 3.1752  Epoch: 10  Global Step: 62050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:49,841-Speed 3890.61 samples/sec  Loss 1.3271  LearningRate 0.0635  ProxyLR: 3.1744  Epoch: 10  Global Step: 62060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:12:52,471-Speed 3894.28 samples/sec  Loss 1.3638  LearningRate 0.0635  ProxyLR: 3.1736  Epoch: 10  Global Step: 62070   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:55,105-Speed 3887.46 samples/sec  Loss 1.3195  LearningRate 0.0635  ProxyLR: 3.1728  Epoch: 10  Global Step: 62080   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:12:57,740-Speed 3888.05 samples/sec  Loss 1.3887  LearningRate 0.0634  ProxyLR: 3.1720  Epoch: 10  Global Step: 62090   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:00,375-Speed 3886.50 samples/sec  Loss 1.4259  LearningRate 0.0634  ProxyLR: 3.1712  Epoch: 10  Global Step: 62100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:03,011-Speed 3886.45 samples/sec  Loss 1.3679  LearningRate 0.0634  ProxyLR: 3.1704  Epoch: 10  Global Step: 62110   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:05,644-Speed 3888.77 samples/sec  Loss 1.4197  LearningRate 0.0634  ProxyLR: 3.1697  Epoch: 10  Global Step: 62120   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:08,279-Speed 3888.11 samples/sec  Loss 1.3643  LearningRate 0.0634  ProxyLR: 3.1689  Epoch: 10  Global Step: 62130   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:10,913-Speed 3888.22 samples/sec  Loss 1.4355  LearningRate 0.0634  ProxyLR: 3.1681  Epoch: 10  Global Step: 62140   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:13,547-Speed 3888.45 samples/sec  Loss 1.4014  LearningRate 0.0633  ProxyLR: 3.1673  Epoch: 10  Global Step: 62150   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:16,183-Speed 3885.58 samples/sec  Loss 1.3435  LearningRate 0.0633  ProxyLR: 3.1665  Epoch: 10  Global Step: 62160   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:18,803-Speed 3909.13 samples/sec  Loss 1.4571  LearningRate 0.0633  ProxyLR: 3.1657  Epoch: 10  Global Step: 62170   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:21,434-Speed 3892.72 samples/sec  Loss 1.3692  LearningRate 0.0633  ProxyLR: 3.1649  Epoch: 10  Global Step: 62180   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:24,065-Speed 3893.36 samples/sec  Loss 1.3452  LearningRate 0.0633  ProxyLR: 3.1641  Epoch: 10  Global Step: 62190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:26,696-Speed 3892.43 samples/sec  Loss 1.3661  LearningRate 0.0633  ProxyLR: 3.1633  Epoch: 10  Global Step: 62200   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:29,327-Speed 3893.75 samples/sec  Loss 1.3226  LearningRate 0.0633  ProxyLR: 3.1625  Epoch: 10  Global Step: 62210   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:31,958-Speed 3892.72 samples/sec  Loss 1.4115  LearningRate 0.0632  ProxyLR: 3.1617  Epoch: 10  Global Step: 62220   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:34,589-Speed 3892.80 samples/sec  Loss 1.3347  LearningRate 0.0632  ProxyLR: 3.1609  Epoch: 10  Global Step: 62230   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:37,219-Speed 3894.78 samples/sec  Loss 1.4292  LearningRate 0.0632  ProxyLR: 3.1602  Epoch: 10  Global Step: 62240   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:39,849-Speed 3894.25 samples/sec  Loss 1.3840  LearningRate 0.0632  ProxyLR: 3.1594  Epoch: 10  Global Step: 62250   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:42,480-Speed 3892.50 samples/sec  Loss 1.3204  LearningRate 0.0632  ProxyLR: 3.1586  Epoch: 10  Global Step: 62260   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:45,112-Speed 3892.35 samples/sec  Loss 1.3787  LearningRate 0.0632  ProxyLR: 3.1578  Epoch: 10  Global Step: 62270   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:13:47,731-Speed 3910.73 samples/sec  Loss 1.3214  LearningRate 0.0631  ProxyLR: 3.1570  Epoch: 10  Global Step: 62280   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:50,363-Speed 3891.58 samples/sec  Loss 1.3614  LearningRate 0.0631  ProxyLR: 3.1562  Epoch: 10  Global Step: 62290   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:52,993-Speed 3894.18 samples/sec  Loss 1.3752  LearningRate 0.0631  ProxyLR: 3.1554  Epoch: 10  Global Step: 62300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:55,624-Speed 3893.52 samples/sec  Loss 1.3565  LearningRate 0.0631  ProxyLR: 3.1546  Epoch: 10  Global Step: 62310   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:13:58,241-Speed 3912.96 samples/sec  Loss 1.3615  LearningRate 0.0631  ProxyLR: 3.1538  Epoch: 10  Global Step: 62320   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:00,871-Speed 3894.51 samples/sec  Loss 1.3516  LearningRate 0.0631  ProxyLR: 3.1530  Epoch: 10  Global Step: 62330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:03,499-Speed 3897.05 samples/sec  Loss 1.4554  LearningRate 0.0630  ProxyLR: 3.1523  Epoch: 10  Global Step: 62340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:06,129-Speed 3895.50 samples/sec  Loss 1.4156  LearningRate 0.0630  ProxyLR: 3.1515  Epoch: 10  Global Step: 62350   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:08,746-Speed 3913.76 samples/sec  Loss 1.3918  LearningRate 0.0630  ProxyLR: 3.1507  Epoch: 10  Global Step: 62360   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:11,376-Speed 3894.17 samples/sec  Loss 1.3941  LearningRate 0.0630  ProxyLR: 3.1499  Epoch: 10  Global Step: 62370   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:14,005-Speed 3895.31 samples/sec  Loss 1.3838  LearningRate 0.0630  ProxyLR: 3.1491  Epoch: 10  Global Step: 62380   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:16,633-Speed 3898.26 samples/sec  Loss 1.2860  LearningRate 0.0630  ProxyLR: 3.1483  Epoch: 10  Global Step: 62390   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:19,263-Speed 3894.68 samples/sec  Loss 1.3679  LearningRate 0.0630  ProxyLR: 3.1475  Epoch: 10  Global Step: 62400   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:21,892-Speed 3895.90 samples/sec  Loss 1.3579  LearningRate 0.0629  ProxyLR: 3.1467  Epoch: 10  Global Step: 62410   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:24,522-Speed 3894.78 samples/sec  Loss 1.4539  LearningRate 0.0629  ProxyLR: 3.1459  Epoch: 10  Global Step: 62420   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:27,150-Speed 3896.38 samples/sec  Loss 1.3653  LearningRate 0.0629  ProxyLR: 3.1451  Epoch: 10  Global Step: 62430   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:29,779-Speed 3896.47 samples/sec  Loss 1.4048  LearningRate 0.0629  ProxyLR: 3.1444  Epoch: 10  Global Step: 62440   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:32,410-Speed 3893.54 samples/sec  Loss 1.3154  LearningRate 0.0629  ProxyLR: 3.1436  Epoch: 10  Global Step: 62450   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:14:35,040-Speed 3894.49 samples/sec  Loss 1.3676  LearningRate 0.0629  ProxyLR: 3.1428  Epoch: 10  Global Step: 62460   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:37,671-Speed 3891.95 samples/sec  Loss 1.3011  LearningRate 0.0628  ProxyLR: 3.1420  Epoch: 10  Global Step: 62470   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:40,301-Speed 3893.98 samples/sec  Loss 1.4418  LearningRate 0.0628  ProxyLR: 3.1412  Epoch: 10  Global Step: 62480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:42,931-Speed 3895.60 samples/sec  Loss 1.3689  LearningRate 0.0628  ProxyLR: 3.1404  Epoch: 10  Global Step: 62490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:45,560-Speed 3895.72 samples/sec  Loss 1.4080  LearningRate 0.0628  ProxyLR: 3.1396  Epoch: 10  Global Step: 62500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:48,191-Speed 3892.35 samples/sec  Loss 1.3588  LearningRate 0.0628  ProxyLR: 3.1388  Epoch: 10  Global Step: 62510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:50,821-Speed 3894.25 samples/sec  Loss 1.3367  LearningRate 0.0628  ProxyLR: 3.1380  Epoch: 10  Global Step: 62520   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:53,451-Speed 3894.35 samples/sec  Loss 1.3968  LearningRate 0.0627  ProxyLR: 3.1373  Epoch: 10  Global Step: 62530   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:14:56,141-Speed 3807.75 samples/sec  Loss 1.3047  LearningRate 0.0627  ProxyLR: 3.1365  Epoch: 10  Global Step: 62540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:15:06,279-Speed 1010.14 samples/sec  Loss 1.2747  LearningRate 0.0627  ProxyLR: 3.1357  Epoch: 11  Global Step: 62550   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:15:08,961-Speed 3819.30 samples/sec  Loss 0.9861  LearningRate 0.0627  ProxyLR: 3.1349  Epoch: 11  Global Step: 62560   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:11,598-Speed 3884.41 samples/sec  Loss 0.9313  LearningRate 0.0627  ProxyLR: 3.1341  Epoch: 11  Global Step: 62570   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:14,228-Speed 3895.13 samples/sec  Loss 0.9200  LearningRate 0.0627  ProxyLR: 3.1333  Epoch: 11  Global Step: 62580   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:16,856-Speed 3896.25 samples/sec  Loss 0.8905  LearningRate 0.0627  ProxyLR: 3.1325  Epoch: 11  Global Step: 62590   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:19,485-Speed 3896.03 samples/sec  Loss 0.8870  LearningRate 0.0626  ProxyLR: 3.1317  Epoch: 11  Global Step: 62600   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:22,118-Speed 3890.68 samples/sec  Loss 0.9062  LearningRate 0.0626  ProxyLR: 3.1310  Epoch: 11  Global Step: 62610   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:24,748-Speed 3895.05 samples/sec  Loss 0.8752  LearningRate 0.0626  ProxyLR: 3.1302  Epoch: 11  Global Step: 62620   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:27,427-Speed 3822.41 samples/sec  Loss 0.9292  LearningRate 0.0626  ProxyLR: 3.1294  Epoch: 11  Global Step: 62630   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:30,056-Speed 3896.22 samples/sec  Loss 0.9042  LearningRate 0.0626  ProxyLR: 3.1286  Epoch: 11  Global Step: 62640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:32,686-Speed 3894.15 samples/sec  Loss 0.9637  LearningRate 0.0626  ProxyLR: 3.1278  Epoch: 11  Global Step: 62650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:35,315-Speed 3895.93 samples/sec  Loss 0.8958  LearningRate 0.0625  ProxyLR: 3.1270  Epoch: 11  Global Step: 62660   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:15:37,966-Speed 3863.91 samples/sec  Loss 0.8879  LearningRate 0.0625  ProxyLR: 3.1262  Epoch: 11  Global Step: 62670   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:15:40,597-Speed 3892.73 samples/sec  Loss 0.8985  LearningRate 0.0625  ProxyLR: 3.1255  Epoch: 11  Global Step: 62680   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:15:43,229-Speed 3892.54 samples/sec  Loss 0.9074  LearningRate 0.0625  ProxyLR: 3.1247  Epoch: 11  Global Step: 62690   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:15:45,858-Speed 3895.00 samples/sec  Loss 0.9145  LearningRate 0.0625  ProxyLR: 3.1239  Epoch: 11  Global Step: 62700   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:15:48,515-Speed 3854.83 samples/sec  Loss 0.8898  LearningRate 0.0625  ProxyLR: 3.1231  Epoch: 11  Global Step: 62710   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:15:51,132-Speed 3914.56 samples/sec  Loss 0.8899  LearningRate 0.0624  ProxyLR: 3.1223  Epoch: 11  Global Step: 62720   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:53,764-Speed 3892.34 samples/sec  Loss 0.8947  LearningRate 0.0624  ProxyLR: 3.1215  Epoch: 11  Global Step: 62730   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:56,394-Speed 3893.42 samples/sec  Loss 0.9009  LearningRate 0.0624  ProxyLR: 3.1207  Epoch: 11  Global Step: 62740   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:15:59,024-Speed 3894.92 samples/sec  Loss 0.9156  LearningRate 0.0624  ProxyLR: 3.1199  Epoch: 11  Global Step: 62750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:01,704-Speed 3822.07 samples/sec  Loss 0.9077  LearningRate 0.0624  ProxyLR: 3.1192  Epoch: 11  Global Step: 62760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:04,335-Speed 3892.67 samples/sec  Loss 0.9442  LearningRate 0.0624  ProxyLR: 3.1184  Epoch: 11  Global Step: 62770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:06,970-Speed 3887.36 samples/sec  Loss 0.9294  LearningRate 0.0624  ProxyLR: 3.1176  Epoch: 11  Global Step: 62780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:09,606-Speed 3885.69 samples/sec  Loss 0.9140  LearningRate 0.0623  ProxyLR: 3.1168  Epoch: 11  Global Step: 62790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:12,240-Speed 3887.94 samples/sec  Loss 0.9354  LearningRate 0.0623  ProxyLR: 3.1160  Epoch: 11  Global Step: 62800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:14,874-Speed 3888.82 samples/sec  Loss 0.8959  LearningRate 0.0623  ProxyLR: 3.1152  Epoch: 11  Global Step: 62810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:17,510-Speed 3885.59 samples/sec  Loss 0.8867  LearningRate 0.0623  ProxyLR: 3.1144  Epoch: 11  Global Step: 62820   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:16:20,134-Speed 3903.48 samples/sec  Loss 0.9169  LearningRate 0.0623  ProxyLR: 3.1137  Epoch: 11  Global Step: 62830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:22,769-Speed 3887.41 samples/sec  Loss 0.9595  LearningRate 0.0623  ProxyLR: 3.1129  Epoch: 11  Global Step: 62840   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:25,404-Speed 3886.98 samples/sec  Loss 0.9160  LearningRate 0.0622  ProxyLR: 3.1121  Epoch: 11  Global Step: 62850   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:28,042-Speed 3882.59 samples/sec  Loss 0.9406  LearningRate 0.0622  ProxyLR: 3.1113  Epoch: 11  Global Step: 62860   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:30,675-Speed 3890.24 samples/sec  Loss 0.8913  LearningRate 0.0622  ProxyLR: 3.1105  Epoch: 11  Global Step: 62870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:33,308-Speed 3891.12 samples/sec  Loss 0.8966  LearningRate 0.0622  ProxyLR: 3.1097  Epoch: 11  Global Step: 62880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:35,942-Speed 3888.37 samples/sec  Loss 0.8951  LearningRate 0.0622  ProxyLR: 3.1090  Epoch: 11  Global Step: 62890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:38,574-Speed 3891.40 samples/sec  Loss 0.9075  LearningRate 0.0622  ProxyLR: 3.1082  Epoch: 11  Global Step: 62900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:41,208-Speed 3888.88 samples/sec  Loss 0.9000  LearningRate 0.0621  ProxyLR: 3.1074  Epoch: 11  Global Step: 62910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:43,842-Speed 3887.93 samples/sec  Loss 0.8879  LearningRate 0.0621  ProxyLR: 3.1066  Epoch: 11  Global Step: 62920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:46,476-Speed 3888.35 samples/sec  Loss 0.9201  LearningRate 0.0621  ProxyLR: 3.1058  Epoch: 11  Global Step: 62930   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:16:49,100-Speed 3904.15 samples/sec  Loss 0.9050  LearningRate 0.0621  ProxyLR: 3.1050  Epoch: 11  Global Step: 62940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:51,736-Speed 3884.83 samples/sec  Loss 0.8685  LearningRate 0.0621  ProxyLR: 3.1042  Epoch: 11  Global Step: 62950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:54,368-Speed 3891.54 samples/sec  Loss 0.9269  LearningRate 0.0621  ProxyLR: 3.1035  Epoch: 11  Global Step: 62960   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:57,001-Speed 3891.12 samples/sec  Loss 0.9025  LearningRate 0.0621  ProxyLR: 3.1027  Epoch: 11  Global Step: 62970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:16:59,635-Speed 3887.88 samples/sec  Loss 0.9248  LearningRate 0.0620  ProxyLR: 3.1019  Epoch: 11  Global Step: 62980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:02,271-Speed 3886.45 samples/sec  Loss 0.9117  LearningRate 0.0620  ProxyLR: 3.1011  Epoch: 11  Global Step: 62990   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:04,905-Speed 3888.77 samples/sec  Loss 0.8939  LearningRate 0.0620  ProxyLR: 3.1003  Epoch: 11  Global Step: 63000   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:07,537-Speed 3891.29 samples/sec  Loss 0.9396  LearningRate 0.0620  ProxyLR: 3.0995  Epoch: 11  Global Step: 63010   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:10,171-Speed 3888.61 samples/sec  Loss 0.9011  LearningRate 0.0620  ProxyLR: 3.0988  Epoch: 11  Global Step: 63020   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:12,803-Speed 3890.97 samples/sec  Loss 0.9178  LearningRate 0.0620  ProxyLR: 3.0980  Epoch: 11  Global Step: 63030   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:15,421-Speed 3912.40 samples/sec  Loss 0.9377  LearningRate 0.0619  ProxyLR: 3.0972  Epoch: 11  Global Step: 63040   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:18,053-Speed 3891.87 samples/sec  Loss 0.8980  LearningRate 0.0619  ProxyLR: 3.0964  Epoch: 11  Global Step: 63050   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:20,686-Speed 3890.21 samples/sec  Loss 0.9639  LearningRate 0.0619  ProxyLR: 3.0956  Epoch: 11  Global Step: 63060   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:23,318-Speed 3891.85 samples/sec  Loss 0.9472  LearningRate 0.0619  ProxyLR: 3.0948  Epoch: 11  Global Step: 63070   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:25,949-Speed 3892.66 samples/sec  Loss 0.9484  LearningRate 0.0619  ProxyLR: 3.0941  Epoch: 11  Global Step: 63080   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:28,584-Speed 3887.13 samples/sec  Loss 0.9113  LearningRate 0.0619  ProxyLR: 3.0933  Epoch: 11  Global Step: 63090   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:31,216-Speed 3890.57 samples/sec  Loss 0.9549  LearningRate 0.0619  ProxyLR: 3.0925  Epoch: 11  Global Step: 63100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:33,848-Speed 3892.63 samples/sec  Loss 0.9250  LearningRate 0.0618  ProxyLR: 3.0917  Epoch: 11  Global Step: 63110   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:36,479-Speed 3892.06 samples/sec  Loss 0.9663  LearningRate 0.0618  ProxyLR: 3.0909  Epoch: 11  Global Step: 63120   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:39,113-Speed 3889.43 samples/sec  Loss 0.9126  LearningRate 0.0618  ProxyLR: 3.0902  Epoch: 11  Global Step: 63130   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:41,747-Speed 3888.69 samples/sec  Loss 0.9301  LearningRate 0.0618  ProxyLR: 3.0894  Epoch: 11  Global Step: 63140   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:17:44,366-Speed 3909.51 samples/sec  Loss 0.9468  LearningRate 0.0618  ProxyLR: 3.0886  Epoch: 11  Global Step: 63150   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:46,999-Speed 3890.55 samples/sec  Loss 0.8907  LearningRate 0.0618  ProxyLR: 3.0878  Epoch: 11  Global Step: 63160   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:49,634-Speed 3887.53 samples/sec  Loss 0.9685  LearningRate 0.0617  ProxyLR: 3.0870  Epoch: 11  Global Step: 63170   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:52,267-Speed 3890.45 samples/sec  Loss 0.9538  LearningRate 0.0617  ProxyLR: 3.0862  Epoch: 11  Global Step: 63180   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:54,898-Speed 3891.90 samples/sec  Loss 0.8962  LearningRate 0.0617  ProxyLR: 3.0855  Epoch: 11  Global Step: 63190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:17:57,533-Speed 3887.70 samples/sec  Loss 0.9174  LearningRate 0.0617  ProxyLR: 3.0847  Epoch: 11  Global Step: 63200   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:00,165-Speed 3891.33 samples/sec  Loss 0.9475  LearningRate 0.0617  ProxyLR: 3.0839  Epoch: 11  Global Step: 63210   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:02,799-Speed 3887.89 samples/sec  Loss 0.9185  LearningRate 0.0617  ProxyLR: 3.0831  Epoch: 11  Global Step: 63220   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:05,432-Speed 3891.19 samples/sec  Loss 0.9230  LearningRate 0.0616  ProxyLR: 3.0823  Epoch: 11  Global Step: 63230   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:08,065-Speed 3889.65 samples/sec  Loss 0.9042  LearningRate 0.0616  ProxyLR: 3.0816  Epoch: 11  Global Step: 63240   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:10,696-Speed 3892.69 samples/sec  Loss 0.9402  LearningRate 0.0616  ProxyLR: 3.0808  Epoch: 11  Global Step: 63250   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:18:13,313-Speed 3913.42 samples/sec  Loss 0.9282  LearningRate 0.0616  ProxyLR: 3.0800  Epoch: 11  Global Step: 63260   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:15,943-Speed 3895.24 samples/sec  Loss 0.9022  LearningRate 0.0616  ProxyLR: 3.0792  Epoch: 11  Global Step: 63270   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:18,575-Speed 3891.99 samples/sec  Loss 1.0009  LearningRate 0.0616  ProxyLR: 3.0784  Epoch: 11  Global Step: 63280   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:21,206-Speed 3892.42 samples/sec  Loss 0.9599  LearningRate 0.0616  ProxyLR: 3.0777  Epoch: 11  Global Step: 63290   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:23,839-Speed 3889.78 samples/sec  Loss 0.9680  LearningRate 0.0615  ProxyLR: 3.0769  Epoch: 11  Global Step: 63300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:26,470-Speed 3893.32 samples/sec  Loss 0.9680  LearningRate 0.0615  ProxyLR: 3.0761  Epoch: 11  Global Step: 63310   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:29,101-Speed 3893.69 samples/sec  Loss 0.9162  LearningRate 0.0615  ProxyLR: 3.0753  Epoch: 11  Global Step: 63320   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:31,731-Speed 3893.13 samples/sec  Loss 0.9852  LearningRate 0.0615  ProxyLR: 3.0745  Epoch: 11  Global Step: 63330   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:34,364-Speed 3890.91 samples/sec  Loss 0.9626  LearningRate 0.0615  ProxyLR: 3.0738  Epoch: 11  Global Step: 63340   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:36,997-Speed 3889.91 samples/sec  Loss 0.9385  LearningRate 0.0615  ProxyLR: 3.0730  Epoch: 11  Global Step: 63350   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:39,629-Speed 3890.71 samples/sec  Loss 0.9193  LearningRate 0.0614  ProxyLR: 3.0722  Epoch: 11  Global Step: 63360   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:18:42,248-Speed 3912.04 samples/sec  Loss 0.9293  LearningRate 0.0614  ProxyLR: 3.0714  Epoch: 11  Global Step: 63370   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:44,883-Speed 3887.44 samples/sec  Loss 0.8565  LearningRate 0.0614  ProxyLR: 3.0706  Epoch: 11  Global Step: 63380   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:47,516-Speed 3889.20 samples/sec  Loss 0.9147  LearningRate 0.0614  ProxyLR: 3.0699  Epoch: 11  Global Step: 63390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:50,151-Speed 3887.21 samples/sec  Loss 0.9455  LearningRate 0.0614  ProxyLR: 3.0691  Epoch: 11  Global Step: 63400   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:52,787-Speed 3886.28 samples/sec  Loss 0.9715  LearningRate 0.0614  ProxyLR: 3.0683  Epoch: 11  Global Step: 63410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:55,423-Speed 3885.30 samples/sec  Loss 0.9335  LearningRate 0.0614  ProxyLR: 3.0675  Epoch: 11  Global Step: 63420   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:18:58,058-Speed 3886.74 samples/sec  Loss 0.9455  LearningRate 0.0613  ProxyLR: 3.0667  Epoch: 11  Global Step: 63430   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:00,693-Speed 3887.02 samples/sec  Loss 0.9419  LearningRate 0.0613  ProxyLR: 3.0660  Epoch: 11  Global Step: 63440   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:03,327-Speed 3888.74 samples/sec  Loss 0.9483  LearningRate 0.0613  ProxyLR: 3.0652  Epoch: 11  Global Step: 63450   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:05,963-Speed 3885.77 samples/sec  Loss 0.9627  LearningRate 0.0613  ProxyLR: 3.0644  Epoch: 11  Global Step: 63460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:08,596-Speed 3890.05 samples/sec  Loss 0.9288  LearningRate 0.0613  ProxyLR: 3.0636  Epoch: 11  Global Step: 63470   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:19:11,231-Speed 3887.12 samples/sec  Loss 0.9266  LearningRate 0.0613  ProxyLR: 3.0628  Epoch: 11  Global Step: 63480   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:19:13,867-Speed 3885.51 samples/sec  Loss 0.8986  LearningRate 0.0612  ProxyLR: 3.0621  Epoch: 11  Global Step: 63490   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:19:16,488-Speed 3907.74 samples/sec  Loss 0.9693  LearningRate 0.0612  ProxyLR: 3.0613  Epoch: 11  Global Step: 63500   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:19,122-Speed 3889.91 samples/sec  Loss 0.9837  LearningRate 0.0612  ProxyLR: 3.0605  Epoch: 11  Global Step: 63510   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:21,757-Speed 3887.13 samples/sec  Loss 1.0030  LearningRate 0.0612  ProxyLR: 3.0597  Epoch: 11  Global Step: 63520   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:24,391-Speed 3888.11 samples/sec  Loss 0.9844  LearningRate 0.0612  ProxyLR: 3.0589  Epoch: 11  Global Step: 63530   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:27,026-Speed 3886.79 samples/sec  Loss 0.9990  LearningRate 0.0612  ProxyLR: 3.0582  Epoch: 11  Global Step: 63540   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:29,661-Speed 3887.51 samples/sec  Loss 0.9438  LearningRate 0.0611  ProxyLR: 3.0574  Epoch: 11  Global Step: 63550   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:32,298-Speed 3883.95 samples/sec  Loss 0.9782  LearningRate 0.0611  ProxyLR: 3.0566  Epoch: 11  Global Step: 63560   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:34,935-Speed 3884.01 samples/sec  Loss 0.9906  LearningRate 0.0611  ProxyLR: 3.0558  Epoch: 11  Global Step: 63570   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:37,570-Speed 3887.37 samples/sec  Loss 0.9450  LearningRate 0.0611  ProxyLR: 3.0551  Epoch: 11  Global Step: 63580   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:40,204-Speed 3888.46 samples/sec  Loss 0.9475  LearningRate 0.0611  ProxyLR: 3.0543  Epoch: 11  Global Step: 63590   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:42,825-Speed 3907.47 samples/sec  Loss 0.9946  LearningRate 0.0611  ProxyLR: 3.0535  Epoch: 11  Global Step: 63600   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:45,464-Speed 3882.32 samples/sec  Loss 0.9575  LearningRate 0.0611  ProxyLR: 3.0527  Epoch: 11  Global Step: 63610   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:48,102-Speed 3882.63 samples/sec  Loss 0.9784  LearningRate 0.0610  ProxyLR: 3.0520  Epoch: 11  Global Step: 63620   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:50,739-Speed 3884.16 samples/sec  Loss 0.9470  LearningRate 0.0610  ProxyLR: 3.0512  Epoch: 11  Global Step: 63630   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:53,376-Speed 3884.24 samples/sec  Loss 0.9592  LearningRate 0.0610  ProxyLR: 3.0504  Epoch: 11  Global Step: 63640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:56,012-Speed 3884.85 samples/sec  Loss 0.9331  LearningRate 0.0610  ProxyLR: 3.0496  Epoch: 11  Global Step: 63650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:19:58,648-Speed 3886.17 samples/sec  Loss 1.0072  LearningRate 0.0610  ProxyLR: 3.0488  Epoch: 11  Global Step: 63660   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:01,286-Speed 3882.56 samples/sec  Loss 0.9910  LearningRate 0.0610  ProxyLR: 3.0481  Epoch: 11  Global Step: 63670   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:03,923-Speed 3884.01 samples/sec  Loss 0.9872  LearningRate 0.0609  ProxyLR: 3.0473  Epoch: 11  Global Step: 63680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:06,558-Speed 3888.25 samples/sec  Loss 0.9981  LearningRate 0.0609  ProxyLR: 3.0465  Epoch: 11  Global Step: 63690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:09,196-Speed 3882.10 samples/sec  Loss 0.9807  LearningRate 0.0609  ProxyLR: 3.0457  Epoch: 11  Global Step: 63700   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:20:11,834-Speed 3883.27 samples/sec  Loss 1.0232  LearningRate 0.0609  ProxyLR: 3.0450  Epoch: 11  Global Step: 63710   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:20:14,456-Speed 3905.28 samples/sec  Loss 0.9818  LearningRate 0.0609  ProxyLR: 3.0442  Epoch: 11  Global Step: 63720   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:17,094-Speed 3883.51 samples/sec  Loss 0.9812  LearningRate 0.0609  ProxyLR: 3.0434  Epoch: 11  Global Step: 63730   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:19,726-Speed 3891.64 samples/sec  Loss 0.9776  LearningRate 0.0609  ProxyLR: 3.0426  Epoch: 11  Global Step: 63740   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:22,358-Speed 3891.15 samples/sec  Loss 0.9791  LearningRate 0.0608  ProxyLR: 3.0419  Epoch: 11  Global Step: 63750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:24,989-Speed 3893.68 samples/sec  Loss 0.9999  LearningRate 0.0608  ProxyLR: 3.0411  Epoch: 11  Global Step: 63760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:27,624-Speed 3887.21 samples/sec  Loss 1.0042  LearningRate 0.0608  ProxyLR: 3.0403  Epoch: 11  Global Step: 63770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:30,257-Speed 3889.11 samples/sec  Loss 1.0039  LearningRate 0.0608  ProxyLR: 3.0395  Epoch: 11  Global Step: 63780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:32,890-Speed 3890.85 samples/sec  Loss 1.0062  LearningRate 0.0608  ProxyLR: 3.0388  Epoch: 11  Global Step: 63790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:35,523-Speed 3889.52 samples/sec  Loss 0.9896  LearningRate 0.0608  ProxyLR: 3.0380  Epoch: 11  Global Step: 63800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:38,156-Speed 3890.51 samples/sec  Loss 0.9972  LearningRate 0.0607  ProxyLR: 3.0372  Epoch: 11  Global Step: 63810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:40,788-Speed 3891.63 samples/sec  Loss 0.9023  LearningRate 0.0607  ProxyLR: 3.0364  Epoch: 11  Global Step: 63820   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:20:43,406-Speed 3912.37 samples/sec  Loss 1.0222  LearningRate 0.0607  ProxyLR: 3.0356  Epoch: 11  Global Step: 63830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:46,036-Speed 3894.24 samples/sec  Loss 0.9321  LearningRate 0.0607  ProxyLR: 3.0349  Epoch: 11  Global Step: 63840   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:48,669-Speed 3890.04 samples/sec  Loss 0.9641  LearningRate 0.0607  ProxyLR: 3.0341  Epoch: 11  Global Step: 63850   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:51,303-Speed 3889.10 samples/sec  Loss 0.9765  LearningRate 0.0607  ProxyLR: 3.0333  Epoch: 11  Global Step: 63860   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:53,937-Speed 3887.81 samples/sec  Loss 0.9867  LearningRate 0.0607  ProxyLR: 3.0325  Epoch: 11  Global Step: 63870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:56,573-Speed 3885.74 samples/sec  Loss 1.0145  LearningRate 0.0606  ProxyLR: 3.0318  Epoch: 11  Global Step: 63880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:20:59,208-Speed 3887.49 samples/sec  Loss 1.0011  LearningRate 0.0606  ProxyLR: 3.0310  Epoch: 11  Global Step: 63890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:01,838-Speed 3893.60 samples/sec  Loss 0.9621  LearningRate 0.0606  ProxyLR: 3.0302  Epoch: 11  Global Step: 63900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:04,470-Speed 3892.18 samples/sec  Loss 1.0092  LearningRate 0.0606  ProxyLR: 3.0295  Epoch: 11  Global Step: 63910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:07,104-Speed 3888.94 samples/sec  Loss 0.9435  LearningRate 0.0606  ProxyLR: 3.0287  Epoch: 11  Global Step: 63920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:09,724-Speed 3909.27 samples/sec  Loss 1.0044  LearningRate 0.0606  ProxyLR: 3.0279  Epoch: 11  Global Step: 63930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:12,358-Speed 3888.89 samples/sec  Loss 1.0338  LearningRate 0.0605  ProxyLR: 3.0271  Epoch: 11  Global Step: 63940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:14,992-Speed 3887.60 samples/sec  Loss 0.9679  LearningRate 0.0605  ProxyLR: 3.0264  Epoch: 11  Global Step: 63950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:17,627-Speed 3887.93 samples/sec  Loss 1.0574  LearningRate 0.0605  ProxyLR: 3.0256  Epoch: 11  Global Step: 63960   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:20,260-Speed 3889.82 samples/sec  Loss 1.0214  LearningRate 0.0605  ProxyLR: 3.0248  Epoch: 11  Global Step: 63970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:22,891-Speed 3893.12 samples/sec  Loss 1.0595  LearningRate 0.0605  ProxyLR: 3.0240  Epoch: 11  Global Step: 63980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:25,522-Speed 3893.13 samples/sec  Loss 0.9914  LearningRate 0.0605  ProxyLR: 3.0233  Epoch: 11  Global Step: 63990   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:28,153-Speed 3892.17 samples/sec  Loss 0.9981  LearningRate 0.0604  ProxyLR: 3.0225  Epoch: 11  Global Step: 64000   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:30,787-Speed 3889.77 samples/sec  Loss 1.0485  LearningRate 0.0604  ProxyLR: 3.0217  Epoch: 11  Global Step: 64010   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:33,417-Speed 3894.72 samples/sec  Loss 1.0533  LearningRate 0.0604  ProxyLR: 3.0209  Epoch: 11  Global Step: 64020   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:21:36,032-Speed 3915.76 samples/sec  Loss 1.0051  LearningRate 0.0604  ProxyLR: 3.0202  Epoch: 11  Global Step: 64030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:38,662-Speed 3895.21 samples/sec  Loss 0.9891  LearningRate 0.0604  ProxyLR: 3.0194  Epoch: 11  Global Step: 64040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:41,293-Speed 3893.12 samples/sec  Loss 1.0105  LearningRate 0.0604  ProxyLR: 3.0186  Epoch: 11  Global Step: 64050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:43,924-Speed 3893.10 samples/sec  Loss 1.0358  LearningRate 0.0604  ProxyLR: 3.0178  Epoch: 11  Global Step: 64060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:46,552-Speed 3897.39 samples/sec  Loss 1.0528  LearningRate 0.0603  ProxyLR: 3.0171  Epoch: 11  Global Step: 64070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:49,182-Speed 3894.39 samples/sec  Loss 0.9779  LearningRate 0.0603  ProxyLR: 3.0163  Epoch: 11  Global Step: 64080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:51,810-Speed 3896.85 samples/sec  Loss 1.0564  LearningRate 0.0603  ProxyLR: 3.0155  Epoch: 11  Global Step: 64090   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:54,438-Speed 3897.62 samples/sec  Loss 0.9983  LearningRate 0.0603  ProxyLR: 3.0148  Epoch: 11  Global Step: 64100   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:57,066-Speed 3897.61 samples/sec  Loss 1.0182  LearningRate 0.0603  ProxyLR: 3.0140  Epoch: 11  Global Step: 64110   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:21:59,696-Speed 3894.67 samples/sec  Loss 1.0176  LearningRate 0.0603  ProxyLR: 3.0132  Epoch: 11  Global Step: 64120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:02,325-Speed 3895.97 samples/sec  Loss 1.0512  LearningRate 0.0602  ProxyLR: 3.0124  Epoch: 11  Global Step: 64130   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:04,955-Speed 3894.78 samples/sec  Loss 1.0120  LearningRate 0.0602  ProxyLR: 3.0117  Epoch: 11  Global Step: 64140   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:07,584-Speed 3895.92 samples/sec  Loss 1.0522  LearningRate 0.0602  ProxyLR: 3.0109  Epoch: 11  Global Step: 64150   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:10,212-Speed 3896.98 samples/sec  Loss 1.0215  LearningRate 0.0602  ProxyLR: 3.0101  Epoch: 11  Global Step: 64160   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:12,840-Speed 3898.27 samples/sec  Loss 1.0209  LearningRate 0.0602  ProxyLR: 3.0094  Epoch: 11  Global Step: 64170   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:15,466-Speed 3899.65 samples/sec  Loss 1.1149  LearningRate 0.0602  ProxyLR: 3.0086  Epoch: 11  Global Step: 64180   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:18,092-Speed 3899.97 samples/sec  Loss 0.9478  LearningRate 0.0602  ProxyLR: 3.0078  Epoch: 11  Global Step: 64190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:20,707-Speed 3918.25 samples/sec  Loss 1.0098  LearningRate 0.0601  ProxyLR: 3.0070  Epoch: 11  Global Step: 64200   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:23,332-Speed 3901.61 samples/sec  Loss 1.0526  LearningRate 0.0601  ProxyLR: 3.0063  Epoch: 11  Global Step: 64210   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:25,958-Speed 3900.39 samples/sec  Loss 1.0179  LearningRate 0.0601  ProxyLR: 3.0055  Epoch: 11  Global Step: 64220   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:28,585-Speed 3899.40 samples/sec  Loss 1.0390  LearningRate 0.0601  ProxyLR: 3.0047  Epoch: 11  Global Step: 64230   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:31,211-Speed 3899.98 samples/sec  Loss 1.0553  LearningRate 0.0601  ProxyLR: 3.0039  Epoch: 11  Global Step: 64240   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:33,837-Speed 3899.79 samples/sec  Loss 1.0713  LearningRate 0.0601  ProxyLR: 3.0032  Epoch: 11  Global Step: 64250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:36,462-Speed 3902.44 samples/sec  Loss 1.0726  LearningRate 0.0600  ProxyLR: 3.0024  Epoch: 11  Global Step: 64260   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:39,088-Speed 3900.96 samples/sec  Loss 1.0374  LearningRate 0.0600  ProxyLR: 3.0016  Epoch: 11  Global Step: 64270   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:41,714-Speed 3899.74 samples/sec  Loss 1.0421  LearningRate 0.0600  ProxyLR: 3.0009  Epoch: 11  Global Step: 64280   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:44,341-Speed 3898.71 samples/sec  Loss 1.0677  LearningRate 0.0600  ProxyLR: 3.0001  Epoch: 11  Global Step: 64290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:22:46,967-Speed 3900.41 samples/sec  Loss 1.1122  LearningRate 0.0600  ProxyLR: 2.9993  Epoch: 11  Global Step: 64300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:49,595-Speed 3897.39 samples/sec  Loss 0.9870  LearningRate 0.0600  ProxyLR: 2.9986  Epoch: 11  Global Step: 64310   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:52,221-Speed 3901.43 samples/sec  Loss 1.0837  LearningRate 0.0600  ProxyLR: 2.9978  Epoch: 11  Global Step: 64320   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:54,847-Speed 3900.51 samples/sec  Loss 1.0769  LearningRate 0.0599  ProxyLR: 2.9970  Epoch: 11  Global Step: 64330   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:22:57,473-Speed 3900.45 samples/sec  Loss 1.0422  LearningRate 0.0599  ProxyLR: 2.9962  Epoch: 11  Global Step: 64340   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:00,100-Speed 3898.73 samples/sec  Loss 0.9937  LearningRate 0.0599  ProxyLR: 2.9955  Epoch: 11  Global Step: 64350   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:02,726-Speed 3900.45 samples/sec  Loss 1.0398  LearningRate 0.0599  ProxyLR: 2.9947  Epoch: 11  Global Step: 64360   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:05,353-Speed 3898.63 samples/sec  Loss 1.0429  LearningRate 0.0599  ProxyLR: 2.9939  Epoch: 11  Global Step: 64370   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:07,978-Speed 3901.30 samples/sec  Loss 1.0381  LearningRate 0.0599  ProxyLR: 2.9932  Epoch: 11  Global Step: 64380   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:10,604-Speed 3900.80 samples/sec  Loss 1.0487  LearningRate 0.0598  ProxyLR: 2.9924  Epoch: 11  Global Step: 64390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:13,231-Speed 3899.87 samples/sec  Loss 1.0558  LearningRate 0.0598  ProxyLR: 2.9916  Epoch: 11  Global Step: 64400   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:23:15,843-Speed 3921.51 samples/sec  Loss 1.0758  LearningRate 0.0598  ProxyLR: 2.9909  Epoch: 11  Global Step: 64410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:18,470-Speed 3899.13 samples/sec  Loss 1.0409  LearningRate 0.0598  ProxyLR: 2.9901  Epoch: 11  Global Step: 64420   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:21,095-Speed 3901.85 samples/sec  Loss 0.9986  LearningRate 0.0598  ProxyLR: 2.9893  Epoch: 11  Global Step: 64430   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:23,721-Speed 3900.54 samples/sec  Loss 1.0282  LearningRate 0.0598  ProxyLR: 2.9885  Epoch: 11  Global Step: 64440   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:26,346-Speed 3901.95 samples/sec  Loss 1.0821  LearningRate 0.0598  ProxyLR: 2.9878  Epoch: 11  Global Step: 64450   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:28,972-Speed 3900.25 samples/sec  Loss 1.0757  LearningRate 0.0597  ProxyLR: 2.9870  Epoch: 11  Global Step: 64460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:31,597-Speed 3901.32 samples/sec  Loss 1.0369  LearningRate 0.0597  ProxyLR: 2.9862  Epoch: 11  Global Step: 64470   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:34,223-Speed 3900.36 samples/sec  Loss 1.0609  LearningRate 0.0597  ProxyLR: 2.9855  Epoch: 11  Global Step: 64480   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:36,849-Speed 3900.28 samples/sec  Loss 1.0773  LearningRate 0.0597  ProxyLR: 2.9847  Epoch: 11  Global Step: 64490   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:39,726-Speed 3560.40 samples/sec  Loss 1.0384  LearningRate 0.0597  ProxyLR: 2.9839  Epoch: 11  Global Step: 64500   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:42,338-Speed 3921.44 samples/sec  Loss 1.0661  LearningRate 0.0597  ProxyLR: 2.9832  Epoch: 11  Global Step: 64510   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:44,964-Speed 3900.85 samples/sec  Loss 1.0730  LearningRate 0.0596  ProxyLR: 2.9824  Epoch: 11  Global Step: 64520   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:47,589-Speed 3901.72 samples/sec  Loss 1.0548  LearningRate 0.0596  ProxyLR: 2.9816  Epoch: 11  Global Step: 64530   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:23:50,202-Speed 3919.16 samples/sec  Loss 1.0264  LearningRate 0.0596  ProxyLR: 2.9809  Epoch: 11  Global Step: 64540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:23:52,828-Speed 3900.94 samples/sec  Loss 1.0964  LearningRate 0.0596  ProxyLR: 2.9801  Epoch: 11  Global Step: 64550   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:23:55,455-Speed 3899.51 samples/sec  Loss 1.0847  LearningRate 0.0596  ProxyLR: 2.9793  Epoch: 11  Global Step: 64560   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:23:58,080-Speed 3901.62 samples/sec  Loss 1.0986  LearningRate 0.0596  ProxyLR: 2.9786  Epoch: 11  Global Step: 64570   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:00,703-Speed 3904.60 samples/sec  Loss 1.0949  LearningRate 0.0596  ProxyLR: 2.9778  Epoch: 11  Global Step: 64580   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:03,329-Speed 3900.75 samples/sec  Loss 1.0609  LearningRate 0.0595  ProxyLR: 2.9770  Epoch: 11  Global Step: 64590   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:05,955-Speed 3901.24 samples/sec  Loss 1.0691  LearningRate 0.0595  ProxyLR: 2.9763  Epoch: 11  Global Step: 64600   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:08,581-Speed 3899.75 samples/sec  Loss 1.0171  LearningRate 0.0595  ProxyLR: 2.9755  Epoch: 11  Global Step: 64610   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:11,206-Speed 3901.98 samples/sec  Loss 1.0123  LearningRate 0.0595  ProxyLR: 2.9747  Epoch: 11  Global Step: 64620   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:13,831-Speed 3902.48 samples/sec  Loss 1.0830  LearningRate 0.0595  ProxyLR: 2.9740  Epoch: 11  Global Step: 64630   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:24:16,457-Speed 3899.62 samples/sec  Loss 1.0848  LearningRate 0.0595  ProxyLR: 2.9732  Epoch: 11  Global Step: 64640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:19,083-Speed 3900.71 samples/sec  Loss 1.0775  LearningRate 0.0594  ProxyLR: 2.9724  Epoch: 11  Global Step: 64650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:21,709-Speed 3900.52 samples/sec  Loss 1.0169  LearningRate 0.0594  ProxyLR: 2.9716  Epoch: 11  Global Step: 64660   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:24,334-Speed 3901.49 samples/sec  Loss 1.0620  LearningRate 0.0594  ProxyLR: 2.9709  Epoch: 11  Global Step: 64670   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:26,962-Speed 3897.95 samples/sec  Loss 1.0937  LearningRate 0.0594  ProxyLR: 2.9701  Epoch: 11  Global Step: 64680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:29,588-Speed 3899.70 samples/sec  Loss 1.0648  LearningRate 0.0594  ProxyLR: 2.9693  Epoch: 11  Global Step: 64690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:32,214-Speed 3900.60 samples/sec  Loss 1.0752  LearningRate 0.0594  ProxyLR: 2.9686  Epoch: 11  Global Step: 64700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:34,841-Speed 3900.00 samples/sec  Loss 1.0976  LearningRate 0.0594  ProxyLR: 2.9678  Epoch: 11  Global Step: 64710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:37,473-Speed 3891.51 samples/sec  Loss 1.0295  LearningRate 0.0593  ProxyLR: 2.9670  Epoch: 11  Global Step: 64720   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:40,099-Speed 3899.95 samples/sec  Loss 1.0743  LearningRate 0.0593  ProxyLR: 2.9663  Epoch: 11  Global Step: 64730   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:42,724-Speed 3901.66 samples/sec  Loss 1.1169  LearningRate 0.0593  ProxyLR: 2.9655  Epoch: 11  Global Step: 64740   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:24:45,339-Speed 3916.90 samples/sec  Loss 1.0115  LearningRate 0.0593  ProxyLR: 2.9648  Epoch: 11  Global Step: 64750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:47,969-Speed 3894.89 samples/sec  Loss 1.0548  LearningRate 0.0593  ProxyLR: 2.9640  Epoch: 11  Global Step: 64760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:50,596-Speed 3898.22 samples/sec  Loss 1.0430  LearningRate 0.0593  ProxyLR: 2.9632  Epoch: 11  Global Step: 64770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:53,222-Speed 3901.32 samples/sec  Loss 1.0177  LearningRate 0.0592  ProxyLR: 2.9625  Epoch: 11  Global Step: 64780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:55,851-Speed 3895.88 samples/sec  Loss 1.0678  LearningRate 0.0592  ProxyLR: 2.9617  Epoch: 11  Global Step: 64790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:24:58,476-Speed 3901.44 samples/sec  Loss 1.0595  LearningRate 0.0592  ProxyLR: 2.9609  Epoch: 11  Global Step: 64800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:01,101-Speed 3901.75 samples/sec  Loss 1.0626  LearningRate 0.0592  ProxyLR: 2.9602  Epoch: 11  Global Step: 64810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:03,727-Speed 3901.41 samples/sec  Loss 1.1001  LearningRate 0.0592  ProxyLR: 2.9594  Epoch: 11  Global Step: 64820   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:06,352-Speed 3900.69 samples/sec  Loss 1.1021  LearningRate 0.0592  ProxyLR: 2.9586  Epoch: 11  Global Step: 64830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:08,980-Speed 3898.31 samples/sec  Loss 1.0823  LearningRate 0.0592  ProxyLR: 2.9579  Epoch: 11  Global Step: 64840   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:11,605-Speed 3901.67 samples/sec  Loss 1.0792  LearningRate 0.0591  ProxyLR: 2.9571  Epoch: 11  Global Step: 64850   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:25:14,232-Speed 3898.14 samples/sec  Loss 1.1141  LearningRate 0.0591  ProxyLR: 2.9563  Epoch: 11  Global Step: 64860   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:25:16,847-Speed 3918.00 samples/sec  Loss 1.0143  LearningRate 0.0591  ProxyLR: 2.9556  Epoch: 11  Global Step: 64870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:19,473-Speed 3899.89 samples/sec  Loss 1.0448  LearningRate 0.0591  ProxyLR: 2.9548  Epoch: 11  Global Step: 64880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:22,097-Speed 3903.09 samples/sec  Loss 1.0799  LearningRate 0.0591  ProxyLR: 2.9540  Epoch: 11  Global Step: 64890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:24,724-Speed 3899.64 samples/sec  Loss 1.0851  LearningRate 0.0591  ProxyLR: 2.9533  Epoch: 11  Global Step: 64900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:27,350-Speed 3899.50 samples/sec  Loss 1.0759  LearningRate 0.0591  ProxyLR: 2.9525  Epoch: 11  Global Step: 64910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:29,975-Speed 3901.74 samples/sec  Loss 1.0380  LearningRate 0.0590  ProxyLR: 2.9517  Epoch: 11  Global Step: 64920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:32,602-Speed 3899.58 samples/sec  Loss 1.1174  LearningRate 0.0590  ProxyLR: 2.9510  Epoch: 11  Global Step: 64930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:35,228-Speed 3900.36 samples/sec  Loss 1.0819  LearningRate 0.0590  ProxyLR: 2.9502  Epoch: 11  Global Step: 64940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:37,855-Speed 3898.92 samples/sec  Loss 1.0542  LearningRate 0.0590  ProxyLR: 2.9494  Epoch: 11  Global Step: 64950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:40,482-Speed 3899.11 samples/sec  Loss 1.0492  LearningRate 0.0590  ProxyLR: 2.9487  Epoch: 11  Global Step: 64960   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:43,109-Speed 3899.12 samples/sec  Loss 1.0575  LearningRate 0.0590  ProxyLR: 2.9479  Epoch: 11  Global Step: 64970   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:25:45,722-Speed 3919.46 samples/sec  Loss 1.0639  LearningRate 0.0589  ProxyLR: 2.9472  Epoch: 11  Global Step: 64980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:25:48,334-Speed 3921.94 samples/sec  Loss 1.1012  LearningRate 0.0589  ProxyLR: 2.9464  Epoch: 11  Global Step: 64990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:25:50,960-Speed 3900.84 samples/sec  Loss 1.0936  LearningRate 0.0589  ProxyLR: 2.9456  Epoch: 11  Global Step: 65000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:25:53,586-Speed 3899.20 samples/sec  Loss 1.0798  LearningRate 0.0589  ProxyLR: 2.9449  Epoch: 11  Global Step: 65010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:25:56,212-Speed 3901.66 samples/sec  Loss 1.0937  LearningRate 0.0589  ProxyLR: 2.9441  Epoch: 11  Global Step: 65020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:25:58,838-Speed 3900.47 samples/sec  Loss 1.0512  LearningRate 0.0589  ProxyLR: 2.9433  Epoch: 11  Global Step: 65030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:26:01,465-Speed 3898.99 samples/sec  Loss 1.1180  LearningRate 0.0589  ProxyLR: 2.9426  Epoch: 11  Global Step: 65040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:26:04,093-Speed 3897.17 samples/sec  Loss 1.0735  LearningRate 0.0588  ProxyLR: 2.9418  Epoch: 11  Global Step: 65050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:26:06,724-Speed 3892.70 samples/sec  Loss 1.0461  LearningRate 0.0588  ProxyLR: 2.9410  Epoch: 11  Global Step: 65060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:26:09,354-Speed 3894.65 samples/sec  Loss 1.1170  LearningRate 0.0588  ProxyLR: 2.9403  Epoch: 11  Global Step: 65070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:26:11,986-Speed 3890.55 samples/sec  Loss 1.0883  LearningRate 0.0588  ProxyLR: 2.9395  Epoch: 11  Global Step: 65080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:26:14,616-Speed 3894.89 samples/sec  Loss 1.0347  LearningRate 0.0588  ProxyLR: 2.9388  Epoch: 11  Global Step: 65090   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:17,246-Speed 3894.51 samples/sec  Loss 1.1125  LearningRate 0.0588  ProxyLR: 2.9380  Epoch: 11  Global Step: 65100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:19,876-Speed 3894.01 samples/sec  Loss 1.0941  LearningRate 0.0587  ProxyLR: 2.9372  Epoch: 11  Global Step: 65110   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:22,505-Speed 3896.93 samples/sec  Loss 1.0803  LearningRate 0.0587  ProxyLR: 2.9365  Epoch: 11  Global Step: 65120   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:25,378-Speed 3564.56 samples/sec  Loss 1.1217  LearningRate 0.0587  ProxyLR: 2.9357  Epoch: 11  Global Step: 65130   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:28,008-Speed 3895.11 samples/sec  Loss 1.0641  LearningRate 0.0587  ProxyLR: 2.9349  Epoch: 11  Global Step: 65140   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:30,638-Speed 3894.45 samples/sec  Loss 1.0753  LearningRate 0.0587  ProxyLR: 2.9342  Epoch: 11  Global Step: 65150   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:33,269-Speed 3892.74 samples/sec  Loss 1.0947  LearningRate 0.0587  ProxyLR: 2.9334  Epoch: 11  Global Step: 65160   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:35,899-Speed 3894.22 samples/sec  Loss 1.0575  LearningRate 0.0587  ProxyLR: 2.9327  Epoch: 11  Global Step: 65170   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:38,530-Speed 3893.37 samples/sec  Loss 1.1125  LearningRate 0.0586  ProxyLR: 2.9319  Epoch: 11  Global Step: 65180   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:41,162-Speed 3892.41 samples/sec  Loss 1.1074  LearningRate 0.0586  ProxyLR: 2.9311  Epoch: 11  Global Step: 65190   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:26:43,793-Speed 3892.26 samples/sec  Loss 1.1265  LearningRate 0.0586  ProxyLR: 2.9304  Epoch: 11  Global Step: 65200   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:26:46,410-Speed 3914.23 samples/sec  Loss 1.0930  LearningRate 0.0586  ProxyLR: 2.9296  Epoch: 11  Global Step: 65210   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:49,041-Speed 3892.63 samples/sec  Loss 1.0981  LearningRate 0.0586  ProxyLR: 2.9289  Epoch: 11  Global Step: 65220   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:51,671-Speed 3894.37 samples/sec  Loss 1.1297  LearningRate 0.0586  ProxyLR: 2.9281  Epoch: 11  Global Step: 65230   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:54,304-Speed 3890.45 samples/sec  Loss 1.1211  LearningRate 0.0585  ProxyLR: 2.9273  Epoch: 11  Global Step: 65240   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:56,934-Speed 3893.88 samples/sec  Loss 1.1051  LearningRate 0.0585  ProxyLR: 2.9266  Epoch: 11  Global Step: 65250   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:26:59,566-Speed 3891.52 samples/sec  Loss 1.1355  LearningRate 0.0585  ProxyLR: 2.9258  Epoch: 11  Global Step: 65260   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:02,197-Speed 3893.99 samples/sec  Loss 1.0763  LearningRate 0.0585  ProxyLR: 2.9250  Epoch: 11  Global Step: 65270   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:04,829-Speed 3891.05 samples/sec  Loss 1.1225  LearningRate 0.0585  ProxyLR: 2.9243  Epoch: 11  Global Step: 65280   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:07,461-Speed 3891.16 samples/sec  Loss 1.0858  LearningRate 0.0585  ProxyLR: 2.9235  Epoch: 11  Global Step: 65290   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:10,091-Speed 3894.24 samples/sec  Loss 1.1393  LearningRate 0.0585  ProxyLR: 2.9228  Epoch: 11  Global Step: 65300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:12,723-Speed 3891.81 samples/sec  Loss 1.0912  LearningRate 0.0584  ProxyLR: 2.9220  Epoch: 11  Global Step: 65310   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:27:15,339-Speed 3915.24 samples/sec  Loss 1.0886  LearningRate 0.0584  ProxyLR: 2.9212  Epoch: 11  Global Step: 65320   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:17,972-Speed 3890.97 samples/sec  Loss 1.0924  LearningRate 0.0584  ProxyLR: 2.9205  Epoch: 11  Global Step: 65330   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:20,602-Speed 3894.32 samples/sec  Loss 1.0357  LearningRate 0.0584  ProxyLR: 2.9197  Epoch: 11  Global Step: 65340   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:23,234-Speed 3890.74 samples/sec  Loss 1.0993  LearningRate 0.0584  ProxyLR: 2.9190  Epoch: 11  Global Step: 65350   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:25,861-Speed 3899.87 samples/sec  Loss 1.0843  LearningRate 0.0584  ProxyLR: 2.9182  Epoch: 11  Global Step: 65360   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:28,491-Speed 3893.89 samples/sec  Loss 1.1640  LearningRate 0.0583  ProxyLR: 2.9174  Epoch: 11  Global Step: 65370   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:31,122-Speed 3893.79 samples/sec  Loss 1.0901  LearningRate 0.0583  ProxyLR: 2.9167  Epoch: 11  Global Step: 65380   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:33,751-Speed 3895.33 samples/sec  Loss 1.1471  LearningRate 0.0583  ProxyLR: 2.9159  Epoch: 11  Global Step: 65390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:36,383-Speed 3892.61 samples/sec  Loss 1.0747  LearningRate 0.0583  ProxyLR: 2.9152  Epoch: 11  Global Step: 65400   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:39,013-Speed 3893.50 samples/sec  Loss 1.1565  LearningRate 0.0583  ProxyLR: 2.9144  Epoch: 11  Global Step: 65410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:41,645-Speed 3892.48 samples/sec  Loss 1.0889  LearningRate 0.0583  ProxyLR: 2.9136  Epoch: 11  Global Step: 65420   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:27:44,276-Speed 3891.70 samples/sec  Loss 1.0782  LearningRate 0.0583  ProxyLR: 2.9129  Epoch: 11  Global Step: 65430   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:27:46,909-Speed 3890.39 samples/sec  Loss 1.1113  LearningRate 0.0582  ProxyLR: 2.9121  Epoch: 11  Global Step: 65440   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:27:49,542-Speed 3889.62 samples/sec  Loss 1.1125  LearningRate 0.0582  ProxyLR: 2.9114  Epoch: 11  Global Step: 65450   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:27:52,161-Speed 3911.86 samples/sec  Loss 1.0980  LearningRate 0.0582  ProxyLR: 2.9106  Epoch: 11  Global Step: 65460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:54,792-Speed 3893.18 samples/sec  Loss 1.1411  LearningRate 0.0582  ProxyLR: 2.9098  Epoch: 11  Global Step: 65470   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:27:57,423-Speed 3892.68 samples/sec  Loss 1.1375  LearningRate 0.0582  ProxyLR: 2.9091  Epoch: 11  Global Step: 65480   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:00,053-Speed 3894.93 samples/sec  Loss 1.1152  LearningRate 0.0582  ProxyLR: 2.9083  Epoch: 11  Global Step: 65490   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:02,684-Speed 3892.03 samples/sec  Loss 1.0927  LearningRate 0.0582  ProxyLR: 2.9076  Epoch: 11  Global Step: 65500   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:05,316-Speed 3891.44 samples/sec  Loss 1.1693  LearningRate 0.0581  ProxyLR: 2.9068  Epoch: 11  Global Step: 65510   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:07,946-Speed 3895.28 samples/sec  Loss 1.0862  LearningRate 0.0581  ProxyLR: 2.9061  Epoch: 11  Global Step: 65520   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:10,576-Speed 3894.19 samples/sec  Loss 1.1001  LearningRate 0.0581  ProxyLR: 2.9053  Epoch: 11  Global Step: 65530   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:13,207-Speed 3892.37 samples/sec  Loss 1.1773  LearningRate 0.0581  ProxyLR: 2.9045  Epoch: 11  Global Step: 65540   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:15,839-Speed 3892.71 samples/sec  Loss 1.1621  LearningRate 0.0581  ProxyLR: 2.9038  Epoch: 11  Global Step: 65550   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:18,456-Speed 3913.67 samples/sec  Loss 1.1555  LearningRate 0.0581  ProxyLR: 2.9030  Epoch: 11  Global Step: 65560   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:21,085-Speed 3895.58 samples/sec  Loss 1.1237  LearningRate 0.0580  ProxyLR: 2.9023  Epoch: 11  Global Step: 65570   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:23,714-Speed 3895.39 samples/sec  Loss 1.1441  LearningRate 0.0580  ProxyLR: 2.9015  Epoch: 11  Global Step: 65580   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:26,344-Speed 3895.84 samples/sec  Loss 1.0924  LearningRate 0.0580  ProxyLR: 2.9007  Epoch: 11  Global Step: 65590   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:28,974-Speed 3894.02 samples/sec  Loss 1.1119  LearningRate 0.0580  ProxyLR: 2.9000  Epoch: 11  Global Step: 65600   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:31,604-Speed 3894.75 samples/sec  Loss 1.1252  LearningRate 0.0580  ProxyLR: 2.8992  Epoch: 11  Global Step: 65610   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:34,234-Speed 3894.17 samples/sec  Loss 1.1193  LearningRate 0.0580  ProxyLR: 2.8985  Epoch: 11  Global Step: 65620   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:36,865-Speed 3892.87 samples/sec  Loss 1.1428  LearningRate 0.0580  ProxyLR: 2.8977  Epoch: 11  Global Step: 65630   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:39,494-Speed 3895.75 samples/sec  Loss 1.1215  LearningRate 0.0579  ProxyLR: 2.8970  Epoch: 11  Global Step: 65640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:42,124-Speed 3895.09 samples/sec  Loss 1.1726  LearningRate 0.0579  ProxyLR: 2.8962  Epoch: 11  Global Step: 65650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:44,755-Speed 3892.87 samples/sec  Loss 1.0932  LearningRate 0.0579  ProxyLR: 2.8954  Epoch: 11  Global Step: 65660   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:28:47,372-Speed 3913.04 samples/sec  Loss 1.1429  LearningRate 0.0579  ProxyLR: 2.8947  Epoch: 11  Global Step: 65670   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:50,237-Speed 3574.75 samples/sec  Loss 1.1117  LearningRate 0.0579  ProxyLR: 2.8939  Epoch: 11  Global Step: 65680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:52,862-Speed 3902.23 samples/sec  Loss 1.1217  LearningRate 0.0579  ProxyLR: 2.8932  Epoch: 11  Global Step: 65690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:55,487-Speed 3902.89 samples/sec  Loss 1.1251  LearningRate 0.0578  ProxyLR: 2.8924  Epoch: 11  Global Step: 65700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:28:58,113-Speed 3900.80 samples/sec  Loss 1.1544  LearningRate 0.0578  ProxyLR: 2.8917  Epoch: 11  Global Step: 65710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:00,739-Speed 3900.64 samples/sec  Loss 1.1581  LearningRate 0.0578  ProxyLR: 2.8909  Epoch: 11  Global Step: 65720   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:03,366-Speed 3899.17 samples/sec  Loss 1.1041  LearningRate 0.0578  ProxyLR: 2.8901  Epoch: 11  Global Step: 65730   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:05,993-Speed 3897.94 samples/sec  Loss 1.1430  LearningRate 0.0578  ProxyLR: 2.8894  Epoch: 11  Global Step: 65740   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:08,626-Speed 3890.30 samples/sec  Loss 1.1094  LearningRate 0.0578  ProxyLR: 2.8886  Epoch: 11  Global Step: 65750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:11,259-Speed 3889.98 samples/sec  Loss 1.0917  LearningRate 0.0578  ProxyLR: 2.8879  Epoch: 11  Global Step: 65760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:13,892-Speed 3890.42 samples/sec  Loss 1.1651  LearningRate 0.0577  ProxyLR: 2.8871  Epoch: 11  Global Step: 65770   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:29:16,525-Speed 3890.07 samples/sec  Loss 1.1689  LearningRate 0.0577  ProxyLR: 2.8864  Epoch: 11  Global Step: 65780   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:29:19,159-Speed 3889.20 samples/sec  Loss 1.1534  LearningRate 0.0577  ProxyLR: 2.8856  Epoch: 11  Global Step: 65790   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:29:21,793-Speed 3887.74 samples/sec  Loss 1.1104  LearningRate 0.0577  ProxyLR: 2.8849  Epoch: 11  Global Step: 65800   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:29:24,426-Speed 3890.72 samples/sec  Loss 1.1029  LearningRate 0.0577  ProxyLR: 2.8841  Epoch: 11  Global Step: 65810   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:29:27,043-Speed 3913.80 samples/sec  Loss 1.1389  LearningRate 0.0577  ProxyLR: 2.8833  Epoch: 11  Global Step: 65820   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:29,677-Speed 3887.88 samples/sec  Loss 1.1348  LearningRate 0.0577  ProxyLR: 2.8826  Epoch: 11  Global Step: 65830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:32,309-Speed 3891.90 samples/sec  Loss 1.1598  LearningRate 0.0576  ProxyLR: 2.8818  Epoch: 11  Global Step: 65840   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:34,940-Speed 3893.34 samples/sec  Loss 1.2010  LearningRate 0.0576  ProxyLR: 2.8811  Epoch: 11  Global Step: 65850   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:37,571-Speed 3892.81 samples/sec  Loss 1.1844  LearningRate 0.0576  ProxyLR: 2.8803  Epoch: 11  Global Step: 65860   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:40,203-Speed 3891.90 samples/sec  Loss 1.1473  LearningRate 0.0576  ProxyLR: 2.8796  Epoch: 11  Global Step: 65870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:42,839-Speed 3885.20 samples/sec  Loss 1.0989  LearningRate 0.0576  ProxyLR: 2.8788  Epoch: 11  Global Step: 65880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:45,473-Speed 3888.59 samples/sec  Loss 1.1798  LearningRate 0.0576  ProxyLR: 2.8781  Epoch: 11  Global Step: 65890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:48,110-Speed 3884.99 samples/sec  Loss 1.1397  LearningRate 0.0575  ProxyLR: 2.8773  Epoch: 11  Global Step: 65900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:50,746-Speed 3885.82 samples/sec  Loss 1.1091  LearningRate 0.0575  ProxyLR: 2.8766  Epoch: 11  Global Step: 65910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:53,370-Speed 3903.21 samples/sec  Loss 1.1378  LearningRate 0.0575  ProxyLR: 2.8758  Epoch: 11  Global Step: 65920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:56,006-Speed 3886.06 samples/sec  Loss 1.0744  LearningRate 0.0575  ProxyLR: 2.8750  Epoch: 11  Global Step: 65930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:29:58,645-Speed 3880.79 samples/sec  Loss 1.1335  LearningRate 0.0575  ProxyLR: 2.8743  Epoch: 11  Global Step: 65940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:01,279-Speed 3889.45 samples/sec  Loss 1.1226  LearningRate 0.0575  ProxyLR: 2.8735  Epoch: 11  Global Step: 65950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:03,910-Speed 3893.03 samples/sec  Loss 1.1167  LearningRate 0.0575  ProxyLR: 2.8728  Epoch: 11  Global Step: 65960   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:06,537-Speed 3898.62 samples/sec  Loss 1.2045  LearningRate 0.0574  ProxyLR: 2.8720  Epoch: 11  Global Step: 65970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:09,165-Speed 3897.02 samples/sec  Loss 1.1325  LearningRate 0.0574  ProxyLR: 2.8713  Epoch: 11  Global Step: 65980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:11,794-Speed 3896.51 samples/sec  Loss 1.1393  LearningRate 0.0574  ProxyLR: 2.8705  Epoch: 11  Global Step: 65990   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:14,424-Speed 3894.10 samples/sec  Loss 1.1583  LearningRate 0.0574  ProxyLR: 2.8698  Epoch: 11  Global Step: 66000   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:17,050-Speed 3900.99 samples/sec  Loss 1.1145  LearningRate 0.0574  ProxyLR: 2.8690  Epoch: 11  Global Step: 66010   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:19,679-Speed 3895.43 samples/sec  Loss 1.1320  LearningRate 0.0574  ProxyLR: 2.8683  Epoch: 11  Global Step: 66020   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:30:22,294-Speed 3916.74 samples/sec  Loss 1.2025  LearningRate 0.0574  ProxyLR: 2.8675  Epoch: 11  Global Step: 66030   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:24,921-Speed 3899.14 samples/sec  Loss 1.1749  LearningRate 0.0573  ProxyLR: 2.8667  Epoch: 11  Global Step: 66040   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:27,549-Speed 3898.21 samples/sec  Loss 1.1518  LearningRate 0.0573  ProxyLR: 2.8660  Epoch: 11  Global Step: 66050   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:30,173-Speed 3902.94 samples/sec  Loss 1.1118  LearningRate 0.0573  ProxyLR: 2.8652  Epoch: 11  Global Step: 66060   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:32,797-Speed 3903.63 samples/sec  Loss 1.1351  LearningRate 0.0573  ProxyLR: 2.8645  Epoch: 11  Global Step: 66070   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:35,422-Speed 3901.01 samples/sec  Loss 1.1390  LearningRate 0.0573  ProxyLR: 2.8637  Epoch: 11  Global Step: 66080   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:38,048-Speed 3901.51 samples/sec  Loss 1.1571  LearningRate 0.0573  ProxyLR: 2.8630  Epoch: 11  Global Step: 66090   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:40,675-Speed 3898.06 samples/sec  Loss 1.1302  LearningRate 0.0572  ProxyLR: 2.8622  Epoch: 11  Global Step: 66100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:43,303-Speed 3897.61 samples/sec  Loss 1.1933  LearningRate 0.0572  ProxyLR: 2.8615  Epoch: 11  Global Step: 66110   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:45,932-Speed 3895.98 samples/sec  Loss 1.1639  LearningRate 0.0572  ProxyLR: 2.8607  Epoch: 11  Global Step: 66120   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:48,550-Speed 3912.21 samples/sec  Loss 1.1805  LearningRate 0.0572  ProxyLR: 2.8600  Epoch: 11  Global Step: 66130   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:51,183-Speed 3890.73 samples/sec  Loss 1.1124  LearningRate 0.0572  ProxyLR: 2.8592  Epoch: 11  Global Step: 66140   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:53,812-Speed 3896.25 samples/sec  Loss 1.1428  LearningRate 0.0572  ProxyLR: 2.8585  Epoch: 11  Global Step: 66150   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:30:56,428-Speed 3914.34 samples/sec  Loss 1.1726  LearningRate 0.0572  ProxyLR: 2.8577  Epoch: 11  Global Step: 66160   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:30:59,058-Speed 3895.76 samples/sec  Loss 1.2094  LearningRate 0.0571  ProxyLR: 2.8570  Epoch: 11  Global Step: 66170   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:01,686-Speed 3897.14 samples/sec  Loss 1.1986  LearningRate 0.0571  ProxyLR: 2.8562  Epoch: 11  Global Step: 66180   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:04,314-Speed 3897.89 samples/sec  Loss 1.1788  LearningRate 0.0571  ProxyLR: 2.8555  Epoch: 11  Global Step: 66190   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:06,942-Speed 3897.13 samples/sec  Loss 1.1396  LearningRate 0.0571  ProxyLR: 2.8547  Epoch: 11  Global Step: 66200   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:09,572-Speed 3894.78 samples/sec  Loss 1.2059  LearningRate 0.0571  ProxyLR: 2.8540  Epoch: 11  Global Step: 66210   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:12,201-Speed 3894.78 samples/sec  Loss 1.1625  LearningRate 0.0571  ProxyLR: 2.8532  Epoch: 11  Global Step: 66220   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:14,830-Speed 3896.60 samples/sec  Loss 1.1434  LearningRate 0.0570  ProxyLR: 2.8525  Epoch: 11  Global Step: 66230   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:17,458-Speed 3897.23 samples/sec  Loss 1.1739  LearningRate 0.0570  ProxyLR: 2.8517  Epoch: 11  Global Step: 66240   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:20,087-Speed 3895.99 samples/sec  Loss 1.1471  LearningRate 0.0570  ProxyLR: 2.8510  Epoch: 11  Global Step: 66250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:22,717-Speed 3894.88 samples/sec  Loss 1.1972  LearningRate 0.0570  ProxyLR: 2.8502  Epoch: 11  Global Step: 66260   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:31:25,346-Speed 3896.15 samples/sec  Loss 1.1763  LearningRate 0.0570  ProxyLR: 2.8494  Epoch: 11  Global Step: 66270   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:31:27,975-Speed 3894.87 samples/sec  Loss 1.0823  LearningRate 0.0570  ProxyLR: 2.8487  Epoch: 11  Global Step: 66280   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:31:30,592-Speed 3914.99 samples/sec  Loss 1.1869  LearningRate 0.0570  ProxyLR: 2.8479  Epoch: 11  Global Step: 66290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:33,221-Speed 3895.33 samples/sec  Loss 1.1386  LearningRate 0.0569  ProxyLR: 2.8472  Epoch: 11  Global Step: 66300   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:35,850-Speed 3896.34 samples/sec  Loss 1.1705  LearningRate 0.0569  ProxyLR: 2.8464  Epoch: 11  Global Step: 66310   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:38,478-Speed 3896.92 samples/sec  Loss 1.1534  LearningRate 0.0569  ProxyLR: 2.8457  Epoch: 11  Global Step: 66320   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:41,106-Speed 3897.87 samples/sec  Loss 1.1224  LearningRate 0.0569  ProxyLR: 2.8449  Epoch: 11  Global Step: 66330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:43,735-Speed 3896.38 samples/sec  Loss 1.1486  LearningRate 0.0569  ProxyLR: 2.8442  Epoch: 11  Global Step: 66340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:46,367-Speed 3891.83 samples/sec  Loss 1.1292  LearningRate 0.0569  ProxyLR: 2.8434  Epoch: 11  Global Step: 66350   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:49,002-Speed 3886.79 samples/sec  Loss 1.1472  LearningRate 0.0569  ProxyLR: 2.8427  Epoch: 11  Global Step: 66360   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:51,634-Speed 3891.73 samples/sec  Loss 1.1340  LearningRate 0.0568  ProxyLR: 2.8419  Epoch: 11  Global Step: 66370   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:54,265-Speed 3892.72 samples/sec  Loss 1.1625  LearningRate 0.0568  ProxyLR: 2.8412  Epoch: 11  Global Step: 66380   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:31:56,897-Speed 3891.70 samples/sec  Loss 1.1656  LearningRate 0.0568  ProxyLR: 2.8404  Epoch: 11  Global Step: 66390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:31:59,528-Speed 3893.19 samples/sec  Loss 1.0972  LearningRate 0.0568  ProxyLR: 2.8397  Epoch: 11  Global Step: 66400   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:02,162-Speed 3888.52 samples/sec  Loss 1.1353  LearningRate 0.0568  ProxyLR: 2.8389  Epoch: 11  Global Step: 66410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:04,794-Speed 3891.48 samples/sec  Loss 1.1858  LearningRate 0.0568  ProxyLR: 2.8382  Epoch: 11  Global Step: 66420   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:07,430-Speed 3885.76 samples/sec  Loss 1.1351  LearningRate 0.0567  ProxyLR: 2.8374  Epoch: 11  Global Step: 66430   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:10,062-Speed 3891.24 samples/sec  Loss 1.1688  LearningRate 0.0567  ProxyLR: 2.8367  Epoch: 11  Global Step: 66440   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:12,696-Speed 3888.91 samples/sec  Loss 1.2136  LearningRate 0.0567  ProxyLR: 2.8359  Epoch: 11  Global Step: 66450   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:15,329-Speed 3890.02 samples/sec  Loss 1.1874  LearningRate 0.0567  ProxyLR: 2.8352  Epoch: 11  Global Step: 66460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:17,962-Speed 3890.16 samples/sec  Loss 1.1692  LearningRate 0.0567  ProxyLR: 2.8344  Epoch: 11  Global Step: 66470   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:32:20,575-Speed 3919.03 samples/sec  Loss 1.1921  LearningRate 0.0567  ProxyLR: 2.8337  Epoch: 11  Global Step: 66480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:23,201-Speed 3900.61 samples/sec  Loss 1.2053  LearningRate 0.0567  ProxyLR: 2.8330  Epoch: 11  Global Step: 66490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:25,827-Speed 3901.27 samples/sec  Loss 1.1725  LearningRate 0.0566  ProxyLR: 2.8322  Epoch: 11  Global Step: 66500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:28,452-Speed 3901.56 samples/sec  Loss 1.1935  LearningRate 0.0566  ProxyLR: 2.8315  Epoch: 11  Global Step: 66510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:31,078-Speed 3900.01 samples/sec  Loss 1.1822  LearningRate 0.0566  ProxyLR: 2.8307  Epoch: 11  Global Step: 66520   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:33,704-Speed 3900.29 samples/sec  Loss 1.1428  LearningRate 0.0566  ProxyLR: 2.8300  Epoch: 11  Global Step: 66530   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:36,331-Speed 3899.19 samples/sec  Loss 1.0941  LearningRate 0.0566  ProxyLR: 2.8292  Epoch: 11  Global Step: 66540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:32:38,944-Speed 3920.06 samples/sec  Loss 1.2103  LearningRate 0.0566  ProxyLR: 2.8285  Epoch: 11  Global Step: 66550   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:41,569-Speed 3902.14 samples/sec  Loss 1.1121  LearningRate 0.0566  ProxyLR: 2.8277  Epoch: 11  Global Step: 66560   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:44,196-Speed 3899.62 samples/sec  Loss 1.0748  LearningRate 0.0565  ProxyLR: 2.8270  Epoch: 11  Global Step: 66570   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:46,820-Speed 3902.45 samples/sec  Loss 1.1626  LearningRate 0.0565  ProxyLR: 2.8262  Epoch: 11  Global Step: 66580   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:49,447-Speed 3899.16 samples/sec  Loss 1.2059  LearningRate 0.0565  ProxyLR: 2.8255  Epoch: 11  Global Step: 66590   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:52,073-Speed 3900.11 samples/sec  Loss 1.1870  LearningRate 0.0565  ProxyLR: 2.8247  Epoch: 11  Global Step: 66600   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:54,699-Speed 3900.91 samples/sec  Loss 1.1969  LearningRate 0.0565  ProxyLR: 2.8240  Epoch: 11  Global Step: 66610   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:57,324-Speed 3901.35 samples/sec  Loss 1.2074  LearningRate 0.0565  ProxyLR: 2.8232  Epoch: 11  Global Step: 66620   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:32:59,948-Speed 3902.94 samples/sec  Loss 1.1426  LearningRate 0.0564  ProxyLR: 2.8225  Epoch: 11  Global Step: 66630   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:33:02,575-Speed 3899.20 samples/sec  Loss 1.1383  LearningRate 0.0564  ProxyLR: 2.8217  Epoch: 11  Global Step: 66640   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:33:05,200-Speed 3901.78 samples/sec  Loss 1.1276  LearningRate 0.0564  ProxyLR: 2.8210  Epoch: 11  Global Step: 66650   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:07,826-Speed 3900.95 samples/sec  Loss 1.1726  LearningRate 0.0564  ProxyLR: 2.8202  Epoch: 11  Global Step: 66660   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:10,451-Speed 3901.25 samples/sec  Loss 1.1507  LearningRate 0.0564  ProxyLR: 2.8195  Epoch: 11  Global Step: 66670   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:13,079-Speed 3898.72 samples/sec  Loss 1.1555  LearningRate 0.0564  ProxyLR: 2.8187  Epoch: 11  Global Step: 66680   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:15,704-Speed 3901.59 samples/sec  Loss 1.1556  LearningRate 0.0564  ProxyLR: 2.8180  Epoch: 11  Global Step: 66690   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:18,330-Speed 3900.73 samples/sec  Loss 1.1762  LearningRate 0.0563  ProxyLR: 2.8172  Epoch: 11  Global Step: 66700   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:20,955-Speed 3901.37 samples/sec  Loss 1.1850  LearningRate 0.0563  ProxyLR: 2.8165  Epoch: 11  Global Step: 66710   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:23,582-Speed 3899.63 samples/sec  Loss 1.1583  LearningRate 0.0563  ProxyLR: 2.8158  Epoch: 11  Global Step: 66720   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:26,206-Speed 3902.16 samples/sec  Loss 1.1625  LearningRate 0.0563  ProxyLR: 2.8150  Epoch: 11  Global Step: 66730   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:28,832-Speed 3900.62 samples/sec  Loss 1.1816  LearningRate 0.0563  ProxyLR: 2.8143  Epoch: 11  Global Step: 66740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:33:31,459-Speed 3900.04 samples/sec  Loss 1.2215  LearningRate 0.0563  ProxyLR: 2.8135  Epoch: 11  Global Step: 66750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:34,085-Speed 3899.28 samples/sec  Loss 1.2238  LearningRate 0.0563  ProxyLR: 2.8128  Epoch: 11  Global Step: 66760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:36,712-Speed 3900.17 samples/sec  Loss 1.1941  LearningRate 0.0562  ProxyLR: 2.8120  Epoch: 11  Global Step: 66770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:39,350-Speed 3881.67 samples/sec  Loss 1.2326  LearningRate 0.0562  ProxyLR: 2.8113  Epoch: 11  Global Step: 66780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:41,987-Speed 3884.33 samples/sec  Loss 1.1424  LearningRate 0.0562  ProxyLR: 2.8105  Epoch: 11  Global Step: 66790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:44,623-Speed 3885.98 samples/sec  Loss 1.1730  LearningRate 0.0562  ProxyLR: 2.8098  Epoch: 11  Global Step: 66800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:47,262-Speed 3881.31 samples/sec  Loss 1.1720  LearningRate 0.0562  ProxyLR: 2.8090  Epoch: 11  Global Step: 66810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:49,901-Speed 3881.72 samples/sec  Loss 1.1616  LearningRate 0.0562  ProxyLR: 2.8083  Epoch: 11  Global Step: 66820   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:52,531-Speed 3894.05 samples/sec  Loss 1.1801  LearningRate 0.0562  ProxyLR: 2.8075  Epoch: 11  Global Step: 66830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:55,157-Speed 3900.68 samples/sec  Loss 1.2246  LearningRate 0.0561  ProxyLR: 2.8068  Epoch: 11  Global Step: 66840   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:33:57,782-Speed 3901.34 samples/sec  Loss 1.2183  LearningRate 0.0561  ProxyLR: 2.8061  Epoch: 11  Global Step: 66850   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:34:00,397-Speed 3917.51 samples/sec  Loss 1.1155  LearningRate 0.0561  ProxyLR: 2.8053  Epoch: 11  Global Step: 66860   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:03,023-Speed 3900.43 samples/sec  Loss 1.0999  LearningRate 0.0561  ProxyLR: 2.8046  Epoch: 11  Global Step: 66870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:05,649-Speed 3900.15 samples/sec  Loss 1.1875  LearningRate 0.0561  ProxyLR: 2.8038  Epoch: 11  Global Step: 66880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:08,277-Speed 3898.30 samples/sec  Loss 1.1867  LearningRate 0.0561  ProxyLR: 2.8031  Epoch: 11  Global Step: 66890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:10,902-Speed 3902.15 samples/sec  Loss 1.1425  LearningRate 0.0560  ProxyLR: 2.8023  Epoch: 11  Global Step: 66900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:13,527-Speed 3901.70 samples/sec  Loss 1.1837  LearningRate 0.0560  ProxyLR: 2.8016  Epoch: 11  Global Step: 66910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:16,152-Speed 3902.27 samples/sec  Loss 1.1311  LearningRate 0.0560  ProxyLR: 2.8008  Epoch: 11  Global Step: 66920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:18,776-Speed 3903.27 samples/sec  Loss 1.1583  LearningRate 0.0560  ProxyLR: 2.8001  Epoch: 11  Global Step: 66930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:21,399-Speed 3904.38 samples/sec  Loss 1.1643  LearningRate 0.0560  ProxyLR: 2.7994  Epoch: 11  Global Step: 66940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:24,023-Speed 3902.69 samples/sec  Loss 1.2818  LearningRate 0.0560  ProxyLR: 2.7986  Epoch: 11  Global Step: 66950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:26,649-Speed 3901.53 samples/sec  Loss 1.1888  LearningRate 0.0560  ProxyLR: 2.7979  Epoch: 11  Global Step: 66960   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:34:29,259-Speed 3923.29 samples/sec  Loss 1.2333  LearningRate 0.0559  ProxyLR: 2.7971  Epoch: 11  Global Step: 66970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:31,885-Speed 3901.23 samples/sec  Loss 1.2043  LearningRate 0.0559  ProxyLR: 2.7964  Epoch: 11  Global Step: 66980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:34,511-Speed 3900.08 samples/sec  Loss 1.2355  LearningRate 0.0559  ProxyLR: 2.7956  Epoch: 11  Global Step: 66990   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:37,136-Speed 3902.03 samples/sec  Loss 1.1665  LearningRate 0.0559  ProxyLR: 2.7949  Epoch: 11  Global Step: 67000   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:39,760-Speed 3902.56 samples/sec  Loss 1.1848  LearningRate 0.0559  ProxyLR: 2.7941  Epoch: 11  Global Step: 67010   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:42,385-Speed 3903.13 samples/sec  Loss 1.1745  LearningRate 0.0559  ProxyLR: 2.7934  Epoch: 11  Global Step: 67020   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:45,011-Speed 3900.63 samples/sec  Loss 1.1477  LearningRate 0.0559  ProxyLR: 2.7927  Epoch: 11  Global Step: 67030   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:47,636-Speed 3902.19 samples/sec  Loss 1.2380  LearningRate 0.0558  ProxyLR: 2.7919  Epoch: 11  Global Step: 67040   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:50,261-Speed 3901.03 samples/sec  Loss 1.1721  LearningRate 0.0558  ProxyLR: 2.7912  Epoch: 11  Global Step: 67050   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:52,889-Speed 3897.02 samples/sec  Loss 1.1577  LearningRate 0.0558  ProxyLR: 2.7904  Epoch: 11  Global Step: 67060   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:34:55,514-Speed 3902.44 samples/sec  Loss 1.1863  LearningRate 0.0558  ProxyLR: 2.7897  Epoch: 11  Global Step: 67070   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:34:58,127-Speed 3920.05 samples/sec  Loss 1.2604  LearningRate 0.0558  ProxyLR: 2.7889  Epoch: 11  Global Step: 67080   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:00,754-Speed 3898.94 samples/sec  Loss 1.1621  LearningRate 0.0558  ProxyLR: 2.7882  Epoch: 11  Global Step: 67090   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:03,378-Speed 3902.99 samples/sec  Loss 1.2099  LearningRate 0.0557  ProxyLR: 2.7875  Epoch: 11  Global Step: 67100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:06,005-Speed 3898.57 samples/sec  Loss 1.2519  LearningRate 0.0557  ProxyLR: 2.7867  Epoch: 11  Global Step: 67110   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:08,629-Speed 3903.76 samples/sec  Loss 1.2012  LearningRate 0.0557  ProxyLR: 2.7860  Epoch: 11  Global Step: 67120   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:11,253-Speed 3903.00 samples/sec  Loss 1.1698  LearningRate 0.0557  ProxyLR: 2.7852  Epoch: 11  Global Step: 67130   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:13,877-Speed 3903.93 samples/sec  Loss 1.1738  LearningRate 0.0557  ProxyLR: 2.7845  Epoch: 11  Global Step: 67140   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:16,504-Speed 3899.27 samples/sec  Loss 1.1152  LearningRate 0.0557  ProxyLR: 2.7837  Epoch: 11  Global Step: 67150   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:19,128-Speed 3902.42 samples/sec  Loss 1.1736  LearningRate 0.0557  ProxyLR: 2.7830  Epoch: 11  Global Step: 67160   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:21,753-Speed 3901.91 samples/sec  Loss 1.1752  LearningRate 0.0556  ProxyLR: 2.7823  Epoch: 11  Global Step: 67170   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:24,365-Speed 3922.50 samples/sec  Loss 1.2192  LearningRate 0.0556  ProxyLR: 2.7815  Epoch: 11  Global Step: 67180   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:26,992-Speed 3898.10 samples/sec  Loss 1.1720  LearningRate 0.0556  ProxyLR: 2.7808  Epoch: 11  Global Step: 67190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:29,619-Speed 3898.45 samples/sec  Loss 1.2347  LearningRate 0.0556  ProxyLR: 2.7800  Epoch: 11  Global Step: 67200   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:32,249-Speed 3895.46 samples/sec  Loss 1.2746  LearningRate 0.0556  ProxyLR: 2.7793  Epoch: 11  Global Step: 67210   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:34,876-Speed 3898.79 samples/sec  Loss 1.0906  LearningRate 0.0556  ProxyLR: 2.7785  Epoch: 11  Global Step: 67220   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:37,501-Speed 3902.33 samples/sec  Loss 1.1730  LearningRate 0.0556  ProxyLR: 2.7778  Epoch: 11  Global Step: 67230   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:40,126-Speed 3901.08 samples/sec  Loss 1.2334  LearningRate 0.0555  ProxyLR: 2.7771  Epoch: 11  Global Step: 67240   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:42,751-Speed 3903.10 samples/sec  Loss 1.2443  LearningRate 0.0555  ProxyLR: 2.7763  Epoch: 11  Global Step: 67250   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:45,378-Speed 3899.35 samples/sec  Loss 1.2249  LearningRate 0.0555  ProxyLR: 2.7756  Epoch: 11  Global Step: 67260   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:48,005-Speed 3898.40 samples/sec  Loss 1.2246  LearningRate 0.0555  ProxyLR: 2.7748  Epoch: 11  Global Step: 67270   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:50,618-Speed 3919.19 samples/sec  Loss 1.1834  LearningRate 0.0555  ProxyLR: 2.7741  Epoch: 11  Global Step: 67280   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:53,245-Speed 3899.38 samples/sec  Loss 1.2118  LearningRate 0.0555  ProxyLR: 2.7734  Epoch: 11  Global Step: 67290   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:55,872-Speed 3898.49 samples/sec  Loss 1.1153  LearningRate 0.0555  ProxyLR: 2.7726  Epoch: 11  Global Step: 67300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:35:58,504-Speed 3891.90 samples/sec  Loss 1.1822  LearningRate 0.0554  ProxyLR: 2.7719  Epoch: 11  Global Step: 67310   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:01,134-Speed 3894.94 samples/sec  Loss 1.1911  LearningRate 0.0554  ProxyLR: 2.7711  Epoch: 11  Global Step: 67320   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:03,764-Speed 3894.47 samples/sec  Loss 1.2162  LearningRate 0.0554  ProxyLR: 2.7704  Epoch: 11  Global Step: 67330   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:06,390-Speed 3899.49 samples/sec  Loss 1.2037  LearningRate 0.0554  ProxyLR: 2.7697  Epoch: 11  Global Step: 67340   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:09,015-Speed 3902.39 samples/sec  Loss 1.1971  LearningRate 0.0554  ProxyLR: 2.7689  Epoch: 11  Global Step: 67350   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:11,641-Speed 3899.84 samples/sec  Loss 1.2271  LearningRate 0.0554  ProxyLR: 2.7682  Epoch: 11  Global Step: 67360   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:14,269-Speed 3897.56 samples/sec  Loss 1.2359  LearningRate 0.0553  ProxyLR: 2.7674  Epoch: 11  Global Step: 67370   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:16,882-Speed 3920.86 samples/sec  Loss 1.1915  LearningRate 0.0553  ProxyLR: 2.7667  Epoch: 11  Global Step: 67380   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:19,507-Speed 3900.99 samples/sec  Loss 1.2167  LearningRate 0.0553  ProxyLR: 2.7660  Epoch: 11  Global Step: 67390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:22,133-Speed 3900.77 samples/sec  Loss 1.1651  LearningRate 0.0553  ProxyLR: 2.7652  Epoch: 11  Global Step: 67400   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:24,758-Speed 3901.85 samples/sec  Loss 1.1494  LearningRate 0.0553  ProxyLR: 2.7645  Epoch: 11  Global Step: 67410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:27,371-Speed 3918.98 samples/sec  Loss 1.1741  LearningRate 0.0553  ProxyLR: 2.7637  Epoch: 11  Global Step: 67420   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:29,997-Speed 3901.30 samples/sec  Loss 1.2344  LearningRate 0.0553  ProxyLR: 2.7630  Epoch: 11  Global Step: 67430   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:32,625-Speed 3898.03 samples/sec  Loss 1.2088  LearningRate 0.0552  ProxyLR: 2.7623  Epoch: 11  Global Step: 67440   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:35,250-Speed 3900.73 samples/sec  Loss 1.2739  LearningRate 0.0552  ProxyLR: 2.7615  Epoch: 11  Global Step: 67450   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:37,877-Speed 3899.37 samples/sec  Loss 1.1458  LearningRate 0.0552  ProxyLR: 2.7608  Epoch: 11  Global Step: 67460   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:40,504-Speed 3899.88 samples/sec  Loss 1.2116  LearningRate 0.0552  ProxyLR: 2.7600  Epoch: 11  Global Step: 67470   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:43,131-Speed 3898.22 samples/sec  Loss 1.2072  LearningRate 0.0552  ProxyLR: 2.7593  Epoch: 11  Global Step: 67480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:45,758-Speed 3899.90 samples/sec  Loss 1.1970  LearningRate 0.0552  ProxyLR: 2.7586  Epoch: 11  Global Step: 67490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:48,384-Speed 3899.16 samples/sec  Loss 1.2118  LearningRate 0.0552  ProxyLR: 2.7578  Epoch: 11  Global Step: 67500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:51,011-Speed 3900.05 samples/sec  Loss 1.2290  LearningRate 0.0551  ProxyLR: 2.7571  Epoch: 11  Global Step: 67510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:36:53,636-Speed 3900.81 samples/sec  Loss 1.1241  LearningRate 0.0551  ProxyLR: 2.7563  Epoch: 11  Global Step: 67520   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:56,262-Speed 3900.70 samples/sec  Loss 1.2123  LearningRate 0.0551  ProxyLR: 2.7556  Epoch: 11  Global Step: 67530   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:36:58,888-Speed 3900.90 samples/sec  Loss 1.2163  LearningRate 0.0551  ProxyLR: 2.7549  Epoch: 11  Global Step: 67540   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:01,512-Speed 3902.91 samples/sec  Loss 1.2045  LearningRate 0.0551  ProxyLR: 2.7541  Epoch: 11  Global Step: 67550   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:04,138-Speed 3900.85 samples/sec  Loss 1.2014  LearningRate 0.0551  ProxyLR: 2.7534  Epoch: 11  Global Step: 67560   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:06,764-Speed 3900.10 samples/sec  Loss 1.2043  LearningRate 0.0551  ProxyLR: 2.7527  Epoch: 11  Global Step: 67570   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:09,390-Speed 3900.24 samples/sec  Loss 1.2013  LearningRate 0.0550  ProxyLR: 2.7519  Epoch: 11  Global Step: 67580   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:12,017-Speed 3899.14 samples/sec  Loss 1.2089  LearningRate 0.0550  ProxyLR: 2.7512  Epoch: 11  Global Step: 67590   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:14,642-Speed 3901.66 samples/sec  Loss 1.1694  LearningRate 0.0550  ProxyLR: 2.7504  Epoch: 11  Global Step: 67600   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:17,269-Speed 3899.23 samples/sec  Loss 1.2195  LearningRate 0.0550  ProxyLR: 2.7497  Epoch: 11  Global Step: 67610   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:19,896-Speed 3899.23 samples/sec  Loss 1.2052  LearningRate 0.0550  ProxyLR: 2.7490  Epoch: 11  Global Step: 67620   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:37:22,521-Speed 3900.90 samples/sec  Loss 1.2247  LearningRate 0.0550  ProxyLR: 2.7482  Epoch: 11  Global Step: 67630   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:37:25,135-Speed 3919.63 samples/sec  Loss 1.2326  LearningRate 0.0549  ProxyLR: 2.7475  Epoch: 11  Global Step: 67640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:27,761-Speed 3899.83 samples/sec  Loss 1.2388  LearningRate 0.0549  ProxyLR: 2.7467  Epoch: 11  Global Step: 67650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:30,387-Speed 3900.02 samples/sec  Loss 1.2445  LearningRate 0.0549  ProxyLR: 2.7460  Epoch: 11  Global Step: 67660   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:33,012-Speed 3902.20 samples/sec  Loss 1.2304  LearningRate 0.0549  ProxyLR: 2.7453  Epoch: 11  Global Step: 67670   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:35,638-Speed 3901.51 samples/sec  Loss 1.1785  LearningRate 0.0549  ProxyLR: 2.7445  Epoch: 11  Global Step: 67680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:38,264-Speed 3900.71 samples/sec  Loss 1.2178  LearningRate 0.0549  ProxyLR: 2.7438  Epoch: 11  Global Step: 67690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:40,890-Speed 3900.37 samples/sec  Loss 1.2432  LearningRate 0.0549  ProxyLR: 2.7431  Epoch: 11  Global Step: 67700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:43,514-Speed 3902.57 samples/sec  Loss 1.1838  LearningRate 0.0548  ProxyLR: 2.7423  Epoch: 11  Global Step: 67710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:46,142-Speed 3897.93 samples/sec  Loss 1.2347  LearningRate 0.0548  ProxyLR: 2.7416  Epoch: 11  Global Step: 67720   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:48,770-Speed 3897.17 samples/sec  Loss 1.1515  LearningRate 0.0548  ProxyLR: 2.7409  Epoch: 11  Global Step: 67730   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:51,383-Speed 3920.04 samples/sec  Loss 1.2641  LearningRate 0.0548  ProxyLR: 2.7401  Epoch: 11  Global Step: 67740   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:54,012-Speed 3896.77 samples/sec  Loss 1.2165  LearningRate 0.0548  ProxyLR: 2.7394  Epoch: 11  Global Step: 67750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:56,640-Speed 3897.15 samples/sec  Loss 1.1790  LearningRate 0.0548  ProxyLR: 2.7386  Epoch: 11  Global Step: 67760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:37:59,266-Speed 3899.90 samples/sec  Loss 1.2297  LearningRate 0.0548  ProxyLR: 2.7379  Epoch: 11  Global Step: 67770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:01,894-Speed 3898.38 samples/sec  Loss 1.1817  LearningRate 0.0547  ProxyLR: 2.7372  Epoch: 11  Global Step: 67780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:04,520-Speed 3899.45 samples/sec  Loss 1.3186  LearningRate 0.0547  ProxyLR: 2.7364  Epoch: 11  Global Step: 67790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:07,148-Speed 3897.32 samples/sec  Loss 1.1796  LearningRate 0.0547  ProxyLR: 2.7357  Epoch: 11  Global Step: 67800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:09,774-Speed 3900.50 samples/sec  Loss 1.1731  LearningRate 0.0547  ProxyLR: 2.7350  Epoch: 11  Global Step: 67810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:12,401-Speed 3899.48 samples/sec  Loss 1.2967  LearningRate 0.0547  ProxyLR: 2.7342  Epoch: 11  Global Step: 67820   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:15,028-Speed 3899.44 samples/sec  Loss 1.2281  LearningRate 0.0547  ProxyLR: 2.7335  Epoch: 11  Global Step: 67830   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:17,654-Speed 3900.21 samples/sec  Loss 1.1808  LearningRate 0.0547  ProxyLR: 2.7328  Epoch: 11  Global Step: 67840   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 19:38:20,267-Speed 3919.75 samples/sec  Loss 1.2438  LearningRate 0.0546  ProxyLR: 2.7320  Epoch: 11  Global Step: 67850   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:22,893-Speed 3900.19 samples/sec  Loss 1.2484  LearningRate 0.0546  ProxyLR: 2.7313  Epoch: 11  Global Step: 67860   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:25,520-Speed 3899.23 samples/sec  Loss 1.1943  LearningRate 0.0546  ProxyLR: 2.7305  Epoch: 11  Global Step: 67870   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:28,148-Speed 3897.68 samples/sec  Loss 1.2467  LearningRate 0.0546  ProxyLR: 2.7298  Epoch: 11  Global Step: 67880   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:30,775-Speed 3898.42 samples/sec  Loss 1.2252  LearningRate 0.0546  ProxyLR: 2.7291  Epoch: 11  Global Step: 67890   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:33,402-Speed 3899.64 samples/sec  Loss 1.1741  LearningRate 0.0546  ProxyLR: 2.7283  Epoch: 11  Global Step: 67900   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:36,029-Speed 3899.09 samples/sec  Loss 1.2126  LearningRate 0.0546  ProxyLR: 2.7276  Epoch: 11  Global Step: 67910   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:38,654-Speed 3901.46 samples/sec  Loss 1.2091  LearningRate 0.0545  ProxyLR: 2.7269  Epoch: 11  Global Step: 67920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:41,281-Speed 3899.48 samples/sec  Loss 1.2183  LearningRate 0.0545  ProxyLR: 2.7261  Epoch: 11  Global Step: 67930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:38:43,892-Speed 3922.52 samples/sec  Loss 1.2058  LearningRate 0.0545  ProxyLR: 2.7254  Epoch: 11  Global Step: 67940   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:38:46,517-Speed 3902.26 samples/sec  Loss 1.2548  LearningRate 0.0545  ProxyLR: 2.7247  Epoch: 11  Global Step: 67950   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:38:49,143-Speed 3899.66 samples/sec  Loss 1.1898  LearningRate 0.0545  ProxyLR: 2.7239  Epoch: 11  Global Step: 67960   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:38:51,769-Speed 3900.73 samples/sec  Loss 1.2046  LearningRate 0.0545  ProxyLR: 2.7232  Epoch: 11  Global Step: 67970   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:38:54,396-Speed 3899.80 samples/sec  Loss 1.1583  LearningRate 0.0544  ProxyLR: 2.7225  Epoch: 11  Global Step: 67980   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:38:57,022-Speed 3899.66 samples/sec  Loss 1.2908  LearningRate 0.0544  ProxyLR: 2.7217  Epoch: 11  Global Step: 67990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:38:59,649-Speed 3899.60 samples/sec  Loss 1.2042  LearningRate 0.0544  ProxyLR: 2.7210  Epoch: 11  Global Step: 68000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:02,276-Speed 3898.93 samples/sec  Loss 1.2225  LearningRate 0.0544  ProxyLR: 2.7203  Epoch: 11  Global Step: 68010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:04,902-Speed 3900.33 samples/sec  Loss 1.2693  LearningRate 0.0544  ProxyLR: 2.7195  Epoch: 11  Global Step: 68020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:07,529-Speed 3897.79 samples/sec  Loss 1.2012  LearningRate 0.0544  ProxyLR: 2.7188  Epoch: 11  Global Step: 68030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:10,154-Speed 3902.36 samples/sec  Loss 1.1468  LearningRate 0.0544  ProxyLR: 2.7181  Epoch: 11  Global Step: 68040   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:39:12,782-Speed 3897.84 samples/sec  Loss 1.2478  LearningRate 0.0543  ProxyLR: 2.7173  Epoch: 11  Global Step: 68050   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:39:15,396-Speed 3917.64 samples/sec  Loss 1.2365  LearningRate 0.0543  ProxyLR: 2.7166  Epoch: 11  Global Step: 68060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:18,021-Speed 3902.61 samples/sec  Loss 1.2158  LearningRate 0.0543  ProxyLR: 2.7159  Epoch: 11  Global Step: 68070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:20,647-Speed 3900.05 samples/sec  Loss 1.2020  LearningRate 0.0543  ProxyLR: 2.7151  Epoch: 11  Global Step: 68080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:23,273-Speed 3900.77 samples/sec  Loss 1.2005  LearningRate 0.0543  ProxyLR: 2.7144  Epoch: 11  Global Step: 68090   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:25,897-Speed 3903.30 samples/sec  Loss 1.1387  LearningRate 0.0543  ProxyLR: 2.7137  Epoch: 11  Global Step: 68100   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:28,523-Speed 3899.76 samples/sec  Loss 1.2375  LearningRate 0.0543  ProxyLR: 2.7129  Epoch: 11  Global Step: 68110   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:31,150-Speed 3899.71 samples/sec  Loss 1.2196  LearningRate 0.0542  ProxyLR: 2.7122  Epoch: 11  Global Step: 68120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:33,775-Speed 3902.42 samples/sec  Loss 1.2232  LearningRate 0.0542  ProxyLR: 2.7115  Epoch: 11  Global Step: 68130   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:39:36,387-Speed 3921.29 samples/sec  Loss 1.2275  LearningRate 0.0542  ProxyLR: 2.7107  Epoch: 11  Global Step: 68140   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:39,012-Speed 3901.04 samples/sec  Loss 1.1973  LearningRate 0.0542  ProxyLR: 2.7100  Epoch: 11  Global Step: 68150   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:41,638-Speed 3900.40 samples/sec  Loss 1.2604  LearningRate 0.0542  ProxyLR: 2.7093  Epoch: 11  Global Step: 68160   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:44,265-Speed 3899.93 samples/sec  Loss 1.2126  LearningRate 0.0542  ProxyLR: 2.7085  Epoch: 11  Global Step: 68170   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:46,893-Speed 3896.95 samples/sec  Loss 1.2354  LearningRate 0.0542  ProxyLR: 2.7078  Epoch: 11  Global Step: 68180   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:49,524-Speed 3893.16 samples/sec  Loss 1.2020  LearningRate 0.0541  ProxyLR: 2.7071  Epoch: 11  Global Step: 68190   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:52,153-Speed 3896.25 samples/sec  Loss 1.2182  LearningRate 0.0541  ProxyLR: 2.7063  Epoch: 11  Global Step: 68200   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:54,782-Speed 3895.99 samples/sec  Loss 1.2646  LearningRate 0.0541  ProxyLR: 2.7056  Epoch: 11  Global Step: 68210   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:39:57,411-Speed 3894.60 samples/sec  Loss 1.2220  LearningRate 0.0541  ProxyLR: 2.7049  Epoch: 11  Global Step: 68220   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:00,098-Speed 3813.06 samples/sec  Loss 1.1808  LearningRate 0.0541  ProxyLR: 2.7041  Epoch: 11  Global Step: 68230   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:09,222-Speed 1122.43 samples/sec  Loss 4.8837  LearningRate 0.0541  ProxyLR: 2.7034  Epoch: 12  Global Step: 68240   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:11,886-Speed 3844.06 samples/sec  Loss 5.6196  LearningRate 0.0541  ProxyLR: 2.7027  Epoch: 12  Global Step: 68250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:14,521-Speed 3887.32 samples/sec  Loss 5.6479  LearningRate 0.0540  ProxyLR: 2.7020  Epoch: 12  Global Step: 68260   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:17,155-Speed 3888.68 samples/sec  Loss 5.5623  LearningRate 0.0540  ProxyLR: 2.7012  Epoch: 12  Global Step: 68270   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:19,789-Speed 3888.12 samples/sec  Loss 5.5977  LearningRate 0.0540  ProxyLR: 2.7005  Epoch: 12  Global Step: 68280   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:22,421-Speed 3891.58 samples/sec  Loss 5.5194  LearningRate 0.0540  ProxyLR: 2.6998  Epoch: 12  Global Step: 68290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:25,053-Speed 3891.60 samples/sec  Loss 5.5111  LearningRate 0.0540  ProxyLR: 2.6990  Epoch: 12  Global Step: 68300   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:27,685-Speed 3891.57 samples/sec  Loss 5.4634  LearningRate 0.0540  ProxyLR: 2.6983  Epoch: 12  Global Step: 68310   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:30,317-Speed 3892.45 samples/sec  Loss 5.4175  LearningRate 0.0540  ProxyLR: 2.6976  Epoch: 12  Global Step: 68320   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:32,995-Speed 3824.22 samples/sec  Loss 5.2999  LearningRate 0.0539  ProxyLR: 2.6968  Epoch: 12  Global Step: 68330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:35,614-Speed 3911.79 samples/sec  Loss 5.2743  LearningRate 0.0539  ProxyLR: 2.6961  Epoch: 12  Global Step: 68340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:38,267-Speed 3859.64 samples/sec  Loss 5.1481  LearningRate 0.0539  ProxyLR: 2.6954  Epoch: 12  Global Step: 68350   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:40:40,884-Speed 3914.98 samples/sec  Loss 5.2359  LearningRate 0.0539  ProxyLR: 2.6946  Epoch: 12  Global Step: 68360   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:43,511-Speed 3898.55 samples/sec  Loss 5.2669  LearningRate 0.0539  ProxyLR: 2.6939  Epoch: 12  Global Step: 68370   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:46,139-Speed 3896.88 samples/sec  Loss 5.1075  LearningRate 0.0539  ProxyLR: 2.6932  Epoch: 12  Global Step: 68380   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:48,773-Speed 3889.62 samples/sec  Loss 5.2046  LearningRate 0.0538  ProxyLR: 2.6925  Epoch: 12  Global Step: 68390   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:51,403-Speed 3894.32 samples/sec  Loss 5.0957  LearningRate 0.0538  ProxyLR: 2.6917  Epoch: 12  Global Step: 68400   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:54,034-Speed 3892.94 samples/sec  Loss 5.1527  LearningRate 0.0538  ProxyLR: 2.6910  Epoch: 12  Global Step: 68410   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:56,666-Speed 3891.40 samples/sec  Loss 4.9872  LearningRate 0.0538  ProxyLR: 2.6903  Epoch: 12  Global Step: 68420   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:40:59,323-Speed 3854.40 samples/sec  Loss 4.9512  LearningRate 0.0538  ProxyLR: 2.6895  Epoch: 12  Global Step: 68430   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:01,958-Speed 3887.11 samples/sec  Loss 4.9504  LearningRate 0.0538  ProxyLR: 2.6888  Epoch: 12  Global Step: 68440   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:04,646-Speed 3811.55 samples/sec  Loss 4.8046  LearningRate 0.0538  ProxyLR: 2.6881  Epoch: 12  Global Step: 68450   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:07,277-Speed 3891.52 samples/sec  Loss 4.8339  LearningRate 0.0537  ProxyLR: 2.6873  Epoch: 12  Global Step: 68460   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:09,909-Speed 3891.99 samples/sec  Loss 4.7542  LearningRate 0.0537  ProxyLR: 2.6866  Epoch: 12  Global Step: 68470   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:12,542-Speed 3890.75 samples/sec  Loss 4.6713  LearningRate 0.0537  ProxyLR: 2.6859  Epoch: 12  Global Step: 68480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:15,174-Speed 3890.55 samples/sec  Loss 4.7556  LearningRate 0.0537  ProxyLR: 2.6852  Epoch: 12  Global Step: 68490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:17,807-Speed 3889.92 samples/sec  Loss 4.6263  LearningRate 0.0537  ProxyLR: 2.6844  Epoch: 12  Global Step: 68500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:20,440-Speed 3891.24 samples/sec  Loss 4.6239  LearningRate 0.0537  ProxyLR: 2.6837  Epoch: 12  Global Step: 68510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:23,071-Speed 3892.99 samples/sec  Loss 4.5884  LearningRate 0.0537  ProxyLR: 2.6830  Epoch: 12  Global Step: 68520   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:25,703-Speed 3890.77 samples/sec  Loss 4.5144  LearningRate 0.0536  ProxyLR: 2.6822  Epoch: 12  Global Step: 68530   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:28,336-Speed 3891.02 samples/sec  Loss 4.4981  LearningRate 0.0536  ProxyLR: 2.6815  Epoch: 12  Global Step: 68540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:30,952-Speed 3915.75 samples/sec  Loss 4.4486  LearningRate 0.0536  ProxyLR: 2.6808  Epoch: 12  Global Step: 68550   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:33,582-Speed 3894.54 samples/sec  Loss 4.6321  LearningRate 0.0536  ProxyLR: 2.6801  Epoch: 12  Global Step: 68560   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:36,214-Speed 3891.09 samples/sec  Loss 4.4591  LearningRate 0.0536  ProxyLR: 2.6793  Epoch: 12  Global Step: 68570   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:38,844-Speed 3893.73 samples/sec  Loss 4.4586  LearningRate 0.0536  ProxyLR: 2.6786  Epoch: 12  Global Step: 68580   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:41,475-Speed 3893.61 samples/sec  Loss 4.4574  LearningRate 0.0536  ProxyLR: 2.6779  Epoch: 12  Global Step: 68590   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:44,106-Speed 3893.77 samples/sec  Loss 4.4262  LearningRate 0.0535  ProxyLR: 2.6771  Epoch: 12  Global Step: 68600   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:46,738-Speed 3891.31 samples/sec  Loss 4.3476  LearningRate 0.0535  ProxyLR: 2.6764  Epoch: 12  Global Step: 68610   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:49,367-Speed 3894.73 samples/sec  Loss 4.3143  LearningRate 0.0535  ProxyLR: 2.6757  Epoch: 12  Global Step: 68620   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:52,000-Speed 3890.36 samples/sec  Loss 4.2614  LearningRate 0.0535  ProxyLR: 2.6750  Epoch: 12  Global Step: 68630   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:54,628-Speed 3897.34 samples/sec  Loss 4.3841  LearningRate 0.0535  ProxyLR: 2.6742  Epoch: 12  Global Step: 68640   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:41:57,259-Speed 3893.95 samples/sec  Loss 4.1746  LearningRate 0.0535  ProxyLR: 2.6735  Epoch: 12  Global Step: 68650   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:41:59,892-Speed 3890.12 samples/sec  Loss 4.2110  LearningRate 0.0535  ProxyLR: 2.6728  Epoch: 12  Global Step: 68660   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:02,524-Speed 3891.79 samples/sec  Loss 4.2956  LearningRate 0.0534  ProxyLR: 2.6720  Epoch: 12  Global Step: 68670   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:05,154-Speed 3894.45 samples/sec  Loss 4.2207  LearningRate 0.0534  ProxyLR: 2.6713  Epoch: 12  Global Step: 68680   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:07,787-Speed 3890.06 samples/sec  Loss 4.1650  LearningRate 0.0534  ProxyLR: 2.6706  Epoch: 12  Global Step: 68690   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:10,419-Speed 3892.07 samples/sec  Loss 4.1895  LearningRate 0.0534  ProxyLR: 2.6699  Epoch: 12  Global Step: 68700   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:13,050-Speed 3892.13 samples/sec  Loss 4.0924  LearningRate 0.0534  ProxyLR: 2.6691  Epoch: 12  Global Step: 68710   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:15,683-Speed 3890.74 samples/sec  Loss 4.0642  LearningRate 0.0534  ProxyLR: 2.6684  Epoch: 12  Global Step: 68720   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:18,314-Speed 3893.05 samples/sec  Loss 3.9167  LearningRate 0.0534  ProxyLR: 2.6677  Epoch: 12  Global Step: 68730   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:20,944-Speed 3893.41 samples/sec  Loss 3.9953  LearningRate 0.0533  ProxyLR: 2.6670  Epoch: 12  Global Step: 68740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:23,575-Speed 3893.65 samples/sec  Loss 4.0160  LearningRate 0.0533  ProxyLR: 2.6662  Epoch: 12  Global Step: 68750   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:42:26,207-Speed 3891.57 samples/sec  Loss 4.0808  LearningRate 0.0533  ProxyLR: 2.6655  Epoch: 12  Global Step: 68760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:42:28,811-Speed 3932.79 samples/sec  Loss 4.0221  LearningRate 0.0533  ProxyLR: 2.6648  Epoch: 12  Global Step: 68770   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:31,442-Speed 3894.05 samples/sec  Loss 3.9853  LearningRate 0.0533  ProxyLR: 2.6641  Epoch: 12  Global Step: 68780   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:34,074-Speed 3891.64 samples/sec  Loss 3.9390  LearningRate 0.0533  ProxyLR: 2.6633  Epoch: 12  Global Step: 68790   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:36,705-Speed 3892.95 samples/sec  Loss 3.9593  LearningRate 0.0533  ProxyLR: 2.6626  Epoch: 12  Global Step: 68800   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:39,336-Speed 3892.46 samples/sec  Loss 4.0172  LearningRate 0.0532  ProxyLR: 2.6619  Epoch: 12  Global Step: 68810   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:41,967-Speed 3893.08 samples/sec  Loss 3.9562  LearningRate 0.0532  ProxyLR: 2.6612  Epoch: 12  Global Step: 68820   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:44,598-Speed 3892.99 samples/sec  Loss 3.8170  LearningRate 0.0532  ProxyLR: 2.6604  Epoch: 12  Global Step: 68830   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:47,230-Speed 3892.54 samples/sec  Loss 3.8800  LearningRate 0.0532  ProxyLR: 2.6597  Epoch: 12  Global Step: 68840   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:49,859-Speed 3895.61 samples/sec  Loss 3.8533  LearningRate 0.0532  ProxyLR: 2.6590  Epoch: 12  Global Step: 68850   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:52,491-Speed 3891.00 samples/sec  Loss 3.8175  LearningRate 0.0532  ProxyLR: 2.6582  Epoch: 12  Global Step: 68860   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:42:55,121-Speed 3894.34 samples/sec  Loss 3.8222  LearningRate 0.0532  ProxyLR: 2.6575  Epoch: 12  Global Step: 68870   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:42:57,754-Speed 3890.87 samples/sec  Loss 3.7871  LearningRate 0.0531  ProxyLR: 2.6568  Epoch: 12  Global Step: 68880   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:00,387-Speed 3889.01 samples/sec  Loss 3.8673  LearningRate 0.0531  ProxyLR: 2.6561  Epoch: 12  Global Step: 68890   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:03,022-Speed 3887.83 samples/sec  Loss 3.7944  LearningRate 0.0531  ProxyLR: 2.6553  Epoch: 12  Global Step: 68900   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:05,657-Speed 3887.32 samples/sec  Loss 3.7744  LearningRate 0.0531  ProxyLR: 2.6546  Epoch: 12  Global Step: 68910   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:08,294-Speed 3884.08 samples/sec  Loss 3.7631  LearningRate 0.0531  ProxyLR: 2.6539  Epoch: 12  Global Step: 68920   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:10,926-Speed 3891.31 samples/sec  Loss 3.7690  LearningRate 0.0531  ProxyLR: 2.6532  Epoch: 12  Global Step: 68930   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:13,560-Speed 3888.70 samples/sec  Loss 3.7072  LearningRate 0.0530  ProxyLR: 2.6524  Epoch: 12  Global Step: 68940   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:16,193-Speed 3890.81 samples/sec  Loss 3.6138  LearningRate 0.0530  ProxyLR: 2.6517  Epoch: 12  Global Step: 68950   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:18,829-Speed 3885.07 samples/sec  Loss 3.6998  LearningRate 0.0530  ProxyLR: 2.6510  Epoch: 12  Global Step: 68960   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:21,465-Speed 3885.19 samples/sec  Loss 3.6305  LearningRate 0.0530  ProxyLR: 2.6503  Epoch: 12  Global Step: 68970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:43:24,101-Speed 3886.54 samples/sec  Loss 3.6716  LearningRate 0.0530  ProxyLR: 2.6496  Epoch: 12  Global Step: 68980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:43:26,721-Speed 3908.94 samples/sec  Loss 3.6469  LearningRate 0.0530  ProxyLR: 2.6488  Epoch: 12  Global Step: 68990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:29,355-Speed 3888.18 samples/sec  Loss 3.5757  LearningRate 0.0530  ProxyLR: 2.6481  Epoch: 12  Global Step: 69000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:31,987-Speed 3891.36 samples/sec  Loss 3.5758  LearningRate 0.0529  ProxyLR: 2.6474  Epoch: 12  Global Step: 69010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:34,620-Speed 3890.59 samples/sec  Loss 3.5254  LearningRate 0.0529  ProxyLR: 2.6467  Epoch: 12  Global Step: 69020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:37,254-Speed 3888.77 samples/sec  Loss 3.5742  LearningRate 0.0529  ProxyLR: 2.6459  Epoch: 12  Global Step: 69030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:39,888-Speed 3887.57 samples/sec  Loss 3.5323  LearningRate 0.0529  ProxyLR: 2.6452  Epoch: 12  Global Step: 69040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:42,522-Speed 3888.54 samples/sec  Loss 3.6176  LearningRate 0.0529  ProxyLR: 2.6445  Epoch: 12  Global Step: 69050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:45,155-Speed 3891.32 samples/sec  Loss 3.5499  LearningRate 0.0529  ProxyLR: 2.6438  Epoch: 12  Global Step: 69060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:47,788-Speed 3889.84 samples/sec  Loss 3.4717  LearningRate 0.0529  ProxyLR: 2.6430  Epoch: 12  Global Step: 69070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:50,419-Speed 3892.11 samples/sec  Loss 3.5777  LearningRate 0.0528  ProxyLR: 2.6423  Epoch: 12  Global Step: 69080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:53,039-Speed 3910.23 samples/sec  Loss 3.5574  LearningRate 0.0528  ProxyLR: 2.6416  Epoch: 12  Global Step: 69090   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:55,672-Speed 3889.40 samples/sec  Loss 3.4592  LearningRate 0.0528  ProxyLR: 2.6409  Epoch: 12  Global Step: 69100   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:43:58,303-Speed 3893.44 samples/sec  Loss 3.5384  LearningRate 0.0528  ProxyLR: 2.6401  Epoch: 12  Global Step: 69110   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:00,932-Speed 3895.09 samples/sec  Loss 3.4734  LearningRate 0.0528  ProxyLR: 2.6394  Epoch: 12  Global Step: 69120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:03,565-Speed 3890.42 samples/sec  Loss 3.5517  LearningRate 0.0528  ProxyLR: 2.6387  Epoch: 12  Global Step: 69130   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:06,198-Speed 3890.65 samples/sec  Loss 3.4300  LearningRate 0.0528  ProxyLR: 2.6380  Epoch: 12  Global Step: 69140   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:08,830-Speed 3891.65 samples/sec  Loss 3.3353  LearningRate 0.0527  ProxyLR: 2.6373  Epoch: 12  Global Step: 69150   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:11,461-Speed 3892.64 samples/sec  Loss 3.3802  LearningRate 0.0527  ProxyLR: 2.6365  Epoch: 12  Global Step: 69160   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:14,094-Speed 3890.85 samples/sec  Loss 3.3996  LearningRate 0.0527  ProxyLR: 2.6358  Epoch: 12  Global Step: 69170   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:16,726-Speed 3891.78 samples/sec  Loss 3.4215  LearningRate 0.0527  ProxyLR: 2.6351  Epoch: 12  Global Step: 69180   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:19,356-Speed 3894.08 samples/sec  Loss 3.3234  LearningRate 0.0527  ProxyLR: 2.6344  Epoch: 12  Global Step: 69190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:44:21,975-Speed 3910.86 samples/sec  Loss 3.2507  LearningRate 0.0527  ProxyLR: 2.6336  Epoch: 12  Global Step: 69200   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:24,607-Speed 3890.78 samples/sec  Loss 3.3289  LearningRate 0.0527  ProxyLR: 2.6329  Epoch: 12  Global Step: 69210   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:27,240-Speed 3891.00 samples/sec  Loss 3.3744  LearningRate 0.0526  ProxyLR: 2.6322  Epoch: 12  Global Step: 69220   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:29,871-Speed 3893.14 samples/sec  Loss 3.2956  LearningRate 0.0526  ProxyLR: 2.6315  Epoch: 12  Global Step: 69230   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:32,502-Speed 3892.62 samples/sec  Loss 3.2903  LearningRate 0.0526  ProxyLR: 2.6308  Epoch: 12  Global Step: 69240   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:35,133-Speed 3892.27 samples/sec  Loss 3.2645  LearningRate 0.0526  ProxyLR: 2.6300  Epoch: 12  Global Step: 69250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:37,766-Speed 3890.22 samples/sec  Loss 3.2338  LearningRate 0.0526  ProxyLR: 2.6293  Epoch: 12  Global Step: 69260   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:40,399-Speed 3889.87 samples/sec  Loss 3.2490  LearningRate 0.0526  ProxyLR: 2.6286  Epoch: 12  Global Step: 69270   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:43,031-Speed 3891.68 samples/sec  Loss 3.2046  LearningRate 0.0526  ProxyLR: 2.6279  Epoch: 12  Global Step: 69280   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:45,662-Speed 3893.10 samples/sec  Loss 3.2431  LearningRate 0.0525  ProxyLR: 2.6271  Epoch: 12  Global Step: 69290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:48,294-Speed 3891.99 samples/sec  Loss 3.2272  LearningRate 0.0525  ProxyLR: 2.6264  Epoch: 12  Global Step: 69300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:44:50,913-Speed 3911.54 samples/sec  Loss 3.2985  LearningRate 0.0525  ProxyLR: 2.6257  Epoch: 12  Global Step: 69310   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:53,544-Speed 3892.37 samples/sec  Loss 3.2942  LearningRate 0.0525  ProxyLR: 2.6250  Epoch: 12  Global Step: 69320   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:56,182-Speed 3883.13 samples/sec  Loss 3.2710  LearningRate 0.0525  ProxyLR: 2.6243  Epoch: 12  Global Step: 69330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:44:58,819-Speed 3883.90 samples/sec  Loss 3.2069  LearningRate 0.0525  ProxyLR: 2.6235  Epoch: 12  Global Step: 69340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:01,457-Speed 3883.12 samples/sec  Loss 3.2732  LearningRate 0.0525  ProxyLR: 2.6228  Epoch: 12  Global Step: 69350   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:04,092-Speed 3887.56 samples/sec  Loss 3.2309  LearningRate 0.0524  ProxyLR: 2.6221  Epoch: 12  Global Step: 69360   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:06,724-Speed 3890.29 samples/sec  Loss 3.1705  LearningRate 0.0524  ProxyLR: 2.6214  Epoch: 12  Global Step: 69370   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:09,357-Speed 3891.32 samples/sec  Loss 3.2388  LearningRate 0.0524  ProxyLR: 2.6207  Epoch: 12  Global Step: 69380   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:11,989-Speed 3891.42 samples/sec  Loss 3.1938  LearningRate 0.0524  ProxyLR: 2.6199  Epoch: 12  Global Step: 69390   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:14,621-Speed 3891.39 samples/sec  Loss 3.1234  LearningRate 0.0524  ProxyLR: 2.6192  Epoch: 12  Global Step: 69400   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:17,238-Speed 3914.20 samples/sec  Loss 3.1638  LearningRate 0.0524  ProxyLR: 2.6185  Epoch: 12  Global Step: 69410   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:19,868-Speed 3894.64 samples/sec  Loss 3.1949  LearningRate 0.0524  ProxyLR: 2.6178  Epoch: 12  Global Step: 69420   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:22,498-Speed 3893.50 samples/sec  Loss 3.1718  LearningRate 0.0523  ProxyLR: 2.6171  Epoch: 12  Global Step: 69430   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:25,128-Speed 3895.00 samples/sec  Loss 3.0823  LearningRate 0.0523  ProxyLR: 2.6163  Epoch: 12  Global Step: 69440   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:27,757-Speed 3895.22 samples/sec  Loss 3.1274  LearningRate 0.0523  ProxyLR: 2.6156  Epoch: 12  Global Step: 69450   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:30,388-Speed 3893.79 samples/sec  Loss 3.0954  LearningRate 0.0523  ProxyLR: 2.6149  Epoch: 12  Global Step: 69460   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:33,019-Speed 3893.84 samples/sec  Loss 3.0933  LearningRate 0.0523  ProxyLR: 2.6142  Epoch: 12  Global Step: 69470   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:35,648-Speed 3895.67 samples/sec  Loss 3.1207  LearningRate 0.0523  ProxyLR: 2.6135  Epoch: 12  Global Step: 69480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:38,278-Speed 3894.58 samples/sec  Loss 3.1301  LearningRate 0.0523  ProxyLR: 2.6127  Epoch: 12  Global Step: 69490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:40,910-Speed 3891.45 samples/sec  Loss 2.9854  LearningRate 0.0522  ProxyLR: 2.6120  Epoch: 12  Global Step: 69500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:43,541-Speed 3892.43 samples/sec  Loss 3.0725  LearningRate 0.0522  ProxyLR: 2.6113  Epoch: 12  Global Step: 69510   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:45:46,172-Speed 3894.42 samples/sec  Loss 3.0564  LearningRate 0.0522  ProxyLR: 2.6106  Epoch: 12  Global Step: 69520   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:45:48,801-Speed 3894.64 samples/sec  Loss 3.0269  LearningRate 0.0522  ProxyLR: 2.6099  Epoch: 12  Global Step: 69530   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:45:51,418-Speed 3914.76 samples/sec  Loss 3.0409  LearningRate 0.0522  ProxyLR: 2.6092  Epoch: 12  Global Step: 69540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:54,049-Speed 3892.18 samples/sec  Loss 3.0177  LearningRate 0.0522  ProxyLR: 2.6084  Epoch: 12  Global Step: 69550   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:56,680-Speed 3893.44 samples/sec  Loss 3.0395  LearningRate 0.0522  ProxyLR: 2.6077  Epoch: 12  Global Step: 69560   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:45:59,309-Speed 3895.66 samples/sec  Loss 2.9950  LearningRate 0.0521  ProxyLR: 2.6070  Epoch: 12  Global Step: 69570   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:01,937-Speed 3897.45 samples/sec  Loss 3.0267  LearningRate 0.0521  ProxyLR: 2.6063  Epoch: 12  Global Step: 69580   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:04,566-Speed 3896.74 samples/sec  Loss 2.9674  LearningRate 0.0521  ProxyLR: 2.6056  Epoch: 12  Global Step: 69590   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:07,198-Speed 3891.09 samples/sec  Loss 3.0780  LearningRate 0.0521  ProxyLR: 2.6048  Epoch: 12  Global Step: 69600   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:09,829-Speed 3893.25 samples/sec  Loss 2.9763  LearningRate 0.0521  ProxyLR: 2.6041  Epoch: 12  Global Step: 69610   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:12,457-Speed 3896.83 samples/sec  Loss 3.0545  LearningRate 0.0521  ProxyLR: 2.6034  Epoch: 12  Global Step: 69620   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:15,087-Speed 3895.19 samples/sec  Loss 2.9298  LearningRate 0.0521  ProxyLR: 2.6027  Epoch: 12  Global Step: 69630   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:17,714-Speed 3898.37 samples/sec  Loss 3.0049  LearningRate 0.0520  ProxyLR: 2.6020  Epoch: 12  Global Step: 69640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:46:20,343-Speed 3895.79 samples/sec  Loss 2.9467  LearningRate 0.0520  ProxyLR: 2.6013  Epoch: 12  Global Step: 69650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:46:22,959-Speed 3915.69 samples/sec  Loss 2.9753  LearningRate 0.0520  ProxyLR: 2.6005  Epoch: 12  Global Step: 69660   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:25,588-Speed 3895.58 samples/sec  Loss 2.9633  LearningRate 0.0520  ProxyLR: 2.5998  Epoch: 12  Global Step: 69670   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:28,216-Speed 3897.79 samples/sec  Loss 2.9868  LearningRate 0.0520  ProxyLR: 2.5991  Epoch: 12  Global Step: 69680   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:30,845-Speed 3896.93 samples/sec  Loss 2.9725  LearningRate 0.0520  ProxyLR: 2.5984  Epoch: 12  Global Step: 69690   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:33,469-Speed 3903.02 samples/sec  Loss 2.9874  LearningRate 0.0520  ProxyLR: 2.5977  Epoch: 12  Global Step: 69700   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:36,097-Speed 3897.79 samples/sec  Loss 2.9291  LearningRate 0.0519  ProxyLR: 2.5969  Epoch: 12  Global Step: 69710   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:38,722-Speed 3901.95 samples/sec  Loss 2.9250  LearningRate 0.0519  ProxyLR: 2.5962  Epoch: 12  Global Step: 69720   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:41,346-Speed 3903.39 samples/sec  Loss 2.9209  LearningRate 0.0519  ProxyLR: 2.5955  Epoch: 12  Global Step: 69730   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:43,971-Speed 3901.93 samples/sec  Loss 2.8886  LearningRate 0.0519  ProxyLR: 2.5948  Epoch: 12  Global Step: 69740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:46,596-Speed 3901.19 samples/sec  Loss 2.8403  LearningRate 0.0519  ProxyLR: 2.5941  Epoch: 12  Global Step: 69750   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:46:49,223-Speed 3899.10 samples/sec  Loss 2.9453  LearningRate 0.0519  ProxyLR: 2.5934  Epoch: 12  Global Step: 69760   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:46:51,850-Speed 3899.58 samples/sec  Loss 2.9639  LearningRate 0.0519  ProxyLR: 2.5926  Epoch: 12  Global Step: 69770   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:46:54,477-Speed 3899.23 samples/sec  Loss 2.8629  LearningRate 0.0518  ProxyLR: 2.5919  Epoch: 12  Global Step: 69780   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:46:57,104-Speed 3898.97 samples/sec  Loss 2.8944  LearningRate 0.0518  ProxyLR: 2.5912  Epoch: 12  Global Step: 69790   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:46:59,732-Speed 3897.99 samples/sec  Loss 2.9352  LearningRate 0.0518  ProxyLR: 2.5905  Epoch: 12  Global Step: 69800   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:47:02,358-Speed 3899.59 samples/sec  Loss 2.8146  LearningRate 0.0518  ProxyLR: 2.5898  Epoch: 12  Global Step: 69810   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:47:04,971-Speed 3919.80 samples/sec  Loss 2.8541  LearningRate 0.0518  ProxyLR: 2.5891  Epoch: 12  Global Step: 69820   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:07,600-Speed 3896.27 samples/sec  Loss 2.8532  LearningRate 0.0518  ProxyLR: 2.5884  Epoch: 12  Global Step: 69830   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:10,230-Speed 3894.42 samples/sec  Loss 2.8222  LearningRate 0.0518  ProxyLR: 2.5876  Epoch: 12  Global Step: 69840   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:12,862-Speed 3891.06 samples/sec  Loss 2.8193  LearningRate 0.0517  ProxyLR: 2.5869  Epoch: 12  Global Step: 69850   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:15,492-Speed 3895.60 samples/sec  Loss 2.8290  LearningRate 0.0517  ProxyLR: 2.5862  Epoch: 12  Global Step: 69860   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:18,121-Speed 3895.30 samples/sec  Loss 2.8538  LearningRate 0.0517  ProxyLR: 2.5855  Epoch: 12  Global Step: 69870   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:20,751-Speed 3893.91 samples/sec  Loss 2.7914  LearningRate 0.0517  ProxyLR: 2.5848  Epoch: 12  Global Step: 69880   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:23,382-Speed 3893.90 samples/sec  Loss 2.8071  LearningRate 0.0517  ProxyLR: 2.5841  Epoch: 12  Global Step: 69890   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:26,012-Speed 3894.39 samples/sec  Loss 2.8247  LearningRate 0.0517  ProxyLR: 2.5833  Epoch: 12  Global Step: 69900   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:28,643-Speed 3893.84 samples/sec  Loss 2.7913  LearningRate 0.0517  ProxyLR: 2.5826  Epoch: 12  Global Step: 69910   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:31,273-Speed 3894.02 samples/sec  Loss 2.7490  LearningRate 0.0516  ProxyLR: 2.5819  Epoch: 12  Global Step: 69920   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:47:33,902-Speed 3896.29 samples/sec  Loss 2.8851  LearningRate 0.0516  ProxyLR: 2.5812  Epoch: 12  Global Step: 69930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:47:36,533-Speed 3893.05 samples/sec  Loss 2.7724  LearningRate 0.0516  ProxyLR: 2.5805  Epoch: 12  Global Step: 69940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:47:39,150-Speed 3913.71 samples/sec  Loss 2.7543  LearningRate 0.0516  ProxyLR: 2.5798  Epoch: 12  Global Step: 69950   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:41,779-Speed 3895.10 samples/sec  Loss 2.8737  LearningRate 0.0516  ProxyLR: 2.5791  Epoch: 12  Global Step: 69960   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:44,411-Speed 3892.19 samples/sec  Loss 2.7370  LearningRate 0.0516  ProxyLR: 2.5783  Epoch: 12  Global Step: 69970   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:47,041-Speed 3895.11 samples/sec  Loss 2.7723  LearningRate 0.0516  ProxyLR: 2.5776  Epoch: 12  Global Step: 69980   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:49,670-Speed 3895.35 samples/sec  Loss 2.7792  LearningRate 0.0515  ProxyLR: 2.5769  Epoch: 12  Global Step: 69990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:47:52,298-Speed 3897.10 samples/sec  Loss 2.7510  LearningRate 0.0515  ProxyLR: 2.5762  Epoch: 12  Global Step: 70000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:48:42,063-[lfw][70000]XNorm: 22.285916
Training: 2023-05-04 19:48:42,063-[lfw][70000]Accuracy-Flip: 0.99750+-0.00261
Training: 2023-05-04 19:48:42,063-[lfw][70000]Accuracy-Highest: 0.99750
Training: 2023-05-04 19:48:42,063-[lfw][70000]TPR@1stNon-Zero-FPR of 0.00033: 0.99567
Training: 2023-05-04 19:48:42,063-[lfw][70000]Highest TPR@FPR: 0.99567
Training: 2023-05-04 19:49:39,641-[cfp_fp][70000]XNorm: 21.765049
Training: 2023-05-04 19:49:39,641-[cfp_fp][70000]Accuracy-Flip: 0.97371+-0.00836
Training: 2023-05-04 19:49:39,642-[cfp_fp][70000]Accuracy-Highest: 0.97371
Training: 2023-05-04 19:49:39,642-[cfp_fp][70000]TPR@1stNon-Zero-FPR of 0.00029: 0.78771
Training: 2023-05-04 19:49:39,642-[cfp_fp][70000]Highest TPR@FPR: 0.78771
Training: 2023-05-04 19:50:29,518-[agedb_30][70000]XNorm: 22.350338
Training: 2023-05-04 19:50:29,518-[agedb_30][70000]Accuracy-Flip: 0.96633+-0.01035
Training: 2023-05-04 19:50:29,519-[agedb_30][70000]Accuracy-Highest: 0.96633
Training: 2023-05-04 19:50:29,519-[agedb_30][70000]TPR@1stNon-Zero-FPR of 0.00033: 0.68767
Training: 2023-05-04 19:50:29,519-[agedb_30][70000]Highest TPR@FPR: 0.68767
Training: 2023-05-04 19:51:20,689-[calfw][70000]XNorm: 22.333764
Training: 2023-05-04 19:51:20,689-[calfw][70000]Accuracy-Flip: 0.95617+-0.01073
Training: 2023-05-04 19:51:20,689-[calfw][70000]Accuracy-Highest: 0.95617
Training: 2023-05-04 19:51:20,689-[calfw][70000]TPR@1stNon-Zero-FPR of 0.00033: 0.83000
Training: 2023-05-04 19:51:20,689-[calfw][70000]Highest TPR@FPR: 0.83000
Training: 2023-05-04 19:52:11,789-[cplfw][70000]XNorm: 21.497431
Training: 2023-05-04 19:52:11,789-[cplfw][70000]Accuracy-Flip: 0.92183+-0.01505
Training: 2023-05-04 19:52:11,789-[cplfw][70000]Accuracy-Highest: 0.92183
Training: 2023-05-04 19:52:11,789-[cplfw][70000]TPR@1stNon-Zero-FPR of 0.00033: 0.01533
Training: 2023-05-04 19:52:11,789-[cplfw][70000]Highest TPR@FPR: 0.01533
Training: 2023-05-04 19:52:14,447-Speed 39.06 samples/sec  Loss 2.8585  LearningRate 0.0515  ProxyLR: 2.5755  Epoch: 12  Global Step: 70010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:17,067-Speed 3909.63 samples/sec  Loss 2.7340  LearningRate 0.0515  ProxyLR: 2.5748  Epoch: 12  Global Step: 70020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:19,687-Speed 3908.66 samples/sec  Loss 2.7293  LearningRate 0.0515  ProxyLR: 2.5741  Epoch: 12  Global Step: 70030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:22,309-Speed 3907.32 samples/sec  Loss 2.7562  LearningRate 0.0515  ProxyLR: 2.5733  Epoch: 12  Global Step: 70040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:24,932-Speed 3904.95 samples/sec  Loss 2.7596  LearningRate 0.0515  ProxyLR: 2.5726  Epoch: 12  Global Step: 70050   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:52:27,553-Speed 3907.54 samples/sec  Loss 2.7124  LearningRate 0.0514  ProxyLR: 2.5719  Epoch: 12  Global Step: 70060   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:52:30,177-Speed 3903.86 samples/sec  Loss 2.7302  LearningRate 0.0514  ProxyLR: 2.5712  Epoch: 12  Global Step: 70070   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:52:32,788-Speed 3922.69 samples/sec  Loss 2.6636  LearningRate 0.0514  ProxyLR: 2.5705  Epoch: 12  Global Step: 70080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:35,412-Speed 3903.08 samples/sec  Loss 2.7047  LearningRate 0.0514  ProxyLR: 2.5698  Epoch: 12  Global Step: 70090   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:38,036-Speed 3904.63 samples/sec  Loss 2.7636  LearningRate 0.0514  ProxyLR: 2.5691  Epoch: 12  Global Step: 70100   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:40,659-Speed 3904.15 samples/sec  Loss 2.7199  LearningRate 0.0514  ProxyLR: 2.5683  Epoch: 12  Global Step: 70110   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:43,283-Speed 3903.52 samples/sec  Loss 2.7732  LearningRate 0.0514  ProxyLR: 2.5676  Epoch: 12  Global Step: 70120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:45,911-Speed 3898.23 samples/sec  Loss 2.7012  LearningRate 0.0513  ProxyLR: 2.5669  Epoch: 12  Global Step: 70130   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:48,536-Speed 3900.67 samples/sec  Loss 2.6863  LearningRate 0.0513  ProxyLR: 2.5662  Epoch: 12  Global Step: 70140   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:51,163-Speed 3899.26 samples/sec  Loss 2.6995  LearningRate 0.0513  ProxyLR: 2.5655  Epoch: 12  Global Step: 70150   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:53,791-Speed 3896.90 samples/sec  Loss 2.6820  LearningRate 0.0513  ProxyLR: 2.5648  Epoch: 12  Global Step: 70160   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:56,419-Speed 3897.77 samples/sec  Loss 2.6581  LearningRate 0.0513  ProxyLR: 2.5641  Epoch: 12  Global Step: 70170   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:52:59,047-Speed 3898.19 samples/sec  Loss 2.7218  LearningRate 0.0513  ProxyLR: 2.5634  Epoch: 12  Global Step: 70180   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:53:01,676-Speed 3895.60 samples/sec  Loss 2.6753  LearningRate 0.0513  ProxyLR: 2.5626  Epoch: 12  Global Step: 70190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:53:04,304-Speed 3898.76 samples/sec  Loss 2.6925  LearningRate 0.0512  ProxyLR: 2.5619  Epoch: 12  Global Step: 70200   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:53:06,933-Speed 3895.92 samples/sec  Loss 2.6970  LearningRate 0.0512  ProxyLR: 2.5612  Epoch: 12  Global Step: 70210   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:53:09,546-Speed 3919.00 samples/sec  Loss 2.6640  LearningRate 0.0512  ProxyLR: 2.5605  Epoch: 12  Global Step: 70220   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:12,160-Speed 3918.11 samples/sec  Loss 2.6264  LearningRate 0.0512  ProxyLR: 2.5598  Epoch: 12  Global Step: 70230   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:14,789-Speed 3897.32 samples/sec  Loss 2.6852  LearningRate 0.0512  ProxyLR: 2.5591  Epoch: 12  Global Step: 70240   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:17,415-Speed 3899.20 samples/sec  Loss 2.6762  LearningRate 0.0512  ProxyLR: 2.5584  Epoch: 12  Global Step: 70250   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:20,043-Speed 3897.57 samples/sec  Loss 2.5683  LearningRate 0.0512  ProxyLR: 2.5577  Epoch: 12  Global Step: 70260   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:22,669-Speed 3900.55 samples/sec  Loss 2.6710  LearningRate 0.0511  ProxyLR: 2.5570  Epoch: 12  Global Step: 70270   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:25,294-Speed 3902.39 samples/sec  Loss 2.5939  LearningRate 0.0511  ProxyLR: 2.5562  Epoch: 12  Global Step: 70280   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:27,917-Speed 3905.16 samples/sec  Loss 2.5923  LearningRate 0.0511  ProxyLR: 2.5555  Epoch: 12  Global Step: 70290   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:30,541-Speed 3903.44 samples/sec  Loss 2.5853  LearningRate 0.0511  ProxyLR: 2.5548  Epoch: 12  Global Step: 70300   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:33,164-Speed 3904.26 samples/sec  Loss 2.6065  LearningRate 0.0511  ProxyLR: 2.5541  Epoch: 12  Global Step: 70310   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:35,787-Speed 3906.06 samples/sec  Loss 2.6217  LearningRate 0.0511  ProxyLR: 2.5534  Epoch: 12  Global Step: 70320   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:53:38,411-Speed 3903.11 samples/sec  Loss 2.6586  LearningRate 0.0511  ProxyLR: 2.5527  Epoch: 12  Global Step: 70330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:41,035-Speed 3903.03 samples/sec  Loss 2.6917  LearningRate 0.0510  ProxyLR: 2.5520  Epoch: 12  Global Step: 70340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:43,661-Speed 3900.62 samples/sec  Loss 2.6651  LearningRate 0.0510  ProxyLR: 2.5513  Epoch: 12  Global Step: 70350   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:46,289-Speed 3897.44 samples/sec  Loss 2.6457  LearningRate 0.0510  ProxyLR: 2.5506  Epoch: 12  Global Step: 70360   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:48,920-Speed 3893.48 samples/sec  Loss 2.6188  LearningRate 0.0510  ProxyLR: 2.5498  Epoch: 12  Global Step: 70370   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:51,549-Speed 3895.17 samples/sec  Loss 2.6151  LearningRate 0.0510  ProxyLR: 2.5491  Epoch: 12  Global Step: 70380   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:54,181-Speed 3892.28 samples/sec  Loss 2.5899  LearningRate 0.0510  ProxyLR: 2.5484  Epoch: 12  Global Step: 70390   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:56,812-Speed 3892.91 samples/sec  Loss 2.5670  LearningRate 0.0510  ProxyLR: 2.5477  Epoch: 12  Global Step: 70400   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:53:59,441-Speed 3895.41 samples/sec  Loss 2.6281  LearningRate 0.0509  ProxyLR: 2.5470  Epoch: 12  Global Step: 70410   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:02,070-Speed 3896.12 samples/sec  Loss 2.5667  LearningRate 0.0509  ProxyLR: 2.5463  Epoch: 12  Global Step: 70420   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:04,700-Speed 3895.02 samples/sec  Loss 2.4831  LearningRate 0.0509  ProxyLR: 2.5456  Epoch: 12  Global Step: 70430   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:07,332-Speed 3892.19 samples/sec  Loss 2.6511  LearningRate 0.0509  ProxyLR: 2.5449  Epoch: 12  Global Step: 70440   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:09,966-Speed 3888.67 samples/sec  Loss 2.5223  LearningRate 0.0509  ProxyLR: 2.5442  Epoch: 12  Global Step: 70450   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:12,603-Speed 3884.33 samples/sec  Loss 2.5681  LearningRate 0.0509  ProxyLR: 2.5435  Epoch: 12  Global Step: 70460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:15,236-Speed 3889.74 samples/sec  Loss 2.5407  LearningRate 0.0509  ProxyLR: 2.5427  Epoch: 12  Global Step: 70470   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:17,871-Speed 3887.29 samples/sec  Loss 2.5561  LearningRate 0.0508  ProxyLR: 2.5420  Epoch: 12  Global Step: 70480   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:20,507-Speed 3886.04 samples/sec  Loss 2.4905  LearningRate 0.0508  ProxyLR: 2.5413  Epoch: 12  Global Step: 70490   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:23,142-Speed 3887.22 samples/sec  Loss 2.6750  LearningRate 0.0508  ProxyLR: 2.5406  Epoch: 12  Global Step: 70500   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:25,776-Speed 3888.54 samples/sec  Loss 2.4669  LearningRate 0.0508  ProxyLR: 2.5399  Epoch: 12  Global Step: 70510   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:28,411-Speed 3886.37 samples/sec  Loss 2.5936  LearningRate 0.0508  ProxyLR: 2.5392  Epoch: 12  Global Step: 70520   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:31,033-Speed 3906.22 samples/sec  Loss 2.6105  LearningRate 0.0508  ProxyLR: 2.5385  Epoch: 12  Global Step: 70530   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:33,669-Speed 3886.58 samples/sec  Loss 2.5745  LearningRate 0.0508  ProxyLR: 2.5378  Epoch: 12  Global Step: 70540   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:36,306-Speed 3884.10 samples/sec  Loss 2.6384  LearningRate 0.0507  ProxyLR: 2.5371  Epoch: 12  Global Step: 70550   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:38,941-Speed 3886.12 samples/sec  Loss 2.4631  LearningRate 0.0507  ProxyLR: 2.5364  Epoch: 12  Global Step: 70560   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:41,575-Speed 3889.08 samples/sec  Loss 2.5382  LearningRate 0.0507  ProxyLR: 2.5357  Epoch: 12  Global Step: 70570   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:54:44,196-Speed 3908.05 samples/sec  Loss 2.5932  LearningRate 0.0507  ProxyLR: 2.5349  Epoch: 12  Global Step: 70580   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:46,830-Speed 3888.11 samples/sec  Loss 2.5873  LearningRate 0.0507  ProxyLR: 2.5342  Epoch: 12  Global Step: 70590   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:49,466-Speed 3885.92 samples/sec  Loss 2.4416  LearningRate 0.0507  ProxyLR: 2.5335  Epoch: 12  Global Step: 70600   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:52,100-Speed 3888.81 samples/sec  Loss 2.5153  LearningRate 0.0507  ProxyLR: 2.5328  Epoch: 12  Global Step: 70610   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:54,735-Speed 3887.08 samples/sec  Loss 2.6032  LearningRate 0.0506  ProxyLR: 2.5321  Epoch: 12  Global Step: 70620   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:54:57,371-Speed 3886.52 samples/sec  Loss 2.5463  LearningRate 0.0506  ProxyLR: 2.5314  Epoch: 12  Global Step: 70630   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:00,005-Speed 3887.89 samples/sec  Loss 2.4443  LearningRate 0.0506  ProxyLR: 2.5307  Epoch: 12  Global Step: 70640   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:02,642-Speed 3884.40 samples/sec  Loss 2.5463  LearningRate 0.0506  ProxyLR: 2.5300  Epoch: 12  Global Step: 70650   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:05,280-Speed 3882.73 samples/sec  Loss 2.4448  LearningRate 0.0506  ProxyLR: 2.5293  Epoch: 12  Global Step: 70660   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:07,915-Speed 3886.89 samples/sec  Loss 2.4740  LearningRate 0.0506  ProxyLR: 2.5286  Epoch: 12  Global Step: 70670   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:10,551-Speed 3885.68 samples/sec  Loss 2.5012  LearningRate 0.0506  ProxyLR: 2.5279  Epoch: 12  Global Step: 70680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:55:13,188-Speed 3883.58 samples/sec  Loss 2.4940  LearningRate 0.0505  ProxyLR: 2.5272  Epoch: 12  Global Step: 70690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:55:15,824-Speed 3885.83 samples/sec  Loss 2.5184  LearningRate 0.0505  ProxyLR: 2.5265  Epoch: 12  Global Step: 70700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:55:18,458-Speed 3889.80 samples/sec  Loss 2.5196  LearningRate 0.0505  ProxyLR: 2.5257  Epoch: 12  Global Step: 70710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:55:21,078-Speed 3907.98 samples/sec  Loss 2.5049  LearningRate 0.0505  ProxyLR: 2.5250  Epoch: 12  Global Step: 70720   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:23,713-Speed 3888.07 samples/sec  Loss 2.5162  LearningRate 0.0505  ProxyLR: 2.5243  Epoch: 12  Global Step: 70730   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:26,346-Speed 3889.59 samples/sec  Loss 2.4906  LearningRate 0.0505  ProxyLR: 2.5236  Epoch: 12  Global Step: 70740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:28,977-Speed 3892.76 samples/sec  Loss 2.4353  LearningRate 0.0505  ProxyLR: 2.5229  Epoch: 12  Global Step: 70750   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:31,610-Speed 3890.22 samples/sec  Loss 2.4727  LearningRate 0.0504  ProxyLR: 2.5222  Epoch: 12  Global Step: 70760   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:55:34,228-Speed 3913.01 samples/sec  Loss 2.4520  LearningRate 0.0504  ProxyLR: 2.5215  Epoch: 12  Global Step: 70770   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:36,859-Speed 3893.00 samples/sec  Loss 2.5499  LearningRate 0.0504  ProxyLR: 2.5208  Epoch: 12  Global Step: 70780   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:39,488-Speed 3894.78 samples/sec  Loss 2.4980  LearningRate 0.0504  ProxyLR: 2.5201  Epoch: 12  Global Step: 70790   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:42,117-Speed 3896.44 samples/sec  Loss 2.4221  LearningRate 0.0504  ProxyLR: 2.5194  Epoch: 12  Global Step: 70800   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:44,746-Speed 3896.38 samples/sec  Loss 2.4863  LearningRate 0.0504  ProxyLR: 2.5187  Epoch: 12  Global Step: 70810   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:47,372-Speed 3900.14 samples/sec  Loss 2.5043  LearningRate 0.0504  ProxyLR: 2.5180  Epoch: 12  Global Step: 70820   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:50,000-Speed 3896.92 samples/sec  Loss 2.4611  LearningRate 0.0503  ProxyLR: 2.5173  Epoch: 12  Global Step: 70830   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:52,628-Speed 3897.36 samples/sec  Loss 2.5043  LearningRate 0.0503  ProxyLR: 2.5166  Epoch: 12  Global Step: 70840   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:55,257-Speed 3897.23 samples/sec  Loss 2.4132  LearningRate 0.0503  ProxyLR: 2.5159  Epoch: 12  Global Step: 70850   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:55:57,884-Speed 3898.76 samples/sec  Loss 2.4902  LearningRate 0.0503  ProxyLR: 2.5151  Epoch: 12  Global Step: 70860   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:56:00,514-Speed 3894.33 samples/sec  Loss 2.4290  LearningRate 0.0503  ProxyLR: 2.5144  Epoch: 12  Global Step: 70870   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:03,142-Speed 3897.63 samples/sec  Loss 2.3970  LearningRate 0.0503  ProxyLR: 2.5137  Epoch: 12  Global Step: 70880   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:05,770-Speed 3896.77 samples/sec  Loss 2.4132  LearningRate 0.0503  ProxyLR: 2.5130  Epoch: 12  Global Step: 70890   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:08,397-Speed 3898.90 samples/sec  Loss 2.3888  LearningRate 0.0502  ProxyLR: 2.5123  Epoch: 12  Global Step: 70900   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:11,027-Speed 3894.96 samples/sec  Loss 2.4488  LearningRate 0.0502  ProxyLR: 2.5116  Epoch: 12  Global Step: 70910   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:13,654-Speed 3899.00 samples/sec  Loss 2.5419  LearningRate 0.0502  ProxyLR: 2.5109  Epoch: 12  Global Step: 70920   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:16,282-Speed 3897.55 samples/sec  Loss 2.3954  LearningRate 0.0502  ProxyLR: 2.5102  Epoch: 12  Global Step: 70930   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:18,909-Speed 3899.39 samples/sec  Loss 2.4989  LearningRate 0.0502  ProxyLR: 2.5095  Epoch: 12  Global Step: 70940   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:21,537-Speed 3896.68 samples/sec  Loss 2.4302  LearningRate 0.0502  ProxyLR: 2.5088  Epoch: 12  Global Step: 70950   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:24,168-Speed 3893.06 samples/sec  Loss 2.4476  LearningRate 0.0502  ProxyLR: 2.5081  Epoch: 12  Global Step: 70960   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:26,797-Speed 3895.74 samples/sec  Loss 2.4406  LearningRate 0.0501  ProxyLR: 2.5074  Epoch: 12  Global Step: 70970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:56:29,412-Speed 3916.49 samples/sec  Loss 2.3794  LearningRate 0.0501  ProxyLR: 2.5067  Epoch: 12  Global Step: 70980   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:32,045-Speed 3891.53 samples/sec  Loss 2.4211  LearningRate 0.0501  ProxyLR: 2.5060  Epoch: 12  Global Step: 70990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:34,678-Speed 3890.00 samples/sec  Loss 2.3943  LearningRate 0.0501  ProxyLR: 2.5053  Epoch: 12  Global Step: 71000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:37,312-Speed 3887.31 samples/sec  Loss 2.3983  LearningRate 0.0501  ProxyLR: 2.5046  Epoch: 12  Global Step: 71010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:39,947-Speed 3887.89 samples/sec  Loss 2.4839  LearningRate 0.0501  ProxyLR: 2.5039  Epoch: 12  Global Step: 71020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:42,580-Speed 3890.44 samples/sec  Loss 2.3901  LearningRate 0.0501  ProxyLR: 2.5032  Epoch: 12  Global Step: 71030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:45,213-Speed 3890.42 samples/sec  Loss 2.3741  LearningRate 0.0500  ProxyLR: 2.5025  Epoch: 12  Global Step: 71040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:47,844-Speed 3893.22 samples/sec  Loss 2.4885  LearningRate 0.0500  ProxyLR: 2.5018  Epoch: 12  Global Step: 71050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:50,473-Speed 3895.01 samples/sec  Loss 2.3603  LearningRate 0.0500  ProxyLR: 2.5011  Epoch: 12  Global Step: 71060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:53,101-Speed 3897.73 samples/sec  Loss 2.4161  LearningRate 0.0500  ProxyLR: 2.5004  Epoch: 12  Global Step: 71070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:56:55,731-Speed 3894.40 samples/sec  Loss 2.3729  LearningRate 0.0500  ProxyLR: 2.4996  Epoch: 12  Global Step: 71080   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:56:58,348-Speed 3913.92 samples/sec  Loss 2.4376  LearningRate 0.0500  ProxyLR: 2.4989  Epoch: 12  Global Step: 71090   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:00,979-Speed 3892.58 samples/sec  Loss 2.4187  LearningRate 0.0500  ProxyLR: 2.4982  Epoch: 12  Global Step: 71100   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:03,608-Speed 3895.93 samples/sec  Loss 2.3881  LearningRate 0.0500  ProxyLR: 2.4975  Epoch: 12  Global Step: 71110   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:06,237-Speed 3896.56 samples/sec  Loss 2.3433  LearningRate 0.0499  ProxyLR: 2.4968  Epoch: 12  Global Step: 71120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:08,867-Speed 3894.18 samples/sec  Loss 2.3937  LearningRate 0.0499  ProxyLR: 2.4961  Epoch: 12  Global Step: 71130   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:11,497-Speed 3893.94 samples/sec  Loss 2.4664  LearningRate 0.0499  ProxyLR: 2.4954  Epoch: 12  Global Step: 71140   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:14,126-Speed 3896.28 samples/sec  Loss 2.3797  LearningRate 0.0499  ProxyLR: 2.4947  Epoch: 12  Global Step: 71150   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:16,758-Speed 3892.44 samples/sec  Loss 2.3356  LearningRate 0.0499  ProxyLR: 2.4940  Epoch: 12  Global Step: 71160   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:19,389-Speed 3892.81 samples/sec  Loss 2.4279  LearningRate 0.0499  ProxyLR: 2.4933  Epoch: 12  Global Step: 71170   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:22,018-Speed 3895.11 samples/sec  Loss 2.4251  LearningRate 0.0499  ProxyLR: 2.4926  Epoch: 12  Global Step: 71180   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:24,649-Speed 3893.31 samples/sec  Loss 2.3437  LearningRate 0.0498  ProxyLR: 2.4919  Epoch: 12  Global Step: 71190   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:57:27,263-Speed 3917.86 samples/sec  Loss 2.4269  LearningRate 0.0498  ProxyLR: 2.4912  Epoch: 12  Global Step: 71200   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:29,891-Speed 3898.04 samples/sec  Loss 2.4076  LearningRate 0.0498  ProxyLR: 2.4905  Epoch: 12  Global Step: 71210   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:32,523-Speed 3891.93 samples/sec  Loss 2.3374  LearningRate 0.0498  ProxyLR: 2.4898  Epoch: 12  Global Step: 71220   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:35,153-Speed 3895.20 samples/sec  Loss 2.4567  LearningRate 0.0498  ProxyLR: 2.4891  Epoch: 12  Global Step: 71230   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:37,780-Speed 3898.17 samples/sec  Loss 2.3104  LearningRate 0.0498  ProxyLR: 2.4884  Epoch: 12  Global Step: 71240   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:40,409-Speed 3895.75 samples/sec  Loss 2.4484  LearningRate 0.0498  ProxyLR: 2.4877  Epoch: 12  Global Step: 71250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:43,038-Speed 3896.00 samples/sec  Loss 2.3826  LearningRate 0.0497  ProxyLR: 2.4870  Epoch: 12  Global Step: 71260   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:45,667-Speed 3896.64 samples/sec  Loss 2.3315  LearningRate 0.0497  ProxyLR: 2.4863  Epoch: 12  Global Step: 71270   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:48,297-Speed 3894.40 samples/sec  Loss 2.4019  LearningRate 0.0497  ProxyLR: 2.4856  Epoch: 12  Global Step: 71280   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:50,927-Speed 3894.05 samples/sec  Loss 2.3557  LearningRate 0.0497  ProxyLR: 2.4849  Epoch: 12  Global Step: 71290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:57:53,558-Speed 3893.04 samples/sec  Loss 2.3164  LearningRate 0.0497  ProxyLR: 2.4842  Epoch: 12  Global Step: 71300   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:57:56,190-Speed 3891.36 samples/sec  Loss 2.3185  LearningRate 0.0497  ProxyLR: 2.4835  Epoch: 12  Global Step: 71310   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:57:58,807-Speed 3913.89 samples/sec  Loss 2.3111  LearningRate 0.0497  ProxyLR: 2.4828  Epoch: 12  Global Step: 71320   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:01,436-Speed 3896.13 samples/sec  Loss 2.3870  LearningRate 0.0496  ProxyLR: 2.4821  Epoch: 12  Global Step: 71330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:04,068-Speed 3891.47 samples/sec  Loss 2.3293  LearningRate 0.0496  ProxyLR: 2.4814  Epoch: 12  Global Step: 71340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:06,699-Speed 3891.90 samples/sec  Loss 2.4141  LearningRate 0.0496  ProxyLR: 2.4807  Epoch: 12  Global Step: 71350   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:09,331-Speed 3892.12 samples/sec  Loss 2.3610  LearningRate 0.0496  ProxyLR: 2.4800  Epoch: 12  Global Step: 71360   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:11,961-Speed 3893.90 samples/sec  Loss 2.3324  LearningRate 0.0496  ProxyLR: 2.4793  Epoch: 12  Global Step: 71370   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:14,594-Speed 3890.13 samples/sec  Loss 2.4735  LearningRate 0.0496  ProxyLR: 2.4786  Epoch: 12  Global Step: 71380   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:17,226-Speed 3891.50 samples/sec  Loss 2.3024  LearningRate 0.0496  ProxyLR: 2.4779  Epoch: 12  Global Step: 71390   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:19,858-Speed 3892.56 samples/sec  Loss 2.3609  LearningRate 0.0495  ProxyLR: 2.4772  Epoch: 12  Global Step: 71400   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:22,488-Speed 3894.46 samples/sec  Loss 2.3272  LearningRate 0.0495  ProxyLR: 2.4765  Epoch: 12  Global Step: 71410   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:25,118-Speed 3893.95 samples/sec  Loss 2.3910  LearningRate 0.0495  ProxyLR: 2.4758  Epoch: 12  Global Step: 71420   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:27,752-Speed 3888.42 samples/sec  Loss 2.3306  LearningRate 0.0495  ProxyLR: 2.4751  Epoch: 12  Global Step: 71430   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:30,382-Speed 3894.80 samples/sec  Loss 2.3567  LearningRate 0.0495  ProxyLR: 2.4744  Epoch: 12  Global Step: 71440   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:33,014-Speed 3891.27 samples/sec  Loss 2.3151  LearningRate 0.0495  ProxyLR: 2.4737  Epoch: 12  Global Step: 71450   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:35,646-Speed 3892.55 samples/sec  Loss 2.3328  LearningRate 0.0495  ProxyLR: 2.4730  Epoch: 12  Global Step: 71460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:38,278-Speed 3890.95 samples/sec  Loss 2.2798  LearningRate 0.0494  ProxyLR: 2.4723  Epoch: 12  Global Step: 71470   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:40,909-Speed 3892.69 samples/sec  Loss 2.3241  LearningRate 0.0494  ProxyLR: 2.4716  Epoch: 12  Global Step: 71480   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:43,539-Speed 3894.48 samples/sec  Loss 2.3069  LearningRate 0.0494  ProxyLR: 2.4709  Epoch: 12  Global Step: 71490   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:46,170-Speed 3892.57 samples/sec  Loss 2.2720  LearningRate 0.0494  ProxyLR: 2.4702  Epoch: 12  Global Step: 71500   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:58:48,789-Speed 3911.06 samples/sec  Loss 2.2929  LearningRate 0.0494  ProxyLR: 2.4695  Epoch: 12  Global Step: 71510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:51,419-Speed 3894.39 samples/sec  Loss 2.2769  LearningRate 0.0494  ProxyLR: 2.4688  Epoch: 12  Global Step: 71520   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:54,051-Speed 3892.48 samples/sec  Loss 2.3504  LearningRate 0.0494  ProxyLR: 2.4681  Epoch: 12  Global Step: 71530   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:56,680-Speed 3894.71 samples/sec  Loss 2.2882  LearningRate 0.0493  ProxyLR: 2.4674  Epoch: 12  Global Step: 71540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:58:59,310-Speed 3894.88 samples/sec  Loss 2.2664  LearningRate 0.0493  ProxyLR: 2.4667  Epoch: 12  Global Step: 71550   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:01,941-Speed 3893.59 samples/sec  Loss 2.3094  LearningRate 0.0493  ProxyLR: 2.4660  Epoch: 12  Global Step: 71560   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:04,571-Speed 3893.93 samples/sec  Loss 2.2079  LearningRate 0.0493  ProxyLR: 2.4653  Epoch: 12  Global Step: 71570   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:07,201-Speed 3895.16 samples/sec  Loss 2.2591  LearningRate 0.0493  ProxyLR: 2.4646  Epoch: 12  Global Step: 71580   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:09,832-Speed 3892.61 samples/sec  Loss 2.3576  LearningRate 0.0493  ProxyLR: 2.4639  Epoch: 12  Global Step: 71590   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:12,460-Speed 3897.54 samples/sec  Loss 2.2271  LearningRate 0.0493  ProxyLR: 2.4632  Epoch: 12  Global Step: 71600   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:15,090-Speed 3895.16 samples/sec  Loss 2.2984  LearningRate 0.0493  ProxyLR: 2.4625  Epoch: 12  Global Step: 71610   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:59:17,721-Speed 3892.37 samples/sec  Loss 2.3512  LearningRate 0.0492  ProxyLR: 2.4618  Epoch: 12  Global Step: 71620   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:59:20,352-Speed 3893.88 samples/sec  Loss 2.3311  LearningRate 0.0492  ProxyLR: 2.4611  Epoch: 12  Global Step: 71630   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 19:59:22,956-Speed 3933.50 samples/sec  Loss 2.2287  LearningRate 0.0492  ProxyLR: 2.4604  Epoch: 12  Global Step: 71640   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:25,585-Speed 3895.38 samples/sec  Loss 2.2968  LearningRate 0.0492  ProxyLR: 2.4597  Epoch: 12  Global Step: 71650   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:28,214-Speed 3896.24 samples/sec  Loss 2.3519  LearningRate 0.0492  ProxyLR: 2.4590  Epoch: 12  Global Step: 71660   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:30,843-Speed 3895.36 samples/sec  Loss 2.3690  LearningRate 0.0492  ProxyLR: 2.4583  Epoch: 12  Global Step: 71670   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:33,474-Speed 3892.65 samples/sec  Loss 2.2917  LearningRate 0.0492  ProxyLR: 2.4576  Epoch: 12  Global Step: 71680   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:36,102-Speed 3897.76 samples/sec  Loss 2.3156  LearningRate 0.0491  ProxyLR: 2.4569  Epoch: 12  Global Step: 71690   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:38,731-Speed 3896.21 samples/sec  Loss 2.2576  LearningRate 0.0491  ProxyLR: 2.4562  Epoch: 12  Global Step: 71700   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:41,359-Speed 3896.74 samples/sec  Loss 2.3182  LearningRate 0.0491  ProxyLR: 2.4555  Epoch: 12  Global Step: 71710   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:43,989-Speed 3895.76 samples/sec  Loss 2.3193  LearningRate 0.0491  ProxyLR: 2.4548  Epoch: 12  Global Step: 71720   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:46,617-Speed 3897.01 samples/sec  Loss 2.1936  LearningRate 0.0491  ProxyLR: 2.4541  Epoch: 12  Global Step: 71730   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 19:59:49,248-Speed 3893.47 samples/sec  Loss 2.3211  LearningRate 0.0491  ProxyLR: 2.4534  Epoch: 12  Global Step: 71740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:51,875-Speed 3899.00 samples/sec  Loss 2.2540  LearningRate 0.0491  ProxyLR: 2.4527  Epoch: 12  Global Step: 71750   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:54,502-Speed 3898.32 samples/sec  Loss 2.2767  LearningRate 0.0490  ProxyLR: 2.4520  Epoch: 12  Global Step: 71760   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:57,130-Speed 3897.19 samples/sec  Loss 2.3340  LearningRate 0.0490  ProxyLR: 2.4513  Epoch: 12  Global Step: 71770   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 19:59:59,757-Speed 3899.33 samples/sec  Loss 2.3323  LearningRate 0.0490  ProxyLR: 2.4507  Epoch: 12  Global Step: 71780   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:02,369-Speed 3920.76 samples/sec  Loss 2.2550  LearningRate 0.0490  ProxyLR: 2.4500  Epoch: 12  Global Step: 71790   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:04,996-Speed 3899.33 samples/sec  Loss 2.2626  LearningRate 0.0490  ProxyLR: 2.4493  Epoch: 12  Global Step: 71800   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:07,626-Speed 3895.32 samples/sec  Loss 2.2291  LearningRate 0.0490  ProxyLR: 2.4486  Epoch: 12  Global Step: 71810   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:10,256-Speed 3893.61 samples/sec  Loss 2.2453  LearningRate 0.0490  ProxyLR: 2.4479  Epoch: 12  Global Step: 71820   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:12,886-Speed 3894.58 samples/sec  Loss 2.2077  LearningRate 0.0489  ProxyLR: 2.4472  Epoch: 12  Global Step: 71830   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:15,519-Speed 3889.86 samples/sec  Loss 2.2647  LearningRate 0.0489  ProxyLR: 2.4465  Epoch: 12  Global Step: 71840   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:18,146-Speed 3899.06 samples/sec  Loss 2.2318  LearningRate 0.0489  ProxyLR: 2.4458  Epoch: 12  Global Step: 71850   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:20,781-Speed 3888.02 samples/sec  Loss 2.2593  LearningRate 0.0489  ProxyLR: 2.4451  Epoch: 12  Global Step: 71860   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:23,415-Speed 3888.74 samples/sec  Loss 2.2522  LearningRate 0.0489  ProxyLR: 2.4444  Epoch: 12  Global Step: 71870   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:26,048-Speed 3889.23 samples/sec  Loss 2.2981  LearningRate 0.0489  ProxyLR: 2.4437  Epoch: 12  Global Step: 71880   Fp16 Grad Scale: 131072  Required: 6 hours
Training: 2023-05-04 20:00:28,680-Speed 3891.95 samples/sec  Loss 2.2435  LearningRate 0.0489  ProxyLR: 2.4430  Epoch: 12  Global Step: 71890   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:31,315-Speed 3886.69 samples/sec  Loss 2.2541  LearningRate 0.0488  ProxyLR: 2.4423  Epoch: 12  Global Step: 71900   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:33,949-Speed 3888.60 samples/sec  Loss 2.2715  LearningRate 0.0488  ProxyLR: 2.4416  Epoch: 12  Global Step: 71910   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:36,584-Speed 3887.09 samples/sec  Loss 2.3140  LearningRate 0.0488  ProxyLR: 2.4409  Epoch: 12  Global Step: 71920   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:39,217-Speed 3889.51 samples/sec  Loss 2.2286  LearningRate 0.0488  ProxyLR: 2.4402  Epoch: 12  Global Step: 71930   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:41,853-Speed 3885.96 samples/sec  Loss 2.2352  LearningRate 0.0488  ProxyLR: 2.4395  Epoch: 12  Global Step: 71940   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:44,486-Speed 3889.96 samples/sec  Loss 2.2258  LearningRate 0.0488  ProxyLR: 2.4388  Epoch: 12  Global Step: 71950   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:47,122-Speed 3886.50 samples/sec  Loss 2.2423  LearningRate 0.0488  ProxyLR: 2.4381  Epoch: 12  Global Step: 71960   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:49,755-Speed 3889.20 samples/sec  Loss 2.2508  LearningRate 0.0487  ProxyLR: 2.4374  Epoch: 12  Global Step: 71970   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:52,387-Speed 3891.87 samples/sec  Loss 2.1700  LearningRate 0.0487  ProxyLR: 2.4367  Epoch: 12  Global Step: 71980   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:00:55,019-Speed 3891.31 samples/sec  Loss 2.2766  LearningRate 0.0487  ProxyLR: 2.4360  Epoch: 12  Global Step: 71990   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:00:57,639-Speed 3910.67 samples/sec  Loss 2.1802  LearningRate 0.0487  ProxyLR: 2.4354  Epoch: 12  Global Step: 72000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:00,272-Speed 3889.49 samples/sec  Loss 2.3138  LearningRate 0.0487  ProxyLR: 2.4347  Epoch: 12  Global Step: 72010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:02,905-Speed 3890.54 samples/sec  Loss 2.2636  LearningRate 0.0487  ProxyLR: 2.4340  Epoch: 12  Global Step: 72020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:05,538-Speed 3889.66 samples/sec  Loss 2.2361  LearningRate 0.0487  ProxyLR: 2.4333  Epoch: 12  Global Step: 72030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:08,171-Speed 3889.68 samples/sec  Loss 2.2033  LearningRate 0.0487  ProxyLR: 2.4326  Epoch: 12  Global Step: 72040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:10,804-Speed 3890.18 samples/sec  Loss 2.2002  LearningRate 0.0486  ProxyLR: 2.4319  Epoch: 12  Global Step: 72050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:13,438-Speed 3889.16 samples/sec  Loss 2.2107  LearningRate 0.0486  ProxyLR: 2.4312  Epoch: 12  Global Step: 72060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:16,071-Speed 3889.45 samples/sec  Loss 2.1727  LearningRate 0.0486  ProxyLR: 2.4305  Epoch: 12  Global Step: 72070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:18,706-Speed 3887.57 samples/sec  Loss 2.2005  LearningRate 0.0486  ProxyLR: 2.4298  Epoch: 12  Global Step: 72080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:21,338-Speed 3890.84 samples/sec  Loss 2.1580  LearningRate 0.0486  ProxyLR: 2.4291  Epoch: 12  Global Step: 72090   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:23,972-Speed 3888.50 samples/sec  Loss 2.2384  LearningRate 0.0486  ProxyLR: 2.4284  Epoch: 12  Global Step: 72100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:01:26,593-Speed 3908.00 samples/sec  Loss 2.1651  LearningRate 0.0486  ProxyLR: 2.4277  Epoch: 12  Global Step: 72110   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:29,224-Speed 3893.06 samples/sec  Loss 2.1929  LearningRate 0.0485  ProxyLR: 2.4270  Epoch: 12  Global Step: 72120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:31,855-Speed 3892.64 samples/sec  Loss 2.1665  LearningRate 0.0485  ProxyLR: 2.4263  Epoch: 12  Global Step: 72130   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:34,486-Speed 3893.73 samples/sec  Loss 2.2336  LearningRate 0.0485  ProxyLR: 2.4256  Epoch: 12  Global Step: 72140   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:37,119-Speed 3890.31 samples/sec  Loss 2.2403  LearningRate 0.0485  ProxyLR: 2.4249  Epoch: 12  Global Step: 72150   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:39,753-Speed 3888.83 samples/sec  Loss 2.2653  LearningRate 0.0485  ProxyLR: 2.4243  Epoch: 12  Global Step: 72160   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:42,384-Speed 3892.79 samples/sec  Loss 2.1785  LearningRate 0.0485  ProxyLR: 2.4236  Epoch: 12  Global Step: 72170   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:45,016-Speed 3890.46 samples/sec  Loss 2.2652  LearningRate 0.0485  ProxyLR: 2.4229  Epoch: 12  Global Step: 72180   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:47,652-Speed 3886.06 samples/sec  Loss 2.2303  LearningRate 0.0484  ProxyLR: 2.4222  Epoch: 12  Global Step: 72190   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:50,284-Speed 3891.67 samples/sec  Loss 2.1739  LearningRate 0.0484  ProxyLR: 2.4215  Epoch: 12  Global Step: 72200   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:01:52,918-Speed 3888.40 samples/sec  Loss 2.2117  LearningRate 0.0484  ProxyLR: 2.4208  Epoch: 12  Global Step: 72210   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:01:55,549-Speed 3893.74 samples/sec  Loss 2.2737  LearningRate 0.0484  ProxyLR: 2.4201  Epoch: 12  Global Step: 72220   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:01:58,181-Speed 3891.46 samples/sec  Loss 2.3122  LearningRate 0.0484  ProxyLR: 2.4194  Epoch: 12  Global Step: 72230   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:00,815-Speed 3887.95 samples/sec  Loss 2.1528  LearningRate 0.0484  ProxyLR: 2.4187  Epoch: 12  Global Step: 72240   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:03,434-Speed 3911.30 samples/sec  Loss 2.2972  LearningRate 0.0484  ProxyLR: 2.4180  Epoch: 12  Global Step: 72250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:06,066-Speed 3891.32 samples/sec  Loss 2.1662  LearningRate 0.0483  ProxyLR: 2.4173  Epoch: 12  Global Step: 72260   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:08,697-Speed 3893.40 samples/sec  Loss 2.2033  LearningRate 0.0483  ProxyLR: 2.4166  Epoch: 12  Global Step: 72270   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:11,330-Speed 3890.44 samples/sec  Loss 2.1701  LearningRate 0.0483  ProxyLR: 2.4159  Epoch: 12  Global Step: 72280   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:13,961-Speed 3892.25 samples/sec  Loss 2.2011  LearningRate 0.0483  ProxyLR: 2.4153  Epoch: 12  Global Step: 72290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:16,593-Speed 3891.35 samples/sec  Loss 2.1742  LearningRate 0.0483  ProxyLR: 2.4146  Epoch: 12  Global Step: 72300   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:19,226-Speed 3891.02 samples/sec  Loss 2.1903  LearningRate 0.0483  ProxyLR: 2.4139  Epoch: 12  Global Step: 72310   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:21,859-Speed 3889.63 samples/sec  Loss 2.2294  LearningRate 0.0483  ProxyLR: 2.4132  Epoch: 12  Global Step: 72320   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:24,493-Speed 3888.74 samples/sec  Loss 2.2200  LearningRate 0.0482  ProxyLR: 2.4125  Epoch: 12  Global Step: 72330   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:27,125-Speed 3891.65 samples/sec  Loss 2.2305  LearningRate 0.0482  ProxyLR: 2.4118  Epoch: 12  Global Step: 72340   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:02:29,755-Speed 3893.82 samples/sec  Loss 2.1659  LearningRate 0.0482  ProxyLR: 2.4111  Epoch: 12  Global Step: 72350   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:32,389-Speed 3889.08 samples/sec  Loss 2.1764  LearningRate 0.0482  ProxyLR: 2.4104  Epoch: 12  Global Step: 72360   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:35,021-Speed 3891.37 samples/sec  Loss 2.1485  LearningRate 0.0482  ProxyLR: 2.4097  Epoch: 12  Global Step: 72370   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:37,653-Speed 3891.94 samples/sec  Loss 2.1940  LearningRate 0.0482  ProxyLR: 2.4090  Epoch: 12  Global Step: 72380   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:40,285-Speed 3891.08 samples/sec  Loss 2.1446  LearningRate 0.0482  ProxyLR: 2.4083  Epoch: 12  Global Step: 72390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:42,919-Speed 3888.93 samples/sec  Loss 2.2264  LearningRate 0.0482  ProxyLR: 2.4077  Epoch: 12  Global Step: 72400   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:45,553-Speed 3887.73 samples/sec  Loss 2.1640  LearningRate 0.0481  ProxyLR: 2.4070  Epoch: 12  Global Step: 72410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:48,188-Speed 3887.10 samples/sec  Loss 2.1442  LearningRate 0.0481  ProxyLR: 2.4063  Epoch: 12  Global Step: 72420   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:50,824-Speed 3886.00 samples/sec  Loss 2.1266  LearningRate 0.0481  ProxyLR: 2.4056  Epoch: 12  Global Step: 72430   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:53,460-Speed 3885.84 samples/sec  Loss 2.1238  LearningRate 0.0481  ProxyLR: 2.4049  Epoch: 12  Global Step: 72440   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:02:56,096-Speed 3885.69 samples/sec  Loss 2.2098  LearningRate 0.0481  ProxyLR: 2.4042  Epoch: 12  Global Step: 72450   Fp16 Grad Scale: 1048576  Required: 6 hours
Training: 2023-05-04 20:02:58,718-Speed 3906.76 samples/sec  Loss 2.1993  LearningRate 0.0481  ProxyLR: 2.4035  Epoch: 12  Global Step: 72460   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:03:01,353-Speed 3886.95 samples/sec  Loss 2.1324  LearningRate 0.0481  ProxyLR: 2.4028  Epoch: 12  Global Step: 72470   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:03:03,980-Speed 3899.30 samples/sec  Loss 2.2000  LearningRate 0.0480  ProxyLR: 2.4021  Epoch: 12  Global Step: 72480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:06,616-Speed 3885.72 samples/sec  Loss 2.1147  LearningRate 0.0480  ProxyLR: 2.4014  Epoch: 12  Global Step: 72490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:09,251-Speed 3887.38 samples/sec  Loss 2.1756  LearningRate 0.0480  ProxyLR: 2.4008  Epoch: 12  Global Step: 72500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:11,888-Speed 3884.36 samples/sec  Loss 2.1995  LearningRate 0.0480  ProxyLR: 2.4001  Epoch: 12  Global Step: 72510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:14,523-Speed 3886.65 samples/sec  Loss 2.1795  LearningRate 0.0480  ProxyLR: 2.3994  Epoch: 12  Global Step: 72520   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:17,159-Speed 3885.80 samples/sec  Loss 2.1494  LearningRate 0.0480  ProxyLR: 2.3987  Epoch: 12  Global Step: 72530   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:19,793-Speed 3888.38 samples/sec  Loss 2.1901  LearningRate 0.0480  ProxyLR: 2.3980  Epoch: 12  Global Step: 72540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:22,428-Speed 3887.48 samples/sec  Loss 2.1146  LearningRate 0.0479  ProxyLR: 2.3973  Epoch: 12  Global Step: 72550   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:25,062-Speed 3887.72 samples/sec  Loss 2.1578  LearningRate 0.0479  ProxyLR: 2.3966  Epoch: 12  Global Step: 72560   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:27,699-Speed 3884.22 samples/sec  Loss 2.1694  LearningRate 0.0479  ProxyLR: 2.3959  Epoch: 12  Global Step: 72570   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:30,332-Speed 3890.49 samples/sec  Loss 2.1617  LearningRate 0.0479  ProxyLR: 2.3952  Epoch: 12  Global Step: 72580   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:03:32,951-Speed 3910.27 samples/sec  Loss 2.1571  LearningRate 0.0479  ProxyLR: 2.3946  Epoch: 12  Global Step: 72590   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:35,586-Speed 3887.21 samples/sec  Loss 2.0975  LearningRate 0.0479  ProxyLR: 2.3939  Epoch: 12  Global Step: 72600   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:38,221-Speed 3887.68 samples/sec  Loss 2.1210  LearningRate 0.0479  ProxyLR: 2.3932  Epoch: 12  Global Step: 72610   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:40,855-Speed 3888.14 samples/sec  Loss 2.2072  LearningRate 0.0478  ProxyLR: 2.3925  Epoch: 12  Global Step: 72620   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:43,490-Speed 3887.51 samples/sec  Loss 2.1845  LearningRate 0.0478  ProxyLR: 2.3918  Epoch: 12  Global Step: 72630   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:46,124-Speed 3888.13 samples/sec  Loss 2.1607  LearningRate 0.0478  ProxyLR: 2.3911  Epoch: 12  Global Step: 72640   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:48,760-Speed 3885.51 samples/sec  Loss 2.2358  LearningRate 0.0478  ProxyLR: 2.3904  Epoch: 12  Global Step: 72650   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:51,394-Speed 3888.88 samples/sec  Loss 2.1411  LearningRate 0.0478  ProxyLR: 2.3897  Epoch: 12  Global Step: 72660   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:54,027-Speed 3890.19 samples/sec  Loss 2.1008  LearningRate 0.0478  ProxyLR: 2.3891  Epoch: 12  Global Step: 72670   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:56,662-Speed 3887.60 samples/sec  Loss 2.1028  LearningRate 0.0478  ProxyLR: 2.3884  Epoch: 12  Global Step: 72680   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:03:59,296-Speed 3887.37 samples/sec  Loss 2.1950  LearningRate 0.0478  ProxyLR: 2.3877  Epoch: 12  Global Step: 72690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:04:01,928-Speed 3892.08 samples/sec  Loss 2.1667  LearningRate 0.0477  ProxyLR: 2.3870  Epoch: 12  Global Step: 72700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:04:04,563-Speed 3886.72 samples/sec  Loss 2.1715  LearningRate 0.0477  ProxyLR: 2.3863  Epoch: 12  Global Step: 72710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:04:07,184-Speed 3908.45 samples/sec  Loss 2.1429  LearningRate 0.0477  ProxyLR: 2.3856  Epoch: 12  Global Step: 72720   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:09,818-Speed 3887.90 samples/sec  Loss 2.1057  LearningRate 0.0477  ProxyLR: 2.3849  Epoch: 12  Global Step: 72730   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:12,452-Speed 3889.26 samples/sec  Loss 2.1403  LearningRate 0.0477  ProxyLR: 2.3842  Epoch: 12  Global Step: 72740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:15,086-Speed 3888.91 samples/sec  Loss 2.0762  LearningRate 0.0477  ProxyLR: 2.3836  Epoch: 12  Global Step: 72750   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:17,722-Speed 3885.18 samples/sec  Loss 2.2098  LearningRate 0.0477  ProxyLR: 2.3829  Epoch: 12  Global Step: 72760   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:20,357-Speed 3886.83 samples/sec  Loss 2.0760  LearningRate 0.0476  ProxyLR: 2.3822  Epoch: 12  Global Step: 72770   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:22,989-Speed 3892.00 samples/sec  Loss 2.0961  LearningRate 0.0476  ProxyLR: 2.3815  Epoch: 12  Global Step: 72780   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:25,621-Speed 3890.19 samples/sec  Loss 2.1665  LearningRate 0.0476  ProxyLR: 2.3808  Epoch: 12  Global Step: 72790   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:28,252-Speed 3893.95 samples/sec  Loss 2.1589  LearningRate 0.0476  ProxyLR: 2.3801  Epoch: 12  Global Step: 72800   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:30,884-Speed 3891.13 samples/sec  Loss 2.1712  LearningRate 0.0476  ProxyLR: 2.3794  Epoch: 12  Global Step: 72810   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:33,518-Speed 3888.83 samples/sec  Loss 2.1983  LearningRate 0.0476  ProxyLR: 2.3787  Epoch: 12  Global Step: 72820   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:04:36,134-Speed 3914.63 samples/sec  Loss 2.1075  LearningRate 0.0476  ProxyLR: 2.3781  Epoch: 12  Global Step: 72830   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:38,765-Speed 3893.88 samples/sec  Loss 2.0521  LearningRate 0.0475  ProxyLR: 2.3774  Epoch: 12  Global Step: 72840   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:41,394-Speed 3896.17 samples/sec  Loss 2.0960  LearningRate 0.0475  ProxyLR: 2.3767  Epoch: 12  Global Step: 72850   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:44,023-Speed 3895.91 samples/sec  Loss 2.1353  LearningRate 0.0475  ProxyLR: 2.3760  Epoch: 12  Global Step: 72860   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:46,653-Speed 3894.14 samples/sec  Loss 2.0824  LearningRate 0.0475  ProxyLR: 2.3753  Epoch: 12  Global Step: 72870   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:49,285-Speed 3892.29 samples/sec  Loss 2.1736  LearningRate 0.0475  ProxyLR: 2.3746  Epoch: 12  Global Step: 72880   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:51,916-Speed 3892.64 samples/sec  Loss 2.1095  LearningRate 0.0475  ProxyLR: 2.3739  Epoch: 12  Global Step: 72890   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:54,546-Speed 3894.16 samples/sec  Loss 2.0903  LearningRate 0.0475  ProxyLR: 2.3733  Epoch: 12  Global Step: 72900   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:57,179-Speed 3889.95 samples/sec  Loss 2.1724  LearningRate 0.0475  ProxyLR: 2.3726  Epoch: 12  Global Step: 72910   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:04:59,825-Speed 3870.91 samples/sec  Loss 2.2010  LearningRate 0.0474  ProxyLR: 2.3719  Epoch: 12  Global Step: 72920   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:02,463-Speed 3883.66 samples/sec  Loss 2.1081  LearningRate 0.0474  ProxyLR: 2.3712  Epoch: 12  Global Step: 72930   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:05,099-Speed 3885.66 samples/sec  Loss 2.1636  LearningRate 0.0474  ProxyLR: 2.3705  Epoch: 12  Global Step: 72940   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:07,734-Speed 3885.86 samples/sec  Loss 2.1260  LearningRate 0.0474  ProxyLR: 2.3698  Epoch: 12  Global Step: 72950   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:10,369-Speed 3887.58 samples/sec  Loss 2.0749  LearningRate 0.0474  ProxyLR: 2.3692  Epoch: 12  Global Step: 72960   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:13,004-Speed 3888.04 samples/sec  Loss 2.1391  LearningRate 0.0474  ProxyLR: 2.3685  Epoch: 12  Global Step: 72970   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:15,636-Speed 3890.64 samples/sec  Loss 2.1683  LearningRate 0.0474  ProxyLR: 2.3678  Epoch: 12  Global Step: 72980   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:18,254-Speed 3912.39 samples/sec  Loss 2.1685  LearningRate 0.0473  ProxyLR: 2.3671  Epoch: 12  Global Step: 72990   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:20,888-Speed 3888.14 samples/sec  Loss 2.1090  LearningRate 0.0473  ProxyLR: 2.3664  Epoch: 12  Global Step: 73000   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:23,519-Speed 3893.51 samples/sec  Loss 2.1955  LearningRate 0.0473  ProxyLR: 2.3657  Epoch: 12  Global Step: 73010   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:26,151-Speed 3891.34 samples/sec  Loss 2.1652  LearningRate 0.0473  ProxyLR: 2.3650  Epoch: 12  Global Step: 73020   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:28,783-Speed 3891.13 samples/sec  Loss 2.1203  LearningRate 0.0473  ProxyLR: 2.3644  Epoch: 12  Global Step: 73030   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:31,415-Speed 3892.20 samples/sec  Loss 2.1084  LearningRate 0.0473  ProxyLR: 2.3637  Epoch: 12  Global Step: 73040   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:34,048-Speed 3890.59 samples/sec  Loss 2.0451  LearningRate 0.0473  ProxyLR: 2.3630  Epoch: 12  Global Step: 73050   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:36,679-Speed 3892.18 samples/sec  Loss 2.1137  LearningRate 0.0472  ProxyLR: 2.3623  Epoch: 12  Global Step: 73060   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:39,310-Speed 3892.90 samples/sec  Loss 2.1904  LearningRate 0.0472  ProxyLR: 2.3616  Epoch: 12  Global Step: 73070   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:41,941-Speed 3893.02 samples/sec  Loss 2.1121  LearningRate 0.0472  ProxyLR: 2.3609  Epoch: 12  Global Step: 73080   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:44,571-Speed 3894.87 samples/sec  Loss 2.1077  LearningRate 0.0472  ProxyLR: 2.3603  Epoch: 12  Global Step: 73090   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:47,203-Speed 3891.66 samples/sec  Loss 2.0652  LearningRate 0.0472  ProxyLR: 2.3596  Epoch: 12  Global Step: 73100   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:49,836-Speed 3890.65 samples/sec  Loss 2.0816  LearningRate 0.0472  ProxyLR: 2.3589  Epoch: 12  Global Step: 73110   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:05:52,459-Speed 3904.85 samples/sec  Loss 2.1913  LearningRate 0.0472  ProxyLR: 2.3582  Epoch: 12  Global Step: 73120   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:55,093-Speed 3887.62 samples/sec  Loss 2.1426  LearningRate 0.0472  ProxyLR: 2.3575  Epoch: 12  Global Step: 73130   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:05:57,730-Speed 3884.64 samples/sec  Loss 2.1423  LearningRate 0.0471  ProxyLR: 2.3568  Epoch: 12  Global Step: 73140   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:00,366-Speed 3885.16 samples/sec  Loss 2.1606  LearningRate 0.0471  ProxyLR: 2.3562  Epoch: 12  Global Step: 73150   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:02,999-Speed 3890.54 samples/sec  Loss 2.0852  LearningRate 0.0471  ProxyLR: 2.3555  Epoch: 12  Global Step: 73160   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:05,636-Speed 3883.28 samples/sec  Loss 2.0940  LearningRate 0.0471  ProxyLR: 2.3548  Epoch: 12  Global Step: 73170   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:08,271-Speed 3888.32 samples/sec  Loss 2.1752  LearningRate 0.0471  ProxyLR: 2.3541  Epoch: 12  Global Step: 73180   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:10,907-Speed 3885.00 samples/sec  Loss 2.0620  LearningRate 0.0471  ProxyLR: 2.3534  Epoch: 12  Global Step: 73190   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:13,542-Speed 3887.53 samples/sec  Loss 2.1476  LearningRate 0.0471  ProxyLR: 2.3527  Epoch: 12  Global Step: 73200   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:16,178-Speed 3884.91 samples/sec  Loss 2.1527  LearningRate 0.0470  ProxyLR: 2.3521  Epoch: 12  Global Step: 73210   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:18,802-Speed 3903.13 samples/sec  Loss 2.1092  LearningRate 0.0470  ProxyLR: 2.3514  Epoch: 12  Global Step: 73220   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:21,437-Speed 3886.56 samples/sec  Loss 2.0989  LearningRate 0.0470  ProxyLR: 2.3507  Epoch: 12  Global Step: 73230   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:24,071-Speed 3888.79 samples/sec  Loss 2.0432  LearningRate 0.0470  ProxyLR: 2.3500  Epoch: 12  Global Step: 73240   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:26,706-Speed 3886.87 samples/sec  Loss 2.0757  LearningRate 0.0470  ProxyLR: 2.3493  Epoch: 12  Global Step: 73250   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:29,341-Speed 3887.63 samples/sec  Loss 2.0667  LearningRate 0.0470  ProxyLR: 2.3487  Epoch: 12  Global Step: 73260   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:31,978-Speed 3884.68 samples/sec  Loss 2.0738  LearningRate 0.0470  ProxyLR: 2.3480  Epoch: 12  Global Step: 73270   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:34,612-Speed 3888.63 samples/sec  Loss 2.0730  LearningRate 0.0469  ProxyLR: 2.3473  Epoch: 12  Global Step: 73280   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:37,244-Speed 3890.44 samples/sec  Loss 2.1075  LearningRate 0.0469  ProxyLR: 2.3466  Epoch: 12  Global Step: 73290   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:39,880-Speed 3886.73 samples/sec  Loss 2.0933  LearningRate 0.0469  ProxyLR: 2.3459  Epoch: 12  Global Step: 73300   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:42,513-Speed 3889.60 samples/sec  Loss 2.1597  LearningRate 0.0469  ProxyLR: 2.3452  Epoch: 12  Global Step: 73310   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:06:45,146-Speed 3890.30 samples/sec  Loss 2.1169  LearningRate 0.0469  ProxyLR: 2.3446  Epoch: 12  Global Step: 73320   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:06:47,781-Speed 3886.63 samples/sec  Loss 2.0965  LearningRate 0.0469  ProxyLR: 2.3439  Epoch: 12  Global Step: 73330   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:06:50,415-Speed 3888.55 samples/sec  Loss 2.1787  LearningRate 0.0469  ProxyLR: 2.3432  Epoch: 12  Global Step: 73340   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:06:53,052-Speed 3885.14 samples/sec  Loss 2.0892  LearningRate 0.0469  ProxyLR: 2.3425  Epoch: 12  Global Step: 73350   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:06:55,686-Speed 3888.14 samples/sec  Loss 2.0824  LearningRate 0.0468  ProxyLR: 2.3418  Epoch: 12  Global Step: 73360   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:06:58,319-Speed 3889.41 samples/sec  Loss 2.0805  LearningRate 0.0468  ProxyLR: 2.3412  Epoch: 12  Global Step: 73370   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:07:00,954-Speed 3887.01 samples/sec  Loss 2.0495  LearningRate 0.0468  ProxyLR: 2.3405  Epoch: 12  Global Step: 73380   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:07:03,587-Speed 3890.10 samples/sec  Loss 2.1237  LearningRate 0.0468  ProxyLR: 2.3398  Epoch: 12  Global Step: 73390   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:07:06,220-Speed 3890.92 samples/sec  Loss 2.0913  LearningRate 0.0468  ProxyLR: 2.3391  Epoch: 12  Global Step: 73400   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:07:08,852-Speed 3890.33 samples/sec  Loss 2.1165  LearningRate 0.0468  ProxyLR: 2.3384  Epoch: 12  Global Step: 73410   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:07:11,471-Speed 3911.07 samples/sec  Loss 2.0832  LearningRate 0.0468  ProxyLR: 2.3378  Epoch: 12  Global Step: 73420   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:14,103-Speed 3891.78 samples/sec  Loss 2.0842  LearningRate 0.0467  ProxyLR: 2.3371  Epoch: 12  Global Step: 73430   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:16,737-Speed 3889.18 samples/sec  Loss 2.0924  LearningRate 0.0467  ProxyLR: 2.3364  Epoch: 12  Global Step: 73440   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:19,368-Speed 3891.72 samples/sec  Loss 2.1437  LearningRate 0.0467  ProxyLR: 2.3357  Epoch: 12  Global Step: 73450   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:21,999-Speed 3893.04 samples/sec  Loss 2.0992  LearningRate 0.0467  ProxyLR: 2.3350  Epoch: 12  Global Step: 73460   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:24,630-Speed 3893.81 samples/sec  Loss 2.1112  LearningRate 0.0467  ProxyLR: 2.3344  Epoch: 12  Global Step: 73470   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:27,263-Speed 3889.79 samples/sec  Loss 2.0323  LearningRate 0.0467  ProxyLR: 2.3337  Epoch: 12  Global Step: 73480   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:29,895-Speed 3891.82 samples/sec  Loss 2.0841  LearningRate 0.0467  ProxyLR: 2.3330  Epoch: 12  Global Step: 73490   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:32,527-Speed 3890.16 samples/sec  Loss 2.1594  LearningRate 0.0466  ProxyLR: 2.3323  Epoch: 12  Global Step: 73500   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:35,156-Speed 3896.60 samples/sec  Loss 2.0001  LearningRate 0.0466  ProxyLR: 2.3316  Epoch: 12  Global Step: 73510   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:37,773-Speed 3914.46 samples/sec  Loss 2.1953  LearningRate 0.0466  ProxyLR: 2.3310  Epoch: 12  Global Step: 73520   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:40,402-Speed 3895.65 samples/sec  Loss 2.0984  LearningRate 0.0466  ProxyLR: 2.3303  Epoch: 12  Global Step: 73530   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:43,030-Speed 3897.67 samples/sec  Loss 2.0638  LearningRate 0.0466  ProxyLR: 2.3296  Epoch: 12  Global Step: 73540   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:45,660-Speed 3894.18 samples/sec  Loss 2.0641  LearningRate 0.0466  ProxyLR: 2.3289  Epoch: 12  Global Step: 73550   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:48,289-Speed 3896.31 samples/sec  Loss 2.0461  LearningRate 0.0466  ProxyLR: 2.3282  Epoch: 12  Global Step: 73560   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:50,916-Speed 3898.11 samples/sec  Loss 2.0976  LearningRate 0.0466  ProxyLR: 2.3276  Epoch: 12  Global Step: 73570   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:53,545-Speed 3896.10 samples/sec  Loss 2.0652  LearningRate 0.0465  ProxyLR: 2.3269  Epoch: 12  Global Step: 73580   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:56,172-Speed 3899.21 samples/sec  Loss 2.0999  LearningRate 0.0465  ProxyLR: 2.3262  Epoch: 12  Global Step: 73590   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:07:58,799-Speed 3898.77 samples/sec  Loss 1.9932  LearningRate 0.0465  ProxyLR: 2.3255  Epoch: 12  Global Step: 73600   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:01,426-Speed 3899.01 samples/sec  Loss 2.0711  LearningRate 0.0465  ProxyLR: 2.3248  Epoch: 12  Global Step: 73610   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:04,053-Speed 3898.40 samples/sec  Loss 2.0325  LearningRate 0.0465  ProxyLR: 2.3242  Epoch: 12  Global Step: 73620   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:06,681-Speed 3897.09 samples/sec  Loss 1.9917  LearningRate 0.0465  ProxyLR: 2.3235  Epoch: 12  Global Step: 73630   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:09,309-Speed 3898.11 samples/sec  Loss 2.0640  LearningRate 0.0465  ProxyLR: 2.3228  Epoch: 12  Global Step: 73640   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:11,936-Speed 3899.44 samples/sec  Loss 2.0968  LearningRate 0.0464  ProxyLR: 2.3221  Epoch: 12  Global Step: 73650   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:14,562-Speed 3899.07 samples/sec  Loss 2.0616  LearningRate 0.0464  ProxyLR: 2.3215  Epoch: 12  Global Step: 73660   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:17,189-Speed 3899.94 samples/sec  Loss 1.9984  LearningRate 0.0464  ProxyLR: 2.3208  Epoch: 12  Global Step: 73670   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:19,817-Speed 3897.79 samples/sec  Loss 2.0544  LearningRate 0.0464  ProxyLR: 2.3201  Epoch: 12  Global Step: 73680   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:22,445-Speed 3896.48 samples/sec  Loss 1.9853  LearningRate 0.0464  ProxyLR: 2.3194  Epoch: 12  Global Step: 73690   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:25,073-Speed 3897.64 samples/sec  Loss 2.0167  LearningRate 0.0464  ProxyLR: 2.3187  Epoch: 12  Global Step: 73700   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:27,700-Speed 3898.40 samples/sec  Loss 2.0188  LearningRate 0.0464  ProxyLR: 2.3181  Epoch: 12  Global Step: 73710   Fp16 Grad Scale: 524288  Required: 6 hours
Training: 2023-05-04 20:08:30,312-Speed 3922.35 samples/sec  Loss 2.1109  LearningRate 0.0463  ProxyLR: 2.3174  Epoch: 12  Global Step: 73720   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:32,939-Speed 3898.53 samples/sec  Loss 2.0835  LearningRate 0.0463  ProxyLR: 2.3167  Epoch: 12  Global Step: 73730   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:35,565-Speed 3900.07 samples/sec  Loss 2.1381  LearningRate 0.0463  ProxyLR: 2.3160  Epoch: 12  Global Step: 73740   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:38,191-Speed 3899.88 samples/sec  Loss 2.0411  LearningRate 0.0463  ProxyLR: 2.3154  Epoch: 12  Global Step: 73750   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:40,816-Speed 3901.87 samples/sec  Loss 2.1319  LearningRate 0.0463  ProxyLR: 2.3147  Epoch: 12  Global Step: 73760   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:43,442-Speed 3901.19 samples/sec  Loss 2.0750  LearningRate 0.0463  ProxyLR: 2.3140  Epoch: 12  Global Step: 73770   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:46,066-Speed 3903.73 samples/sec  Loss 2.0952  LearningRate 0.0463  ProxyLR: 2.3133  Epoch: 12  Global Step: 73780   Fp16 Grad Scale: 262144  Required: 6 hours
Training: 2023-05-04 20:08:48,691-Speed 3900.78 samples/sec  Loss 2.0497  LearningRate 0.0463  ProxyLR: 2.3127  Epoch: 12  Global Step: 73790   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:08:51,316-Speed 3902.47 samples/sec  Loss 2.0110  LearningRate 0.0462  ProxyLR: 2.3120  Epoch: 12  Global Step: 73800   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:08:53,940-Speed 3902.49 samples/sec  Loss 2.0794  LearningRate 0.0462  ProxyLR: 2.3113  Epoch: 12  Global Step: 73810   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:08:56,566-Speed 3900.32 samples/sec  Loss 2.0506  LearningRate 0.0462  ProxyLR: 2.3106  Epoch: 12  Global Step: 73820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:08:59,191-Speed 3902.25 samples/sec  Loss 2.1570  LearningRate 0.0462  ProxyLR: 2.3099  Epoch: 12  Global Step: 73830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:01,822-Speed 3892.46 samples/sec  Loss 1.9851  LearningRate 0.0462  ProxyLR: 2.3093  Epoch: 12  Global Step: 73840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:04,453-Speed 3894.10 samples/sec  Loss 2.1052  LearningRate 0.0462  ProxyLR: 2.3086  Epoch: 12  Global Step: 73850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:07,072-Speed 3910.48 samples/sec  Loss 2.0902  LearningRate 0.0462  ProxyLR: 2.3079  Epoch: 12  Global Step: 73860   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:09,704-Speed 3892.21 samples/sec  Loss 2.0838  LearningRate 0.0461  ProxyLR: 2.3072  Epoch: 12  Global Step: 73870   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:12,337-Speed 3888.91 samples/sec  Loss 2.1025  LearningRate 0.0461  ProxyLR: 2.3066  Epoch: 12  Global Step: 73880   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:14,971-Speed 3888.32 samples/sec  Loss 1.9699  LearningRate 0.0461  ProxyLR: 2.3059  Epoch: 12  Global Step: 73890   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:17,604-Speed 3890.40 samples/sec  Loss 2.0411  LearningRate 0.0461  ProxyLR: 2.3052  Epoch: 12  Global Step: 73900   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:20,292-Speed 3810.66 samples/sec  Loss 1.9863  LearningRate 0.0461  ProxyLR: 2.3045  Epoch: 12  Global Step: 73910   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:32,757-Speed 821.54 samples/sec  Loss 1.8312  LearningRate 0.0461  ProxyLR: 2.3039  Epoch: 13  Global Step: 73920   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:35,415-Speed 3853.30 samples/sec  Loss 1.6360  LearningRate 0.0461  ProxyLR: 2.3032  Epoch: 13  Global Step: 73930   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:38,080-Speed 3844.26 samples/sec  Loss 1.6051  LearningRate 0.0461  ProxyLR: 2.3025  Epoch: 13  Global Step: 73940   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:40,733-Speed 3859.88 samples/sec  Loss 1.5862  LearningRate 0.0460  ProxyLR: 2.3018  Epoch: 13  Global Step: 73950   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:09:43,361-Speed 3897.84 samples/sec  Loss 1.6383  LearningRate 0.0460  ProxyLR: 2.3012  Epoch: 13  Global Step: 73960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:46,042-Speed 3820.31 samples/sec  Loss 1.5931  LearningRate 0.0460  ProxyLR: 2.3005  Epoch: 13  Global Step: 73970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:48,673-Speed 3893.56 samples/sec  Loss 1.6164  LearningRate 0.0460  ProxyLR: 2.2998  Epoch: 13  Global Step: 73980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:51,303-Speed 3894.35 samples/sec  Loss 1.5567  LearningRate 0.0460  ProxyLR: 2.2991  Epoch: 13  Global Step: 73990   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:53,976-Speed 3832.33 samples/sec  Loss 1.6023  LearningRate 0.0460  ProxyLR: 2.2985  Epoch: 13  Global Step: 74000   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:56,605-Speed 3895.44 samples/sec  Loss 1.5807  LearningRate 0.0460  ProxyLR: 2.2978  Epoch: 13  Global Step: 74010   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:09:59,236-Speed 3892.80 samples/sec  Loss 1.6458  LearningRate 0.0459  ProxyLR: 2.2971  Epoch: 13  Global Step: 74020   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:01,867-Speed 3892.48 samples/sec  Loss 1.6093  LearningRate 0.0459  ProxyLR: 2.2964  Epoch: 13  Global Step: 74030   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:04,497-Speed 3894.62 samples/sec  Loss 1.6206  LearningRate 0.0459  ProxyLR: 2.2958  Epoch: 13  Global Step: 74040   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:07,127-Speed 3894.31 samples/sec  Loss 1.5543  LearningRate 0.0459  ProxyLR: 2.2951  Epoch: 13  Global Step: 74050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:09,757-Speed 3894.45 samples/sec  Loss 1.5794  LearningRate 0.0459  ProxyLR: 2.2944  Epoch: 13  Global Step: 74060   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:10:12,377-Speed 3909.95 samples/sec  Loss 1.6077  LearningRate 0.0459  ProxyLR: 2.2937  Epoch: 13  Global Step: 74070   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:15,006-Speed 3895.74 samples/sec  Loss 1.5673  LearningRate 0.0459  ProxyLR: 2.2931  Epoch: 13  Global Step: 74080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:17,636-Speed 3894.89 samples/sec  Loss 1.5911  LearningRate 0.0458  ProxyLR: 2.2924  Epoch: 13  Global Step: 74090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:20,267-Speed 3893.11 samples/sec  Loss 1.6196  LearningRate 0.0458  ProxyLR: 2.2917  Epoch: 13  Global Step: 74100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:22,900-Speed 3889.34 samples/sec  Loss 1.5559  LearningRate 0.0458  ProxyLR: 2.2911  Epoch: 13  Global Step: 74110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:25,531-Speed 3893.10 samples/sec  Loss 1.5694  LearningRate 0.0458  ProxyLR: 2.2904  Epoch: 13  Global Step: 74120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:28,163-Speed 3891.44 samples/sec  Loss 1.5861  LearningRate 0.0458  ProxyLR: 2.2897  Epoch: 13  Global Step: 74130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:30,784-Speed 3908.35 samples/sec  Loss 1.5285  LearningRate 0.0458  ProxyLR: 2.2890  Epoch: 13  Global Step: 74140   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:33,444-Speed 3850.83 samples/sec  Loss 1.6056  LearningRate 0.0458  ProxyLR: 2.2884  Epoch: 13  Global Step: 74150   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:36,077-Speed 3890.16 samples/sec  Loss 1.5441  LearningRate 0.0458  ProxyLR: 2.2877  Epoch: 13  Global Step: 74160   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:38,710-Speed 3889.88 samples/sec  Loss 1.6080  LearningRate 0.0457  ProxyLR: 2.2870  Epoch: 13  Global Step: 74170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:41,345-Speed 3886.69 samples/sec  Loss 1.6337  LearningRate 0.0457  ProxyLR: 2.2863  Epoch: 13  Global Step: 74180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:43,977-Speed 3891.31 samples/sec  Loss 1.5755  LearningRate 0.0457  ProxyLR: 2.2857  Epoch: 13  Global Step: 74190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:46,609-Speed 3892.21 samples/sec  Loss 1.6167  LearningRate 0.0457  ProxyLR: 2.2850  Epoch: 13  Global Step: 74200   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:49,243-Speed 3888.13 samples/sec  Loss 1.5797  LearningRate 0.0457  ProxyLR: 2.2843  Epoch: 13  Global Step: 74210   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:51,877-Speed 3887.55 samples/sec  Loss 1.6662  LearningRate 0.0457  ProxyLR: 2.2836  Epoch: 13  Global Step: 74220   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:54,510-Speed 3890.60 samples/sec  Loss 1.6169  LearningRate 0.0457  ProxyLR: 2.2830  Epoch: 13  Global Step: 74230   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:10:57,141-Speed 3892.36 samples/sec  Loss 1.5459  LearningRate 0.0456  ProxyLR: 2.2823  Epoch: 13  Global Step: 74240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:10:59,761-Speed 3909.56 samples/sec  Loss 1.6040  LearningRate 0.0456  ProxyLR: 2.2816  Epoch: 13  Global Step: 74250   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:02,395-Speed 3889.34 samples/sec  Loss 1.5696  LearningRate 0.0456  ProxyLR: 2.2810  Epoch: 13  Global Step: 74260   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:05,028-Speed 3889.39 samples/sec  Loss 1.5764  LearningRate 0.0456  ProxyLR: 2.2803  Epoch: 13  Global Step: 74270   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:07,661-Speed 3890.57 samples/sec  Loss 1.5951  LearningRate 0.0456  ProxyLR: 2.2796  Epoch: 13  Global Step: 74280   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:10,292-Speed 3892.31 samples/sec  Loss 1.6423  LearningRate 0.0456  ProxyLR: 2.2789  Epoch: 13  Global Step: 74290   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:12,924-Speed 3892.16 samples/sec  Loss 1.6631  LearningRate 0.0456  ProxyLR: 2.2783  Epoch: 13  Global Step: 74300   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:15,556-Speed 3891.18 samples/sec  Loss 1.6099  LearningRate 0.0456  ProxyLR: 2.2776  Epoch: 13  Global Step: 74310   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:18,190-Speed 3888.25 samples/sec  Loss 1.6478  LearningRate 0.0455  ProxyLR: 2.2769  Epoch: 13  Global Step: 74320   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:20,822-Speed 3892.49 samples/sec  Loss 1.6443  LearningRate 0.0455  ProxyLR: 2.2763  Epoch: 13  Global Step: 74330   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:23,455-Speed 3889.93 samples/sec  Loss 1.5959  LearningRate 0.0455  ProxyLR: 2.2756  Epoch: 13  Global Step: 74340   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:11:26,086-Speed 3892.10 samples/sec  Loss 1.6689  LearningRate 0.0455  ProxyLR: 2.2749  Epoch: 13  Global Step: 74350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:28,718-Speed 3891.81 samples/sec  Loss 1.5519  LearningRate 0.0455  ProxyLR: 2.2742  Epoch: 13  Global Step: 74360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:31,350-Speed 3891.25 samples/sec  Loss 1.6053  LearningRate 0.0455  ProxyLR: 2.2736  Epoch: 13  Global Step: 74370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:33,984-Speed 3888.84 samples/sec  Loss 1.5735  LearningRate 0.0455  ProxyLR: 2.2729  Epoch: 13  Global Step: 74380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:36,618-Speed 3888.59 samples/sec  Loss 1.6033  LearningRate 0.0454  ProxyLR: 2.2722  Epoch: 13  Global Step: 74390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:39,251-Speed 3890.71 samples/sec  Loss 1.5740  LearningRate 0.0454  ProxyLR: 2.2716  Epoch: 13  Global Step: 74400   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:41,884-Speed 3890.05 samples/sec  Loss 1.6182  LearningRate 0.0454  ProxyLR: 2.2709  Epoch: 13  Global Step: 74410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:44,517-Speed 3888.78 samples/sec  Loss 1.6196  LearningRate 0.0454  ProxyLR: 2.2702  Epoch: 13  Global Step: 74420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:47,150-Speed 3890.10 samples/sec  Loss 1.6341  LearningRate 0.0454  ProxyLR: 2.2696  Epoch: 13  Global Step: 74430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:49,782-Speed 3891.71 samples/sec  Loss 1.6718  LearningRate 0.0454  ProxyLR: 2.2689  Epoch: 13  Global Step: 74440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:52,412-Speed 3894.41 samples/sec  Loss 1.5731  LearningRate 0.0454  ProxyLR: 2.2682  Epoch: 13  Global Step: 74450   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:11:55,030-Speed 3912.43 samples/sec  Loss 1.5953  LearningRate 0.0454  ProxyLR: 2.2675  Epoch: 13  Global Step: 74460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:11:57,662-Speed 3891.01 samples/sec  Loss 1.6255  LearningRate 0.0453  ProxyLR: 2.2669  Epoch: 13  Global Step: 74470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:00,294-Speed 3891.81 samples/sec  Loss 1.6521  LearningRate 0.0453  ProxyLR: 2.2662  Epoch: 13  Global Step: 74480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:02,927-Speed 3890.53 samples/sec  Loss 1.6643  LearningRate 0.0453  ProxyLR: 2.2655  Epoch: 13  Global Step: 74490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:05,560-Speed 3889.64 samples/sec  Loss 1.6123  LearningRate 0.0453  ProxyLR: 2.2649  Epoch: 13  Global Step: 74500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:08,190-Speed 3895.05 samples/sec  Loss 1.6098  LearningRate 0.0453  ProxyLR: 2.2642  Epoch: 13  Global Step: 74510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:10,822-Speed 3891.68 samples/sec  Loss 1.5612  LearningRate 0.0453  ProxyLR: 2.2635  Epoch: 13  Global Step: 74520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:13,452-Speed 3894.29 samples/sec  Loss 1.6248  LearningRate 0.0453  ProxyLR: 2.2629  Epoch: 13  Global Step: 74530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:16,084-Speed 3891.96 samples/sec  Loss 1.6121  LearningRate 0.0452  ProxyLR: 2.2622  Epoch: 13  Global Step: 74540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:18,713-Speed 3895.08 samples/sec  Loss 1.5627  LearningRate 0.0452  ProxyLR: 2.2615  Epoch: 13  Global Step: 74550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:21,331-Speed 3912.33 samples/sec  Loss 1.6369  LearningRate 0.0452  ProxyLR: 2.2608  Epoch: 13  Global Step: 74560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:23,962-Speed 3893.78 samples/sec  Loss 1.5413  LearningRate 0.0452  ProxyLR: 2.2602  Epoch: 13  Global Step: 74570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:26,592-Speed 3893.88 samples/sec  Loss 1.6028  LearningRate 0.0452  ProxyLR: 2.2595  Epoch: 13  Global Step: 74580   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:29,221-Speed 3895.54 samples/sec  Loss 1.6056  LearningRate 0.0452  ProxyLR: 2.2588  Epoch: 13  Global Step: 74590   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:31,851-Speed 3894.96 samples/sec  Loss 1.6276  LearningRate 0.0452  ProxyLR: 2.2582  Epoch: 13  Global Step: 74600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:12:34,468-Speed 3913.01 samples/sec  Loss 1.6723  LearningRate 0.0452  ProxyLR: 2.2575  Epoch: 13  Global Step: 74610   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:37,097-Speed 3896.65 samples/sec  Loss 1.5951  LearningRate 0.0451  ProxyLR: 2.2568  Epoch: 13  Global Step: 74620   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:39,727-Speed 3895.00 samples/sec  Loss 1.6535  LearningRate 0.0451  ProxyLR: 2.2562  Epoch: 13  Global Step: 74630   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:42,356-Speed 3895.07 samples/sec  Loss 1.6448  LearningRate 0.0451  ProxyLR: 2.2555  Epoch: 13  Global Step: 74640   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:44,986-Speed 3895.39 samples/sec  Loss 1.6060  LearningRate 0.0451  ProxyLR: 2.2548  Epoch: 13  Global Step: 74650   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:47,617-Speed 3893.21 samples/sec  Loss 1.5651  LearningRate 0.0451  ProxyLR: 2.2542  Epoch: 13  Global Step: 74660   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:50,246-Speed 3895.17 samples/sec  Loss 1.6427  LearningRate 0.0451  ProxyLR: 2.2535  Epoch: 13  Global Step: 74670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:52,877-Speed 3892.80 samples/sec  Loss 1.5763  LearningRate 0.0451  ProxyLR: 2.2528  Epoch: 13  Global Step: 74680   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:55,508-Speed 3894.04 samples/sec  Loss 1.6835  LearningRate 0.0450  ProxyLR: 2.2522  Epoch: 13  Global Step: 74690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:12:58,137-Speed 3895.45 samples/sec  Loss 1.5497  LearningRate 0.0450  ProxyLR: 2.2515  Epoch: 13  Global Step: 74700   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:00,767-Speed 3894.94 samples/sec  Loss 1.6666  LearningRate 0.0450  ProxyLR: 2.2508  Epoch: 13  Global Step: 74710   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:03,397-Speed 3893.59 samples/sec  Loss 1.6216  LearningRate 0.0450  ProxyLR: 2.2502  Epoch: 13  Global Step: 74720   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:06,027-Speed 3894.10 samples/sec  Loss 1.6105  LearningRate 0.0450  ProxyLR: 2.2495  Epoch: 13  Global Step: 74730   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:08,657-Speed 3895.07 samples/sec  Loss 1.5953  LearningRate 0.0450  ProxyLR: 2.2488  Epoch: 13  Global Step: 74740   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:11,287-Speed 3893.91 samples/sec  Loss 1.6565  LearningRate 0.0450  ProxyLR: 2.2482  Epoch: 13  Global Step: 74750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:13,916-Speed 3895.73 samples/sec  Loss 1.6082  LearningRate 0.0449  ProxyLR: 2.2475  Epoch: 13  Global Step: 74760   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:16,547-Speed 3894.24 samples/sec  Loss 1.6118  LearningRate 0.0449  ProxyLR: 2.2468  Epoch: 13  Global Step: 74770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:19,176-Speed 3894.86 samples/sec  Loss 1.6288  LearningRate 0.0449  ProxyLR: 2.2462  Epoch: 13  Global Step: 74780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:21,808-Speed 3892.85 samples/sec  Loss 1.6145  LearningRate 0.0449  ProxyLR: 2.2455  Epoch: 13  Global Step: 74790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:24,437-Speed 3895.60 samples/sec  Loss 1.5835  LearningRate 0.0449  ProxyLR: 2.2448  Epoch: 13  Global Step: 74800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:27,052-Speed 3916.56 samples/sec  Loss 1.6127  LearningRate 0.0449  ProxyLR: 2.2442  Epoch: 13  Global Step: 74810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:29,667-Speed 3916.52 samples/sec  Loss 1.5911  LearningRate 0.0449  ProxyLR: 2.2435  Epoch: 13  Global Step: 74820   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:32,298-Speed 3893.94 samples/sec  Loss 1.5826  LearningRate 0.0449  ProxyLR: 2.2428  Epoch: 13  Global Step: 74830   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:34,925-Speed 3897.90 samples/sec  Loss 1.6095  LearningRate 0.0448  ProxyLR: 2.2422  Epoch: 13  Global Step: 74840   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:37,554-Speed 3896.92 samples/sec  Loss 1.7093  LearningRate 0.0448  ProxyLR: 2.2415  Epoch: 13  Global Step: 74850   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:40,182-Speed 3896.35 samples/sec  Loss 1.6125  LearningRate 0.0448  ProxyLR: 2.2408  Epoch: 13  Global Step: 74860   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:42,811-Speed 3896.98 samples/sec  Loss 1.5833  LearningRate 0.0448  ProxyLR: 2.2402  Epoch: 13  Global Step: 74870   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:45,439-Speed 3896.78 samples/sec  Loss 1.6305  LearningRate 0.0448  ProxyLR: 2.2395  Epoch: 13  Global Step: 74880   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:48,069-Speed 3893.85 samples/sec  Loss 1.6506  LearningRate 0.0448  ProxyLR: 2.2388  Epoch: 13  Global Step: 74890   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:50,699-Speed 3894.24 samples/sec  Loss 1.6375  LearningRate 0.0448  ProxyLR: 2.2382  Epoch: 13  Global Step: 74900   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:53,329-Speed 3895.08 samples/sec  Loss 1.6146  LearningRate 0.0447  ProxyLR: 2.2375  Epoch: 13  Global Step: 74910   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:13:55,959-Speed 3894.32 samples/sec  Loss 1.6796  LearningRate 0.0447  ProxyLR: 2.2368  Epoch: 13  Global Step: 74920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:13:58,591-Speed 3891.00 samples/sec  Loss 1.6177  LearningRate 0.0447  ProxyLR: 2.2362  Epoch: 13  Global Step: 74930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:01,221-Speed 3895.25 samples/sec  Loss 1.6280  LearningRate 0.0447  ProxyLR: 2.2355  Epoch: 13  Global Step: 74940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:03,851-Speed 3894.26 samples/sec  Loss 1.6738  LearningRate 0.0447  ProxyLR: 2.2348  Epoch: 13  Global Step: 74950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:06,481-Speed 3894.75 samples/sec  Loss 1.5914  LearningRate 0.0447  ProxyLR: 2.2342  Epoch: 13  Global Step: 74960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:09,113-Speed 3892.02 samples/sec  Loss 1.6135  LearningRate 0.0447  ProxyLR: 2.2335  Epoch: 13  Global Step: 74970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:11,744-Speed 3892.65 samples/sec  Loss 1.5950  LearningRate 0.0447  ProxyLR: 2.2328  Epoch: 13  Global Step: 74980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:14,375-Speed 3893.59 samples/sec  Loss 1.6277  LearningRate 0.0446  ProxyLR: 2.2322  Epoch: 13  Global Step: 74990   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:17,004-Speed 3895.68 samples/sec  Loss 1.6436  LearningRate 0.0446  ProxyLR: 2.2315  Epoch: 13  Global Step: 75000   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:19,635-Speed 3892.34 samples/sec  Loss 1.6460  LearningRate 0.0446  ProxyLR: 2.2308  Epoch: 13  Global Step: 75010   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:22,253-Speed 3912.08 samples/sec  Loss 1.5902  LearningRate 0.0446  ProxyLR: 2.2302  Epoch: 13  Global Step: 75020   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:24,884-Speed 3893.89 samples/sec  Loss 1.6049  LearningRate 0.0446  ProxyLR: 2.2295  Epoch: 13  Global Step: 75030   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:27,514-Speed 3894.43 samples/sec  Loss 1.6214  LearningRate 0.0446  ProxyLR: 2.2288  Epoch: 13  Global Step: 75040   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:30,145-Speed 3892.96 samples/sec  Loss 1.6444  LearningRate 0.0446  ProxyLR: 2.2282  Epoch: 13  Global Step: 75050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:32,776-Speed 3893.05 samples/sec  Loss 1.6418  LearningRate 0.0446  ProxyLR: 2.2275  Epoch: 13  Global Step: 75060   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:35,406-Speed 3893.58 samples/sec  Loss 1.6628  LearningRate 0.0445  ProxyLR: 2.2269  Epoch: 13  Global Step: 75070   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:38,038-Speed 3892.07 samples/sec  Loss 1.6311  LearningRate 0.0445  ProxyLR: 2.2262  Epoch: 13  Global Step: 75080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:40,674-Speed 3885.07 samples/sec  Loss 1.5830  LearningRate 0.0445  ProxyLR: 2.2255  Epoch: 13  Global Step: 75090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:14:43,300-Speed 3901.49 samples/sec  Loss 1.6189  LearningRate 0.0445  ProxyLR: 2.2249  Epoch: 13  Global Step: 75100   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:14:45,935-Speed 3886.61 samples/sec  Loss 1.6141  LearningRate 0.0445  ProxyLR: 2.2242  Epoch: 13  Global Step: 75110   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:14:48,572-Speed 3884.39 samples/sec  Loss 1.6412  LearningRate 0.0445  ProxyLR: 2.2235  Epoch: 13  Global Step: 75120   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:14:51,207-Speed 3885.61 samples/sec  Loss 1.5408  LearningRate 0.0445  ProxyLR: 2.2229  Epoch: 13  Global Step: 75130   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:14:53,843-Speed 3886.63 samples/sec  Loss 1.6926  LearningRate 0.0444  ProxyLR: 2.2222  Epoch: 13  Global Step: 75140   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:14:56,480-Speed 3884.15 samples/sec  Loss 1.7081  LearningRate 0.0444  ProxyLR: 2.2215  Epoch: 13  Global Step: 75150   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:14:59,116-Speed 3885.49 samples/sec  Loss 1.6009  LearningRate 0.0444  ProxyLR: 2.2209  Epoch: 13  Global Step: 75160   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:15:01,752-Speed 3885.58 samples/sec  Loss 1.6530  LearningRate 0.0444  ProxyLR: 2.2202  Epoch: 13  Global Step: 75170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:15:04,388-Speed 3886.19 samples/sec  Loss 1.7207  LearningRate 0.0444  ProxyLR: 2.2196  Epoch: 13  Global Step: 75180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:15:07,024-Speed 3885.58 samples/sec  Loss 1.6568  LearningRate 0.0444  ProxyLR: 2.2189  Epoch: 13  Global Step: 75190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:15:09,660-Speed 3884.46 samples/sec  Loss 1.5953  LearningRate 0.0444  ProxyLR: 2.2182  Epoch: 13  Global Step: 75200   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:12,300-Speed 3880.86 samples/sec  Loss 1.6758  LearningRate 0.0444  ProxyLR: 2.2176  Epoch: 13  Global Step: 75210   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:14,938-Speed 3881.69 samples/sec  Loss 1.5742  LearningRate 0.0443  ProxyLR: 2.2169  Epoch: 13  Global Step: 75220   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:17,578-Speed 3880.29 samples/sec  Loss 1.6192  LearningRate 0.0443  ProxyLR: 2.2162  Epoch: 13  Global Step: 75230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:20,215-Speed 3883.97 samples/sec  Loss 1.6950  LearningRate 0.0443  ProxyLR: 2.2156  Epoch: 13  Global Step: 75240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:22,851-Speed 3885.88 samples/sec  Loss 1.6369  LearningRate 0.0443  ProxyLR: 2.2149  Epoch: 13  Global Step: 75250   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:25,487-Speed 3885.70 samples/sec  Loss 1.6722  LearningRate 0.0443  ProxyLR: 2.2143  Epoch: 13  Global Step: 75260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:28,122-Speed 3887.02 samples/sec  Loss 1.6756  LearningRate 0.0443  ProxyLR: 2.2136  Epoch: 13  Global Step: 75270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:30,754-Speed 3891.57 samples/sec  Loss 1.5899  LearningRate 0.0443  ProxyLR: 2.2129  Epoch: 13  Global Step: 75280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:33,386-Speed 3891.17 samples/sec  Loss 1.6396  LearningRate 0.0442  ProxyLR: 2.2123  Epoch: 13  Global Step: 75290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:36,005-Speed 3912.09 samples/sec  Loss 1.6263  LearningRate 0.0442  ProxyLR: 2.2116  Epoch: 13  Global Step: 75300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:38,636-Speed 3892.27 samples/sec  Loss 1.6530  LearningRate 0.0442  ProxyLR: 2.2110  Epoch: 13  Global Step: 75310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:41,268-Speed 3891.15 samples/sec  Loss 1.6755  LearningRate 0.0442  ProxyLR: 2.2103  Epoch: 13  Global Step: 75320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:43,900-Speed 3891.75 samples/sec  Loss 1.6008  LearningRate 0.0442  ProxyLR: 2.2096  Epoch: 13  Global Step: 75330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:46,532-Speed 3891.56 samples/sec  Loss 1.6809  LearningRate 0.0442  ProxyLR: 2.2090  Epoch: 13  Global Step: 75340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:49,165-Speed 3890.95 samples/sec  Loss 1.6607  LearningRate 0.0442  ProxyLR: 2.2083  Epoch: 13  Global Step: 75350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:51,798-Speed 3889.74 samples/sec  Loss 1.6028  LearningRate 0.0442  ProxyLR: 2.2076  Epoch: 13  Global Step: 75360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:54,428-Speed 3894.59 samples/sec  Loss 1.6519  LearningRate 0.0441  ProxyLR: 2.2070  Epoch: 13  Global Step: 75370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:57,057-Speed 3896.05 samples/sec  Loss 1.5997  LearningRate 0.0441  ProxyLR: 2.2063  Epoch: 13  Global Step: 75380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:15:59,689-Speed 3891.87 samples/sec  Loss 1.6706  LearningRate 0.0441  ProxyLR: 2.2057  Epoch: 13  Global Step: 75390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:02,321-Speed 3891.44 samples/sec  Loss 1.6634  LearningRate 0.0441  ProxyLR: 2.2050  Epoch: 13  Global Step: 75400   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:16:04,941-Speed 3909.90 samples/sec  Loss 1.6166  LearningRate 0.0441  ProxyLR: 2.2043  Epoch: 13  Global Step: 75410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:07,573-Speed 3891.16 samples/sec  Loss 1.6967  LearningRate 0.0441  ProxyLR: 2.2037  Epoch: 13  Global Step: 75420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:10,208-Speed 3886.49 samples/sec  Loss 1.6594  LearningRate 0.0441  ProxyLR: 2.2030  Epoch: 13  Global Step: 75430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:12,841-Speed 3891.22 samples/sec  Loss 1.6463  LearningRate 0.0440  ProxyLR: 2.2024  Epoch: 13  Global Step: 75440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:15,471-Speed 3893.71 samples/sec  Loss 1.6175  LearningRate 0.0440  ProxyLR: 2.2017  Epoch: 13  Global Step: 75450   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:18,103-Speed 3892.15 samples/sec  Loss 1.6791  LearningRate 0.0440  ProxyLR: 2.2010  Epoch: 13  Global Step: 75460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:20,735-Speed 3891.64 samples/sec  Loss 1.6323  LearningRate 0.0440  ProxyLR: 2.2004  Epoch: 13  Global Step: 75470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:23,367-Speed 3891.32 samples/sec  Loss 1.6683  LearningRate 0.0440  ProxyLR: 2.1997  Epoch: 13  Global Step: 75480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:25,998-Speed 3892.41 samples/sec  Loss 1.5824  LearningRate 0.0440  ProxyLR: 2.1991  Epoch: 13  Global Step: 75490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:28,616-Speed 3912.17 samples/sec  Loss 1.6774  LearningRate 0.0440  ProxyLR: 2.1984  Epoch: 13  Global Step: 75500   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:31,247-Speed 3892.89 samples/sec  Loss 1.6732  LearningRate 0.0440  ProxyLR: 2.1977  Epoch: 13  Global Step: 75510   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:33,877-Speed 3895.92 samples/sec  Loss 1.6918  LearningRate 0.0439  ProxyLR: 2.1971  Epoch: 13  Global Step: 75520   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:36,507-Speed 3893.27 samples/sec  Loss 1.7124  LearningRate 0.0439  ProxyLR: 2.1964  Epoch: 13  Global Step: 75530   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:39,140-Speed 3890.28 samples/sec  Loss 1.6540  LearningRate 0.0439  ProxyLR: 2.1958  Epoch: 13  Global Step: 75540   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:41,772-Speed 3892.35 samples/sec  Loss 1.7065  LearningRate 0.0439  ProxyLR: 2.1951  Epoch: 13  Global Step: 75550   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:44,401-Speed 3896.09 samples/sec  Loss 1.6585  LearningRate 0.0439  ProxyLR: 2.1944  Epoch: 13  Global Step: 75560   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:47,033-Speed 3891.17 samples/sec  Loss 1.6347  LearningRate 0.0439  ProxyLR: 2.1938  Epoch: 13  Global Step: 75570   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:49,666-Speed 3890.15 samples/sec  Loss 1.6376  LearningRate 0.0439  ProxyLR: 2.1931  Epoch: 13  Global Step: 75580   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:52,295-Speed 3895.71 samples/sec  Loss 1.6908  LearningRate 0.0438  ProxyLR: 2.1925  Epoch: 13  Global Step: 75590   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:16:54,925-Speed 3895.15 samples/sec  Loss 1.7121  LearningRate 0.0438  ProxyLR: 2.1918  Epoch: 13  Global Step: 75600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:16:57,558-Speed 3889.46 samples/sec  Loss 1.7263  LearningRate 0.0438  ProxyLR: 2.1911  Epoch: 13  Global Step: 75610   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:00,190-Speed 3892.17 samples/sec  Loss 1.6845  LearningRate 0.0438  ProxyLR: 2.1905  Epoch: 13  Global Step: 75620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:02,822-Speed 3890.93 samples/sec  Loss 1.6399  LearningRate 0.0438  ProxyLR: 2.1898  Epoch: 13  Global Step: 75630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:05,451-Speed 3896.42 samples/sec  Loss 1.6215  LearningRate 0.0438  ProxyLR: 2.1892  Epoch: 13  Global Step: 75640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:08,065-Speed 3918.07 samples/sec  Loss 1.6501  LearningRate 0.0438  ProxyLR: 2.1885  Epoch: 13  Global Step: 75650   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:10,694-Speed 3896.73 samples/sec  Loss 1.6447  LearningRate 0.0438  ProxyLR: 2.1879  Epoch: 13  Global Step: 75660   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:13,324-Speed 3894.77 samples/sec  Loss 1.6343  LearningRate 0.0437  ProxyLR: 2.1872  Epoch: 13  Global Step: 75670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:15,953-Speed 3895.70 samples/sec  Loss 1.6250  LearningRate 0.0437  ProxyLR: 2.1865  Epoch: 13  Global Step: 75680   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:18,581-Speed 3896.98 samples/sec  Loss 1.5790  LearningRate 0.0437  ProxyLR: 2.1859  Epoch: 13  Global Step: 75690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:21,210-Speed 3896.07 samples/sec  Loss 1.7471  LearningRate 0.0437  ProxyLR: 2.1852  Epoch: 13  Global Step: 75700   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:23,837-Speed 3898.98 samples/sec  Loss 1.6637  LearningRate 0.0437  ProxyLR: 2.1846  Epoch: 13  Global Step: 75710   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:26,467-Speed 3895.10 samples/sec  Loss 1.6556  LearningRate 0.0437  ProxyLR: 2.1839  Epoch: 13  Global Step: 75720   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:29,095-Speed 3896.95 samples/sec  Loss 1.6572  LearningRate 0.0437  ProxyLR: 2.1833  Epoch: 13  Global Step: 75730   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:31,725-Speed 3895.32 samples/sec  Loss 1.6441  LearningRate 0.0437  ProxyLR: 2.1826  Epoch: 13  Global Step: 75740   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:17:34,354-Speed 3896.18 samples/sec  Loss 1.6531  LearningRate 0.0436  ProxyLR: 2.1819  Epoch: 13  Global Step: 75750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:36,983-Speed 3895.95 samples/sec  Loss 1.6439  LearningRate 0.0436  ProxyLR: 2.1813  Epoch: 13  Global Step: 75760   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:39,611-Speed 3897.04 samples/sec  Loss 1.6695  LearningRate 0.0436  ProxyLR: 2.1806  Epoch: 13  Global Step: 75770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:42,240-Speed 3895.19 samples/sec  Loss 1.7391  LearningRate 0.0436  ProxyLR: 2.1800  Epoch: 13  Global Step: 75780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:44,870-Speed 3895.64 samples/sec  Loss 1.7318  LearningRate 0.0436  ProxyLR: 2.1793  Epoch: 13  Global Step: 75790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:47,499-Speed 3895.79 samples/sec  Loss 1.6402  LearningRate 0.0436  ProxyLR: 2.1787  Epoch: 13  Global Step: 75800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:50,128-Speed 3895.64 samples/sec  Loss 1.6789  LearningRate 0.0436  ProxyLR: 2.1780  Epoch: 13  Global Step: 75810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:52,755-Speed 3898.20 samples/sec  Loss 1.7147  LearningRate 0.0435  ProxyLR: 2.1773  Epoch: 13  Global Step: 75820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:55,384-Speed 3895.90 samples/sec  Loss 1.7144  LearningRate 0.0435  ProxyLR: 2.1767  Epoch: 13  Global Step: 75830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:17:58,000-Speed 3915.11 samples/sec  Loss 1.6813  LearningRate 0.0435  ProxyLR: 2.1760  Epoch: 13  Global Step: 75840   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:00,629-Speed 3895.95 samples/sec  Loss 1.6881  LearningRate 0.0435  ProxyLR: 2.1754  Epoch: 13  Global Step: 75850   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:03,260-Speed 3894.05 samples/sec  Loss 1.6731  LearningRate 0.0435  ProxyLR: 2.1747  Epoch: 13  Global Step: 75860   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:05,887-Speed 3897.82 samples/sec  Loss 1.6552  LearningRate 0.0435  ProxyLR: 2.1741  Epoch: 13  Global Step: 75870   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:08,514-Speed 3899.53 samples/sec  Loss 1.6851  LearningRate 0.0435  ProxyLR: 2.1734  Epoch: 13  Global Step: 75880   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:11,143-Speed 3895.44 samples/sec  Loss 1.6384  LearningRate 0.0435  ProxyLR: 2.1727  Epoch: 13  Global Step: 75890   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:13,773-Speed 3894.05 samples/sec  Loss 1.6746  LearningRate 0.0434  ProxyLR: 2.1721  Epoch: 13  Global Step: 75900   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:16,402-Speed 3896.79 samples/sec  Loss 1.6883  LearningRate 0.0434  ProxyLR: 2.1714  Epoch: 13  Global Step: 75910   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:19,029-Speed 3898.17 samples/sec  Loss 1.7081  LearningRate 0.0434  ProxyLR: 2.1708  Epoch: 13  Global Step: 75920   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:21,658-Speed 3897.08 samples/sec  Loss 1.7564  LearningRate 0.0434  ProxyLR: 2.1701  Epoch: 13  Global Step: 75930   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:24,274-Speed 3914.64 samples/sec  Loss 1.6460  LearningRate 0.0434  ProxyLR: 2.1695  Epoch: 13  Global Step: 75940   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:26,901-Speed 3898.73 samples/sec  Loss 1.6002  LearningRate 0.0434  ProxyLR: 2.1688  Epoch: 13  Global Step: 75950   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:29,529-Speed 3898.32 samples/sec  Loss 1.6669  LearningRate 0.0434  ProxyLR: 2.1682  Epoch: 13  Global Step: 75960   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:32,157-Speed 3897.60 samples/sec  Loss 1.6351  LearningRate 0.0434  ProxyLR: 2.1675  Epoch: 13  Global Step: 75970   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:34,784-Speed 3898.37 samples/sec  Loss 1.6165  LearningRate 0.0433  ProxyLR: 2.1668  Epoch: 13  Global Step: 75980   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:37,413-Speed 3895.94 samples/sec  Loss 1.6005  LearningRate 0.0433  ProxyLR: 2.1662  Epoch: 13  Global Step: 75990   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:40,040-Speed 3898.48 samples/sec  Loss 1.6786  LearningRate 0.0433  ProxyLR: 2.1655  Epoch: 13  Global Step: 76000   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:42,669-Speed 3896.37 samples/sec  Loss 1.6929  LearningRate 0.0433  ProxyLR: 2.1649  Epoch: 13  Global Step: 76010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:45,296-Speed 3898.43 samples/sec  Loss 1.6943  LearningRate 0.0433  ProxyLR: 2.1642  Epoch: 13  Global Step: 76020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:47,921-Speed 3901.88 samples/sec  Loss 1.6448  LearningRate 0.0433  ProxyLR: 2.1636  Epoch: 13  Global Step: 76030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:18:50,550-Speed 3897.00 samples/sec  Loss 1.6948  LearningRate 0.0433  ProxyLR: 2.1629  Epoch: 13  Global Step: 76040   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:18:53,178-Speed 3897.50 samples/sec  Loss 1.6842  LearningRate 0.0432  ProxyLR: 2.1623  Epoch: 13  Global Step: 76050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:18:55,807-Speed 3895.93 samples/sec  Loss 1.5610  LearningRate 0.0432  ProxyLR: 2.1616  Epoch: 13  Global Step: 76060   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:18:58,435-Speed 3896.98 samples/sec  Loss 1.6483  LearningRate 0.0432  ProxyLR: 2.1610  Epoch: 13  Global Step: 76070   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:01,062-Speed 3899.12 samples/sec  Loss 1.6307  LearningRate 0.0432  ProxyLR: 2.1603  Epoch: 13  Global Step: 76080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:03,691-Speed 3896.42 samples/sec  Loss 1.6783  LearningRate 0.0432  ProxyLR: 2.1597  Epoch: 13  Global Step: 76090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:06,319-Speed 3897.30 samples/sec  Loss 1.6866  LearningRate 0.0432  ProxyLR: 2.1590  Epoch: 13  Global Step: 76100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:08,947-Speed 3897.41 samples/sec  Loss 1.7307  LearningRate 0.0432  ProxyLR: 2.1583  Epoch: 13  Global Step: 76110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:11,574-Speed 3899.05 samples/sec  Loss 1.6882  LearningRate 0.0432  ProxyLR: 2.1577  Epoch: 13  Global Step: 76120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:14,202-Speed 3897.44 samples/sec  Loss 1.6941  LearningRate 0.0431  ProxyLR: 2.1570  Epoch: 13  Global Step: 76130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:16,830-Speed 3897.69 samples/sec  Loss 1.7337  LearningRate 0.0431  ProxyLR: 2.1564  Epoch: 13  Global Step: 76140   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:19:19,444-Speed 3918.45 samples/sec  Loss 1.6733  LearningRate 0.0431  ProxyLR: 2.1557  Epoch: 13  Global Step: 76150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:22,071-Speed 3898.19 samples/sec  Loss 1.7480  LearningRate 0.0431  ProxyLR: 2.1551  Epoch: 13  Global Step: 76160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:24,699-Speed 3898.09 samples/sec  Loss 1.6418  LearningRate 0.0431  ProxyLR: 2.1544  Epoch: 13  Global Step: 76170   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:27,327-Speed 3896.81 samples/sec  Loss 1.7040  LearningRate 0.0431  ProxyLR: 2.1538  Epoch: 13  Global Step: 76180   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:29,954-Speed 3899.17 samples/sec  Loss 1.6823  LearningRate 0.0431  ProxyLR: 2.1531  Epoch: 13  Global Step: 76190   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:32,583-Speed 3896.80 samples/sec  Loss 1.6803  LearningRate 0.0430  ProxyLR: 2.1525  Epoch: 13  Global Step: 76200   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:35,212-Speed 3896.00 samples/sec  Loss 1.6651  LearningRate 0.0430  ProxyLR: 2.1518  Epoch: 13  Global Step: 76210   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:37,840-Speed 3897.11 samples/sec  Loss 1.7185  LearningRate 0.0430  ProxyLR: 2.1512  Epoch: 13  Global Step: 76220   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:40,467-Speed 3898.32 samples/sec  Loss 1.6792  LearningRate 0.0430  ProxyLR: 2.1505  Epoch: 13  Global Step: 76230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:43,094-Speed 3899.66 samples/sec  Loss 1.6355  LearningRate 0.0430  ProxyLR: 2.1499  Epoch: 13  Global Step: 76240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:45,721-Speed 3898.33 samples/sec  Loss 1.6591  LearningRate 0.0430  ProxyLR: 2.1492  Epoch: 13  Global Step: 76250   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:19:48,337-Speed 3916.50 samples/sec  Loss 1.7126  LearningRate 0.0430  ProxyLR: 2.1485  Epoch: 13  Global Step: 76260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:50,964-Speed 3898.34 samples/sec  Loss 1.6448  LearningRate 0.0430  ProxyLR: 2.1479  Epoch: 13  Global Step: 76270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:53,592-Speed 3897.02 samples/sec  Loss 1.6880  LearningRate 0.0429  ProxyLR: 2.1472  Epoch: 13  Global Step: 76280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:56,220-Speed 3898.53 samples/sec  Loss 1.7283  LearningRate 0.0429  ProxyLR: 2.1466  Epoch: 13  Global Step: 76290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:19:58,849-Speed 3895.08 samples/sec  Loss 1.6678  LearningRate 0.0429  ProxyLR: 2.1459  Epoch: 13  Global Step: 76300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:01,477-Speed 3897.13 samples/sec  Loss 1.7475  LearningRate 0.0429  ProxyLR: 2.1453  Epoch: 13  Global Step: 76310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:04,105-Speed 3898.85 samples/sec  Loss 1.7097  LearningRate 0.0429  ProxyLR: 2.1446  Epoch: 13  Global Step: 76320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:06,731-Speed 3900.11 samples/sec  Loss 1.6682  LearningRate 0.0429  ProxyLR: 2.1440  Epoch: 13  Global Step: 76330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:09,358-Speed 3899.18 samples/sec  Loss 1.7044  LearningRate 0.0429  ProxyLR: 2.1433  Epoch: 13  Global Step: 76340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:11,984-Speed 3899.73 samples/sec  Loss 1.6760  LearningRate 0.0429  ProxyLR: 2.1427  Epoch: 13  Global Step: 76350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:14,598-Speed 3918.27 samples/sec  Loss 1.7070  LearningRate 0.0428  ProxyLR: 2.1420  Epoch: 13  Global Step: 76360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:17,224-Speed 3899.95 samples/sec  Loss 1.6883  LearningRate 0.0428  ProxyLR: 2.1414  Epoch: 13  Global Step: 76370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:19,851-Speed 3900.12 samples/sec  Loss 1.6714  LearningRate 0.0428  ProxyLR: 2.1407  Epoch: 13  Global Step: 76380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:22,464-Speed 3919.66 samples/sec  Loss 1.6215  LearningRate 0.0428  ProxyLR: 2.1401  Epoch: 13  Global Step: 76390   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:25,092-Speed 3897.83 samples/sec  Loss 1.6400  LearningRate 0.0428  ProxyLR: 2.1394  Epoch: 13  Global Step: 76400   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:27,719-Speed 3897.67 samples/sec  Loss 1.6996  LearningRate 0.0428  ProxyLR: 2.1388  Epoch: 13  Global Step: 76410   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:30,346-Speed 3898.90 samples/sec  Loss 1.6975  LearningRate 0.0428  ProxyLR: 2.1381  Epoch: 13  Global Step: 76420   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:32,973-Speed 3899.55 samples/sec  Loss 1.7429  LearningRate 0.0427  ProxyLR: 2.1375  Epoch: 13  Global Step: 76430   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:35,601-Speed 3896.84 samples/sec  Loss 1.6856  LearningRate 0.0427  ProxyLR: 2.1368  Epoch: 13  Global Step: 76440   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:38,232-Speed 3893.63 samples/sec  Loss 1.7681  LearningRate 0.0427  ProxyLR: 2.1362  Epoch: 13  Global Step: 76450   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:40,863-Speed 3892.30 samples/sec  Loss 1.6559  LearningRate 0.0427  ProxyLR: 2.1355  Epoch: 13  Global Step: 76460   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:43,493-Speed 3895.12 samples/sec  Loss 1.6621  LearningRate 0.0427  ProxyLR: 2.1349  Epoch: 13  Global Step: 76470   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:46,121-Speed 3897.48 samples/sec  Loss 1.5939  LearningRate 0.0427  ProxyLR: 2.1342  Epoch: 13  Global Step: 76480   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:20:48,748-Speed 3898.31 samples/sec  Loss 1.7447  LearningRate 0.0427  ProxyLR: 2.1336  Epoch: 13  Global Step: 76490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:51,377-Speed 3896.19 samples/sec  Loss 1.7139  LearningRate 0.0427  ProxyLR: 2.1329  Epoch: 13  Global Step: 76500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:54,006-Speed 3895.89 samples/sec  Loss 1.6916  LearningRate 0.0426  ProxyLR: 2.1323  Epoch: 13  Global Step: 76510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:56,641-Speed 3886.20 samples/sec  Loss 1.6807  LearningRate 0.0426  ProxyLR: 2.1316  Epoch: 13  Global Step: 76520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:20:59,278-Speed 3885.18 samples/sec  Loss 1.7169  LearningRate 0.0426  ProxyLR: 2.1310  Epoch: 13  Global Step: 76530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:01,915-Speed 3883.87 samples/sec  Loss 1.7645  LearningRate 0.0426  ProxyLR: 2.1303  Epoch: 13  Global Step: 76540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:04,552-Speed 3883.38 samples/sec  Loss 1.6734  LearningRate 0.0426  ProxyLR: 2.1297  Epoch: 13  Global Step: 76550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:07,188-Speed 3885.87 samples/sec  Loss 1.7016  LearningRate 0.0426  ProxyLR: 2.1290  Epoch: 13  Global Step: 76560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:09,822-Speed 3887.77 samples/sec  Loss 1.7051  LearningRate 0.0426  ProxyLR: 2.1284  Epoch: 13  Global Step: 76570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:12,444-Speed 3906.49 samples/sec  Loss 1.6628  LearningRate 0.0426  ProxyLR: 2.1277  Epoch: 13  Global Step: 76580   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:15,078-Speed 3889.75 samples/sec  Loss 1.7355  LearningRate 0.0425  ProxyLR: 2.1271  Epoch: 13  Global Step: 76590   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:17,712-Speed 3887.78 samples/sec  Loss 1.6870  LearningRate 0.0425  ProxyLR: 2.1264  Epoch: 13  Global Step: 76600   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:20,351-Speed 3881.92 samples/sec  Loss 1.7095  LearningRate 0.0425  ProxyLR: 2.1258  Epoch: 13  Global Step: 76610   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:22,989-Speed 3882.41 samples/sec  Loss 1.6905  LearningRate 0.0425  ProxyLR: 2.1251  Epoch: 13  Global Step: 76620   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:25,624-Speed 3887.82 samples/sec  Loss 1.6874  LearningRate 0.0425  ProxyLR: 2.1245  Epoch: 13  Global Step: 76630   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:28,258-Speed 3887.08 samples/sec  Loss 1.7817  LearningRate 0.0425  ProxyLR: 2.1238  Epoch: 13  Global Step: 76640   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:30,895-Speed 3884.89 samples/sec  Loss 1.7069  LearningRate 0.0425  ProxyLR: 2.1232  Epoch: 13  Global Step: 76650   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:33,529-Speed 3888.86 samples/sec  Loss 1.7423  LearningRate 0.0425  ProxyLR: 2.1225  Epoch: 13  Global Step: 76660   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:36,163-Speed 3887.86 samples/sec  Loss 1.7333  LearningRate 0.0424  ProxyLR: 2.1219  Epoch: 13  Global Step: 76670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:38,796-Speed 3890.09 samples/sec  Loss 1.6985  LearningRate 0.0424  ProxyLR: 2.1212  Epoch: 13  Global Step: 76680   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:41,430-Speed 3888.86 samples/sec  Loss 1.7817  LearningRate 0.0424  ProxyLR: 2.1206  Epoch: 13  Global Step: 76690   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:44,062-Speed 3890.61 samples/sec  Loss 1.7564  LearningRate 0.0424  ProxyLR: 2.1199  Epoch: 13  Global Step: 76700   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:46,696-Speed 3888.87 samples/sec  Loss 1.6488  LearningRate 0.0424  ProxyLR: 2.1193  Epoch: 13  Global Step: 76710   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:49,330-Speed 3888.94 samples/sec  Loss 1.7229  LearningRate 0.0424  ProxyLR: 2.1187  Epoch: 13  Global Step: 76720   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:51,962-Speed 3890.92 samples/sec  Loss 1.7614  LearningRate 0.0424  ProxyLR: 2.1180  Epoch: 13  Global Step: 76730   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:54,596-Speed 3889.49 samples/sec  Loss 1.6745  LearningRate 0.0423  ProxyLR: 2.1174  Epoch: 13  Global Step: 76740   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:21:57,217-Speed 3907.63 samples/sec  Loss 1.7164  LearningRate 0.0423  ProxyLR: 2.1167  Epoch: 13  Global Step: 76750   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:21:59,850-Speed 3889.61 samples/sec  Loss 1.6611  LearningRate 0.0423  ProxyLR: 2.1161  Epoch: 13  Global Step: 76760   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:02,483-Speed 3890.37 samples/sec  Loss 1.6938  LearningRate 0.0423  ProxyLR: 2.1154  Epoch: 13  Global Step: 76770   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:05,118-Speed 3887.18 samples/sec  Loss 1.7439  LearningRate 0.0423  ProxyLR: 2.1148  Epoch: 13  Global Step: 76780   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:07,752-Speed 3888.28 samples/sec  Loss 1.6744  LearningRate 0.0423  ProxyLR: 2.1141  Epoch: 13  Global Step: 76790   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:10,386-Speed 3888.09 samples/sec  Loss 1.6949  LearningRate 0.0423  ProxyLR: 2.1135  Epoch: 13  Global Step: 76800   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:13,020-Speed 3889.40 samples/sec  Loss 1.6612  LearningRate 0.0423  ProxyLR: 2.1128  Epoch: 13  Global Step: 76810   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:15,653-Speed 3889.40 samples/sec  Loss 1.7212  LearningRate 0.0422  ProxyLR: 2.1122  Epoch: 13  Global Step: 76820   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:18,287-Speed 3888.58 samples/sec  Loss 1.7625  LearningRate 0.0422  ProxyLR: 2.1115  Epoch: 13  Global Step: 76830   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:20,921-Speed 3889.64 samples/sec  Loss 1.7268  LearningRate 0.0422  ProxyLR: 2.1109  Epoch: 13  Global Step: 76840   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:22:23,556-Speed 3886.51 samples/sec  Loss 1.7197  LearningRate 0.0422  ProxyLR: 2.1102  Epoch: 13  Global Step: 76850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:26,193-Speed 3883.54 samples/sec  Loss 1.7272  LearningRate 0.0422  ProxyLR: 2.1096  Epoch: 13  Global Step: 76860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:28,827-Speed 3888.31 samples/sec  Loss 1.6732  LearningRate 0.0422  ProxyLR: 2.1090  Epoch: 13  Global Step: 76870   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:31,461-Speed 3888.52 samples/sec  Loss 1.6512  LearningRate 0.0422  ProxyLR: 2.1083  Epoch: 13  Global Step: 76880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:34,099-Speed 3882.62 samples/sec  Loss 1.6830  LearningRate 0.0422  ProxyLR: 2.1077  Epoch: 13  Global Step: 76890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:36,735-Speed 3886.17 samples/sec  Loss 1.6940  LearningRate 0.0421  ProxyLR: 2.1070  Epoch: 13  Global Step: 76900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:39,371-Speed 3884.98 samples/sec  Loss 1.6560  LearningRate 0.0421  ProxyLR: 2.1064  Epoch: 13  Global Step: 76910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:42,004-Speed 3891.21 samples/sec  Loss 1.6613  LearningRate 0.0421  ProxyLR: 2.1057  Epoch: 13  Global Step: 76920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:44,637-Speed 3889.43 samples/sec  Loss 1.6882  LearningRate 0.0421  ProxyLR: 2.1051  Epoch: 13  Global Step: 76930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:47,272-Speed 3886.35 samples/sec  Loss 1.6554  LearningRate 0.0421  ProxyLR: 2.1044  Epoch: 13  Global Step: 76940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:49,892-Speed 3909.41 samples/sec  Loss 1.6649  LearningRate 0.0421  ProxyLR: 2.1038  Epoch: 13  Global Step: 76950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:52,531-Speed 3882.00 samples/sec  Loss 1.7103  LearningRate 0.0421  ProxyLR: 2.1031  Epoch: 13  Global Step: 76960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:55,165-Speed 3887.58 samples/sec  Loss 1.6836  LearningRate 0.0420  ProxyLR: 2.1025  Epoch: 13  Global Step: 76970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:22:57,786-Speed 3908.72 samples/sec  Loss 1.6974  LearningRate 0.0420  ProxyLR: 2.1018  Epoch: 13  Global Step: 76980   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:00,416-Speed 3893.66 samples/sec  Loss 1.7003  LearningRate 0.0420  ProxyLR: 2.1012  Epoch: 13  Global Step: 76990   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:03,045-Speed 3896.77 samples/sec  Loss 1.6643  LearningRate 0.0420  ProxyLR: 2.1006  Epoch: 13  Global Step: 77000   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:05,674-Speed 3896.08 samples/sec  Loss 1.6887  LearningRate 0.0420  ProxyLR: 2.0999  Epoch: 13  Global Step: 77010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:08,302-Speed 3896.34 samples/sec  Loss 1.7229  LearningRate 0.0420  ProxyLR: 2.0993  Epoch: 13  Global Step: 77020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:10,929-Speed 3898.75 samples/sec  Loss 1.7008  LearningRate 0.0420  ProxyLR: 2.0986  Epoch: 13  Global Step: 77030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:13,557-Speed 3898.70 samples/sec  Loss 1.6850  LearningRate 0.0420  ProxyLR: 2.0980  Epoch: 13  Global Step: 77040   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:16,183-Speed 3899.91 samples/sec  Loss 1.7792  LearningRate 0.0419  ProxyLR: 2.0973  Epoch: 13  Global Step: 77050   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:18,812-Speed 3896.72 samples/sec  Loss 1.6654  LearningRate 0.0419  ProxyLR: 2.0967  Epoch: 13  Global Step: 77060   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:21,441-Speed 3896.08 samples/sec  Loss 1.6710  LearningRate 0.0419  ProxyLR: 2.0960  Epoch: 13  Global Step: 77070   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:24,069-Speed 3896.63 samples/sec  Loss 1.7593  LearningRate 0.0419  ProxyLR: 2.0954  Epoch: 13  Global Step: 77080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:26,698-Speed 3896.30 samples/sec  Loss 1.7241  LearningRate 0.0419  ProxyLR: 2.0948  Epoch: 13  Global Step: 77090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:29,325-Speed 3898.95 samples/sec  Loss 1.7482  LearningRate 0.0419  ProxyLR: 2.0941  Epoch: 13  Global Step: 77100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:31,953-Speed 3897.18 samples/sec  Loss 1.7649  LearningRate 0.0419  ProxyLR: 2.0935  Epoch: 13  Global Step: 77110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:34,580-Speed 3898.84 samples/sec  Loss 1.7385  LearningRate 0.0419  ProxyLR: 2.0928  Epoch: 13  Global Step: 77120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:37,207-Speed 3898.65 samples/sec  Loss 1.7453  LearningRate 0.0418  ProxyLR: 2.0922  Epoch: 13  Global Step: 77130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:39,838-Speed 3893.65 samples/sec  Loss 1.6936  LearningRate 0.0418  ProxyLR: 2.0915  Epoch: 13  Global Step: 77140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:42,466-Speed 3896.54 samples/sec  Loss 1.6992  LearningRate 0.0418  ProxyLR: 2.0909  Epoch: 13  Global Step: 77150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:45,094-Speed 3897.58 samples/sec  Loss 1.6846  LearningRate 0.0418  ProxyLR: 2.0903  Epoch: 13  Global Step: 77160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:23:47,710-Speed 3914.66 samples/sec  Loss 1.7730  LearningRate 0.0418  ProxyLR: 2.0896  Epoch: 13  Global Step: 77170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:50,339-Speed 3896.24 samples/sec  Loss 1.7664  LearningRate 0.0418  ProxyLR: 2.0890  Epoch: 13  Global Step: 77180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:52,967-Speed 3897.02 samples/sec  Loss 1.7110  LearningRate 0.0418  ProxyLR: 2.0883  Epoch: 13  Global Step: 77190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:55,596-Speed 3896.61 samples/sec  Loss 1.7071  LearningRate 0.0418  ProxyLR: 2.0877  Epoch: 13  Global Step: 77200   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:23:58,226-Speed 3894.91 samples/sec  Loss 1.6919  LearningRate 0.0417  ProxyLR: 2.0870  Epoch: 13  Global Step: 77210   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:00,854-Speed 3896.34 samples/sec  Loss 1.6675  LearningRate 0.0417  ProxyLR: 2.0864  Epoch: 13  Global Step: 77220   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:03,483-Speed 3897.27 samples/sec  Loss 1.7279  LearningRate 0.0417  ProxyLR: 2.0858  Epoch: 13  Global Step: 77230   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:06,110-Speed 3897.53 samples/sec  Loss 1.7233  LearningRate 0.0417  ProxyLR: 2.0851  Epoch: 13  Global Step: 77240   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:08,737-Speed 3898.89 samples/sec  Loss 1.7288  LearningRate 0.0417  ProxyLR: 2.0845  Epoch: 13  Global Step: 77250   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:11,365-Speed 3899.14 samples/sec  Loss 1.6680  LearningRate 0.0417  ProxyLR: 2.0838  Epoch: 13  Global Step: 77260   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:13,993-Speed 3896.35 samples/sec  Loss 1.7243  LearningRate 0.0417  ProxyLR: 2.0832  Epoch: 13  Global Step: 77270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:16,624-Speed 3894.29 samples/sec  Loss 1.7439  LearningRate 0.0417  ProxyLR: 2.0825  Epoch: 13  Global Step: 77280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:19,250-Speed 3899.43 samples/sec  Loss 1.6841  LearningRate 0.0416  ProxyLR: 2.0819  Epoch: 13  Global Step: 77290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:21,879-Speed 3896.23 samples/sec  Loss 1.7314  LearningRate 0.0416  ProxyLR: 2.0813  Epoch: 13  Global Step: 77300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:24,508-Speed 3895.53 samples/sec  Loss 1.7493  LearningRate 0.0416  ProxyLR: 2.0806  Epoch: 13  Global Step: 77310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:27,123-Speed 3916.55 samples/sec  Loss 1.7590  LearningRate 0.0416  ProxyLR: 2.0800  Epoch: 13  Global Step: 77320   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:29,751-Speed 3898.03 samples/sec  Loss 1.7064  LearningRate 0.0416  ProxyLR: 2.0793  Epoch: 13  Global Step: 77330   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:32,378-Speed 3899.13 samples/sec  Loss 1.6774  LearningRate 0.0416  ProxyLR: 2.0787  Epoch: 13  Global Step: 77340   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:35,006-Speed 3897.64 samples/sec  Loss 1.6957  LearningRate 0.0416  ProxyLR: 2.0781  Epoch: 13  Global Step: 77350   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:37,633-Speed 3897.85 samples/sec  Loss 1.7503  LearningRate 0.0415  ProxyLR: 2.0774  Epoch: 13  Global Step: 77360   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:40,260-Speed 3899.42 samples/sec  Loss 1.7033  LearningRate 0.0415  ProxyLR: 2.0768  Epoch: 13  Global Step: 77370   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:42,888-Speed 3897.35 samples/sec  Loss 1.6992  LearningRate 0.0415  ProxyLR: 2.0761  Epoch: 13  Global Step: 77380   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:45,515-Speed 3898.21 samples/sec  Loss 1.7043  LearningRate 0.0415  ProxyLR: 2.0755  Epoch: 13  Global Step: 77390   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:48,143-Speed 3897.54 samples/sec  Loss 1.6818  LearningRate 0.0415  ProxyLR: 2.0748  Epoch: 13  Global Step: 77400   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:50,773-Speed 3895.12 samples/sec  Loss 1.7086  LearningRate 0.0415  ProxyLR: 2.0742  Epoch: 13  Global Step: 77410   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:24:53,402-Speed 3895.30 samples/sec  Loss 1.7203  LearningRate 0.0415  ProxyLR: 2.0736  Epoch: 13  Global Step: 77420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:56,031-Speed 3895.79 samples/sec  Loss 1.7532  LearningRate 0.0415  ProxyLR: 2.0729  Epoch: 13  Global Step: 77430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:24:58,662-Speed 3892.97 samples/sec  Loss 1.7220  LearningRate 0.0414  ProxyLR: 2.0723  Epoch: 13  Global Step: 77440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:01,293-Speed 3893.46 samples/sec  Loss 1.6954  LearningRate 0.0414  ProxyLR: 2.0716  Epoch: 13  Global Step: 77450   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:03,922-Speed 3895.55 samples/sec  Loss 1.7339  LearningRate 0.0414  ProxyLR: 2.0710  Epoch: 13  Global Step: 77460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:06,553-Speed 3893.40 samples/sec  Loss 1.7224  LearningRate 0.0414  ProxyLR: 2.0704  Epoch: 13  Global Step: 77470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:09,184-Speed 3893.67 samples/sec  Loss 1.7680  LearningRate 0.0414  ProxyLR: 2.0697  Epoch: 13  Global Step: 77480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:11,813-Speed 3896.22 samples/sec  Loss 1.6871  LearningRate 0.0414  ProxyLR: 2.0691  Epoch: 13  Global Step: 77490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:14,443-Speed 3893.55 samples/sec  Loss 1.7411  LearningRate 0.0414  ProxyLR: 2.0684  Epoch: 13  Global Step: 77500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:17,075-Speed 3891.87 samples/sec  Loss 1.7543  LearningRate 0.0414  ProxyLR: 2.0678  Epoch: 13  Global Step: 77510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:19,692-Speed 3914.74 samples/sec  Loss 1.7531  LearningRate 0.0413  ProxyLR: 2.0672  Epoch: 13  Global Step: 77520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:22,322-Speed 3893.39 samples/sec  Loss 1.7275  LearningRate 0.0413  ProxyLR: 2.0665  Epoch: 13  Global Step: 77530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:24,954-Speed 3891.26 samples/sec  Loss 1.7024  LearningRate 0.0413  ProxyLR: 2.0659  Epoch: 13  Global Step: 77540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:27,584-Speed 3894.67 samples/sec  Loss 1.7134  LearningRate 0.0413  ProxyLR: 2.0652  Epoch: 13  Global Step: 77550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:30,215-Speed 3893.18 samples/sec  Loss 1.7765  LearningRate 0.0413  ProxyLR: 2.0646  Epoch: 13  Global Step: 77560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:32,845-Speed 3893.95 samples/sec  Loss 1.7117  LearningRate 0.0413  ProxyLR: 2.0640  Epoch: 13  Global Step: 77570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:25:35,460-Speed 3916.46 samples/sec  Loss 1.7305  LearningRate 0.0413  ProxyLR: 2.0633  Epoch: 13  Global Step: 77580   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:38,090-Speed 3894.93 samples/sec  Loss 1.7583  LearningRate 0.0413  ProxyLR: 2.0627  Epoch: 13  Global Step: 77590   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:40,720-Speed 3895.05 samples/sec  Loss 1.7956  LearningRate 0.0412  ProxyLR: 2.0620  Epoch: 13  Global Step: 77600   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:43,347-Speed 3898.23 samples/sec  Loss 1.7869  LearningRate 0.0412  ProxyLR: 2.0614  Epoch: 13  Global Step: 77610   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:45,974-Speed 3898.43 samples/sec  Loss 1.7522  LearningRate 0.0412  ProxyLR: 2.0608  Epoch: 13  Global Step: 77620   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:48,601-Speed 3898.73 samples/sec  Loss 1.7664  LearningRate 0.0412  ProxyLR: 2.0601  Epoch: 13  Global Step: 77630   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:51,229-Speed 3898.17 samples/sec  Loss 1.7152  LearningRate 0.0412  ProxyLR: 2.0595  Epoch: 13  Global Step: 77640   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:53,857-Speed 3897.81 samples/sec  Loss 1.7841  LearningRate 0.0412  ProxyLR: 2.0589  Epoch: 13  Global Step: 77650   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:56,483-Speed 3900.04 samples/sec  Loss 1.7930  LearningRate 0.0412  ProxyLR: 2.0582  Epoch: 13  Global Step: 77660   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:25:59,110-Speed 3898.55 samples/sec  Loss 1.7631  LearningRate 0.0412  ProxyLR: 2.0576  Epoch: 13  Global Step: 77670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:01,737-Speed 3899.28 samples/sec  Loss 1.7578  LearningRate 0.0411  ProxyLR: 2.0569  Epoch: 13  Global Step: 77680   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:04,364-Speed 3899.69 samples/sec  Loss 1.7893  LearningRate 0.0411  ProxyLR: 2.0563  Epoch: 13  Global Step: 77690   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:06,991-Speed 3898.50 samples/sec  Loss 1.7077  LearningRate 0.0411  ProxyLR: 2.0557  Epoch: 13  Global Step: 77700   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:09,618-Speed 3898.70 samples/sec  Loss 1.7575  LearningRate 0.0411  ProxyLR: 2.0550  Epoch: 13  Global Step: 77710   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:12,246-Speed 3896.65 samples/sec  Loss 1.6761  LearningRate 0.0411  ProxyLR: 2.0544  Epoch: 13  Global Step: 77720   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:14,874-Speed 3898.23 samples/sec  Loss 1.7095  LearningRate 0.0411  ProxyLR: 2.0538  Epoch: 13  Global Step: 77730   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:17,503-Speed 3895.72 samples/sec  Loss 1.7772  LearningRate 0.0411  ProxyLR: 2.0531  Epoch: 13  Global Step: 77740   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:20,130-Speed 3899.00 samples/sec  Loss 1.7060  LearningRate 0.0410  ProxyLR: 2.0525  Epoch: 13  Global Step: 77750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:22,758-Speed 3897.03 samples/sec  Loss 1.7516  LearningRate 0.0410  ProxyLR: 2.0518  Epoch: 13  Global Step: 77760   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:25,386-Speed 3897.94 samples/sec  Loss 1.7476  LearningRate 0.0410  ProxyLR: 2.0512  Epoch: 13  Global Step: 77770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:28,013-Speed 3898.18 samples/sec  Loss 1.7024  LearningRate 0.0410  ProxyLR: 2.0506  Epoch: 13  Global Step: 77780   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:26:30,628-Speed 3916.55 samples/sec  Loss 1.6554  LearningRate 0.0410  ProxyLR: 2.0499  Epoch: 13  Global Step: 77790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:33,255-Speed 3899.04 samples/sec  Loss 1.7267  LearningRate 0.0410  ProxyLR: 2.0493  Epoch: 13  Global Step: 77800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:35,883-Speed 3897.84 samples/sec  Loss 1.7329  LearningRate 0.0410  ProxyLR: 2.0487  Epoch: 13  Global Step: 77810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:38,509-Speed 3900.63 samples/sec  Loss 1.7027  LearningRate 0.0410  ProxyLR: 2.0480  Epoch: 13  Global Step: 77820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:26:41,124-Speed 3916.61 samples/sec  Loss 1.7881  LearningRate 0.0409  ProxyLR: 2.0474  Epoch: 13  Global Step: 77830   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:43,751-Speed 3898.44 samples/sec  Loss 1.7672  LearningRate 0.0409  ProxyLR: 2.0467  Epoch: 13  Global Step: 77840   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:46,379-Speed 3897.97 samples/sec  Loss 1.7319  LearningRate 0.0409  ProxyLR: 2.0461  Epoch: 13  Global Step: 77850   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:49,008-Speed 3895.76 samples/sec  Loss 1.7053  LearningRate 0.0409  ProxyLR: 2.0455  Epoch: 13  Global Step: 77860   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:51,635-Speed 3898.48 samples/sec  Loss 1.6964  LearningRate 0.0409  ProxyLR: 2.0448  Epoch: 13  Global Step: 77870   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:54,264-Speed 3896.14 samples/sec  Loss 1.7389  LearningRate 0.0409  ProxyLR: 2.0442  Epoch: 13  Global Step: 77880   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:56,890-Speed 3899.98 samples/sec  Loss 1.7499  LearningRate 0.0409  ProxyLR: 2.0436  Epoch: 13  Global Step: 77890   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:26:59,519-Speed 3896.96 samples/sec  Loss 1.7280  LearningRate 0.0409  ProxyLR: 2.0429  Epoch: 13  Global Step: 77900   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:02,147-Speed 3897.51 samples/sec  Loss 1.7421  LearningRate 0.0408  ProxyLR: 2.0423  Epoch: 13  Global Step: 77910   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:04,774-Speed 3898.73 samples/sec  Loss 1.7634  LearningRate 0.0408  ProxyLR: 2.0417  Epoch: 13  Global Step: 77920   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:07,403-Speed 3896.47 samples/sec  Loss 1.6979  LearningRate 0.0408  ProxyLR: 2.0410  Epoch: 13  Global Step: 77930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:10,033-Speed 3893.67 samples/sec  Loss 1.6942  LearningRate 0.0408  ProxyLR: 2.0404  Epoch: 13  Global Step: 77940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:12,664-Speed 3892.85 samples/sec  Loss 1.8622  LearningRate 0.0408  ProxyLR: 2.0397  Epoch: 13  Global Step: 77950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:15,295-Speed 3893.57 samples/sec  Loss 1.8503  LearningRate 0.0408  ProxyLR: 2.0391  Epoch: 13  Global Step: 77960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:17,927-Speed 3891.31 samples/sec  Loss 1.7410  LearningRate 0.0408  ProxyLR: 2.0385  Epoch: 13  Global Step: 77970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:20,547-Speed 3909.48 samples/sec  Loss 1.6967  LearningRate 0.0408  ProxyLR: 2.0378  Epoch: 13  Global Step: 77980   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:23,180-Speed 3890.55 samples/sec  Loss 1.7150  LearningRate 0.0407  ProxyLR: 2.0372  Epoch: 13  Global Step: 77990   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:25,811-Speed 3892.15 samples/sec  Loss 1.7282  LearningRate 0.0407  ProxyLR: 2.0366  Epoch: 13  Global Step: 78000   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:28,443-Speed 3891.80 samples/sec  Loss 1.7777  LearningRate 0.0407  ProxyLR: 2.0359  Epoch: 13  Global Step: 78010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:31,079-Speed 3885.85 samples/sec  Loss 1.7040  LearningRate 0.0407  ProxyLR: 2.0353  Epoch: 13  Global Step: 78020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:33,713-Speed 3888.97 samples/sec  Loss 1.6957  LearningRate 0.0407  ProxyLR: 2.0347  Epoch: 13  Global Step: 78030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:36,347-Speed 3887.33 samples/sec  Loss 1.6097  LearningRate 0.0407  ProxyLR: 2.0340  Epoch: 13  Global Step: 78040   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:38,981-Speed 3889.76 samples/sec  Loss 1.7900  LearningRate 0.0407  ProxyLR: 2.0334  Epoch: 13  Global Step: 78050   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:41,613-Speed 3890.84 samples/sec  Loss 1.7488  LearningRate 0.0407  ProxyLR: 2.0328  Epoch: 13  Global Step: 78060   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:44,245-Speed 3891.22 samples/sec  Loss 1.6888  LearningRate 0.0406  ProxyLR: 2.0321  Epoch: 13  Global Step: 78070   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:27:46,879-Speed 3887.99 samples/sec  Loss 1.7797  LearningRate 0.0406  ProxyLR: 2.0315  Epoch: 13  Global Step: 78080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:49,514-Speed 3887.93 samples/sec  Loss 1.7546  LearningRate 0.0406  ProxyLR: 2.0309  Epoch: 13  Global Step: 78090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:52,148-Speed 3888.07 samples/sec  Loss 1.7470  LearningRate 0.0406  ProxyLR: 2.0302  Epoch: 13  Global Step: 78100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:54,782-Speed 3889.83 samples/sec  Loss 1.7588  LearningRate 0.0406  ProxyLR: 2.0296  Epoch: 13  Global Step: 78110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:27:57,415-Speed 3889.74 samples/sec  Loss 1.7356  LearningRate 0.0406  ProxyLR: 2.0290  Epoch: 13  Global Step: 78120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:00,049-Speed 3888.45 samples/sec  Loss 1.7248  LearningRate 0.0406  ProxyLR: 2.0283  Epoch: 13  Global Step: 78130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:02,669-Speed 3908.39 samples/sec  Loss 1.7381  LearningRate 0.0406  ProxyLR: 2.0277  Epoch: 13  Global Step: 78140   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:05,304-Speed 3887.67 samples/sec  Loss 1.6861  LearningRate 0.0405  ProxyLR: 2.0271  Epoch: 13  Global Step: 78150   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:07,937-Speed 3889.90 samples/sec  Loss 1.7198  LearningRate 0.0405  ProxyLR: 2.0264  Epoch: 13  Global Step: 78160   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:10,570-Speed 3890.00 samples/sec  Loss 1.7132  LearningRate 0.0405  ProxyLR: 2.0258  Epoch: 13  Global Step: 78170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:13,205-Speed 3887.12 samples/sec  Loss 1.7668  LearningRate 0.0405  ProxyLR: 2.0252  Epoch: 13  Global Step: 78180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:15,839-Speed 3888.15 samples/sec  Loss 1.7327  LearningRate 0.0405  ProxyLR: 2.0245  Epoch: 13  Global Step: 78190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:18,473-Speed 3888.70 samples/sec  Loss 1.7669  LearningRate 0.0405  ProxyLR: 2.0239  Epoch: 13  Global Step: 78200   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:21,109-Speed 3885.91 samples/sec  Loss 1.6968  LearningRate 0.0405  ProxyLR: 2.0233  Epoch: 13  Global Step: 78210   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:23,743-Speed 3888.43 samples/sec  Loss 1.7372  LearningRate 0.0405  ProxyLR: 2.0226  Epoch: 13  Global Step: 78220   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:26,376-Speed 3889.91 samples/sec  Loss 1.6420  LearningRate 0.0404  ProxyLR: 2.0220  Epoch: 13  Global Step: 78230   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:28:29,012-Speed 3886.03 samples/sec  Loss 1.6885  LearningRate 0.0404  ProxyLR: 2.0214  Epoch: 13  Global Step: 78240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:31,648-Speed 3884.47 samples/sec  Loss 1.7747  LearningRate 0.0404  ProxyLR: 2.0207  Epoch: 13  Global Step: 78250   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:34,283-Speed 3888.11 samples/sec  Loss 1.6846  LearningRate 0.0404  ProxyLR: 2.0201  Epoch: 13  Global Step: 78260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:36,919-Speed 3885.42 samples/sec  Loss 1.7298  LearningRate 0.0404  ProxyLR: 2.0195  Epoch: 13  Global Step: 78270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:39,557-Speed 3882.67 samples/sec  Loss 1.7062  LearningRate 0.0404  ProxyLR: 2.0188  Epoch: 13  Global Step: 78280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:42,193-Speed 3885.08 samples/sec  Loss 1.6710  LearningRate 0.0404  ProxyLR: 2.0182  Epoch: 13  Global Step: 78290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:44,829-Speed 3885.66 samples/sec  Loss 1.7157  LearningRate 0.0404  ProxyLR: 2.0176  Epoch: 13  Global Step: 78300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:47,465-Speed 3884.77 samples/sec  Loss 1.7625  LearningRate 0.0403  ProxyLR: 2.0169  Epoch: 13  Global Step: 78310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:50,099-Speed 3889.11 samples/sec  Loss 1.7532  LearningRate 0.0403  ProxyLR: 2.0163  Epoch: 13  Global Step: 78320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:52,736-Speed 3884.69 samples/sec  Loss 1.7324  LearningRate 0.0403  ProxyLR: 2.0157  Epoch: 13  Global Step: 78330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:55,359-Speed 3905.12 samples/sec  Loss 1.7186  LearningRate 0.0403  ProxyLR: 2.0150  Epoch: 13  Global Step: 78340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:28:57,994-Speed 3886.06 samples/sec  Loss 1.7730  LearningRate 0.0403  ProxyLR: 2.0144  Epoch: 13  Global Step: 78350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:00,632-Speed 3883.87 samples/sec  Loss 1.7225  LearningRate 0.0403  ProxyLR: 2.0138  Epoch: 13  Global Step: 78360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:03,265-Speed 3888.56 samples/sec  Loss 1.7508  LearningRate 0.0403  ProxyLR: 2.0131  Epoch: 13  Global Step: 78370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:05,900-Speed 3887.21 samples/sec  Loss 1.8037  LearningRate 0.0403  ProxyLR: 2.0125  Epoch: 13  Global Step: 78380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:08,536-Speed 3886.75 samples/sec  Loss 1.7603  LearningRate 0.0402  ProxyLR: 2.0119  Epoch: 13  Global Step: 78390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:11,171-Speed 3886.89 samples/sec  Loss 1.7713  LearningRate 0.0402  ProxyLR: 2.0113  Epoch: 13  Global Step: 78400   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:13,806-Speed 3887.32 samples/sec  Loss 1.7153  LearningRate 0.0402  ProxyLR: 2.0106  Epoch: 13  Global Step: 78410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:16,440-Speed 3888.58 samples/sec  Loss 1.7552  LearningRate 0.0402  ProxyLR: 2.0100  Epoch: 13  Global Step: 78420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:19,075-Speed 3886.83 samples/sec  Loss 1.7335  LearningRate 0.0402  ProxyLR: 2.0094  Epoch: 13  Global Step: 78430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:21,694-Speed 3910.00 samples/sec  Loss 1.7923  LearningRate 0.0402  ProxyLR: 2.0087  Epoch: 13  Global Step: 78440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:24,328-Speed 3888.22 samples/sec  Loss 1.6972  LearningRate 0.0402  ProxyLR: 2.0081  Epoch: 13  Global Step: 78450   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:26,963-Speed 3887.73 samples/sec  Loss 1.7050  LearningRate 0.0401  ProxyLR: 2.0075  Epoch: 13  Global Step: 78460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:29,594-Speed 3893.04 samples/sec  Loss 1.7826  LearningRate 0.0401  ProxyLR: 2.0068  Epoch: 13  Global Step: 78470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:32,229-Speed 3886.52 samples/sec  Loss 1.7563  LearningRate 0.0401  ProxyLR: 2.0062  Epoch: 13  Global Step: 78480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:34,858-Speed 3896.12 samples/sec  Loss 1.7445  LearningRate 0.0401  ProxyLR: 2.0056  Epoch: 13  Global Step: 78490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:37,490-Speed 3892.12 samples/sec  Loss 1.6457  LearningRate 0.0401  ProxyLR: 2.0049  Epoch: 13  Global Step: 78500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:40,122-Speed 3891.60 samples/sec  Loss 1.7451  LearningRate 0.0401  ProxyLR: 2.0043  Epoch: 13  Global Step: 78510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:42,753-Speed 3892.57 samples/sec  Loss 1.7628  LearningRate 0.0401  ProxyLR: 2.0037  Epoch: 13  Global Step: 78520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:45,385-Speed 3891.20 samples/sec  Loss 1.6887  LearningRate 0.0401  ProxyLR: 2.0031  Epoch: 13  Global Step: 78530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:48,001-Speed 3915.47 samples/sec  Loss 1.6927  LearningRate 0.0400  ProxyLR: 2.0024  Epoch: 13  Global Step: 78540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:50,631-Speed 3894.50 samples/sec  Loss 1.6968  LearningRate 0.0400  ProxyLR: 2.0018  Epoch: 13  Global Step: 78550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:53,262-Speed 3892.41 samples/sec  Loss 1.7988  LearningRate 0.0400  ProxyLR: 2.0012  Epoch: 13  Global Step: 78560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:55,895-Speed 3890.87 samples/sec  Loss 1.6464  LearningRate 0.0400  ProxyLR: 2.0005  Epoch: 13  Global Step: 78570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:29:58,523-Speed 3898.05 samples/sec  Loss 1.7135  LearningRate 0.0400  ProxyLR: 1.9999  Epoch: 13  Global Step: 78580   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:01,151-Speed 3896.89 samples/sec  Loss 1.7166  LearningRate 0.0400  ProxyLR: 1.9993  Epoch: 13  Global Step: 78590   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:03,780-Speed 3895.62 samples/sec  Loss 1.7100  LearningRate 0.0400  ProxyLR: 1.9987  Epoch: 13  Global Step: 78600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:06,409-Speed 3896.22 samples/sec  Loss 1.7543  LearningRate 0.0400  ProxyLR: 1.9980  Epoch: 13  Global Step: 78610   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:09,039-Speed 3894.88 samples/sec  Loss 1.7797  LearningRate 0.0399  ProxyLR: 1.9974  Epoch: 13  Global Step: 78620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:11,667-Speed 3897.10 samples/sec  Loss 1.6954  LearningRate 0.0399  ProxyLR: 1.9968  Epoch: 13  Global Step: 78630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:14,282-Speed 3915.79 samples/sec  Loss 1.8116  LearningRate 0.0399  ProxyLR: 1.9961  Epoch: 13  Global Step: 78640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:16,910-Speed 3898.46 samples/sec  Loss 1.7222  LearningRate 0.0399  ProxyLR: 1.9955  Epoch: 13  Global Step: 78650   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:19,538-Speed 3896.54 samples/sec  Loss 1.7976  LearningRate 0.0399  ProxyLR: 1.9949  Epoch: 13  Global Step: 78660   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:22,166-Speed 3897.26 samples/sec  Loss 1.8180  LearningRate 0.0399  ProxyLR: 1.9943  Epoch: 13  Global Step: 78670   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:24,795-Speed 3896.21 samples/sec  Loss 1.6859  LearningRate 0.0399  ProxyLR: 1.9936  Epoch: 13  Global Step: 78680   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:27,409-Speed 3918.17 samples/sec  Loss 1.7383  LearningRate 0.0399  ProxyLR: 1.9930  Epoch: 13  Global Step: 78690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:30,036-Speed 3898.96 samples/sec  Loss 1.7563  LearningRate 0.0398  ProxyLR: 1.9924  Epoch: 13  Global Step: 78700   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:32,664-Speed 3898.17 samples/sec  Loss 1.7135  LearningRate 0.0398  ProxyLR: 1.9917  Epoch: 13  Global Step: 78710   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:35,296-Speed 3890.30 samples/sec  Loss 1.6868  LearningRate 0.0398  ProxyLR: 1.9911  Epoch: 13  Global Step: 78720   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:37,929-Speed 3890.75 samples/sec  Loss 1.7786  LearningRate 0.0398  ProxyLR: 1.9905  Epoch: 13  Global Step: 78730   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:40,558-Speed 3895.01 samples/sec  Loss 1.7481  LearningRate 0.0398  ProxyLR: 1.9899  Epoch: 13  Global Step: 78740   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:43,186-Speed 3897.52 samples/sec  Loss 1.7099  LearningRate 0.0398  ProxyLR: 1.9892  Epoch: 13  Global Step: 78750   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:45,816-Speed 3894.68 samples/sec  Loss 1.7769  LearningRate 0.0398  ProxyLR: 1.9886  Epoch: 13  Global Step: 78760   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:48,445-Speed 3896.88 samples/sec  Loss 1.6800  LearningRate 0.0398  ProxyLR: 1.9880  Epoch: 13  Global Step: 78770   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:51,074-Speed 3895.45 samples/sec  Loss 1.7410  LearningRate 0.0397  ProxyLR: 1.9873  Epoch: 13  Global Step: 78780   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:30:53,702-Speed 3896.88 samples/sec  Loss 1.8025  LearningRate 0.0397  ProxyLR: 1.9867  Epoch: 13  Global Step: 78790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:56,330-Speed 3897.50 samples/sec  Loss 1.7564  LearningRate 0.0397  ProxyLR: 1.9861  Epoch: 13  Global Step: 78800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:30:58,958-Speed 3898.18 samples/sec  Loss 1.6702  LearningRate 0.0397  ProxyLR: 1.9855  Epoch: 13  Global Step: 78810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:01,587-Speed 3895.25 samples/sec  Loss 1.7444  LearningRate 0.0397  ProxyLR: 1.9848  Epoch: 13  Global Step: 78820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:04,214-Speed 3899.40 samples/sec  Loss 1.7120  LearningRate 0.0397  ProxyLR: 1.9842  Epoch: 13  Global Step: 78830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:06,843-Speed 3896.59 samples/sec  Loss 1.7449  LearningRate 0.0397  ProxyLR: 1.9836  Epoch: 13  Global Step: 78840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:09,470-Speed 3898.27 samples/sec  Loss 1.7017  LearningRate 0.0397  ProxyLR: 1.9830  Epoch: 13  Global Step: 78850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:12,097-Speed 3899.03 samples/sec  Loss 1.7045  LearningRate 0.0396  ProxyLR: 1.9823  Epoch: 13  Global Step: 78860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:14,725-Speed 3896.53 samples/sec  Loss 1.7651  LearningRate 0.0396  ProxyLR: 1.9817  Epoch: 13  Global Step: 78870   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:17,354-Speed 3896.60 samples/sec  Loss 1.7634  LearningRate 0.0396  ProxyLR: 1.9811  Epoch: 13  Global Step: 78880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:19,969-Speed 3917.18 samples/sec  Loss 1.7281  LearningRate 0.0396  ProxyLR: 1.9805  Epoch: 13  Global Step: 78890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:22,598-Speed 3895.78 samples/sec  Loss 1.7985  LearningRate 0.0396  ProxyLR: 1.9798  Epoch: 13  Global Step: 78900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:25,225-Speed 3898.34 samples/sec  Loss 1.7518  LearningRate 0.0396  ProxyLR: 1.9792  Epoch: 13  Global Step: 78910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:27,856-Speed 3893.44 samples/sec  Loss 1.7047  LearningRate 0.0396  ProxyLR: 1.9786  Epoch: 13  Global Step: 78920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:30,490-Speed 3888.51 samples/sec  Loss 1.7424  LearningRate 0.0396  ProxyLR: 1.9779  Epoch: 13  Global Step: 78930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:33,125-Speed 3887.25 samples/sec  Loss 1.7216  LearningRate 0.0395  ProxyLR: 1.9773  Epoch: 13  Global Step: 78940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:35,758-Speed 3889.53 samples/sec  Loss 1.6871  LearningRate 0.0395  ProxyLR: 1.9767  Epoch: 13  Global Step: 78950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:38,391-Speed 3890.42 samples/sec  Loss 1.7302  LearningRate 0.0395  ProxyLR: 1.9761  Epoch: 13  Global Step: 78960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:41,022-Speed 3892.59 samples/sec  Loss 1.7336  LearningRate 0.0395  ProxyLR: 1.9754  Epoch: 13  Global Step: 78970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:31:43,641-Speed 3910.68 samples/sec  Loss 1.7425  LearningRate 0.0395  ProxyLR: 1.9748  Epoch: 13  Global Step: 78980   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:31:46,273-Speed 3891.96 samples/sec  Loss 1.7331  LearningRate 0.0395  ProxyLR: 1.9742  Epoch: 13  Global Step: 78990   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:31:48,906-Speed 3889.27 samples/sec  Loss 1.6708  LearningRate 0.0395  ProxyLR: 1.9736  Epoch: 13  Global Step: 79000   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:31:51,537-Speed 3892.90 samples/sec  Loss 1.7577  LearningRate 0.0395  ProxyLR: 1.9729  Epoch: 13  Global Step: 79010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:31:54,170-Speed 3889.98 samples/sec  Loss 1.8012  LearningRate 0.0394  ProxyLR: 1.9723  Epoch: 13  Global Step: 79020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:31:56,802-Speed 3891.97 samples/sec  Loss 1.6535  LearningRate 0.0394  ProxyLR: 1.9717  Epoch: 13  Global Step: 79030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:31:59,440-Speed 3883.28 samples/sec  Loss 1.7309  LearningRate 0.0394  ProxyLR: 1.9711  Epoch: 13  Global Step: 79040   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:02,074-Speed 3888.44 samples/sec  Loss 1.8146  LearningRate 0.0394  ProxyLR: 1.9704  Epoch: 13  Global Step: 79050   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:04,706-Speed 3890.63 samples/sec  Loss 1.6718  LearningRate 0.0394  ProxyLR: 1.9698  Epoch: 13  Global Step: 79060   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:07,343-Speed 3884.94 samples/sec  Loss 1.7527  LearningRate 0.0394  ProxyLR: 1.9692  Epoch: 13  Global Step: 79070   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:09,976-Speed 3889.76 samples/sec  Loss 1.7439  LearningRate 0.0394  ProxyLR: 1.9686  Epoch: 13  Global Step: 79080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:32:12,608-Speed 3891.41 samples/sec  Loss 1.7764  LearningRate 0.0394  ProxyLR: 1.9680  Epoch: 13  Global Step: 79090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:32:15,239-Speed 3893.15 samples/sec  Loss 1.7730  LearningRate 0.0393  ProxyLR: 1.9673  Epoch: 13  Global Step: 79100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:32:17,872-Speed 3889.51 samples/sec  Loss 1.7467  LearningRate 0.0393  ProxyLR: 1.9667  Epoch: 13  Global Step: 79110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:32:20,505-Speed 3889.60 samples/sec  Loss 1.8215  LearningRate 0.0393  ProxyLR: 1.9661  Epoch: 13  Global Step: 79120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:32:23,126-Speed 3908.94 samples/sec  Loss 1.7960  LearningRate 0.0393  ProxyLR: 1.9655  Epoch: 13  Global Step: 79130   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:25,762-Speed 3884.55 samples/sec  Loss 1.6519  LearningRate 0.0393  ProxyLR: 1.9648  Epoch: 13  Global Step: 79140   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:28,397-Speed 3887.97 samples/sec  Loss 1.7369  LearningRate 0.0393  ProxyLR: 1.9642  Epoch: 13  Global Step: 79150   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:31,030-Speed 3888.88 samples/sec  Loss 1.8343  LearningRate 0.0393  ProxyLR: 1.9636  Epoch: 13  Global Step: 79160   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:33,665-Speed 3887.70 samples/sec  Loss 1.7581  LearningRate 0.0393  ProxyLR: 1.9630  Epoch: 13  Global Step: 79170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:36,298-Speed 3890.28 samples/sec  Loss 1.7773  LearningRate 0.0392  ProxyLR: 1.9623  Epoch: 13  Global Step: 79180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:38,932-Speed 3888.25 samples/sec  Loss 1.7520  LearningRate 0.0392  ProxyLR: 1.9617  Epoch: 13  Global Step: 79190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:41,565-Speed 3890.13 samples/sec  Loss 1.7208  LearningRate 0.0392  ProxyLR: 1.9611  Epoch: 13  Global Step: 79200   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:44,200-Speed 3886.76 samples/sec  Loss 1.8073  LearningRate 0.0392  ProxyLR: 1.9605  Epoch: 13  Global Step: 79210   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:46,833-Speed 3890.34 samples/sec  Loss 1.7134  LearningRate 0.0392  ProxyLR: 1.9598  Epoch: 13  Global Step: 79220   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:49,468-Speed 3887.45 samples/sec  Loss 1.6947  LearningRate 0.0392  ProxyLR: 1.9592  Epoch: 13  Global Step: 79230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:32:52,087-Speed 3910.12 samples/sec  Loss 1.7965  LearningRate 0.0392  ProxyLR: 1.9586  Epoch: 13  Global Step: 79240   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:54,717-Speed 3894.24 samples/sec  Loss 1.7885  LearningRate 0.0392  ProxyLR: 1.9580  Epoch: 13  Global Step: 79250   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:57,348-Speed 3893.16 samples/sec  Loss 1.7489  LearningRate 0.0391  ProxyLR: 1.9574  Epoch: 13  Global Step: 79260   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:32:59,977-Speed 3895.90 samples/sec  Loss 1.7029  LearningRate 0.0391  ProxyLR: 1.9567  Epoch: 13  Global Step: 79270   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:02,609-Speed 3891.92 samples/sec  Loss 1.8311  LearningRate 0.0391  ProxyLR: 1.9561  Epoch: 13  Global Step: 79280   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:05,240-Speed 3892.47 samples/sec  Loss 1.8186  LearningRate 0.0391  ProxyLR: 1.9555  Epoch: 13  Global Step: 79290   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:07,871-Speed 3893.86 samples/sec  Loss 1.6996  LearningRate 0.0391  ProxyLR: 1.9549  Epoch: 13  Global Step: 79300   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:10,503-Speed 3890.92 samples/sec  Loss 1.7354  LearningRate 0.0391  ProxyLR: 1.9542  Epoch: 13  Global Step: 79310   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:13,135-Speed 3892.44 samples/sec  Loss 1.7001  LearningRate 0.0391  ProxyLR: 1.9536  Epoch: 13  Global Step: 79320   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:15,766-Speed 3891.75 samples/sec  Loss 1.7520  LearningRate 0.0391  ProxyLR: 1.9530  Epoch: 13  Global Step: 79330   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:18,399-Speed 3890.98 samples/sec  Loss 1.7802  LearningRate 0.0390  ProxyLR: 1.9524  Epoch: 13  Global Step: 79340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:33:21,031-Speed 3891.26 samples/sec  Loss 1.8047  LearningRate 0.0390  ProxyLR: 1.9518  Epoch: 13  Global Step: 79350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:33:23,653-Speed 3906.29 samples/sec  Loss 1.7667  LearningRate 0.0390  ProxyLR: 1.9511  Epoch: 13  Global Step: 79360   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:26,286-Speed 3890.43 samples/sec  Loss 1.7273  LearningRate 0.0390  ProxyLR: 1.9505  Epoch: 13  Global Step: 79370   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:28,918-Speed 3890.14 samples/sec  Loss 1.6820  LearningRate 0.0390  ProxyLR: 1.9499  Epoch: 13  Global Step: 79380   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:31,554-Speed 3886.69 samples/sec  Loss 1.7190  LearningRate 0.0390  ProxyLR: 1.9493  Epoch: 13  Global Step: 79390   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:34,186-Speed 3890.84 samples/sec  Loss 1.7878  LearningRate 0.0390  ProxyLR: 1.9486  Epoch: 13  Global Step: 79400   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:36,819-Speed 3890.04 samples/sec  Loss 1.6874  LearningRate 0.0390  ProxyLR: 1.9480  Epoch: 13  Global Step: 79410   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:39,454-Speed 3887.80 samples/sec  Loss 1.7543  LearningRate 0.0389  ProxyLR: 1.9474  Epoch: 13  Global Step: 79420   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:42,084-Speed 3893.14 samples/sec  Loss 1.8047  LearningRate 0.0389  ProxyLR: 1.9468  Epoch: 13  Global Step: 79430   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:44,717-Speed 3890.45 samples/sec  Loss 1.7582  LearningRate 0.0389  ProxyLR: 1.9462  Epoch: 13  Global Step: 79440   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:47,347-Speed 3894.31 samples/sec  Loss 1.7518  LearningRate 0.0389  ProxyLR: 1.9455  Epoch: 13  Global Step: 79450   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:49,965-Speed 3912.76 samples/sec  Loss 1.7092  LearningRate 0.0389  ProxyLR: 1.9449  Epoch: 13  Global Step: 79460   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:52,595-Speed 3893.63 samples/sec  Loss 1.7613  LearningRate 0.0389  ProxyLR: 1.9443  Epoch: 13  Global Step: 79470   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:55,227-Speed 3892.42 samples/sec  Loss 1.7020  LearningRate 0.0389  ProxyLR: 1.9437  Epoch: 13  Global Step: 79480   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:33:57,861-Speed 3888.66 samples/sec  Loss 1.6949  LearningRate 0.0389  ProxyLR: 1.9431  Epoch: 13  Global Step: 79490   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:00,497-Speed 3885.94 samples/sec  Loss 1.7067  LearningRate 0.0388  ProxyLR: 1.9424  Epoch: 13  Global Step: 79500   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:03,134-Speed 3882.89 samples/sec  Loss 1.8147  LearningRate 0.0388  ProxyLR: 1.9418  Epoch: 13  Global Step: 79510   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:05,772-Speed 3883.26 samples/sec  Loss 1.7471  LearningRate 0.0388  ProxyLR: 1.9412  Epoch: 13  Global Step: 79520   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:08,407-Speed 3887.66 samples/sec  Loss 1.7222  LearningRate 0.0388  ProxyLR: 1.9406  Epoch: 13  Global Step: 79530   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:11,035-Speed 3896.76 samples/sec  Loss 1.7730  LearningRate 0.0388  ProxyLR: 1.9400  Epoch: 13  Global Step: 79540   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:13,662-Speed 3898.45 samples/sec  Loss 1.6392  LearningRate 0.0388  ProxyLR: 1.9393  Epoch: 13  Global Step: 79550   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:34:16,293-Speed 3894.07 samples/sec  Loss 1.7933  LearningRate 0.0388  ProxyLR: 1.9387  Epoch: 13  Global Step: 79560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:18,923-Speed 3893.11 samples/sec  Loss 1.6788  LearningRate 0.0388  ProxyLR: 1.9381  Epoch: 13  Global Step: 79570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:21,553-Speed 3894.47 samples/sec  Loss 1.7836  LearningRate 0.0387  ProxyLR: 1.9375  Epoch: 13  Global Step: 79580   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:24,184-Speed 3893.70 samples/sec  Loss 1.7302  LearningRate 0.0387  ProxyLR: 1.9369  Epoch: 13  Global Step: 79590   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:26,869-Speed 3814.16 samples/sec  Loss 1.7548  LearningRate 0.0387  ProxyLR: 1.9362  Epoch: 13  Global Step: 79600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:35,729-Speed 1155.99 samples/sec  Loss 1.5732  LearningRate 0.0387  ProxyLR: 1.9356  Epoch: 14  Global Step: 79610   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:38,361-Speed 3890.86 samples/sec  Loss 1.4841  LearningRate 0.0387  ProxyLR: 1.9350  Epoch: 14  Global Step: 79620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:40,999-Speed 3882.38 samples/sec  Loss 1.4353  LearningRate 0.0387  ProxyLR: 1.9344  Epoch: 14  Global Step: 79630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:43,654-Speed 3858.11 samples/sec  Loss 1.4022  LearningRate 0.0387  ProxyLR: 1.9338  Epoch: 14  Global Step: 79640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:46,289-Speed 3887.23 samples/sec  Loss 1.3852  LearningRate 0.0387  ProxyLR: 1.9332  Epoch: 14  Global Step: 79650   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:48,910-Speed 3908.30 samples/sec  Loss 1.4384  LearningRate 0.0387  ProxyLR: 1.9325  Epoch: 14  Global Step: 79660   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:51,544-Speed 3888.13 samples/sec  Loss 1.3947  LearningRate 0.0386  ProxyLR: 1.9319  Epoch: 14  Global Step: 79670   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:54,178-Speed 3888.43 samples/sec  Loss 1.3676  LearningRate 0.0386  ProxyLR: 1.9313  Epoch: 14  Global Step: 79680   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:56,814-Speed 3886.45 samples/sec  Loss 1.3869  LearningRate 0.0386  ProxyLR: 1.9307  Epoch: 14  Global Step: 79690   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:34:59,450-Speed 3884.77 samples/sec  Loss 1.3949  LearningRate 0.0386  ProxyLR: 1.9301  Epoch: 14  Global Step: 79700   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:02,087-Speed 3884.88 samples/sec  Loss 1.3836  LearningRate 0.0386  ProxyLR: 1.9294  Epoch: 14  Global Step: 79710   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:04,747-Speed 3850.13 samples/sec  Loss 1.3738  LearningRate 0.0386  ProxyLR: 1.9288  Epoch: 14  Global Step: 79720   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:07,384-Speed 3884.71 samples/sec  Loss 1.3971  LearningRate 0.0386  ProxyLR: 1.9282  Epoch: 14  Global Step: 79730   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:10,018-Speed 3889.20 samples/sec  Loss 1.4009  LearningRate 0.0386  ProxyLR: 1.9276  Epoch: 14  Global Step: 79740   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:12,653-Speed 3886.99 samples/sec  Loss 1.4441  LearningRate 0.0385  ProxyLR: 1.9270  Epoch: 14  Global Step: 79750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:15,286-Speed 3890.22 samples/sec  Loss 1.3211  LearningRate 0.0385  ProxyLR: 1.9264  Epoch: 14  Global Step: 79760   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:35:17,905-Speed 3910.53 samples/sec  Loss 1.3509  LearningRate 0.0385  ProxyLR: 1.9257  Epoch: 14  Global Step: 79770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:20,538-Speed 3889.92 samples/sec  Loss 1.3841  LearningRate 0.0385  ProxyLR: 1.9251  Epoch: 14  Global Step: 79780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:23,170-Speed 3891.50 samples/sec  Loss 1.4356  LearningRate 0.0385  ProxyLR: 1.9245  Epoch: 14  Global Step: 79790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:25,803-Speed 3890.79 samples/sec  Loss 1.3825  LearningRate 0.0385  ProxyLR: 1.9239  Epoch: 14  Global Step: 79800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:28,435-Speed 3891.57 samples/sec  Loss 1.4632  LearningRate 0.0385  ProxyLR: 1.9233  Epoch: 14  Global Step: 79810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:31,117-Speed 3817.86 samples/sec  Loss 1.4208  LearningRate 0.0385  ProxyLR: 1.9227  Epoch: 14  Global Step: 79820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:33,749-Speed 3892.21 samples/sec  Loss 1.4187  LearningRate 0.0384  ProxyLR: 1.9220  Epoch: 14  Global Step: 79830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:36,380-Speed 3893.14 samples/sec  Loss 1.3931  LearningRate 0.0384  ProxyLR: 1.9214  Epoch: 14  Global Step: 79840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:39,011-Speed 3893.21 samples/sec  Loss 1.3716  LearningRate 0.0384  ProxyLR: 1.9208  Epoch: 14  Global Step: 79850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:41,644-Speed 3889.92 samples/sec  Loss 1.3702  LearningRate 0.0384  ProxyLR: 1.9202  Epoch: 14  Global Step: 79860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:44,262-Speed 3913.55 samples/sec  Loss 1.3775  LearningRate 0.0384  ProxyLR: 1.9196  Epoch: 14  Global Step: 79870   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:46,944-Speed 3818.77 samples/sec  Loss 1.3996  LearningRate 0.0384  ProxyLR: 1.9190  Epoch: 14  Global Step: 79880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:49,574-Speed 3894.07 samples/sec  Loss 1.4108  LearningRate 0.0384  ProxyLR: 1.9183  Epoch: 14  Global Step: 79890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:52,205-Speed 3893.54 samples/sec  Loss 1.4204  LearningRate 0.0384  ProxyLR: 1.9177  Epoch: 14  Global Step: 79900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:54,835-Speed 3894.27 samples/sec  Loss 1.3954  LearningRate 0.0383  ProxyLR: 1.9171  Epoch: 14  Global Step: 79910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:35:57,467-Speed 3891.35 samples/sec  Loss 1.4013  LearningRate 0.0383  ProxyLR: 1.9165  Epoch: 14  Global Step: 79920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:00,100-Speed 3890.74 samples/sec  Loss 1.4108  LearningRate 0.0383  ProxyLR: 1.9159  Epoch: 14  Global Step: 79930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:02,731-Speed 3893.29 samples/sec  Loss 1.3520  LearningRate 0.0383  ProxyLR: 1.9153  Epoch: 14  Global Step: 79940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:05,361-Speed 3894.13 samples/sec  Loss 1.4329  LearningRate 0.0383  ProxyLR: 1.9146  Epoch: 14  Global Step: 79950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:07,992-Speed 3893.38 samples/sec  Loss 1.4398  LearningRate 0.0383  ProxyLR: 1.9140  Epoch: 14  Global Step: 79960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:10,610-Speed 3912.16 samples/sec  Loss 1.4489  LearningRate 0.0383  ProxyLR: 1.9134  Epoch: 14  Global Step: 79970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:13,240-Speed 3894.00 samples/sec  Loss 1.4473  LearningRate 0.0383  ProxyLR: 1.9128  Epoch: 14  Global Step: 79980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:36:15,857-Speed 3914.00 samples/sec  Loss 1.4274  LearningRate 0.0382  ProxyLR: 1.9122  Epoch: 14  Global Step: 79990   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:36:18,487-Speed 3894.55 samples/sec  Loss 1.3888  LearningRate 0.0382  ProxyLR: 1.9116  Epoch: 14  Global Step: 80000   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:37:08,541-[lfw][80000]XNorm: 22.163542
Training: 2023-05-04 20:37:08,541-[lfw][80000]Accuracy-Flip: 0.99717+-0.00342
Training: 2023-05-04 20:37:08,542-[lfw][80000]Accuracy-Highest: 0.99750
Training: 2023-05-04 20:37:08,542-[lfw][80000]TPR@1stNon-Zero-FPR of 0.00033: 0.99500
Training: 2023-05-04 20:37:08,542-[lfw][80000]Highest TPR@FPR: 0.99567
Training: 2023-05-04 20:38:05,846-[cfp_fp][80000]XNorm: 21.761494
Training: 2023-05-04 20:38:05,847-[cfp_fp][80000]Accuracy-Flip: 0.97629+-0.00811
Training: 2023-05-04 20:38:05,847-[cfp_fp][80000]Accuracy-Highest: 0.97629
Training: 2023-05-04 20:38:05,847-[cfp_fp][80000]TPR@1stNon-Zero-FPR of 0.00029: 0.86514
Training: 2023-05-04 20:38:05,847-[cfp_fp][80000]Highest TPR@FPR: 0.86514
Training: 2023-05-04 20:38:55,690-[agedb_30][80000]XNorm: 22.149824
Training: 2023-05-04 20:38:55,691-[agedb_30][80000]Accuracy-Flip: 0.97067+-0.01020
Training: 2023-05-04 20:38:55,691-[agedb_30][80000]Accuracy-Highest: 0.97067
Training: 2023-05-04 20:38:55,691-[agedb_30][80000]TPR@1stNon-Zero-FPR of 0.00033: 0.77467
Training: 2023-05-04 20:38:55,691-[agedb_30][80000]Highest TPR@FPR: 0.77467
Training: 2023-05-04 20:39:46,955-[calfw][80000]XNorm: 22.214291
Training: 2023-05-04 20:39:46,955-[calfw][80000]Accuracy-Flip: 0.95817+-0.01109
Training: 2023-05-04 20:39:46,956-[calfw][80000]Accuracy-Highest: 0.95817
Training: 2023-05-04 20:39:46,956-[calfw][80000]TPR@1stNon-Zero-FPR of 0.00033: 0.85533
Training: 2023-05-04 20:39:46,956-[calfw][80000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 20:40:38,348-[cplfw][80000]XNorm: 21.545323
Training: 2023-05-04 20:40:38,348-[cplfw][80000]Accuracy-Flip: 0.91883+-0.01220
Training: 2023-05-04 20:40:38,348-[cplfw][80000]Accuracy-Highest: 0.92183
Training: 2023-05-04 20:40:38,349-[cplfw][80000]TPR@1stNon-Zero-FPR of 0.00033: 0.00600
Training: 2023-05-04 20:40:38,349-[cplfw][80000]Highest TPR@FPR: 0.01533
Training: 2023-05-04 20:40:41,015-Speed 39.01 samples/sec  Loss 1.4099  LearningRate 0.0382  ProxyLR: 1.9109  Epoch: 14  Global Step: 80010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:43,636-Speed 3907.91 samples/sec  Loss 1.4353  LearningRate 0.0382  ProxyLR: 1.9103  Epoch: 14  Global Step: 80020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:46,257-Speed 3906.78 samples/sec  Loss 1.4535  LearningRate 0.0382  ProxyLR: 1.9097  Epoch: 14  Global Step: 80030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:48,879-Speed 3906.63 samples/sec  Loss 1.3913  LearningRate 0.0382  ProxyLR: 1.9091  Epoch: 14  Global Step: 80040   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:51,504-Speed 3902.09 samples/sec  Loss 1.4389  LearningRate 0.0382  ProxyLR: 1.9085  Epoch: 14  Global Step: 80050   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:54,128-Speed 3903.58 samples/sec  Loss 1.4561  LearningRate 0.0382  ProxyLR: 1.9079  Epoch: 14  Global Step: 80060   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:56,754-Speed 3900.30 samples/sec  Loss 1.3622  LearningRate 0.0381  ProxyLR: 1.9073  Epoch: 14  Global Step: 80070   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:40:59,380-Speed 3900.58 samples/sec  Loss 1.4195  LearningRate 0.0381  ProxyLR: 1.9066  Epoch: 14  Global Step: 80080   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:41:02,007-Speed 3899.11 samples/sec  Loss 1.4230  LearningRate 0.0381  ProxyLR: 1.9060  Epoch: 14  Global Step: 80090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:04,634-Speed 3899.39 samples/sec  Loss 1.4311  LearningRate 0.0381  ProxyLR: 1.9054  Epoch: 14  Global Step: 80100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:07,261-Speed 3898.98 samples/sec  Loss 1.3812  LearningRate 0.0381  ProxyLR: 1.9048  Epoch: 14  Global Step: 80110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:09,886-Speed 3902.14 samples/sec  Loss 1.4139  LearningRate 0.0381  ProxyLR: 1.9042  Epoch: 14  Global Step: 80120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:12,513-Speed 3899.06 samples/sec  Loss 1.4394  LearningRate 0.0381  ProxyLR: 1.9036  Epoch: 14  Global Step: 80130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:15,139-Speed 3900.35 samples/sec  Loss 1.4148  LearningRate 0.0381  ProxyLR: 1.9030  Epoch: 14  Global Step: 80140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:17,766-Speed 3898.82 samples/sec  Loss 1.4351  LearningRate 0.0380  ProxyLR: 1.9023  Epoch: 14  Global Step: 80150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:20,394-Speed 3898.32 samples/sec  Loss 1.3819  LearningRate 0.0380  ProxyLR: 1.9017  Epoch: 14  Global Step: 80160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:23,028-Speed 3888.60 samples/sec  Loss 1.4212  LearningRate 0.0380  ProxyLR: 1.9011  Epoch: 14  Global Step: 80170   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:25,658-Speed 3894.65 samples/sec  Loss 1.4321  LearningRate 0.0380  ProxyLR: 1.9005  Epoch: 14  Global Step: 80180   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:28,287-Speed 3896.01 samples/sec  Loss 1.4390  LearningRate 0.0380  ProxyLR: 1.8999  Epoch: 14  Global Step: 80190   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:41:30,922-Speed 3887.49 samples/sec  Loss 1.4570  LearningRate 0.0380  ProxyLR: 1.8993  Epoch: 14  Global Step: 80200   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:41:33,537-Speed 3916.75 samples/sec  Loss 1.4662  LearningRate 0.0380  ProxyLR: 1.8987  Epoch: 14  Global Step: 80210   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:36,166-Speed 3895.15 samples/sec  Loss 1.4012  LearningRate 0.0380  ProxyLR: 1.8981  Epoch: 14  Global Step: 80220   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:38,795-Speed 3896.32 samples/sec  Loss 1.4190  LearningRate 0.0379  ProxyLR: 1.8974  Epoch: 14  Global Step: 80230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:41,423-Speed 3897.80 samples/sec  Loss 1.3973  LearningRate 0.0379  ProxyLR: 1.8968  Epoch: 14  Global Step: 80240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:44,051-Speed 3897.02 samples/sec  Loss 1.4893  LearningRate 0.0379  ProxyLR: 1.8962  Epoch: 14  Global Step: 80250   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:46,677-Speed 3900.62 samples/sec  Loss 1.4143  LearningRate 0.0379  ProxyLR: 1.8956  Epoch: 14  Global Step: 80260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:49,306-Speed 3896.25 samples/sec  Loss 1.4066  LearningRate 0.0379  ProxyLR: 1.8950  Epoch: 14  Global Step: 80270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:51,934-Speed 3898.10 samples/sec  Loss 1.4116  LearningRate 0.0379  ProxyLR: 1.8944  Epoch: 14  Global Step: 80280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:54,561-Speed 3899.15 samples/sec  Loss 1.4145  LearningRate 0.0379  ProxyLR: 1.8938  Epoch: 14  Global Step: 80290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:57,187-Speed 3899.43 samples/sec  Loss 1.3885  LearningRate 0.0379  ProxyLR: 1.8932  Epoch: 14  Global Step: 80300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:41:59,802-Speed 3917.23 samples/sec  Loss 1.4062  LearningRate 0.0379  ProxyLR: 1.8925  Epoch: 14  Global Step: 80310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:02,430-Speed 3898.50 samples/sec  Loss 1.3823  LearningRate 0.0378  ProxyLR: 1.8919  Epoch: 14  Global Step: 80320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:05,056-Speed 3900.20 samples/sec  Loss 1.4345  LearningRate 0.0378  ProxyLR: 1.8913  Epoch: 14  Global Step: 80330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:07,685-Speed 3896.17 samples/sec  Loss 1.4585  LearningRate 0.0378  ProxyLR: 1.8907  Epoch: 14  Global Step: 80340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:10,312-Speed 3899.25 samples/sec  Loss 1.4223  LearningRate 0.0378  ProxyLR: 1.8901  Epoch: 14  Global Step: 80350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:12,940-Speed 3897.75 samples/sec  Loss 1.4870  LearningRate 0.0378  ProxyLR: 1.8895  Epoch: 14  Global Step: 80360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:15,566-Speed 3899.55 samples/sec  Loss 1.4544  LearningRate 0.0378  ProxyLR: 1.8889  Epoch: 14  Global Step: 80370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:18,191-Speed 3902.16 samples/sec  Loss 1.4557  LearningRate 0.0378  ProxyLR: 1.8883  Epoch: 14  Global Step: 80380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:20,816-Speed 3901.96 samples/sec  Loss 1.3973  LearningRate 0.0378  ProxyLR: 1.8876  Epoch: 14  Global Step: 80390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:23,441-Speed 3902.62 samples/sec  Loss 1.4885  LearningRate 0.0377  ProxyLR: 1.8870  Epoch: 14  Global Step: 80400   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:26,053-Speed 3920.50 samples/sec  Loss 1.4433  LearningRate 0.0377  ProxyLR: 1.8864  Epoch: 14  Global Step: 80410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:28,679-Speed 3901.01 samples/sec  Loss 1.4273  LearningRate 0.0377  ProxyLR: 1.8858  Epoch: 14  Global Step: 80420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:31,308-Speed 3895.91 samples/sec  Loss 1.3544  LearningRate 0.0377  ProxyLR: 1.8852  Epoch: 14  Global Step: 80430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:33,935-Speed 3899.83 samples/sec  Loss 1.4053  LearningRate 0.0377  ProxyLR: 1.8846  Epoch: 14  Global Step: 80440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:36,564-Speed 3896.34 samples/sec  Loss 1.4711  LearningRate 0.0377  ProxyLR: 1.8840  Epoch: 14  Global Step: 80450   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:39,191-Speed 3898.29 samples/sec  Loss 1.4111  LearningRate 0.0377  ProxyLR: 1.8834  Epoch: 14  Global Step: 80460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:41,821-Speed 3893.97 samples/sec  Loss 1.4101  LearningRate 0.0377  ProxyLR: 1.8828  Epoch: 14  Global Step: 80470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:44,451-Speed 3895.90 samples/sec  Loss 1.4192  LearningRate 0.0376  ProxyLR: 1.8821  Epoch: 14  Global Step: 80480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:47,081-Speed 3894.30 samples/sec  Loss 1.4329  LearningRate 0.0376  ProxyLR: 1.8815  Epoch: 14  Global Step: 80490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:49,714-Speed 3889.69 samples/sec  Loss 1.4286  LearningRate 0.0376  ProxyLR: 1.8809  Epoch: 14  Global Step: 80500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:52,332-Speed 3912.05 samples/sec  Loss 1.4406  LearningRate 0.0376  ProxyLR: 1.8803  Epoch: 14  Global Step: 80510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:54,963-Speed 3893.08 samples/sec  Loss 1.4728  LearningRate 0.0376  ProxyLR: 1.8797  Epoch: 14  Global Step: 80520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:42:57,582-Speed 3910.72 samples/sec  Loss 1.4109  LearningRate 0.0376  ProxyLR: 1.8791  Epoch: 14  Global Step: 80530   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:00,217-Speed 3888.23 samples/sec  Loss 1.4449  LearningRate 0.0376  ProxyLR: 1.8785  Epoch: 14  Global Step: 80540   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:02,846-Speed 3895.08 samples/sec  Loss 1.4301  LearningRate 0.0376  ProxyLR: 1.8779  Epoch: 14  Global Step: 80550   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:05,478-Speed 3891.71 samples/sec  Loss 1.4128  LearningRate 0.0375  ProxyLR: 1.8773  Epoch: 14  Global Step: 80560   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:08,109-Speed 3893.29 samples/sec  Loss 1.5062  LearningRate 0.0375  ProxyLR: 1.8767  Epoch: 14  Global Step: 80570   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:10,741-Speed 3892.05 samples/sec  Loss 1.4292  LearningRate 0.0375  ProxyLR: 1.8761  Epoch: 14  Global Step: 80580   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:13,371-Speed 3894.00 samples/sec  Loss 1.4016  LearningRate 0.0375  ProxyLR: 1.8754  Epoch: 14  Global Step: 80590   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:16,003-Speed 3892.41 samples/sec  Loss 1.3698  LearningRate 0.0375  ProxyLR: 1.8748  Epoch: 14  Global Step: 80600   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:18,634-Speed 3892.60 samples/sec  Loss 1.5168  LearningRate 0.0375  ProxyLR: 1.8742  Epoch: 14  Global Step: 80610   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:21,264-Speed 3894.78 samples/sec  Loss 1.4533  LearningRate 0.0375  ProxyLR: 1.8736  Epoch: 14  Global Step: 80620   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:43:23,893-Speed 3895.92 samples/sec  Loss 1.4289  LearningRate 0.0375  ProxyLR: 1.8730  Epoch: 14  Global Step: 80630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:26,525-Speed 3892.39 samples/sec  Loss 1.4119  LearningRate 0.0374  ProxyLR: 1.8724  Epoch: 14  Global Step: 80640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:29,156-Speed 3892.45 samples/sec  Loss 1.4731  LearningRate 0.0374  ProxyLR: 1.8718  Epoch: 14  Global Step: 80650   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:31,787-Speed 3893.17 samples/sec  Loss 1.4094  LearningRate 0.0374  ProxyLR: 1.8712  Epoch: 14  Global Step: 80660   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:34,415-Speed 3897.54 samples/sec  Loss 1.4844  LearningRate 0.0374  ProxyLR: 1.8706  Epoch: 14  Global Step: 80670   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:37,044-Speed 3895.19 samples/sec  Loss 1.4674  LearningRate 0.0374  ProxyLR: 1.8700  Epoch: 14  Global Step: 80680   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:39,678-Speed 3888.80 samples/sec  Loss 1.4356  LearningRate 0.0374  ProxyLR: 1.8694  Epoch: 14  Global Step: 80690   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:42,311-Speed 3890.75 samples/sec  Loss 1.4568  LearningRate 0.0374  ProxyLR: 1.8687  Epoch: 14  Global Step: 80700   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:44,943-Speed 3891.55 samples/sec  Loss 1.4714  LearningRate 0.0374  ProxyLR: 1.8681  Epoch: 14  Global Step: 80710   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:47,574-Speed 3893.56 samples/sec  Loss 1.4583  LearningRate 0.0374  ProxyLR: 1.8675  Epoch: 14  Global Step: 80720   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:50,191-Speed 3913.13 samples/sec  Loss 1.4698  LearningRate 0.0373  ProxyLR: 1.8669  Epoch: 14  Global Step: 80730   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:52,823-Speed 3892.25 samples/sec  Loss 1.4522  LearningRate 0.0373  ProxyLR: 1.8663  Epoch: 14  Global Step: 80740   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:55,456-Speed 3890.60 samples/sec  Loss 1.5004  LearningRate 0.0373  ProxyLR: 1.8657  Epoch: 14  Global Step: 80750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:43:58,087-Speed 3892.33 samples/sec  Loss 1.4329  LearningRate 0.0373  ProxyLR: 1.8651  Epoch: 14  Global Step: 80760   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:00,721-Speed 3888.62 samples/sec  Loss 1.4809  LearningRate 0.0373  ProxyLR: 1.8645  Epoch: 14  Global Step: 80770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:03,352-Speed 3893.45 samples/sec  Loss 1.4481  LearningRate 0.0373  ProxyLR: 1.8639  Epoch: 14  Global Step: 80780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:05,988-Speed 3884.63 samples/sec  Loss 1.4302  LearningRate 0.0373  ProxyLR: 1.8633  Epoch: 14  Global Step: 80790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:08,620-Speed 3892.80 samples/sec  Loss 1.4710  LearningRate 0.0373  ProxyLR: 1.8627  Epoch: 14  Global Step: 80800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:11,252-Speed 3890.80 samples/sec  Loss 1.4517  LearningRate 0.0372  ProxyLR: 1.8621  Epoch: 14  Global Step: 80810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:13,883-Speed 3893.49 samples/sec  Loss 1.4529  LearningRate 0.0372  ProxyLR: 1.8615  Epoch: 14  Global Step: 80820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:16,501-Speed 3911.46 samples/sec  Loss 1.4875  LearningRate 0.0372  ProxyLR: 1.8608  Epoch: 14  Global Step: 80830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:19,137-Speed 3886.23 samples/sec  Loss 1.4571  LearningRate 0.0372  ProxyLR: 1.8602  Epoch: 14  Global Step: 80840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:21,772-Speed 3887.99 samples/sec  Loss 1.4988  LearningRate 0.0372  ProxyLR: 1.8596  Epoch: 14  Global Step: 80850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:24,406-Speed 3888.44 samples/sec  Loss 1.4586  LearningRate 0.0372  ProxyLR: 1.8590  Epoch: 14  Global Step: 80860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:27,040-Speed 3888.36 samples/sec  Loss 1.4166  LearningRate 0.0372  ProxyLR: 1.8584  Epoch: 14  Global Step: 80870   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:29,678-Speed 3883.22 samples/sec  Loss 1.4854  LearningRate 0.0372  ProxyLR: 1.8578  Epoch: 14  Global Step: 80880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:32,315-Speed 3883.76 samples/sec  Loss 1.4805  LearningRate 0.0371  ProxyLR: 1.8572  Epoch: 14  Global Step: 80890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:34,954-Speed 3882.07 samples/sec  Loss 1.4852  LearningRate 0.0371  ProxyLR: 1.8566  Epoch: 14  Global Step: 80900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:37,591-Speed 3884.18 samples/sec  Loss 1.4816  LearningRate 0.0371  ProxyLR: 1.8560  Epoch: 14  Global Step: 80910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:40,227-Speed 3884.88 samples/sec  Loss 1.5219  LearningRate 0.0371  ProxyLR: 1.8554  Epoch: 14  Global Step: 80920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:42,851-Speed 3903.95 samples/sec  Loss 1.4422  LearningRate 0.0371  ProxyLR: 1.8548  Epoch: 14  Global Step: 80930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:45,488-Speed 3884.36 samples/sec  Loss 1.5103  LearningRate 0.0371  ProxyLR: 1.8542  Epoch: 14  Global Step: 80940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:44:48,111-Speed 3904.05 samples/sec  Loss 1.4859  LearningRate 0.0371  ProxyLR: 1.8536  Epoch: 14  Global Step: 80950   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:44:50,747-Speed 3885.75 samples/sec  Loss 1.4935  LearningRate 0.0371  ProxyLR: 1.8530  Epoch: 14  Global Step: 80960   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:44:53,382-Speed 3887.13 samples/sec  Loss 1.5453  LearningRate 0.0370  ProxyLR: 1.8524  Epoch: 14  Global Step: 80970   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:44:56,019-Speed 3884.17 samples/sec  Loss 1.4298  LearningRate 0.0370  ProxyLR: 1.8518  Epoch: 14  Global Step: 80980   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:44:58,655-Speed 3885.91 samples/sec  Loss 1.4239  LearningRate 0.0370  ProxyLR: 1.8511  Epoch: 14  Global Step: 80990   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:45:01,290-Speed 3886.91 samples/sec  Loss 1.5377  LearningRate 0.0370  ProxyLR: 1.8505  Epoch: 14  Global Step: 81000   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:45:03,927-Speed 3885.02 samples/sec  Loss 1.4715  LearningRate 0.0370  ProxyLR: 1.8499  Epoch: 14  Global Step: 81010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:45:06,564-Speed 3883.68 samples/sec  Loss 1.4920  LearningRate 0.0370  ProxyLR: 1.8493  Epoch: 14  Global Step: 81020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:45:09,199-Speed 3887.33 samples/sec  Loss 1.4556  LearningRate 0.0370  ProxyLR: 1.8487  Epoch: 14  Global Step: 81030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:45:11,834-Speed 3887.32 samples/sec  Loss 1.4609  LearningRate 0.0370  ProxyLR: 1.8481  Epoch: 14  Global Step: 81040   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:45:14,469-Speed 3886.80 samples/sec  Loss 1.5485  LearningRate 0.0370  ProxyLR: 1.8475  Epoch: 14  Global Step: 81050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:17,106-Speed 3884.79 samples/sec  Loss 1.4330  LearningRate 0.0369  ProxyLR: 1.8469  Epoch: 14  Global Step: 81060   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:19,743-Speed 3883.81 samples/sec  Loss 1.4762  LearningRate 0.0369  ProxyLR: 1.8463  Epoch: 14  Global Step: 81070   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:22,373-Speed 3894.61 samples/sec  Loss 1.4598  LearningRate 0.0369  ProxyLR: 1.8457  Epoch: 14  Global Step: 81080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:25,003-Speed 3895.03 samples/sec  Loss 1.4745  LearningRate 0.0369  ProxyLR: 1.8451  Epoch: 14  Global Step: 81090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:27,635-Speed 3891.31 samples/sec  Loss 1.4777  LearningRate 0.0369  ProxyLR: 1.8445  Epoch: 14  Global Step: 81100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:30,267-Speed 3892.59 samples/sec  Loss 1.4600  LearningRate 0.0369  ProxyLR: 1.8439  Epoch: 14  Global Step: 81110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:32,896-Speed 3895.64 samples/sec  Loss 1.5074  LearningRate 0.0369  ProxyLR: 1.8433  Epoch: 14  Global Step: 81120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:35,525-Speed 3895.16 samples/sec  Loss 1.4597  LearningRate 0.0369  ProxyLR: 1.8427  Epoch: 14  Global Step: 81130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:38,157-Speed 3892.72 samples/sec  Loss 1.5169  LearningRate 0.0368  ProxyLR: 1.8421  Epoch: 14  Global Step: 81140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:40,773-Speed 3914.63 samples/sec  Loss 1.4557  LearningRate 0.0368  ProxyLR: 1.8415  Epoch: 14  Global Step: 81150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:43,400-Speed 3898.53 samples/sec  Loss 1.4824  LearningRate 0.0368  ProxyLR: 1.8409  Epoch: 14  Global Step: 81160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:46,027-Speed 3898.76 samples/sec  Loss 1.4359  LearningRate 0.0368  ProxyLR: 1.8403  Epoch: 14  Global Step: 81170   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:48,656-Speed 3896.32 samples/sec  Loss 1.4955  LearningRate 0.0368  ProxyLR: 1.8397  Epoch: 14  Global Step: 81180   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:51,282-Speed 3900.72 samples/sec  Loss 1.4555  LearningRate 0.0368  ProxyLR: 1.8391  Epoch: 14  Global Step: 81190   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:53,911-Speed 3896.76 samples/sec  Loss 1.4747  LearningRate 0.0368  ProxyLR: 1.8385  Epoch: 14  Global Step: 81200   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:56,539-Speed 3897.05 samples/sec  Loss 1.4958  LearningRate 0.0368  ProxyLR: 1.8379  Epoch: 14  Global Step: 81210   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:45:59,167-Speed 3898.88 samples/sec  Loss 1.4276  LearningRate 0.0367  ProxyLR: 1.8373  Epoch: 14  Global Step: 81220   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:01,794-Speed 3897.80 samples/sec  Loss 1.4499  LearningRate 0.0367  ProxyLR: 1.8366  Epoch: 14  Global Step: 81230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:04,421-Speed 3900.33 samples/sec  Loss 1.4546  LearningRate 0.0367  ProxyLR: 1.8360  Epoch: 14  Global Step: 81240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:07,036-Speed 3916.39 samples/sec  Loss 1.5104  LearningRate 0.0367  ProxyLR: 1.8354  Epoch: 14  Global Step: 81250   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:09,663-Speed 3898.23 samples/sec  Loss 1.4776  LearningRate 0.0367  ProxyLR: 1.8348  Epoch: 14  Global Step: 81260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:12,291-Speed 3898.39 samples/sec  Loss 1.4687  LearningRate 0.0367  ProxyLR: 1.8342  Epoch: 14  Global Step: 81270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:14,918-Speed 3899.17 samples/sec  Loss 1.5353  LearningRate 0.0367  ProxyLR: 1.8336  Epoch: 14  Global Step: 81280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:17,546-Speed 3897.19 samples/sec  Loss 1.4712  LearningRate 0.0367  ProxyLR: 1.8330  Epoch: 14  Global Step: 81290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:20,174-Speed 3896.73 samples/sec  Loss 1.4562  LearningRate 0.0366  ProxyLR: 1.8324  Epoch: 14  Global Step: 81300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:22,802-Speed 3898.30 samples/sec  Loss 1.5767  LearningRate 0.0366  ProxyLR: 1.8318  Epoch: 14  Global Step: 81310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:25,428-Speed 3899.99 samples/sec  Loss 1.4653  LearningRate 0.0366  ProxyLR: 1.8312  Epoch: 14  Global Step: 81320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:28,057-Speed 3896.43 samples/sec  Loss 1.4876  LearningRate 0.0366  ProxyLR: 1.8306  Epoch: 14  Global Step: 81330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:30,687-Speed 3894.04 samples/sec  Loss 1.4623  LearningRate 0.0366  ProxyLR: 1.8300  Epoch: 14  Global Step: 81340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:33,301-Speed 3919.50 samples/sec  Loss 1.5257  LearningRate 0.0366  ProxyLR: 1.8294  Epoch: 14  Global Step: 81350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:35,928-Speed 3898.81 samples/sec  Loss 1.4680  LearningRate 0.0366  ProxyLR: 1.8288  Epoch: 14  Global Step: 81360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:38,557-Speed 3894.93 samples/sec  Loss 1.4853  LearningRate 0.0366  ProxyLR: 1.8282  Epoch: 14  Global Step: 81370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:41,186-Speed 3896.52 samples/sec  Loss 1.4453  LearningRate 0.0366  ProxyLR: 1.8276  Epoch: 14  Global Step: 81380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:43,812-Speed 3900.76 samples/sec  Loss 1.4509  LearningRate 0.0365  ProxyLR: 1.8270  Epoch: 14  Global Step: 81390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:46,439-Speed 3899.14 samples/sec  Loss 1.5326  LearningRate 0.0365  ProxyLR: 1.8264  Epoch: 14  Global Step: 81400   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:49,066-Speed 3899.78 samples/sec  Loss 1.4288  LearningRate 0.0365  ProxyLR: 1.8258  Epoch: 14  Global Step: 81410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:51,695-Speed 3895.85 samples/sec  Loss 1.4758  LearningRate 0.0365  ProxyLR: 1.8252  Epoch: 14  Global Step: 81420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:54,324-Speed 3894.92 samples/sec  Loss 1.4741  LearningRate 0.0365  ProxyLR: 1.8246  Epoch: 14  Global Step: 81430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:56,957-Speed 3889.89 samples/sec  Loss 1.5620  LearningRate 0.0365  ProxyLR: 1.8240  Epoch: 14  Global Step: 81440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:46:59,587-Speed 3894.66 samples/sec  Loss 1.4480  LearningRate 0.0365  ProxyLR: 1.8234  Epoch: 14  Global Step: 81450   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:47:02,201-Speed 3918.16 samples/sec  Loss 1.4851  LearningRate 0.0365  ProxyLR: 1.8228  Epoch: 14  Global Step: 81460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:04,828-Speed 3900.22 samples/sec  Loss 1.4776  LearningRate 0.0364  ProxyLR: 1.8222  Epoch: 14  Global Step: 81470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:07,459-Speed 3892.93 samples/sec  Loss 1.4728  LearningRate 0.0364  ProxyLR: 1.8216  Epoch: 14  Global Step: 81480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:10,088-Speed 3895.98 samples/sec  Loss 1.5192  LearningRate 0.0364  ProxyLR: 1.8210  Epoch: 14  Global Step: 81490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:12,717-Speed 3894.74 samples/sec  Loss 1.5299  LearningRate 0.0364  ProxyLR: 1.8204  Epoch: 14  Global Step: 81500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:15,348-Speed 3893.34 samples/sec  Loss 1.4883  LearningRate 0.0364  ProxyLR: 1.8198  Epoch: 14  Global Step: 81510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:17,978-Speed 3895.55 samples/sec  Loss 1.4934  LearningRate 0.0364  ProxyLR: 1.8192  Epoch: 14  Global Step: 81520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:20,612-Speed 3887.45 samples/sec  Loss 1.5473  LearningRate 0.0364  ProxyLR: 1.8186  Epoch: 14  Global Step: 81530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:23,245-Speed 3891.11 samples/sec  Loss 1.5129  LearningRate 0.0364  ProxyLR: 1.8180  Epoch: 14  Global Step: 81540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:25,876-Speed 3892.71 samples/sec  Loss 1.4524  LearningRate 0.0363  ProxyLR: 1.8174  Epoch: 14  Global Step: 81550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:28,497-Speed 3907.64 samples/sec  Loss 1.4509  LearningRate 0.0363  ProxyLR: 1.8168  Epoch: 14  Global Step: 81560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:31,127-Speed 3894.56 samples/sec  Loss 1.5009  LearningRate 0.0363  ProxyLR: 1.8162  Epoch: 14  Global Step: 81570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:33,758-Speed 3892.89 samples/sec  Loss 1.4864  LearningRate 0.0363  ProxyLR: 1.8156  Epoch: 14  Global Step: 81580   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:36,389-Speed 3893.51 samples/sec  Loss 1.4763  LearningRate 0.0363  ProxyLR: 1.8150  Epoch: 14  Global Step: 81590   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:39,022-Speed 3890.31 samples/sec  Loss 1.5276  LearningRate 0.0363  ProxyLR: 1.8144  Epoch: 14  Global Step: 81600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:41,653-Speed 3892.67 samples/sec  Loss 1.4580  LearningRate 0.0363  ProxyLR: 1.8138  Epoch: 14  Global Step: 81610   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:44,284-Speed 3893.59 samples/sec  Loss 1.5254  LearningRate 0.0363  ProxyLR: 1.8132  Epoch: 14  Global Step: 81620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:46,915-Speed 3892.57 samples/sec  Loss 1.5302  LearningRate 0.0363  ProxyLR: 1.8126  Epoch: 14  Global Step: 81630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:49,549-Speed 3888.86 samples/sec  Loss 1.5147  LearningRate 0.0362  ProxyLR: 1.8120  Epoch: 14  Global Step: 81640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:52,184-Speed 3887.29 samples/sec  Loss 1.5050  LearningRate 0.0362  ProxyLR: 1.8114  Epoch: 14  Global Step: 81650   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:54,803-Speed 3911.05 samples/sec  Loss 1.5436  LearningRate 0.0362  ProxyLR: 1.8108  Epoch: 14  Global Step: 81660   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:47:57,423-Speed 3909.50 samples/sec  Loss 1.5525  LearningRate 0.0362  ProxyLR: 1.8102  Epoch: 14  Global Step: 81670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:00,057-Speed 3888.70 samples/sec  Loss 1.5200  LearningRate 0.0362  ProxyLR: 1.8096  Epoch: 14  Global Step: 81680   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:02,692-Speed 3886.36 samples/sec  Loss 1.4940  LearningRate 0.0362  ProxyLR: 1.8090  Epoch: 14  Global Step: 81690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:05,331-Speed 3881.57 samples/sec  Loss 1.4998  LearningRate 0.0362  ProxyLR: 1.8084  Epoch: 14  Global Step: 81700   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:07,971-Speed 3880.04 samples/sec  Loss 1.4219  LearningRate 0.0362  ProxyLR: 1.8078  Epoch: 14  Global Step: 81710   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:10,608-Speed 3883.12 samples/sec  Loss 1.5829  LearningRate 0.0361  ProxyLR: 1.8072  Epoch: 14  Global Step: 81720   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:13,254-Speed 3870.96 samples/sec  Loss 1.5124  LearningRate 0.0361  ProxyLR: 1.8066  Epoch: 14  Global Step: 81730   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:15,891-Speed 3884.64 samples/sec  Loss 1.4980  LearningRate 0.0361  ProxyLR: 1.8060  Epoch: 14  Global Step: 81740   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:18,529-Speed 3883.10 samples/sec  Loss 1.4865  LearningRate 0.0361  ProxyLR: 1.8054  Epoch: 14  Global Step: 81750   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:21,167-Speed 3882.41 samples/sec  Loss 1.5275  LearningRate 0.0361  ProxyLR: 1.8048  Epoch: 14  Global Step: 81760   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:48:23,805-Speed 3882.33 samples/sec  Loss 1.4756  LearningRate 0.0361  ProxyLR: 1.8042  Epoch: 14  Global Step: 81770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:26,446-Speed 3878.46 samples/sec  Loss 1.5000  LearningRate 0.0361  ProxyLR: 1.8036  Epoch: 14  Global Step: 81780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:29,083-Speed 3884.17 samples/sec  Loss 1.5419  LearningRate 0.0361  ProxyLR: 1.8030  Epoch: 14  Global Step: 81790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:31,723-Speed 3879.29 samples/sec  Loss 1.5349  LearningRate 0.0360  ProxyLR: 1.8024  Epoch: 14  Global Step: 81800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:34,362-Speed 3882.14 samples/sec  Loss 1.4895  LearningRate 0.0360  ProxyLR: 1.8018  Epoch: 14  Global Step: 81810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:36,999-Speed 3883.29 samples/sec  Loss 1.5798  LearningRate 0.0360  ProxyLR: 1.8012  Epoch: 14  Global Step: 81820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:39,638-Speed 3882.06 samples/sec  Loss 1.5198  LearningRate 0.0360  ProxyLR: 1.8006  Epoch: 14  Global Step: 81830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:42,276-Speed 3882.69 samples/sec  Loss 1.5005  LearningRate 0.0360  ProxyLR: 1.8001  Epoch: 14  Global Step: 81840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:45,155-Speed 3557.91 samples/sec  Loss 1.5464  LearningRate 0.0360  ProxyLR: 1.7995  Epoch: 14  Global Step: 81850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:47,790-Speed 3886.34 samples/sec  Loss 1.5343  LearningRate 0.0360  ProxyLR: 1.7989  Epoch: 14  Global Step: 81860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:50,427-Speed 3884.20 samples/sec  Loss 1.4910  LearningRate 0.0360  ProxyLR: 1.7983  Epoch: 14  Global Step: 81870   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:48:53,052-Speed 3901.69 samples/sec  Loss 1.5608  LearningRate 0.0360  ProxyLR: 1.7977  Epoch: 14  Global Step: 81880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:55,691-Speed 3881.82 samples/sec  Loss 1.5329  LearningRate 0.0359  ProxyLR: 1.7971  Epoch: 14  Global Step: 81890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:48:58,331-Speed 3879.37 samples/sec  Loss 1.5885  LearningRate 0.0359  ProxyLR: 1.7965  Epoch: 14  Global Step: 81900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:00,971-Speed 3880.59 samples/sec  Loss 1.4944  LearningRate 0.0359  ProxyLR: 1.7959  Epoch: 14  Global Step: 81910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:03,609-Speed 3881.72 samples/sec  Loss 1.5390  LearningRate 0.0359  ProxyLR: 1.7953  Epoch: 14  Global Step: 81920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:06,247-Speed 3882.74 samples/sec  Loss 1.5205  LearningRate 0.0359  ProxyLR: 1.7947  Epoch: 14  Global Step: 81930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:08,884-Speed 3884.42 samples/sec  Loss 1.5305  LearningRate 0.0359  ProxyLR: 1.7941  Epoch: 14  Global Step: 81940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:11,519-Speed 3887.72 samples/sec  Loss 1.5379  LearningRate 0.0359  ProxyLR: 1.7935  Epoch: 14  Global Step: 81950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:14,151-Speed 3891.72 samples/sec  Loss 1.4874  LearningRate 0.0359  ProxyLR: 1.7929  Epoch: 14  Global Step: 81960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:16,782-Speed 3891.93 samples/sec  Loss 1.5409  LearningRate 0.0358  ProxyLR: 1.7923  Epoch: 14  Global Step: 81970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:19,400-Speed 3912.45 samples/sec  Loss 1.4991  LearningRate 0.0358  ProxyLR: 1.7917  Epoch: 14  Global Step: 81980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:22,032-Speed 3891.70 samples/sec  Loss 1.5973  LearningRate 0.0358  ProxyLR: 1.7911  Epoch: 14  Global Step: 81990   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:24,903-Speed 3568.59 samples/sec  Loss 1.4967  LearningRate 0.0358  ProxyLR: 1.7905  Epoch: 14  Global Step: 82000   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:27,535-Speed 3891.78 samples/sec  Loss 1.5298  LearningRate 0.0358  ProxyLR: 1.7899  Epoch: 14  Global Step: 82010   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:30,165-Speed 3894.56 samples/sec  Loss 1.5548  LearningRate 0.0358  ProxyLR: 1.7893  Epoch: 14  Global Step: 82020   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:32,797-Speed 3891.10 samples/sec  Loss 1.6052  LearningRate 0.0358  ProxyLR: 1.7887  Epoch: 14  Global Step: 82030   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:35,431-Speed 3889.36 samples/sec  Loss 1.5725  LearningRate 0.0358  ProxyLR: 1.7881  Epoch: 14  Global Step: 82040   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:38,062-Speed 3891.81 samples/sec  Loss 1.5693  LearningRate 0.0358  ProxyLR: 1.7875  Epoch: 14  Global Step: 82050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:40,695-Speed 3890.18 samples/sec  Loss 1.5420  LearningRate 0.0357  ProxyLR: 1.7869  Epoch: 14  Global Step: 82060   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:43,328-Speed 3890.32 samples/sec  Loss 1.5304  LearningRate 0.0357  ProxyLR: 1.7863  Epoch: 14  Global Step: 82070   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:45,948-Speed 3909.99 samples/sec  Loss 1.5361  LearningRate 0.0357  ProxyLR: 1.7858  Epoch: 14  Global Step: 82080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:48,581-Speed 3889.34 samples/sec  Loss 1.5054  LearningRate 0.0357  ProxyLR: 1.7852  Epoch: 14  Global Step: 82090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:51,213-Speed 3892.15 samples/sec  Loss 1.5279  LearningRate 0.0357  ProxyLR: 1.7846  Epoch: 14  Global Step: 82100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:53,845-Speed 3890.65 samples/sec  Loss 1.5394  LearningRate 0.0357  ProxyLR: 1.7840  Epoch: 14  Global Step: 82110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:56,477-Speed 3892.76 samples/sec  Loss 1.5301  LearningRate 0.0357  ProxyLR: 1.7834  Epoch: 14  Global Step: 82120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:49:59,105-Speed 3897.30 samples/sec  Loss 1.5331  LearningRate 0.0357  ProxyLR: 1.7828  Epoch: 14  Global Step: 82130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:01,736-Speed 3893.27 samples/sec  Loss 1.5186  LearningRate 0.0356  ProxyLR: 1.7822  Epoch: 14  Global Step: 82140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:04,370-Speed 3888.54 samples/sec  Loss 1.5165  LearningRate 0.0356  ProxyLR: 1.7816  Epoch: 14  Global Step: 82150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:07,006-Speed 3884.70 samples/sec  Loss 1.5728  LearningRate 0.0356  ProxyLR: 1.7810  Epoch: 14  Global Step: 82160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:09,640-Speed 3888.77 samples/sec  Loss 1.5299  LearningRate 0.0356  ProxyLR: 1.7804  Epoch: 14  Global Step: 82170   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:12,262-Speed 3907.66 samples/sec  Loss 1.5841  LearningRate 0.0356  ProxyLR: 1.7798  Epoch: 14  Global Step: 82180   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:14,898-Speed 3885.16 samples/sec  Loss 1.5497  LearningRate 0.0356  ProxyLR: 1.7792  Epoch: 14  Global Step: 82190   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:17,531-Speed 3890.06 samples/sec  Loss 1.4901  LearningRate 0.0356  ProxyLR: 1.7786  Epoch: 14  Global Step: 82200   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:20,165-Speed 3888.93 samples/sec  Loss 1.4767  LearningRate 0.0356  ProxyLR: 1.7780  Epoch: 14  Global Step: 82210   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:22,797-Speed 3890.32 samples/sec  Loss 1.5266  LearningRate 0.0355  ProxyLR: 1.7774  Epoch: 14  Global Step: 82220   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:25,430-Speed 3891.18 samples/sec  Loss 1.5227  LearningRate 0.0355  ProxyLR: 1.7768  Epoch: 14  Global Step: 82230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:28,062-Speed 3892.42 samples/sec  Loss 1.5286  LearningRate 0.0355  ProxyLR: 1.7763  Epoch: 14  Global Step: 82240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:30,694-Speed 3891.39 samples/sec  Loss 1.5555  LearningRate 0.0355  ProxyLR: 1.7757  Epoch: 14  Global Step: 82250   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:33,327-Speed 3890.21 samples/sec  Loss 1.5519  LearningRate 0.0355  ProxyLR: 1.7751  Epoch: 14  Global Step: 82260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:35,960-Speed 3890.27 samples/sec  Loss 1.5106  LearningRate 0.0355  ProxyLR: 1.7745  Epoch: 14  Global Step: 82270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:38,592-Speed 3891.09 samples/sec  Loss 1.5395  LearningRate 0.0355  ProxyLR: 1.7739  Epoch: 14  Global Step: 82280   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:50:41,210-Speed 3912.87 samples/sec  Loss 1.5344  LearningRate 0.0355  ProxyLR: 1.7733  Epoch: 14  Global Step: 82290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:43,844-Speed 3888.83 samples/sec  Loss 1.5365  LearningRate 0.0355  ProxyLR: 1.7727  Epoch: 14  Global Step: 82300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:46,474-Speed 3893.73 samples/sec  Loss 1.5293  LearningRate 0.0354  ProxyLR: 1.7721  Epoch: 14  Global Step: 82310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:49,105-Speed 3893.59 samples/sec  Loss 1.5133  LearningRate 0.0354  ProxyLR: 1.7715  Epoch: 14  Global Step: 82320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:51,736-Speed 3893.23 samples/sec  Loss 1.5545  LearningRate 0.0354  ProxyLR: 1.7709  Epoch: 14  Global Step: 82330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:54,368-Speed 3891.54 samples/sec  Loss 1.5225  LearningRate 0.0354  ProxyLR: 1.7703  Epoch: 14  Global Step: 82340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:57,000-Speed 3890.96 samples/sec  Loss 1.5178  LearningRate 0.0354  ProxyLR: 1.7697  Epoch: 14  Global Step: 82350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:50:59,631-Speed 3893.35 samples/sec  Loss 1.4691  LearningRate 0.0354  ProxyLR: 1.7691  Epoch: 14  Global Step: 82360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:02,262-Speed 3892.72 samples/sec  Loss 1.5631  LearningRate 0.0354  ProxyLR: 1.7686  Epoch: 14  Global Step: 82370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:04,898-Speed 3885.80 samples/sec  Loss 1.5434  LearningRate 0.0354  ProxyLR: 1.7680  Epoch: 14  Global Step: 82380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:07,521-Speed 3905.69 samples/sec  Loss 1.5409  LearningRate 0.0353  ProxyLR: 1.7674  Epoch: 14  Global Step: 82390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:10,156-Speed 3887.53 samples/sec  Loss 1.5275  LearningRate 0.0353  ProxyLR: 1.7668  Epoch: 14  Global Step: 82400   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:12,794-Speed 3881.95 samples/sec  Loss 1.5101  LearningRate 0.0353  ProxyLR: 1.7662  Epoch: 14  Global Step: 82410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:15,430-Speed 3886.26 samples/sec  Loss 1.5455  LearningRate 0.0353  ProxyLR: 1.7656  Epoch: 14  Global Step: 82420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:18,065-Speed 3886.86 samples/sec  Loss 1.5605  LearningRate 0.0353  ProxyLR: 1.7650  Epoch: 14  Global Step: 82430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:20,701-Speed 3886.20 samples/sec  Loss 1.5269  LearningRate 0.0353  ProxyLR: 1.7644  Epoch: 14  Global Step: 82440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:23,340-Speed 3881.12 samples/sec  Loss 1.5089  LearningRate 0.0353  ProxyLR: 1.7638  Epoch: 14  Global Step: 82450   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:25,979-Speed 3879.99 samples/sec  Loss 1.5611  LearningRate 0.0353  ProxyLR: 1.7632  Epoch: 14  Global Step: 82460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:28,620-Speed 3879.46 samples/sec  Loss 1.5247  LearningRate 0.0353  ProxyLR: 1.7626  Epoch: 14  Global Step: 82470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:31,258-Speed 3882.03 samples/sec  Loss 1.5218  LearningRate 0.0352  ProxyLR: 1.7621  Epoch: 14  Global Step: 82480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:33,885-Speed 3900.04 samples/sec  Loss 1.5605  LearningRate 0.0352  ProxyLR: 1.7615  Epoch: 14  Global Step: 82490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:36,527-Speed 3876.28 samples/sec  Loss 1.5209  LearningRate 0.0352  ProxyLR: 1.7609  Epoch: 14  Global Step: 82500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:39,166-Speed 3881.94 samples/sec  Loss 1.5611  LearningRate 0.0352  ProxyLR: 1.7603  Epoch: 14  Global Step: 82510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:41,804-Speed 3882.19 samples/sec  Loss 1.5855  LearningRate 0.0352  ProxyLR: 1.7597  Epoch: 14  Global Step: 82520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:44,443-Speed 3881.64 samples/sec  Loss 1.5008  LearningRate 0.0352  ProxyLR: 1.7591  Epoch: 14  Global Step: 82530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:47,082-Speed 3880.96 samples/sec  Loss 1.5630  LearningRate 0.0352  ProxyLR: 1.7585  Epoch: 14  Global Step: 82540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:49,721-Speed 3880.50 samples/sec  Loss 1.5395  LearningRate 0.0352  ProxyLR: 1.7579  Epoch: 14  Global Step: 82550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:52,364-Speed 3876.33 samples/sec  Loss 1.5458  LearningRate 0.0351  ProxyLR: 1.7573  Epoch: 14  Global Step: 82560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:55,004-Speed 3879.62 samples/sec  Loss 1.6093  LearningRate 0.0351  ProxyLR: 1.7567  Epoch: 14  Global Step: 82570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:51:57,644-Speed 3880.01 samples/sec  Loss 1.5335  LearningRate 0.0351  ProxyLR: 1.7561  Epoch: 14  Global Step: 82580   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:00,285-Speed 3877.54 samples/sec  Loss 1.5881  LearningRate 0.0351  ProxyLR: 1.7556  Epoch: 14  Global Step: 82590   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:52:02,911-Speed 3901.09 samples/sec  Loss 1.5564  LearningRate 0.0351  ProxyLR: 1.7550  Epoch: 14  Global Step: 82600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:05,552-Speed 3878.75 samples/sec  Loss 1.5855  LearningRate 0.0351  ProxyLR: 1.7544  Epoch: 14  Global Step: 82610   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:08,189-Speed 3883.63 samples/sec  Loss 1.4939  LearningRate 0.0351  ProxyLR: 1.7538  Epoch: 14  Global Step: 82620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:10,828-Speed 3881.49 samples/sec  Loss 1.5341  LearningRate 0.0351  ProxyLR: 1.7532  Epoch: 14  Global Step: 82630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:13,463-Speed 3886.23 samples/sec  Loss 1.5045  LearningRate 0.0351  ProxyLR: 1.7526  Epoch: 14  Global Step: 82640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:16,099-Speed 3885.77 samples/sec  Loss 1.5497  LearningRate 0.0350  ProxyLR: 1.7520  Epoch: 14  Global Step: 82650   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:18,736-Speed 3884.76 samples/sec  Loss 1.4982  LearningRate 0.0350  ProxyLR: 1.7514  Epoch: 14  Global Step: 82660   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:21,364-Speed 3896.89 samples/sec  Loss 1.5092  LearningRate 0.0350  ProxyLR: 1.7508  Epoch: 14  Global Step: 82670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:23,997-Speed 3890.15 samples/sec  Loss 1.5194  LearningRate 0.0350  ProxyLR: 1.7503  Epoch: 14  Global Step: 82680   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:26,634-Speed 3885.40 samples/sec  Loss 1.5499  LearningRate 0.0350  ProxyLR: 1.7497  Epoch: 14  Global Step: 82690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:29,267-Speed 3889.05 samples/sec  Loss 1.5432  LearningRate 0.0350  ProxyLR: 1.7491  Epoch: 14  Global Step: 82700   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:31,902-Speed 3887.33 samples/sec  Loss 1.5051  LearningRate 0.0350  ProxyLR: 1.7485  Epoch: 14  Global Step: 82710   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:34,534-Speed 3892.37 samples/sec  Loss 1.6059  LearningRate 0.0350  ProxyLR: 1.7479  Epoch: 14  Global Step: 82720   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:37,166-Speed 3892.07 samples/sec  Loss 1.5536  LearningRate 0.0349  ProxyLR: 1.7473  Epoch: 14  Global Step: 82730   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:39,800-Speed 3888.36 samples/sec  Loss 1.5534  LearningRate 0.0349  ProxyLR: 1.7467  Epoch: 14  Global Step: 82740   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:42,433-Speed 3890.65 samples/sec  Loss 1.5980  LearningRate 0.0349  ProxyLR: 1.7461  Epoch: 14  Global Step: 82750   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:45,064-Speed 3893.10 samples/sec  Loss 1.5594  LearningRate 0.0349  ProxyLR: 1.7456  Epoch: 14  Global Step: 82760   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:52:47,698-Speed 3888.71 samples/sec  Loss 1.6168  LearningRate 0.0349  ProxyLR: 1.7450  Epoch: 14  Global Step: 82770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:50,332-Speed 3887.87 samples/sec  Loss 1.6004  LearningRate 0.0349  ProxyLR: 1.7444  Epoch: 14  Global Step: 82780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:52,964-Speed 3892.49 samples/sec  Loss 1.5350  LearningRate 0.0349  ProxyLR: 1.7438  Epoch: 14  Global Step: 82790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:55,595-Speed 3892.83 samples/sec  Loss 1.5440  LearningRate 0.0349  ProxyLR: 1.7432  Epoch: 14  Global Step: 82800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:52:58,227-Speed 3891.94 samples/sec  Loss 1.5699  LearningRate 0.0349  ProxyLR: 1.7426  Epoch: 14  Global Step: 82810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:00,859-Speed 3891.06 samples/sec  Loss 1.5237  LearningRate 0.0348  ProxyLR: 1.7420  Epoch: 14  Global Step: 82820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:03,490-Speed 3893.80 samples/sec  Loss 1.6518  LearningRate 0.0348  ProxyLR: 1.7414  Epoch: 14  Global Step: 82830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:06,120-Speed 3893.97 samples/sec  Loss 1.5562  LearningRate 0.0348  ProxyLR: 1.7409  Epoch: 14  Global Step: 82840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:08,753-Speed 3890.59 samples/sec  Loss 1.5624  LearningRate 0.0348  ProxyLR: 1.7403  Epoch: 14  Global Step: 82850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:11,385-Speed 3890.62 samples/sec  Loss 1.5549  LearningRate 0.0348  ProxyLR: 1.7397  Epoch: 14  Global Step: 82860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:14,251-Speed 3574.18 samples/sec  Loss 1.5427  LearningRate 0.0348  ProxyLR: 1.7391  Epoch: 14  Global Step: 82870   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:16,884-Speed 3889.66 samples/sec  Loss 1.5232  LearningRate 0.0348  ProxyLR: 1.7385  Epoch: 14  Global Step: 82880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:19,519-Speed 3887.87 samples/sec  Loss 1.5200  LearningRate 0.0348  ProxyLR: 1.7379  Epoch: 14  Global Step: 82890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:22,151-Speed 3890.66 samples/sec  Loss 1.5062  LearningRate 0.0347  ProxyLR: 1.7373  Epoch: 14  Global Step: 82900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:24,787-Speed 3885.71 samples/sec  Loss 1.6003  LearningRate 0.0347  ProxyLR: 1.7367  Epoch: 14  Global Step: 82910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:27,421-Speed 3889.27 samples/sec  Loss 1.5714  LearningRate 0.0347  ProxyLR: 1.7362  Epoch: 14  Global Step: 82920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:30,052-Speed 3893.25 samples/sec  Loss 1.5817  LearningRate 0.0347  ProxyLR: 1.7356  Epoch: 14  Global Step: 82930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:32,684-Speed 3891.60 samples/sec  Loss 1.5305  LearningRate 0.0347  ProxyLR: 1.7350  Epoch: 14  Global Step: 82940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:35,315-Speed 3892.95 samples/sec  Loss 1.5962  LearningRate 0.0347  ProxyLR: 1.7344  Epoch: 14  Global Step: 82950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:37,947-Speed 3891.34 samples/sec  Loss 1.6258  LearningRate 0.0347  ProxyLR: 1.7338  Epoch: 14  Global Step: 82960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:40,565-Speed 3912.80 samples/sec  Loss 1.5006  LearningRate 0.0347  ProxyLR: 1.7332  Epoch: 14  Global Step: 82970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:43,197-Speed 3891.05 samples/sec  Loss 1.5448  LearningRate 0.0347  ProxyLR: 1.7326  Epoch: 14  Global Step: 82980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:45,829-Speed 3892.09 samples/sec  Loss 1.5678  LearningRate 0.0346  ProxyLR: 1.7321  Epoch: 14  Global Step: 82990   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:48,461-Speed 3892.10 samples/sec  Loss 1.4888  LearningRate 0.0346  ProxyLR: 1.7315  Epoch: 14  Global Step: 83000   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:51,095-Speed 3887.62 samples/sec  Loss 1.4878  LearningRate 0.0346  ProxyLR: 1.7309  Epoch: 14  Global Step: 83010   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:53,726-Speed 3893.21 samples/sec  Loss 1.5606  LearningRate 0.0346  ProxyLR: 1.7303  Epoch: 14  Global Step: 83020   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:56,358-Speed 3891.88 samples/sec  Loss 1.5540  LearningRate 0.0346  ProxyLR: 1.7297  Epoch: 14  Global Step: 83030   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:53:58,989-Speed 3893.12 samples/sec  Loss 1.5913  LearningRate 0.0346  ProxyLR: 1.7291  Epoch: 14  Global Step: 83040   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:01,621-Speed 3891.92 samples/sec  Loss 1.5720  LearningRate 0.0346  ProxyLR: 1.7285  Epoch: 14  Global Step: 83050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:04,255-Speed 3887.93 samples/sec  Loss 1.4997  LearningRate 0.0346  ProxyLR: 1.7280  Epoch: 14  Global Step: 83060   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:06,886-Speed 3892.69 samples/sec  Loss 1.5570  LearningRate 0.0345  ProxyLR: 1.7274  Epoch: 14  Global Step: 83070   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:54:09,506-Speed 3910.43 samples/sec  Loss 1.5404  LearningRate 0.0345  ProxyLR: 1.7268  Epoch: 14  Global Step: 83080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:12,138-Speed 3890.84 samples/sec  Loss 1.5422  LearningRate 0.0345  ProxyLR: 1.7262  Epoch: 14  Global Step: 83090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:14,771-Speed 3890.22 samples/sec  Loss 1.5427  LearningRate 0.0345  ProxyLR: 1.7256  Epoch: 14  Global Step: 83100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:17,404-Speed 3890.55 samples/sec  Loss 1.5754  LearningRate 0.0345  ProxyLR: 1.7250  Epoch: 14  Global Step: 83110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:20,038-Speed 3889.09 samples/sec  Loss 1.5547  LearningRate 0.0345  ProxyLR: 1.7245  Epoch: 14  Global Step: 83120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:22,671-Speed 3889.44 samples/sec  Loss 1.6185  LearningRate 0.0345  ProxyLR: 1.7239  Epoch: 14  Global Step: 83130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:25,304-Speed 3890.16 samples/sec  Loss 1.5333  LearningRate 0.0345  ProxyLR: 1.7233  Epoch: 14  Global Step: 83140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:27,937-Speed 3890.80 samples/sec  Loss 1.5617  LearningRate 0.0345  ProxyLR: 1.7227  Epoch: 14  Global Step: 83150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:30,569-Speed 3890.75 samples/sec  Loss 1.5852  LearningRate 0.0344  ProxyLR: 1.7221  Epoch: 14  Global Step: 83160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:54:33,187-Speed 3912.49 samples/sec  Loss 1.5669  LearningRate 0.0344  ProxyLR: 1.7215  Epoch: 14  Global Step: 83170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:35,819-Speed 3891.11 samples/sec  Loss 1.5212  LearningRate 0.0344  ProxyLR: 1.7210  Epoch: 14  Global Step: 83180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:38,452-Speed 3890.87 samples/sec  Loss 1.5675  LearningRate 0.0344  ProxyLR: 1.7204  Epoch: 14  Global Step: 83190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:41,083-Speed 3894.20 samples/sec  Loss 1.5592  LearningRate 0.0344  ProxyLR: 1.7198  Epoch: 14  Global Step: 83200   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:43,714-Speed 3892.06 samples/sec  Loss 1.5477  LearningRate 0.0344  ProxyLR: 1.7192  Epoch: 14  Global Step: 83210   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:46,347-Speed 3891.01 samples/sec  Loss 1.5357  LearningRate 0.0344  ProxyLR: 1.7186  Epoch: 14  Global Step: 83220   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:48,978-Speed 3893.19 samples/sec  Loss 1.5406  LearningRate 0.0344  ProxyLR: 1.7180  Epoch: 14  Global Step: 83230   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:51,610-Speed 3890.70 samples/sec  Loss 1.5581  LearningRate 0.0343  ProxyLR: 1.7175  Epoch: 14  Global Step: 83240   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:54,242-Speed 3891.11 samples/sec  Loss 1.5976  LearningRate 0.0343  ProxyLR: 1.7169  Epoch: 14  Global Step: 83250   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:56,873-Speed 3893.30 samples/sec  Loss 1.5625  LearningRate 0.0343  ProxyLR: 1.7163  Epoch: 14  Global Step: 83260   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:54:59,505-Speed 3891.58 samples/sec  Loss 1.5558  LearningRate 0.0343  ProxyLR: 1.7157  Epoch: 14  Global Step: 83270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:02,137-Speed 3892.05 samples/sec  Loss 1.5164  LearningRate 0.0343  ProxyLR: 1.7151  Epoch: 14  Global Step: 83280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:04,767-Speed 3893.76 samples/sec  Loss 1.5122  LearningRate 0.0343  ProxyLR: 1.7145  Epoch: 14  Global Step: 83290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:07,402-Speed 3887.75 samples/sec  Loss 1.5552  LearningRate 0.0343  ProxyLR: 1.7140  Epoch: 14  Global Step: 83300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:10,034-Speed 3891.09 samples/sec  Loss 1.5297  LearningRate 0.0343  ProxyLR: 1.7134  Epoch: 14  Global Step: 83310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:12,666-Speed 3891.84 samples/sec  Loss 1.5996  LearningRate 0.0343  ProxyLR: 1.7128  Epoch: 14  Global Step: 83320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:15,297-Speed 3892.93 samples/sec  Loss 1.5616  LearningRate 0.0342  ProxyLR: 1.7122  Epoch: 14  Global Step: 83330   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:17,932-Speed 3887.48 samples/sec  Loss 1.5108  LearningRate 0.0342  ProxyLR: 1.7116  Epoch: 14  Global Step: 83340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:20,564-Speed 3891.88 samples/sec  Loss 1.5381  LearningRate 0.0342  ProxyLR: 1.7110  Epoch: 14  Global Step: 83350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:23,196-Speed 3891.34 samples/sec  Loss 1.5443  LearningRate 0.0342  ProxyLR: 1.7105  Epoch: 14  Global Step: 83360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:25,830-Speed 3888.47 samples/sec  Loss 1.5319  LearningRate 0.0342  ProxyLR: 1.7099  Epoch: 14  Global Step: 83370   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:55:28,451-Speed 3908.25 samples/sec  Loss 1.5465  LearningRate 0.0342  ProxyLR: 1.7093  Epoch: 14  Global Step: 83380   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:31,082-Speed 3892.96 samples/sec  Loss 1.5489  LearningRate 0.0342  ProxyLR: 1.7087  Epoch: 14  Global Step: 83390   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:33,714-Speed 3891.26 samples/sec  Loss 1.5549  LearningRate 0.0342  ProxyLR: 1.7081  Epoch: 14  Global Step: 83400   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:36,346-Speed 3891.20 samples/sec  Loss 1.6183  LearningRate 0.0342  ProxyLR: 1.7076  Epoch: 14  Global Step: 83410   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:38,980-Speed 3889.41 samples/sec  Loss 1.5387  LearningRate 0.0341  ProxyLR: 1.7070  Epoch: 14  Global Step: 83420   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:41,616-Speed 3885.87 samples/sec  Loss 1.5810  LearningRate 0.0341  ProxyLR: 1.7064  Epoch: 14  Global Step: 83430   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:44,248-Speed 3891.19 samples/sec  Loss 1.5570  LearningRate 0.0341  ProxyLR: 1.7058  Epoch: 14  Global Step: 83440   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:46,880-Speed 3892.01 samples/sec  Loss 1.5331  LearningRate 0.0341  ProxyLR: 1.7052  Epoch: 14  Global Step: 83450   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:49,513-Speed 3889.69 samples/sec  Loss 1.6244  LearningRate 0.0341  ProxyLR: 1.7046  Epoch: 14  Global Step: 83460   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:52,146-Speed 3891.03 samples/sec  Loss 1.5743  LearningRate 0.0341  ProxyLR: 1.7041  Epoch: 14  Global Step: 83470   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:54,765-Speed 3909.75 samples/sec  Loss 1.4722  LearningRate 0.0341  ProxyLR: 1.7035  Epoch: 14  Global Step: 83480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:55:57,397-Speed 3892.59 samples/sec  Loss 1.5326  LearningRate 0.0341  ProxyLR: 1.7029  Epoch: 14  Global Step: 83490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:00,031-Speed 3888.24 samples/sec  Loss 1.5308  LearningRate 0.0340  ProxyLR: 1.7023  Epoch: 14  Global Step: 83500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:02,662-Speed 3893.10 samples/sec  Loss 1.5807  LearningRate 0.0340  ProxyLR: 1.7017  Epoch: 14  Global Step: 83510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:05,279-Speed 3913.53 samples/sec  Loss 1.5388  LearningRate 0.0340  ProxyLR: 1.7012  Epoch: 14  Global Step: 83520   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:07,912-Speed 3890.41 samples/sec  Loss 1.5843  LearningRate 0.0340  ProxyLR: 1.7006  Epoch: 14  Global Step: 83530   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:10,543-Speed 3893.92 samples/sec  Loss 1.6257  LearningRate 0.0340  ProxyLR: 1.7000  Epoch: 14  Global Step: 83540   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:13,173-Speed 3894.39 samples/sec  Loss 1.6636  LearningRate 0.0340  ProxyLR: 1.6994  Epoch: 14  Global Step: 83550   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:15,805-Speed 3891.33 samples/sec  Loss 1.5909  LearningRate 0.0340  ProxyLR: 1.6988  Epoch: 14  Global Step: 83560   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:18,438-Speed 3890.52 samples/sec  Loss 1.6018  LearningRate 0.0340  ProxyLR: 1.6983  Epoch: 14  Global Step: 83570   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:21,069-Speed 3892.16 samples/sec  Loss 1.5429  LearningRate 0.0340  ProxyLR: 1.6977  Epoch: 14  Global Step: 83580   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:23,701-Speed 3891.31 samples/sec  Loss 1.5073  LearningRate 0.0339  ProxyLR: 1.6971  Epoch: 14  Global Step: 83590   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:26,332-Speed 3893.07 samples/sec  Loss 1.5397  LearningRate 0.0339  ProxyLR: 1.6965  Epoch: 14  Global Step: 83600   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:28,964-Speed 3892.11 samples/sec  Loss 1.5601  LearningRate 0.0339  ProxyLR: 1.6959  Epoch: 14  Global Step: 83610   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:56:31,595-Speed 3893.13 samples/sec  Loss 1.5604  LearningRate 0.0339  ProxyLR: 1.6954  Epoch: 14  Global Step: 83620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:34,227-Speed 3891.15 samples/sec  Loss 1.5171  LearningRate 0.0339  ProxyLR: 1.6948  Epoch: 14  Global Step: 83630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:36,861-Speed 3888.98 samples/sec  Loss 1.5946  LearningRate 0.0339  ProxyLR: 1.6942  Epoch: 14  Global Step: 83640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:39,498-Speed 3884.72 samples/sec  Loss 1.5295  LearningRate 0.0339  ProxyLR: 1.6936  Epoch: 14  Global Step: 83650   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:42,129-Speed 3892.99 samples/sec  Loss 1.6303  LearningRate 0.0339  ProxyLR: 1.6930  Epoch: 14  Global Step: 83660   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:44,758-Speed 3895.99 samples/sec  Loss 1.5255  LearningRate 0.0338  ProxyLR: 1.6925  Epoch: 14  Global Step: 83670   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:47,388-Speed 3894.43 samples/sec  Loss 1.6278  LearningRate 0.0338  ProxyLR: 1.6919  Epoch: 14  Global Step: 83680   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:50,023-Speed 3887.28 samples/sec  Loss 1.6316  LearningRate 0.0338  ProxyLR: 1.6913  Epoch: 14  Global Step: 83690   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:52,655-Speed 3892.56 samples/sec  Loss 1.5582  LearningRate 0.0338  ProxyLR: 1.6907  Epoch: 14  Global Step: 83700   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:55,286-Speed 3892.11 samples/sec  Loss 1.6485  LearningRate 0.0338  ProxyLR: 1.6902  Epoch: 14  Global Step: 83710   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:56:57,903-Speed 3914.41 samples/sec  Loss 1.6059  LearningRate 0.0338  ProxyLR: 1.6896  Epoch: 14  Global Step: 83720   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:00,533-Speed 3893.93 samples/sec  Loss 1.5882  LearningRate 0.0338  ProxyLR: 1.6890  Epoch: 14  Global Step: 83730   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:03,164-Speed 3893.74 samples/sec  Loss 1.5416  LearningRate 0.0338  ProxyLR: 1.6884  Epoch: 14  Global Step: 83740   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:05,794-Speed 3894.90 samples/sec  Loss 1.5522  LearningRate 0.0338  ProxyLR: 1.6878  Epoch: 14  Global Step: 83750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:08,424-Speed 3894.20 samples/sec  Loss 1.5662  LearningRate 0.0337  ProxyLR: 1.6873  Epoch: 14  Global Step: 83760   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:11,055-Speed 3892.95 samples/sec  Loss 1.5628  LearningRate 0.0337  ProxyLR: 1.6867  Epoch: 14  Global Step: 83770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:13,685-Speed 3895.09 samples/sec  Loss 1.4704  LearningRate 0.0337  ProxyLR: 1.6861  Epoch: 14  Global Step: 83780   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:16,316-Speed 3892.29 samples/sec  Loss 1.4827  LearningRate 0.0337  ProxyLR: 1.6855  Epoch: 14  Global Step: 83790   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:18,947-Speed 3893.30 samples/sec  Loss 1.5859  LearningRate 0.0337  ProxyLR: 1.6850  Epoch: 14  Global Step: 83800   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:21,577-Speed 3894.02 samples/sec  Loss 1.6025  LearningRate 0.0337  ProxyLR: 1.6844  Epoch: 14  Global Step: 83810   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:24,195-Speed 3913.26 samples/sec  Loss 1.5685  LearningRate 0.0337  ProxyLR: 1.6838  Epoch: 14  Global Step: 83820   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:26,825-Speed 3893.93 samples/sec  Loss 1.6083  LearningRate 0.0337  ProxyLR: 1.6832  Epoch: 14  Global Step: 83830   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:29,454-Speed 3896.23 samples/sec  Loss 1.5495  LearningRate 0.0337  ProxyLR: 1.6826  Epoch: 14  Global Step: 83840   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:32,083-Speed 3895.24 samples/sec  Loss 1.5419  LearningRate 0.0336  ProxyLR: 1.6821  Epoch: 14  Global Step: 83850   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:34,715-Speed 3892.89 samples/sec  Loss 1.5394  LearningRate 0.0336  ProxyLR: 1.6815  Epoch: 14  Global Step: 83860   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:37,347-Speed 3890.96 samples/sec  Loss 1.6134  LearningRate 0.0336  ProxyLR: 1.6809  Epoch: 14  Global Step: 83870   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:39,976-Speed 3895.67 samples/sec  Loss 1.5679  LearningRate 0.0336  ProxyLR: 1.6803  Epoch: 14  Global Step: 83880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:42,607-Speed 3894.18 samples/sec  Loss 1.6065  LearningRate 0.0336  ProxyLR: 1.6798  Epoch: 14  Global Step: 83890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:45,237-Speed 3894.68 samples/sec  Loss 1.5632  LearningRate 0.0336  ProxyLR: 1.6792  Epoch: 14  Global Step: 83900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:47,867-Speed 3894.04 samples/sec  Loss 1.5877  LearningRate 0.0336  ProxyLR: 1.6786  Epoch: 14  Global Step: 83910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:50,484-Speed 3913.03 samples/sec  Loss 1.6318  LearningRate 0.0336  ProxyLR: 1.6780  Epoch: 14  Global Step: 83920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:53,118-Speed 3889.89 samples/sec  Loss 1.5832  LearningRate 0.0335  ProxyLR: 1.6775  Epoch: 14  Global Step: 83930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:55,749-Speed 3892.31 samples/sec  Loss 1.5776  LearningRate 0.0335  ProxyLR: 1.6769  Epoch: 14  Global Step: 83940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:57:58,380-Speed 3893.17 samples/sec  Loss 1.5831  LearningRate 0.0335  ProxyLR: 1.6763  Epoch: 14  Global Step: 83950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:01,010-Speed 3894.99 samples/sec  Loss 1.5899  LearningRate 0.0335  ProxyLR: 1.6757  Epoch: 14  Global Step: 83960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:03,639-Speed 3895.07 samples/sec  Loss 1.5832  LearningRate 0.0335  ProxyLR: 1.6752  Epoch: 14  Global Step: 83970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:06,271-Speed 3891.74 samples/sec  Loss 1.5158  LearningRate 0.0335  ProxyLR: 1.6746  Epoch: 14  Global Step: 83980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:08,902-Speed 3894.06 samples/sec  Loss 1.5898  LearningRate 0.0335  ProxyLR: 1.6740  Epoch: 14  Global Step: 83990   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:11,534-Speed 3890.84 samples/sec  Loss 1.5875  LearningRate 0.0335  ProxyLR: 1.6734  Epoch: 14  Global Step: 84000   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:14,166-Speed 3891.53 samples/sec  Loss 1.4892  LearningRate 0.0335  ProxyLR: 1.6728  Epoch: 14  Global Step: 84010   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:16,797-Speed 3893.26 samples/sec  Loss 1.5654  LearningRate 0.0334  ProxyLR: 1.6723  Epoch: 14  Global Step: 84020   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:58:19,416-Speed 3911.52 samples/sec  Loss 1.5464  LearningRate 0.0334  ProxyLR: 1.6717  Epoch: 14  Global Step: 84030   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:22,045-Speed 3895.23 samples/sec  Loss 1.6135  LearningRate 0.0334  ProxyLR: 1.6711  Epoch: 14  Global Step: 84040   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:24,677-Speed 3892.49 samples/sec  Loss 1.5747  LearningRate 0.0334  ProxyLR: 1.6705  Epoch: 14  Global Step: 84050   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:27,307-Speed 3893.42 samples/sec  Loss 1.5618  LearningRate 0.0334  ProxyLR: 1.6700  Epoch: 14  Global Step: 84060   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:29,938-Speed 3893.59 samples/sec  Loss 1.5441  LearningRate 0.0334  ProxyLR: 1.6694  Epoch: 14  Global Step: 84070   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:32,570-Speed 3891.64 samples/sec  Loss 1.6229  LearningRate 0.0334  ProxyLR: 1.6688  Epoch: 14  Global Step: 84080   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:35,198-Speed 3897.38 samples/sec  Loss 1.6054  LearningRate 0.0334  ProxyLR: 1.6682  Epoch: 14  Global Step: 84090   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:37,830-Speed 3892.54 samples/sec  Loss 1.5813  LearningRate 0.0334  ProxyLR: 1.6677  Epoch: 14  Global Step: 84100   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:40,462-Speed 3890.84 samples/sec  Loss 1.5751  LearningRate 0.0333  ProxyLR: 1.6671  Epoch: 14  Global Step: 84110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:43,094-Speed 3891.85 samples/sec  Loss 1.4825  LearningRate 0.0333  ProxyLR: 1.6665  Epoch: 14  Global Step: 84120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:45,712-Speed 3911.98 samples/sec  Loss 1.6341  LearningRate 0.0333  ProxyLR: 1.6659  Epoch: 14  Global Step: 84130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:48,347-Speed 3887.94 samples/sec  Loss 1.6249  LearningRate 0.0333  ProxyLR: 1.6654  Epoch: 14  Global Step: 84140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:50,980-Speed 3888.61 samples/sec  Loss 1.6055  LearningRate 0.0333  ProxyLR: 1.6648  Epoch: 14  Global Step: 84150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:53,615-Speed 3886.97 samples/sec  Loss 1.6322  LearningRate 0.0333  ProxyLR: 1.6642  Epoch: 14  Global Step: 84160   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:56,250-Speed 3887.53 samples/sec  Loss 1.6054  LearningRate 0.0333  ProxyLR: 1.6637  Epoch: 14  Global Step: 84170   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:58:58,883-Speed 3889.69 samples/sec  Loss 1.5763  LearningRate 0.0333  ProxyLR: 1.6631  Epoch: 14  Global Step: 84180   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:01,519-Speed 3886.56 samples/sec  Loss 1.5945  LearningRate 0.0333  ProxyLR: 1.6625  Epoch: 14  Global Step: 84190   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:04,151-Speed 3890.33 samples/sec  Loss 1.6098  LearningRate 0.0332  ProxyLR: 1.6619  Epoch: 14  Global Step: 84200   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:06,785-Speed 3888.88 samples/sec  Loss 1.5496  LearningRate 0.0332  ProxyLR: 1.6614  Epoch: 14  Global Step: 84210   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:09,419-Speed 3888.32 samples/sec  Loss 1.5512  LearningRate 0.0332  ProxyLR: 1.6608  Epoch: 14  Global Step: 84220   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:12,039-Speed 3910.21 samples/sec  Loss 1.6133  LearningRate 0.0332  ProxyLR: 1.6602  Epoch: 14  Global Step: 84230   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:14,674-Speed 3887.10 samples/sec  Loss 1.5795  LearningRate 0.0332  ProxyLR: 1.6596  Epoch: 14  Global Step: 84240   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:17,307-Speed 3889.81 samples/sec  Loss 1.5546  LearningRate 0.0332  ProxyLR: 1.6591  Epoch: 14  Global Step: 84250   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:19,941-Speed 3887.79 samples/sec  Loss 1.5834  LearningRate 0.0332  ProxyLR: 1.6585  Epoch: 14  Global Step: 84260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:22,575-Speed 3889.45 samples/sec  Loss 1.5836  LearningRate 0.0332  ProxyLR: 1.6579  Epoch: 14  Global Step: 84270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:25,208-Speed 3889.62 samples/sec  Loss 1.5703  LearningRate 0.0331  ProxyLR: 1.6573  Epoch: 14  Global Step: 84280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:27,842-Speed 3887.66 samples/sec  Loss 1.5831  LearningRate 0.0331  ProxyLR: 1.6568  Epoch: 14  Global Step: 84290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:30,477-Speed 3888.05 samples/sec  Loss 1.5555  LearningRate 0.0331  ProxyLR: 1.6562  Epoch: 14  Global Step: 84300   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:33,108-Speed 3892.36 samples/sec  Loss 1.6220  LearningRate 0.0331  ProxyLR: 1.6556  Epoch: 14  Global Step: 84310   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:35,740-Speed 3892.81 samples/sec  Loss 1.5714  LearningRate 0.0331  ProxyLR: 1.6551  Epoch: 14  Global Step: 84320   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:38,371-Speed 3892.69 samples/sec  Loss 1.5939  LearningRate 0.0331  ProxyLR: 1.6545  Epoch: 14  Global Step: 84330   Fp16 Grad Scale: 1048576  Required: 5 hours
Training: 2023-05-04 20:59:40,989-Speed 3911.70 samples/sec  Loss 1.5936  LearningRate 0.0331  ProxyLR: 1.6539  Epoch: 14  Global Step: 84340   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:43,621-Speed 3891.75 samples/sec  Loss 1.5632  LearningRate 0.0331  ProxyLR: 1.6533  Epoch: 14  Global Step: 84350   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:46,252-Speed 3892.80 samples/sec  Loss 1.5970  LearningRate 0.0331  ProxyLR: 1.6528  Epoch: 14  Global Step: 84360   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:48,883-Speed 3893.50 samples/sec  Loss 1.5574  LearningRate 0.0330  ProxyLR: 1.6522  Epoch: 14  Global Step: 84370   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 20:59:51,500-Speed 3913.39 samples/sec  Loss 1.6098  LearningRate 0.0330  ProxyLR: 1.6516  Epoch: 14  Global Step: 84380   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:59:54,129-Speed 3895.56 samples/sec  Loss 1.6658  LearningRate 0.0330  ProxyLR: 1.6511  Epoch: 14  Global Step: 84390   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:59:56,759-Speed 3894.89 samples/sec  Loss 1.5812  LearningRate 0.0330  ProxyLR: 1.6505  Epoch: 14  Global Step: 84400   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 20:59:59,390-Speed 3893.55 samples/sec  Loss 1.6091  LearningRate 0.0330  ProxyLR: 1.6499  Epoch: 14  Global Step: 84410   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:02,019-Speed 3895.50 samples/sec  Loss 1.6139  LearningRate 0.0330  ProxyLR: 1.6493  Epoch: 14  Global Step: 84420   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:04,652-Speed 3889.38 samples/sec  Loss 1.5603  LearningRate 0.0330  ProxyLR: 1.6488  Epoch: 14  Global Step: 84430   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:07,285-Speed 3890.07 samples/sec  Loss 1.6148  LearningRate 0.0330  ProxyLR: 1.6482  Epoch: 14  Global Step: 84440   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:09,917-Speed 3891.88 samples/sec  Loss 1.5950  LearningRate 0.0330  ProxyLR: 1.6476  Epoch: 14  Global Step: 84450   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:12,551-Speed 3889.46 samples/sec  Loss 1.6071  LearningRate 0.0329  ProxyLR: 1.6471  Epoch: 14  Global Step: 84460   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:15,183-Speed 3890.98 samples/sec  Loss 1.6023  LearningRate 0.0329  ProxyLR: 1.6465  Epoch: 14  Global Step: 84470   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:00:17,816-Speed 3889.98 samples/sec  Loss 1.6170  LearningRate 0.0329  ProxyLR: 1.6459  Epoch: 14  Global Step: 84480   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:20,451-Speed 3887.54 samples/sec  Loss 1.5979  LearningRate 0.0329  ProxyLR: 1.6453  Epoch: 14  Global Step: 84490   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:23,084-Speed 3889.60 samples/sec  Loss 1.5969  LearningRate 0.0329  ProxyLR: 1.6448  Epoch: 14  Global Step: 84500   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:25,715-Speed 3892.23 samples/sec  Loss 1.5791  LearningRate 0.0329  ProxyLR: 1.6442  Epoch: 14  Global Step: 84510   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:28,348-Speed 3891.00 samples/sec  Loss 1.6233  LearningRate 0.0329  ProxyLR: 1.6436  Epoch: 14  Global Step: 84520   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:30,980-Speed 3890.66 samples/sec  Loss 1.5813  LearningRate 0.0329  ProxyLR: 1.6431  Epoch: 14  Global Step: 84530   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:33,612-Speed 3892.26 samples/sec  Loss 1.6206  LearningRate 0.0328  ProxyLR: 1.6425  Epoch: 14  Global Step: 84540   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:36,244-Speed 3891.60 samples/sec  Loss 1.5558  LearningRate 0.0328  ProxyLR: 1.6419  Epoch: 14  Global Step: 84550   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:38,876-Speed 3891.15 samples/sec  Loss 1.6210  LearningRate 0.0328  ProxyLR: 1.6413  Epoch: 14  Global Step: 84560   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:41,508-Speed 3891.16 samples/sec  Loss 1.6599  LearningRate 0.0328  ProxyLR: 1.6408  Epoch: 14  Global Step: 84570   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:44,125-Speed 3914.48 samples/sec  Loss 1.6036  LearningRate 0.0328  ProxyLR: 1.6402  Epoch: 14  Global Step: 84580   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:46,758-Speed 3889.22 samples/sec  Loss 1.5770  LearningRate 0.0328  ProxyLR: 1.6396  Epoch: 14  Global Step: 84590   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:49,388-Speed 3894.74 samples/sec  Loss 1.6174  LearningRate 0.0328  ProxyLR: 1.6391  Epoch: 14  Global Step: 84600   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:52,019-Speed 3893.52 samples/sec  Loss 1.6168  LearningRate 0.0328  ProxyLR: 1.6385  Epoch: 14  Global Step: 84610   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:54,650-Speed 3893.46 samples/sec  Loss 1.5725  LearningRate 0.0328  ProxyLR: 1.6379  Epoch: 14  Global Step: 84620   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:57,282-Speed 3890.30 samples/sec  Loss 1.6282  LearningRate 0.0327  ProxyLR: 1.6374  Epoch: 14  Global Step: 84630   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:00:59,912-Speed 3894.89 samples/sec  Loss 1.5998  LearningRate 0.0327  ProxyLR: 1.6368  Epoch: 14  Global Step: 84640   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:01:02,530-Speed 3912.09 samples/sec  Loss 1.6187  LearningRate 0.0327  ProxyLR: 1.6362  Epoch: 14  Global Step: 84650   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:05,159-Speed 3896.93 samples/sec  Loss 1.5990  LearningRate 0.0327  ProxyLR: 1.6357  Epoch: 14  Global Step: 84660   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:07,788-Speed 3894.76 samples/sec  Loss 1.6475  LearningRate 0.0327  ProxyLR: 1.6351  Epoch: 14  Global Step: 84670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:10,418-Speed 3895.41 samples/sec  Loss 1.5899  LearningRate 0.0327  ProxyLR: 1.6345  Epoch: 14  Global Step: 84680   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:13,048-Speed 3894.18 samples/sec  Loss 1.5465  LearningRate 0.0327  ProxyLR: 1.6339  Epoch: 14  Global Step: 84690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:15,680-Speed 3891.27 samples/sec  Loss 1.5678  LearningRate 0.0327  ProxyLR: 1.6334  Epoch: 14  Global Step: 84700   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:18,313-Speed 3889.98 samples/sec  Loss 1.5641  LearningRate 0.0327  ProxyLR: 1.6328  Epoch: 14  Global Step: 84710   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:20,948-Speed 3887.73 samples/sec  Loss 1.5138  LearningRate 0.0326  ProxyLR: 1.6322  Epoch: 14  Global Step: 84720   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:23,584-Speed 3885.32 samples/sec  Loss 1.5329  LearningRate 0.0326  ProxyLR: 1.6317  Epoch: 14  Global Step: 84730   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:26,222-Speed 3882.06 samples/sec  Loss 1.5654  LearningRate 0.0326  ProxyLR: 1.6311  Epoch: 14  Global Step: 84740   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:28,858-Speed 3885.17 samples/sec  Loss 1.6588  LearningRate 0.0326  ProxyLR: 1.6305  Epoch: 14  Global Step: 84750   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:01:31,499-Speed 3879.00 samples/sec  Loss 1.5586  LearningRate 0.0326  ProxyLR: 1.6300  Epoch: 14  Global Step: 84760   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:01:34,137-Speed 3882.98 samples/sec  Loss 1.6074  LearningRate 0.0326  ProxyLR: 1.6294  Epoch: 14  Global Step: 84770   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:01:36,760-Speed 3905.03 samples/sec  Loss 1.5986  LearningRate 0.0326  ProxyLR: 1.6288  Epoch: 14  Global Step: 84780   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:39,395-Speed 3886.19 samples/sec  Loss 1.5839  LearningRate 0.0326  ProxyLR: 1.6283  Epoch: 14  Global Step: 84790   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:42,030-Speed 3887.40 samples/sec  Loss 1.5418  LearningRate 0.0326  ProxyLR: 1.6277  Epoch: 14  Global Step: 84800   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:44,665-Speed 3887.70 samples/sec  Loss 1.6211  LearningRate 0.0325  ProxyLR: 1.6271  Epoch: 14  Global Step: 84810   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:47,301-Speed 3885.13 samples/sec  Loss 1.5976  LearningRate 0.0325  ProxyLR: 1.6266  Epoch: 14  Global Step: 84820   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:49,936-Speed 3886.15 samples/sec  Loss 1.6234  LearningRate 0.0325  ProxyLR: 1.6260  Epoch: 14  Global Step: 84830   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:52,568-Speed 3891.95 samples/sec  Loss 1.6517  LearningRate 0.0325  ProxyLR: 1.6254  Epoch: 14  Global Step: 84840   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:55,204-Speed 3886.25 samples/sec  Loss 1.6009  LearningRate 0.0325  ProxyLR: 1.6249  Epoch: 14  Global Step: 84850   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:01:57,841-Speed 3883.97 samples/sec  Loss 1.6172  LearningRate 0.0325  ProxyLR: 1.6243  Epoch: 14  Global Step: 84860   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:00,477-Speed 3884.75 samples/sec  Loss 1.6106  LearningRate 0.0325  ProxyLR: 1.6237  Epoch: 14  Global Step: 84870   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:03,117-Speed 3879.63 samples/sec  Loss 1.5940  LearningRate 0.0325  ProxyLR: 1.6232  Epoch: 14  Global Step: 84880   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:05,756-Speed 3880.87 samples/sec  Loss 1.6267  LearningRate 0.0325  ProxyLR: 1.6226  Epoch: 14  Global Step: 84890   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:08,392-Speed 3885.63 samples/sec  Loss 1.5512  LearningRate 0.0324  ProxyLR: 1.6220  Epoch: 14  Global Step: 84900   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:11,030-Speed 3883.15 samples/sec  Loss 1.5776  LearningRate 0.0324  ProxyLR: 1.6215  Epoch: 14  Global Step: 84910   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:13,666-Speed 3885.43 samples/sec  Loss 1.5813  LearningRate 0.0324  ProxyLR: 1.6209  Epoch: 14  Global Step: 84920   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:16,295-Speed 3896.23 samples/sec  Loss 1.6608  LearningRate 0.0324  ProxyLR: 1.6203  Epoch: 14  Global Step: 84930   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:18,920-Speed 3901.58 samples/sec  Loss 1.6259  LearningRate 0.0324  ProxyLR: 1.6198  Epoch: 14  Global Step: 84940   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:21,547-Speed 3899.56 samples/sec  Loss 1.5814  LearningRate 0.0324  ProxyLR: 1.6192  Epoch: 14  Global Step: 84950   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:24,173-Speed 3899.67 samples/sec  Loss 1.6100  LearningRate 0.0324  ProxyLR: 1.6186  Epoch: 14  Global Step: 84960   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:26,798-Speed 3901.89 samples/sec  Loss 1.6269  LearningRate 0.0324  ProxyLR: 1.6181  Epoch: 14  Global Step: 84970   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:29,410-Speed 3920.92 samples/sec  Loss 1.6324  LearningRate 0.0323  ProxyLR: 1.6175  Epoch: 14  Global Step: 84980   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:32,036-Speed 3900.58 samples/sec  Loss 1.5398  LearningRate 0.0323  ProxyLR: 1.6169  Epoch: 14  Global Step: 84990   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:34,661-Speed 3902.03 samples/sec  Loss 1.5691  LearningRate 0.0323  ProxyLR: 1.6164  Epoch: 14  Global Step: 85000   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:02:37,273-Speed 3921.37 samples/sec  Loss 1.5828  LearningRate 0.0323  ProxyLR: 1.6158  Epoch: 14  Global Step: 85010   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:39,899-Speed 3900.41 samples/sec  Loss 1.6602  LearningRate 0.0323  ProxyLR: 1.6152  Epoch: 14  Global Step: 85020   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:42,524-Speed 3901.63 samples/sec  Loss 1.6299  LearningRate 0.0323  ProxyLR: 1.6147  Epoch: 14  Global Step: 85030   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:45,150-Speed 3901.07 samples/sec  Loss 1.5945  LearningRate 0.0323  ProxyLR: 1.6141  Epoch: 14  Global Step: 85040   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:47,774-Speed 3902.73 samples/sec  Loss 1.6362  LearningRate 0.0323  ProxyLR: 1.6135  Epoch: 14  Global Step: 85050   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:50,400-Speed 3900.80 samples/sec  Loss 1.5997  LearningRate 0.0323  ProxyLR: 1.6130  Epoch: 14  Global Step: 85060   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:53,026-Speed 3900.35 samples/sec  Loss 1.6189  LearningRate 0.0322  ProxyLR: 1.6124  Epoch: 14  Global Step: 85070   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:55,651-Speed 3900.89 samples/sec  Loss 1.6138  LearningRate 0.0322  ProxyLR: 1.6118  Epoch: 14  Global Step: 85080   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:02:58,275-Speed 3904.23 samples/sec  Loss 1.5844  LearningRate 0.0322  ProxyLR: 1.6113  Epoch: 14  Global Step: 85090   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:00,900-Speed 3902.12 samples/sec  Loss 1.6070  LearningRate 0.0322  ProxyLR: 1.6107  Epoch: 14  Global Step: 85100   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:03,524-Speed 3903.44 samples/sec  Loss 1.5508  LearningRate 0.0322  ProxyLR: 1.6101  Epoch: 14  Global Step: 85110   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:06,150-Speed 3899.08 samples/sec  Loss 1.5662  LearningRate 0.0322  ProxyLR: 1.6096  Epoch: 14  Global Step: 85120   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:08,776-Speed 3900.60 samples/sec  Loss 1.5117  LearningRate 0.0322  ProxyLR: 1.6090  Epoch: 14  Global Step: 85130   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:11,400-Speed 3903.52 samples/sec  Loss 1.6311  LearningRate 0.0322  ProxyLR: 1.6085  Epoch: 14  Global Step: 85140   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:14,030-Speed 3895.03 samples/sec  Loss 1.5750  LearningRate 0.0322  ProxyLR: 1.6079  Epoch: 14  Global Step: 85150   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:16,649-Speed 3910.79 samples/sec  Loss 1.5809  LearningRate 0.0321  ProxyLR: 1.6073  Epoch: 14  Global Step: 85160   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:19,277-Speed 3896.18 samples/sec  Loss 1.5981  LearningRate 0.0321  ProxyLR: 1.6068  Epoch: 14  Global Step: 85170   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:21,905-Speed 3897.88 samples/sec  Loss 1.5992  LearningRate 0.0321  ProxyLR: 1.6062  Epoch: 14  Global Step: 85180   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:24,533-Speed 3896.69 samples/sec  Loss 1.5725  LearningRate 0.0321  ProxyLR: 1.6056  Epoch: 14  Global Step: 85190   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:27,160-Speed 3899.05 samples/sec  Loss 1.6129  LearningRate 0.0321  ProxyLR: 1.6051  Epoch: 14  Global Step: 85200   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:29,790-Speed 3895.22 samples/sec  Loss 1.5237  LearningRate 0.0321  ProxyLR: 1.6045  Epoch: 14  Global Step: 85210   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:32,418-Speed 3898.20 samples/sec  Loss 1.5687  LearningRate 0.0321  ProxyLR: 1.6039  Epoch: 14  Global Step: 85220   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:35,045-Speed 3897.61 samples/sec  Loss 1.6821  LearningRate 0.0321  ProxyLR: 1.6034  Epoch: 14  Global Step: 85230   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:37,673-Speed 3897.70 samples/sec  Loss 1.6460  LearningRate 0.0321  ProxyLR: 1.6028  Epoch: 14  Global Step: 85240   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:40,300-Speed 3898.71 samples/sec  Loss 1.5252  LearningRate 0.0320  ProxyLR: 1.6023  Epoch: 14  Global Step: 85250   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:03:42,927-Speed 3899.66 samples/sec  Loss 1.6238  LearningRate 0.0320  ProxyLR: 1.6017  Epoch: 14  Global Step: 85260   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:45,555-Speed 3897.65 samples/sec  Loss 1.6151  LearningRate 0.0320  ProxyLR: 1.6011  Epoch: 14  Global Step: 85270   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:48,240-Speed 3813.94 samples/sec  Loss 1.5815  LearningRate 0.0320  ProxyLR: 1.6006  Epoch: 14  Global Step: 85280   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:50,865-Speed 3901.56 samples/sec  Loss 1.6120  LearningRate 0.0320  ProxyLR: 1.6000  Epoch: 14  Global Step: 85290   Fp16 Grad Scale: 524288  Required: 5 hours
Training: 2023-05-04 21:03:59,705-Speed 1158.59 samples/sec  Loss 14.0609  LearningRate 0.0320  ProxyLR: 1.5994  Epoch: 15  Global Step: 85300   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:02,340-Speed 3886.62 samples/sec  Loss 13.9814  LearningRate 0.0320  ProxyLR: 1.5989  Epoch: 15  Global Step: 85310   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:04,977-Speed 3884.58 samples/sec  Loss 14.1778  LearningRate 0.0320  ProxyLR: 1.5983  Epoch: 15  Global Step: 85320   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:07,639-Speed 3848.07 samples/sec  Loss 14.1553  LearningRate 0.0320  ProxyLR: 1.5977  Epoch: 15  Global Step: 85330   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:10,277-Speed 3882.59 samples/sec  Loss 14.1633  LearningRate 0.0319  ProxyLR: 1.5972  Epoch: 15  Global Step: 85340   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:12,914-Speed 3883.78 samples/sec  Loss 14.3454  LearningRate 0.0319  ProxyLR: 1.5966  Epoch: 15  Global Step: 85350   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:15,552-Speed 3882.49 samples/sec  Loss 14.2711  LearningRate 0.0319  ProxyLR: 1.5961  Epoch: 15  Global Step: 85360   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:18,186-Speed 3888.22 samples/sec  Loss 14.0856  LearningRate 0.0319  ProxyLR: 1.5955  Epoch: 15  Global Step: 85370   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:20,821-Speed 3887.06 samples/sec  Loss 14.1496  LearningRate 0.0319  ProxyLR: 1.5949  Epoch: 15  Global Step: 85380   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:23,456-Speed 3887.20 samples/sec  Loss 14.1151  LearningRate 0.0319  ProxyLR: 1.5944  Epoch: 15  Global Step: 85390   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:26,075-Speed 3911.77 samples/sec  Loss 13.8606  LearningRate 0.0319  ProxyLR: 1.5938  Epoch: 15  Global Step: 85400   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:28,759-Speed 3814.98 samples/sec  Loss 14.0962  LearningRate 0.0319  ProxyLR: 1.5933  Epoch: 15  Global Step: 85410   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:31,389-Speed 3894.64 samples/sec  Loss 13.9314  LearningRate 0.0319  ProxyLR: 1.5927  Epoch: 15  Global Step: 85420   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:34,043-Speed 3860.40 samples/sec  Loss 13.8490  LearningRate 0.0318  ProxyLR: 1.5921  Epoch: 15  Global Step: 85430   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:36,679-Speed 3885.35 samples/sec  Loss 13.8150  LearningRate 0.0318  ProxyLR: 1.5916  Epoch: 15  Global Step: 85440   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:39,315-Speed 3885.27 samples/sec  Loss 13.6879  LearningRate 0.0318  ProxyLR: 1.5910  Epoch: 15  Global Step: 85450   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:42,000-Speed 3814.48 samples/sec  Loss 13.7429  LearningRate 0.0318  ProxyLR: 1.5904  Epoch: 15  Global Step: 85460   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:44,633-Speed 3889.68 samples/sec  Loss 13.6215  LearningRate 0.0318  ProxyLR: 1.5899  Epoch: 15  Global Step: 85470   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:47,267-Speed 3888.20 samples/sec  Loss 13.6496  LearningRate 0.0318  ProxyLR: 1.5893  Epoch: 15  Global Step: 85480   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:49,905-Speed 3883.09 samples/sec  Loss 13.6508  LearningRate 0.0318  ProxyLR: 1.5888  Epoch: 15  Global Step: 85490   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:04:52,538-Speed 3890.63 samples/sec  Loss 13.5183  LearningRate 0.0318  ProxyLR: 1.5882  Epoch: 15  Global Step: 85500   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:04:55,173-Speed 3886.14 samples/sec  Loss 13.4301  LearningRate 0.0318  ProxyLR: 1.5876  Epoch: 15  Global Step: 85510   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:04:57,791-Speed 3912.94 samples/sec  Loss 13.4344  LearningRate 0.0317  ProxyLR: 1.5871  Epoch: 15  Global Step: 85520   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:00,423-Speed 3891.46 samples/sec  Loss 13.5111  LearningRate 0.0317  ProxyLR: 1.5865  Epoch: 15  Global Step: 85530   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:03,054-Speed 3893.92 samples/sec  Loss 13.1516  LearningRate 0.0317  ProxyLR: 1.5860  Epoch: 15  Global Step: 85540   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:05,686-Speed 3891.32 samples/sec  Loss 13.2997  LearningRate 0.0317  ProxyLR: 1.5854  Epoch: 15  Global Step: 85550   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:08,316-Speed 3894.56 samples/sec  Loss 13.1240  LearningRate 0.0317  ProxyLR: 1.5848  Epoch: 15  Global Step: 85560   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:10,947-Speed 3892.69 samples/sec  Loss 13.0785  LearningRate 0.0317  ProxyLR: 1.5843  Epoch: 15  Global Step: 85570   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:13,578-Speed 3893.02 samples/sec  Loss 13.2111  LearningRate 0.0317  ProxyLR: 1.5837  Epoch: 15  Global Step: 85580   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:16,261-Speed 3817.37 samples/sec  Loss 13.0876  LearningRate 0.0317  ProxyLR: 1.5832  Epoch: 15  Global Step: 85590   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:18,892-Speed 3893.37 samples/sec  Loss 12.9909  LearningRate 0.0317  ProxyLR: 1.5826  Epoch: 15  Global Step: 85600   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:21,523-Speed 3892.62 samples/sec  Loss 13.0153  LearningRate 0.0316  ProxyLR: 1.5820  Epoch: 15  Global Step: 85610   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:24,154-Speed 3892.88 samples/sec  Loss 13.0783  LearningRate 0.0316  ProxyLR: 1.5815  Epoch: 15  Global Step: 85620   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:26,784-Speed 3894.54 samples/sec  Loss 12.9354  LearningRate 0.0316  ProxyLR: 1.5809  Epoch: 15  Global Step: 85630   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:29,415-Speed 3892.48 samples/sec  Loss 12.8647  LearningRate 0.0316  ProxyLR: 1.5804  Epoch: 15  Global Step: 85640   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:32,046-Speed 3892.74 samples/sec  Loss 12.8512  LearningRate 0.0316  ProxyLR: 1.5798  Epoch: 15  Global Step: 85650   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:34,677-Speed 3894.44 samples/sec  Loss 12.8092  LearningRate 0.0316  ProxyLR: 1.5792  Epoch: 15  Global Step: 85660   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:37,307-Speed 3894.46 samples/sec  Loss 12.9798  LearningRate 0.0316  ProxyLR: 1.5787  Epoch: 15  Global Step: 85670   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:39,936-Speed 3895.08 samples/sec  Loss 12.6059  LearningRate 0.0316  ProxyLR: 1.5781  Epoch: 15  Global Step: 85680   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:42,567-Speed 3893.58 samples/sec  Loss 12.6305  LearningRate 0.0316  ProxyLR: 1.5776  Epoch: 15  Global Step: 85690   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:05:45,189-Speed 3906.18 samples/sec  Loss 12.5159  LearningRate 0.0315  ProxyLR: 1.5770  Epoch: 15  Global Step: 85700   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:47,823-Speed 3887.94 samples/sec  Loss 12.6573  LearningRate 0.0315  ProxyLR: 1.5765  Epoch: 15  Global Step: 85710   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:50,454-Speed 3893.16 samples/sec  Loss 12.6145  LearningRate 0.0315  ProxyLR: 1.5759  Epoch: 15  Global Step: 85720   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:53,084-Speed 3894.14 samples/sec  Loss 12.5625  LearningRate 0.0315  ProxyLR: 1.5753  Epoch: 15  Global Step: 85730   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:55,718-Speed 3888.28 samples/sec  Loss 12.5149  LearningRate 0.0315  ProxyLR: 1.5748  Epoch: 15  Global Step: 85740   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:05:58,355-Speed 3885.39 samples/sec  Loss 12.3671  LearningRate 0.0315  ProxyLR: 1.5742  Epoch: 15  Global Step: 85750   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:00,993-Speed 3882.48 samples/sec  Loss 12.3082  LearningRate 0.0315  ProxyLR: 1.5737  Epoch: 15  Global Step: 85760   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:03,631-Speed 3882.68 samples/sec  Loss 12.3394  LearningRate 0.0315  ProxyLR: 1.5731  Epoch: 15  Global Step: 85770   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:06,267-Speed 3885.02 samples/sec  Loss 12.3735  LearningRate 0.0315  ProxyLR: 1.5725  Epoch: 15  Global Step: 85780   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:08,907-Speed 3880.26 samples/sec  Loss 12.2845  LearningRate 0.0314  ProxyLR: 1.5720  Epoch: 15  Global Step: 85790   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:11,529-Speed 3905.41 samples/sec  Loss 12.0994  LearningRate 0.0314  ProxyLR: 1.5714  Epoch: 15  Global Step: 85800   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:14,164-Speed 3888.38 samples/sec  Loss 12.2923  LearningRate 0.0314  ProxyLR: 1.5709  Epoch: 15  Global Step: 85810   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:16,799-Speed 3886.89 samples/sec  Loss 12.1417  LearningRate 0.0314  ProxyLR: 1.5703  Epoch: 15  Global Step: 85820   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:19,435-Speed 3885.49 samples/sec  Loss 12.1641  LearningRate 0.0314  ProxyLR: 1.5698  Epoch: 15  Global Step: 85830   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:22,071-Speed 3885.45 samples/sec  Loss 12.0732  LearningRate 0.0314  ProxyLR: 1.5692  Epoch: 15  Global Step: 85840   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:24,706-Speed 3886.60 samples/sec  Loss 11.9893  LearningRate 0.0314  ProxyLR: 1.5686  Epoch: 15  Global Step: 85850   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:27,341-Speed 3887.83 samples/sec  Loss 11.9840  LearningRate 0.0314  ProxyLR: 1.5681  Epoch: 15  Global Step: 85860   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:29,973-Speed 3890.59 samples/sec  Loss 11.9194  LearningRate 0.0314  ProxyLR: 1.5675  Epoch: 15  Global Step: 85870   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:32,609-Speed 3885.79 samples/sec  Loss 11.9426  LearningRate 0.0313  ProxyLR: 1.5670  Epoch: 15  Global Step: 85880   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:35,246-Speed 3885.13 samples/sec  Loss 11.7469  LearningRate 0.0313  ProxyLR: 1.5664  Epoch: 15  Global Step: 85890   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:37,881-Speed 3886.29 samples/sec  Loss 11.8682  LearningRate 0.0313  ProxyLR: 1.5659  Epoch: 15  Global Step: 85900   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:40,520-Speed 3881.39 samples/sec  Loss 11.7960  LearningRate 0.0313  ProxyLR: 1.5653  Epoch: 15  Global Step: 85910   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:43,157-Speed 3883.98 samples/sec  Loss 11.9118  LearningRate 0.0313  ProxyLR: 1.5647  Epoch: 15  Global Step: 85920   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:45,794-Speed 3884.45 samples/sec  Loss 11.8275  LearningRate 0.0313  ProxyLR: 1.5642  Epoch: 15  Global Step: 85930   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:48,431-Speed 3883.67 samples/sec  Loss 11.6979  LearningRate 0.0313  ProxyLR: 1.5636  Epoch: 15  Global Step: 85940   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:51,072-Speed 3879.15 samples/sec  Loss 11.7544  LearningRate 0.0313  ProxyLR: 1.5631  Epoch: 15  Global Step: 85950   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:53,710-Speed 3882.45 samples/sec  Loss 11.7506  LearningRate 0.0313  ProxyLR: 1.5625  Epoch: 15  Global Step: 85960   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:06:56,332-Speed 3906.73 samples/sec  Loss 11.5479  LearningRate 0.0312  ProxyLR: 1.5620  Epoch: 15  Global Step: 85970   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:06:58,968-Speed 3884.82 samples/sec  Loss 11.5392  LearningRate 0.0312  ProxyLR: 1.5614  Epoch: 15  Global Step: 85980   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:01,608-Speed 3880.30 samples/sec  Loss 11.5313  LearningRate 0.0312  ProxyLR: 1.5608  Epoch: 15  Global Step: 85990   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:04,246-Speed 3882.59 samples/sec  Loss 11.4955  LearningRate 0.0312  ProxyLR: 1.5603  Epoch: 15  Global Step: 86000   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:06,884-Speed 3882.52 samples/sec  Loss 11.3976  LearningRate 0.0312  ProxyLR: 1.5597  Epoch: 15  Global Step: 86010   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:09,521-Speed 3884.46 samples/sec  Loss 11.5623  LearningRate 0.0312  ProxyLR: 1.5592  Epoch: 15  Global Step: 86020   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:12,156-Speed 3886.25 samples/sec  Loss 11.3246  LearningRate 0.0312  ProxyLR: 1.5586  Epoch: 15  Global Step: 86030   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:14,791-Speed 3887.59 samples/sec  Loss 11.3642  LearningRate 0.0312  ProxyLR: 1.5581  Epoch: 15  Global Step: 86040   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:17,426-Speed 3886.65 samples/sec  Loss 11.2626  LearningRate 0.0312  ProxyLR: 1.5575  Epoch: 15  Global Step: 86050   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:20,060-Speed 3888.88 samples/sec  Loss 11.2924  LearningRate 0.0311  ProxyLR: 1.5570  Epoch: 15  Global Step: 86060   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:22,694-Speed 3888.94 samples/sec  Loss 11.1800  LearningRate 0.0311  ProxyLR: 1.5564  Epoch: 15  Global Step: 86070   Fp16 Grad Scale: 262144  Required: 5 hours
Training: 2023-05-04 21:07:25,311-Speed 3913.50 samples/sec  Loss 11.2661  LearningRate 0.0311  ProxyLR: 1.5558  Epoch: 15  Global Step: 86080   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:27,942-Speed 3892.63 samples/sec  Loss 11.2251  LearningRate 0.0311  ProxyLR: 1.5553  Epoch: 15  Global Step: 86090   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:30,572-Speed 3893.86 samples/sec  Loss 11.1677  LearningRate 0.0311  ProxyLR: 1.5547  Epoch: 15  Global Step: 86100   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:33,204-Speed 3892.82 samples/sec  Loss 11.0840  LearningRate 0.0311  ProxyLR: 1.5542  Epoch: 15  Global Step: 86110   Fp16 Grad Scale: 131072  Required: 5 hours
Training: 2023-05-04 21:07:35,835-Speed 3891.75 samples/sec  Loss 11.1031  LearningRate 0.0311  ProxyLR: 1.5536  Epoch: 15  Global Step: 86120   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:07:38,469-Speed 3889.70 samples/sec  Loss 11.1625  LearningRate 0.0311  ProxyLR: 1.5531  Epoch: 15  Global Step: 86130   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:07:41,100-Speed 3892.36 samples/sec  Loss 10.9723  LearningRate 0.0311  ProxyLR: 1.5525  Epoch: 15  Global Step: 86140   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:07:43,732-Speed 3890.81 samples/sec  Loss 11.0068  LearningRate 0.0310  ProxyLR: 1.5520  Epoch: 15  Global Step: 86150   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:07:46,364-Speed 3892.65 samples/sec  Loss 10.9536  LearningRate 0.0310  ProxyLR: 1.5514  Epoch: 15  Global Step: 86160   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:07:48,995-Speed 3891.90 samples/sec  Loss 10.9096  LearningRate 0.0310  ProxyLR: 1.5509  Epoch: 15  Global Step: 86170   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:07:51,629-Speed 3888.57 samples/sec  Loss 10.9290  LearningRate 0.0310  ProxyLR: 1.5503  Epoch: 15  Global Step: 86180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:07:54,260-Speed 3893.24 samples/sec  Loss 10.8856  LearningRate 0.0310  ProxyLR: 1.5498  Epoch: 15  Global Step: 86190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:07:56,893-Speed 3890.54 samples/sec  Loss 10.8422  LearningRate 0.0310  ProxyLR: 1.5492  Epoch: 15  Global Step: 86200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:07:59,524-Speed 3892.97 samples/sec  Loss 10.9118  LearningRate 0.0310  ProxyLR: 1.5486  Epoch: 15  Global Step: 86210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:02,156-Speed 3890.73 samples/sec  Loss 10.8450  LearningRate 0.0310  ProxyLR: 1.5481  Epoch: 15  Global Step: 86220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:04,789-Speed 3890.23 samples/sec  Loss 10.7458  LearningRate 0.0310  ProxyLR: 1.5475  Epoch: 15  Global Step: 86230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:07,418-Speed 3896.04 samples/sec  Loss 10.8549  LearningRate 0.0309  ProxyLR: 1.5470  Epoch: 15  Global Step: 86240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:10,037-Speed 3910.52 samples/sec  Loss 10.7430  LearningRate 0.0309  ProxyLR: 1.5464  Epoch: 15  Global Step: 86250   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:12,673-Speed 3886.14 samples/sec  Loss 10.6866  LearningRate 0.0309  ProxyLR: 1.5459  Epoch: 15  Global Step: 86260   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:15,308-Speed 3887.83 samples/sec  Loss 10.7405  LearningRate 0.0309  ProxyLR: 1.5453  Epoch: 15  Global Step: 86270   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:17,942-Speed 3888.03 samples/sec  Loss 10.5938  LearningRate 0.0309  ProxyLR: 1.5448  Epoch: 15  Global Step: 86280   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:20,577-Speed 3886.95 samples/sec  Loss 10.5869  LearningRate 0.0309  ProxyLR: 1.5442  Epoch: 15  Global Step: 86290   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:23,212-Speed 3887.50 samples/sec  Loss 10.5756  LearningRate 0.0309  ProxyLR: 1.5437  Epoch: 15  Global Step: 86300   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:25,848-Speed 3884.48 samples/sec  Loss 10.5908  LearningRate 0.0309  ProxyLR: 1.5431  Epoch: 15  Global Step: 86310   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:28,484-Speed 3885.81 samples/sec  Loss 10.4627  LearningRate 0.0309  ProxyLR: 1.5426  Epoch: 15  Global Step: 86320   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:31,124-Speed 3879.45 samples/sec  Loss 10.5660  LearningRate 0.0308  ProxyLR: 1.5420  Epoch: 15  Global Step: 86330   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:33,761-Speed 3884.34 samples/sec  Loss 10.5019  LearningRate 0.0308  ProxyLR: 1.5415  Epoch: 15  Global Step: 86340   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:08:36,400-Speed 3881.81 samples/sec  Loss 10.4187  LearningRate 0.0308  ProxyLR: 1.5409  Epoch: 15  Global Step: 86350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:39,038-Speed 3882.83 samples/sec  Loss 10.4746  LearningRate 0.0308  ProxyLR: 1.5403  Epoch: 15  Global Step: 86360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:41,672-Speed 3887.53 samples/sec  Loss 10.4301  LearningRate 0.0308  ProxyLR: 1.5398  Epoch: 15  Global Step: 86370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:44,307-Speed 3886.77 samples/sec  Loss 10.4533  LearningRate 0.0308  ProxyLR: 1.5392  Epoch: 15  Global Step: 86380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:46,944-Speed 3885.07 samples/sec  Loss 10.3219  LearningRate 0.0308  ProxyLR: 1.5387  Epoch: 15  Global Step: 86390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:49,580-Speed 3884.89 samples/sec  Loss 10.2393  LearningRate 0.0308  ProxyLR: 1.5381  Epoch: 15  Global Step: 86400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:52,218-Speed 3884.20 samples/sec  Loss 10.3111  LearningRate 0.0308  ProxyLR: 1.5376  Epoch: 15  Global Step: 86410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:54,856-Speed 3881.86 samples/sec  Loss 10.3644  LearningRate 0.0307  ProxyLR: 1.5370  Epoch: 15  Global Step: 86420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:08:57,497-Speed 3878.53 samples/sec  Loss 10.2614  LearningRate 0.0307  ProxyLR: 1.5365  Epoch: 15  Global Step: 86430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:00,138-Speed 3878.59 samples/sec  Loss 10.1250  LearningRate 0.0307  ProxyLR: 1.5359  Epoch: 15  Global Step: 86440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:02,764-Speed 3900.31 samples/sec  Loss 10.2530  LearningRate 0.0307  ProxyLR: 1.5354  Epoch: 15  Global Step: 86450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:05,402-Speed 3882.59 samples/sec  Loss 9.9790  LearningRate 0.0307  ProxyLR: 1.5348  Epoch: 15  Global Step: 86460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:08,038-Speed 3884.49 samples/sec  Loss 10.1291  LearningRate 0.0307  ProxyLR: 1.5343  Epoch: 15  Global Step: 86470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:10,675-Speed 3884.98 samples/sec  Loss 10.2257  LearningRate 0.0307  ProxyLR: 1.5337  Epoch: 15  Global Step: 86480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:13,309-Speed 3887.43 samples/sec  Loss 10.0500  LearningRate 0.0307  ProxyLR: 1.5332  Epoch: 15  Global Step: 86490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:15,947-Speed 3882.60 samples/sec  Loss 10.1280  LearningRate 0.0307  ProxyLR: 1.5326  Epoch: 15  Global Step: 86500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:18,584-Speed 3884.40 samples/sec  Loss 10.0703  LearningRate 0.0306  ProxyLR: 1.5321  Epoch: 15  Global Step: 86510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:21,219-Speed 3887.05 samples/sec  Loss 9.9411  LearningRate 0.0306  ProxyLR: 1.5315  Epoch: 15  Global Step: 86520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:23,856-Speed 3884.39 samples/sec  Loss 9.9737  LearningRate 0.0306  ProxyLR: 1.5310  Epoch: 15  Global Step: 86530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:26,490-Speed 3888.88 samples/sec  Loss 10.0217  LearningRate 0.0306  ProxyLR: 1.5304  Epoch: 15  Global Step: 86540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:29,109-Speed 3910.83 samples/sec  Loss 9.8193  LearningRate 0.0306  ProxyLR: 1.5299  Epoch: 15  Global Step: 86550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:31,741-Speed 3890.79 samples/sec  Loss 9.9440  LearningRate 0.0306  ProxyLR: 1.5293  Epoch: 15  Global Step: 86560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:09:34,357-Speed 3915.42 samples/sec  Loss 10.0054  LearningRate 0.0306  ProxyLR: 1.5288  Epoch: 15  Global Step: 86570   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:36,986-Speed 3895.99 samples/sec  Loss 9.7642  LearningRate 0.0306  ProxyLR: 1.5282  Epoch: 15  Global Step: 86580   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:39,616-Speed 3893.98 samples/sec  Loss 9.8253  LearningRate 0.0306  ProxyLR: 1.5277  Epoch: 15  Global Step: 86590   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:42,246-Speed 3895.89 samples/sec  Loss 9.8967  LearningRate 0.0305  ProxyLR: 1.5271  Epoch: 15  Global Step: 86600   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:44,877-Speed 3892.34 samples/sec  Loss 9.8058  LearningRate 0.0305  ProxyLR: 1.5266  Epoch: 15  Global Step: 86610   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:47,503-Speed 3900.11 samples/sec  Loss 9.8141  LearningRate 0.0305  ProxyLR: 1.5260  Epoch: 15  Global Step: 86620   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:50,129-Speed 3901.41 samples/sec  Loss 9.8001  LearningRate 0.0305  ProxyLR: 1.5255  Epoch: 15  Global Step: 86630   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:52,758-Speed 3896.20 samples/sec  Loss 9.6006  LearningRate 0.0305  ProxyLR: 1.5249  Epoch: 15  Global Step: 86640   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:55,386-Speed 3896.70 samples/sec  Loss 9.6848  LearningRate 0.0305  ProxyLR: 1.5244  Epoch: 15  Global Step: 86650   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:09:58,013-Speed 3899.82 samples/sec  Loss 9.6588  LearningRate 0.0305  ProxyLR: 1.5238  Epoch: 15  Global Step: 86660   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:00,641-Speed 3896.56 samples/sec  Loss 9.6827  LearningRate 0.0305  ProxyLR: 1.5233  Epoch: 15  Global Step: 86670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:03,269-Speed 3898.24 samples/sec  Loss 9.5043  LearningRate 0.0305  ProxyLR: 1.5227  Epoch: 15  Global Step: 86680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:05,899-Speed 3893.65 samples/sec  Loss 9.5319  LearningRate 0.0304  ProxyLR: 1.5222  Epoch: 15  Global Step: 86690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:08,528-Speed 3896.89 samples/sec  Loss 9.5636  LearningRate 0.0304  ProxyLR: 1.5216  Epoch: 15  Global Step: 86700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:11,156-Speed 3896.23 samples/sec  Loss 9.6743  LearningRate 0.0304  ProxyLR: 1.5211  Epoch: 15  Global Step: 86710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:13,785-Speed 3896.09 samples/sec  Loss 9.5165  LearningRate 0.0304  ProxyLR: 1.5205  Epoch: 15  Global Step: 86720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:16,415-Speed 3895.36 samples/sec  Loss 9.4991  LearningRate 0.0304  ProxyLR: 1.5200  Epoch: 15  Global Step: 86730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:19,042-Speed 3898.07 samples/sec  Loss 9.5387  LearningRate 0.0304  ProxyLR: 1.5194  Epoch: 15  Global Step: 86740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:21,670-Speed 3897.52 samples/sec  Loss 9.5335  LearningRate 0.0304  ProxyLR: 1.5189  Epoch: 15  Global Step: 86750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:24,299-Speed 3896.74 samples/sec  Loss 9.3615  LearningRate 0.0304  ProxyLR: 1.5183  Epoch: 15  Global Step: 86760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:26,915-Speed 3915.73 samples/sec  Loss 9.3438  LearningRate 0.0304  ProxyLR: 1.5178  Epoch: 15  Global Step: 86770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:29,539-Speed 3903.13 samples/sec  Loss 9.3553  LearningRate 0.0303  ProxyLR: 1.5172  Epoch: 15  Global Step: 86780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:10:32,151-Speed 3921.12 samples/sec  Loss 9.3914  LearningRate 0.0303  ProxyLR: 1.5167  Epoch: 15  Global Step: 86790   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:34,779-Speed 3897.51 samples/sec  Loss 9.3989  LearningRate 0.0303  ProxyLR: 1.5161  Epoch: 15  Global Step: 86800   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:37,404-Speed 3902.06 samples/sec  Loss 9.3320  LearningRate 0.0303  ProxyLR: 1.5156  Epoch: 15  Global Step: 86810   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:40,031-Speed 3899.91 samples/sec  Loss 9.2834  LearningRate 0.0303  ProxyLR: 1.5151  Epoch: 15  Global Step: 86820   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:42,658-Speed 3897.93 samples/sec  Loss 9.3237  LearningRate 0.0303  ProxyLR: 1.5145  Epoch: 15  Global Step: 86830   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:45,284-Speed 3900.44 samples/sec  Loss 9.3466  LearningRate 0.0303  ProxyLR: 1.5140  Epoch: 15  Global Step: 86840   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:47,909-Speed 3901.70 samples/sec  Loss 9.0461  LearningRate 0.0303  ProxyLR: 1.5134  Epoch: 15  Global Step: 86850   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:50,534-Speed 3902.73 samples/sec  Loss 9.2540  LearningRate 0.0303  ProxyLR: 1.5129  Epoch: 15  Global Step: 86860   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:53,160-Speed 3900.75 samples/sec  Loss 9.2434  LearningRate 0.0302  ProxyLR: 1.5123  Epoch: 15  Global Step: 86870   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:55,785-Speed 3901.67 samples/sec  Loss 9.2677  LearningRate 0.0302  ProxyLR: 1.5118  Epoch: 15  Global Step: 86880   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:10:58,409-Speed 3902.50 samples/sec  Loss 9.2905  LearningRate 0.0302  ProxyLR: 1.5112  Epoch: 15  Global Step: 86890   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:01,034-Speed 3902.38 samples/sec  Loss 9.0814  LearningRate 0.0302  ProxyLR: 1.5107  Epoch: 15  Global Step: 86900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:03,661-Speed 3898.40 samples/sec  Loss 9.0892  LearningRate 0.0302  ProxyLR: 1.5101  Epoch: 15  Global Step: 86910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:06,285-Speed 3903.65 samples/sec  Loss 9.0876  LearningRate 0.0302  ProxyLR: 1.5096  Epoch: 15  Global Step: 86920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:08,911-Speed 3900.92 samples/sec  Loss 9.0319  LearningRate 0.0302  ProxyLR: 1.5090  Epoch: 15  Global Step: 86930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:11,536-Speed 3901.27 samples/sec  Loss 9.1053  LearningRate 0.0302  ProxyLR: 1.5085  Epoch: 15  Global Step: 86940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:14,162-Speed 3900.86 samples/sec  Loss 9.0759  LearningRate 0.0302  ProxyLR: 1.5079  Epoch: 15  Global Step: 86950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:16,789-Speed 3898.56 samples/sec  Loss 9.0159  LearningRate 0.0301  ProxyLR: 1.5074  Epoch: 15  Global Step: 86960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:19,414-Speed 3901.62 samples/sec  Loss 8.8897  LearningRate 0.0301  ProxyLR: 1.5068  Epoch: 15  Global Step: 86970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:22,039-Speed 3902.36 samples/sec  Loss 8.9147  LearningRate 0.0301  ProxyLR: 1.5063  Epoch: 15  Global Step: 86980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:24,665-Speed 3899.93 samples/sec  Loss 9.0963  LearningRate 0.0301  ProxyLR: 1.5058  Epoch: 15  Global Step: 86990   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:11:27,277-Speed 3921.63 samples/sec  Loss 8.8685  LearningRate 0.0301  ProxyLR: 1.5052  Epoch: 15  Global Step: 87000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:29,903-Speed 3900.64 samples/sec  Loss 8.9273  LearningRate 0.0301  ProxyLR: 1.5047  Epoch: 15  Global Step: 87010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:32,528-Speed 3902.08 samples/sec  Loss 9.0200  LearningRate 0.0301  ProxyLR: 1.5041  Epoch: 15  Global Step: 87020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:35,152-Speed 3902.07 samples/sec  Loss 8.7905  LearningRate 0.0301  ProxyLR: 1.5036  Epoch: 15  Global Step: 87030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:37,777-Speed 3901.98 samples/sec  Loss 8.7800  LearningRate 0.0301  ProxyLR: 1.5030  Epoch: 15  Global Step: 87040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:40,402-Speed 3902.83 samples/sec  Loss 8.9288  LearningRate 0.0300  ProxyLR: 1.5025  Epoch: 15  Global Step: 87050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:43,027-Speed 3900.83 samples/sec  Loss 8.8217  LearningRate 0.0300  ProxyLR: 1.5019  Epoch: 15  Global Step: 87060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:45,652-Speed 3902.28 samples/sec  Loss 8.8613  LearningRate 0.0300  ProxyLR: 1.5014  Epoch: 15  Global Step: 87070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:48,277-Speed 3902.26 samples/sec  Loss 8.7716  LearningRate 0.0300  ProxyLR: 1.5008  Epoch: 15  Global Step: 87080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:50,902-Speed 3901.38 samples/sec  Loss 8.8876  LearningRate 0.0300  ProxyLR: 1.5003  Epoch: 15  Global Step: 87090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:53,515-Speed 3919.61 samples/sec  Loss 8.8446  LearningRate 0.0300  ProxyLR: 1.4998  Epoch: 15  Global Step: 87100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:56,142-Speed 3899.19 samples/sec  Loss 8.7811  LearningRate 0.0300  ProxyLR: 1.4992  Epoch: 15  Global Step: 87110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:11:58,765-Speed 3904.66 samples/sec  Loss 8.5436  LearningRate 0.0300  ProxyLR: 1.4987  Epoch: 15  Global Step: 87120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:01,392-Speed 3899.37 samples/sec  Loss 8.7644  LearningRate 0.0300  ProxyLR: 1.4981  Epoch: 15  Global Step: 87130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:04,018-Speed 3901.17 samples/sec  Loss 8.7562  LearningRate 0.0300  ProxyLR: 1.4976  Epoch: 15  Global Step: 87140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:06,644-Speed 3899.61 samples/sec  Loss 8.6819  LearningRate 0.0299  ProxyLR: 1.4970  Epoch: 15  Global Step: 87150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:09,270-Speed 3900.75 samples/sec  Loss 8.6081  LearningRate 0.0299  ProxyLR: 1.4965  Epoch: 15  Global Step: 87160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:11,896-Speed 3900.11 samples/sec  Loss 8.5128  LearningRate 0.0299  ProxyLR: 1.4959  Epoch: 15  Global Step: 87170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:14,523-Speed 3900.05 samples/sec  Loss 8.5868  LearningRate 0.0299  ProxyLR: 1.4954  Epoch: 15  Global Step: 87180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:17,146-Speed 3905.02 samples/sec  Loss 8.7155  LearningRate 0.0299  ProxyLR: 1.4949  Epoch: 15  Global Step: 87190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:19,758-Speed 3920.88 samples/sec  Loss 8.6994  LearningRate 0.0299  ProxyLR: 1.4943  Epoch: 15  Global Step: 87200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:22,383-Speed 3902.00 samples/sec  Loss 8.6593  LearningRate 0.0299  ProxyLR: 1.4938  Epoch: 15  Global Step: 87210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:25,008-Speed 3901.27 samples/sec  Loss 8.5596  LearningRate 0.0299  ProxyLR: 1.4932  Epoch: 15  Global Step: 87220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:27,633-Speed 3902.33 samples/sec  Loss 8.4492  LearningRate 0.0299  ProxyLR: 1.4927  Epoch: 15  Global Step: 87230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:30,258-Speed 3900.95 samples/sec  Loss 8.5334  LearningRate 0.0298  ProxyLR: 1.4921  Epoch: 15  Global Step: 87240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:32,885-Speed 3899.79 samples/sec  Loss 8.6011  LearningRate 0.0298  ProxyLR: 1.4916  Epoch: 15  Global Step: 87250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:35,511-Speed 3900.11 samples/sec  Loss 8.3968  LearningRate 0.0298  ProxyLR: 1.4911  Epoch: 15  Global Step: 87260   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:38,141-Speed 3894.07 samples/sec  Loss 8.4319  LearningRate 0.0298  ProxyLR: 1.4905  Epoch: 15  Global Step: 87270   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:40,767-Speed 3900.19 samples/sec  Loss 8.4683  LearningRate 0.0298  ProxyLR: 1.4900  Epoch: 15  Global Step: 87280   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:43,392-Speed 3901.78 samples/sec  Loss 8.3041  LearningRate 0.0298  ProxyLR: 1.4894  Epoch: 15  Global Step: 87290   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:46,018-Speed 3900.72 samples/sec  Loss 8.4873  LearningRate 0.0298  ProxyLR: 1.4889  Epoch: 15  Global Step: 87300   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:12:48,631-Speed 3919.69 samples/sec  Loss 8.3928  LearningRate 0.0298  ProxyLR: 1.4883  Epoch: 15  Global Step: 87310   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:51,257-Speed 3900.71 samples/sec  Loss 8.3584  LearningRate 0.0298  ProxyLR: 1.4878  Epoch: 15  Global Step: 87320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:53,884-Speed 3899.48 samples/sec  Loss 8.4540  LearningRate 0.0297  ProxyLR: 1.4873  Epoch: 15  Global Step: 87330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:56,509-Speed 3901.40 samples/sec  Loss 8.2203  LearningRate 0.0297  ProxyLR: 1.4867  Epoch: 15  Global Step: 87340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:12:59,134-Speed 3901.08 samples/sec  Loss 8.2658  LearningRate 0.0297  ProxyLR: 1.4862  Epoch: 15  Global Step: 87350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:01,761-Speed 3899.88 samples/sec  Loss 8.2394  LearningRate 0.0297  ProxyLR: 1.4856  Epoch: 15  Global Step: 87360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:04,387-Speed 3900.14 samples/sec  Loss 8.1810  LearningRate 0.0297  ProxyLR: 1.4851  Epoch: 15  Global Step: 87370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:07,013-Speed 3901.25 samples/sec  Loss 8.4002  LearningRate 0.0297  ProxyLR: 1.4845  Epoch: 15  Global Step: 87380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:09,638-Speed 3901.33 samples/sec  Loss 8.1736  LearningRate 0.0297  ProxyLR: 1.4840  Epoch: 15  Global Step: 87390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:12,265-Speed 3898.84 samples/sec  Loss 8.2611  LearningRate 0.0297  ProxyLR: 1.4835  Epoch: 15  Global Step: 87400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:14,890-Speed 3902.40 samples/sec  Loss 8.1984  LearningRate 0.0297  ProxyLR: 1.4829  Epoch: 15  Global Step: 87410   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:13:17,501-Speed 3922.31 samples/sec  Loss 8.1417  LearningRate 0.0296  ProxyLR: 1.4824  Epoch: 15  Global Step: 87420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:20,128-Speed 3898.09 samples/sec  Loss 8.2425  LearningRate 0.0296  ProxyLR: 1.4818  Epoch: 15  Global Step: 87430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:22,753-Speed 3902.29 samples/sec  Loss 8.2405  LearningRate 0.0296  ProxyLR: 1.4813  Epoch: 15  Global Step: 87440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:25,379-Speed 3900.59 samples/sec  Loss 8.0990  LearningRate 0.0296  ProxyLR: 1.4807  Epoch: 15  Global Step: 87450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:28,004-Speed 3902.28 samples/sec  Loss 8.0968  LearningRate 0.0296  ProxyLR: 1.4802  Epoch: 15  Global Step: 87460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:30,631-Speed 3898.92 samples/sec  Loss 8.1255  LearningRate 0.0296  ProxyLR: 1.4797  Epoch: 15  Global Step: 87470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:33,257-Speed 3900.31 samples/sec  Loss 8.1120  LearningRate 0.0296  ProxyLR: 1.4791  Epoch: 15  Global Step: 87480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:35,883-Speed 3899.05 samples/sec  Loss 8.0218  LearningRate 0.0296  ProxyLR: 1.4786  Epoch: 15  Global Step: 87490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:38,508-Speed 3901.87 samples/sec  Loss 8.1386  LearningRate 0.0296  ProxyLR: 1.4780  Epoch: 15  Global Step: 87500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:41,135-Speed 3899.58 samples/sec  Loss 8.1993  LearningRate 0.0296  ProxyLR: 1.4775  Epoch: 15  Global Step: 87510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:43,748-Speed 3919.42 samples/sec  Loss 8.0291  LearningRate 0.0295  ProxyLR: 1.4770  Epoch: 15  Global Step: 87520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:46,375-Speed 3899.69 samples/sec  Loss 8.0777  LearningRate 0.0295  ProxyLR: 1.4764  Epoch: 15  Global Step: 87530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:49,001-Speed 3899.62 samples/sec  Loss 7.9932  LearningRate 0.0295  ProxyLR: 1.4759  Epoch: 15  Global Step: 87540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:51,626-Speed 3902.67 samples/sec  Loss 8.0637  LearningRate 0.0295  ProxyLR: 1.4753  Epoch: 15  Global Step: 87550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:54,253-Speed 3898.11 samples/sec  Loss 8.0342  LearningRate 0.0295  ProxyLR: 1.4748  Epoch: 15  Global Step: 87560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:56,879-Speed 3901.38 samples/sec  Loss 7.9507  LearningRate 0.0295  ProxyLR: 1.4743  Epoch: 15  Global Step: 87570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:13:59,503-Speed 3902.22 samples/sec  Loss 7.8558  LearningRate 0.0295  ProxyLR: 1.4737  Epoch: 15  Global Step: 87580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:02,116-Speed 3920.62 samples/sec  Loss 8.0481  LearningRate 0.0295  ProxyLR: 1.4732  Epoch: 15  Global Step: 87590   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:04,743-Speed 3898.72 samples/sec  Loss 7.9317  LearningRate 0.0295  ProxyLR: 1.4726  Epoch: 15  Global Step: 87600   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:07,370-Speed 3898.96 samples/sec  Loss 7.9284  LearningRate 0.0294  ProxyLR: 1.4721  Epoch: 15  Global Step: 87610   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:09,996-Speed 3900.92 samples/sec  Loss 7.7784  LearningRate 0.0294  ProxyLR: 1.4716  Epoch: 15  Global Step: 87620   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:12,926-Speed 3494.96 samples/sec  Loss 7.9297  LearningRate 0.0294  ProxyLR: 1.4710  Epoch: 15  Global Step: 87630   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:15,551-Speed 3902.78 samples/sec  Loss 7.8512  LearningRate 0.0294  ProxyLR: 1.4705  Epoch: 15  Global Step: 87640   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:18,174-Speed 3904.21 samples/sec  Loss 7.7897  LearningRate 0.0294  ProxyLR: 1.4699  Epoch: 15  Global Step: 87650   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:20,799-Speed 3902.41 samples/sec  Loss 7.8962  LearningRate 0.0294  ProxyLR: 1.4694  Epoch: 15  Global Step: 87660   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:23,423-Speed 3902.25 samples/sec  Loss 8.0221  LearningRate 0.0294  ProxyLR: 1.4689  Epoch: 15  Global Step: 87670   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:26,048-Speed 3902.68 samples/sec  Loss 7.8647  LearningRate 0.0294  ProxyLR: 1.4683  Epoch: 15  Global Step: 87680   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:14:28,671-Speed 3904.25 samples/sec  Loss 7.7301  LearningRate 0.0294  ProxyLR: 1.4678  Epoch: 15  Global Step: 87690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:31,296-Speed 3902.49 samples/sec  Loss 7.7636  LearningRate 0.0293  ProxyLR: 1.4672  Epoch: 15  Global Step: 87700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:33,921-Speed 3901.74 samples/sec  Loss 7.5640  LearningRate 0.0293  ProxyLR: 1.4667  Epoch: 15  Global Step: 87710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:36,547-Speed 3900.35 samples/sec  Loss 7.7650  LearningRate 0.0293  ProxyLR: 1.4662  Epoch: 15  Global Step: 87720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:39,171-Speed 3902.86 samples/sec  Loss 7.7557  LearningRate 0.0293  ProxyLR: 1.4656  Epoch: 15  Global Step: 87730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:41,798-Speed 3899.40 samples/sec  Loss 7.8214  LearningRate 0.0293  ProxyLR: 1.4651  Epoch: 15  Global Step: 87740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:44,422-Speed 3903.29 samples/sec  Loss 7.7224  LearningRate 0.0293  ProxyLR: 1.4645  Epoch: 15  Global Step: 87750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:47,047-Speed 3901.87 samples/sec  Loss 7.6960  LearningRate 0.0293  ProxyLR: 1.4640  Epoch: 15  Global Step: 87760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:49,671-Speed 3902.75 samples/sec  Loss 7.7771  LearningRate 0.0293  ProxyLR: 1.4635  Epoch: 15  Global Step: 87770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:52,297-Speed 3900.43 samples/sec  Loss 7.6439  LearningRate 0.0293  ProxyLR: 1.4629  Epoch: 15  Global Step: 87780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:54,908-Speed 3922.83 samples/sec  Loss 7.5846  LearningRate 0.0292  ProxyLR: 1.4624  Epoch: 15  Global Step: 87790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:14:57,536-Speed 3898.01 samples/sec  Loss 7.6578  LearningRate 0.0292  ProxyLR: 1.4619  Epoch: 15  Global Step: 87800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:00,162-Speed 3901.06 samples/sec  Loss 7.7256  LearningRate 0.0292  ProxyLR: 1.4613  Epoch: 15  Global Step: 87810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:02,787-Speed 3901.74 samples/sec  Loss 7.7198  LearningRate 0.0292  ProxyLR: 1.4608  Epoch: 15  Global Step: 87820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:05,412-Speed 3901.56 samples/sec  Loss 7.7035  LearningRate 0.0292  ProxyLR: 1.4602  Epoch: 15  Global Step: 87830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:08,043-Speed 3893.14 samples/sec  Loss 7.6600  LearningRate 0.0292  ProxyLR: 1.4597  Epoch: 15  Global Step: 87840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:10,674-Speed 3892.54 samples/sec  Loss 7.7073  LearningRate 0.0292  ProxyLR: 1.4592  Epoch: 15  Global Step: 87850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:13,295-Speed 3908.09 samples/sec  Loss 7.5004  LearningRate 0.0292  ProxyLR: 1.4586  Epoch: 15  Global Step: 87860   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:15,928-Speed 3890.23 samples/sec  Loss 7.5614  LearningRate 0.0292  ProxyLR: 1.4581  Epoch: 15  Global Step: 87870   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:18,561-Speed 3890.25 samples/sec  Loss 7.4078  LearningRate 0.0292  ProxyLR: 1.4576  Epoch: 15  Global Step: 87880   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:21,193-Speed 3891.57 samples/sec  Loss 7.4206  LearningRate 0.0291  ProxyLR: 1.4570  Epoch: 15  Global Step: 87890   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:23,827-Speed 3888.04 samples/sec  Loss 7.5429  LearningRate 0.0291  ProxyLR: 1.4565  Epoch: 15  Global Step: 87900   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:26,460-Speed 3889.99 samples/sec  Loss 7.5602  LearningRate 0.0291  ProxyLR: 1.4559  Epoch: 15  Global Step: 87910   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:29,091-Speed 3892.65 samples/sec  Loss 7.4131  LearningRate 0.0291  ProxyLR: 1.4554  Epoch: 15  Global Step: 87920   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:31,724-Speed 3890.62 samples/sec  Loss 7.5786  LearningRate 0.0291  ProxyLR: 1.4549  Epoch: 15  Global Step: 87930   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:34,360-Speed 3885.28 samples/sec  Loss 7.5064  LearningRate 0.0291  ProxyLR: 1.4543  Epoch: 15  Global Step: 87940   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:36,996-Speed 3885.58 samples/sec  Loss 7.3114  LearningRate 0.0291  ProxyLR: 1.4538  Epoch: 15  Global Step: 87950   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:15:39,632-Speed 3885.94 samples/sec  Loss 7.3213  LearningRate 0.0291  ProxyLR: 1.4533  Epoch: 15  Global Step: 87960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:42,270-Speed 3882.36 samples/sec  Loss 7.4987  LearningRate 0.0291  ProxyLR: 1.4527  Epoch: 15  Global Step: 87970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:44,906-Speed 3885.83 samples/sec  Loss 7.4185  LearningRate 0.0290  ProxyLR: 1.4522  Epoch: 15  Global Step: 87980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:47,541-Speed 3886.77 samples/sec  Loss 7.4833  LearningRate 0.0290  ProxyLR: 1.4517  Epoch: 15  Global Step: 87990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:50,178-Speed 3883.44 samples/sec  Loss 7.4267  LearningRate 0.0290  ProxyLR: 1.4511  Epoch: 15  Global Step: 88000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:52,814-Speed 3885.19 samples/sec  Loss 7.2361  LearningRate 0.0290  ProxyLR: 1.4506  Epoch: 15  Global Step: 88010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:55,451-Speed 3884.82 samples/sec  Loss 7.3534  LearningRate 0.0290  ProxyLR: 1.4500  Epoch: 15  Global Step: 88020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:15:58,088-Speed 3883.85 samples/sec  Loss 7.2367  LearningRate 0.0290  ProxyLR: 1.4495  Epoch: 15  Global Step: 88030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:00,725-Speed 3884.49 samples/sec  Loss 7.3022  LearningRate 0.0290  ProxyLR: 1.4490  Epoch: 15  Global Step: 88040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:03,361-Speed 3884.66 samples/sec  Loss 7.4571  LearningRate 0.0290  ProxyLR: 1.4484  Epoch: 15  Global Step: 88050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:05,997-Speed 3885.56 samples/sec  Loss 7.3694  LearningRate 0.0290  ProxyLR: 1.4479  Epoch: 15  Global Step: 88060   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:16:08,621-Speed 3904.18 samples/sec  Loss 7.2210  LearningRate 0.0289  ProxyLR: 1.4474  Epoch: 15  Global Step: 88070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:11,256-Speed 3886.92 samples/sec  Loss 7.3059  LearningRate 0.0289  ProxyLR: 1.4468  Epoch: 15  Global Step: 88080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:13,892-Speed 3886.13 samples/sec  Loss 7.1454  LearningRate 0.0289  ProxyLR: 1.4463  Epoch: 15  Global Step: 88090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:16,527-Speed 3887.16 samples/sec  Loss 7.4730  LearningRate 0.0289  ProxyLR: 1.4458  Epoch: 15  Global Step: 88100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:19,163-Speed 3885.31 samples/sec  Loss 7.1473  LearningRate 0.0289  ProxyLR: 1.4452  Epoch: 15  Global Step: 88110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:21,799-Speed 3885.18 samples/sec  Loss 7.3104  LearningRate 0.0289  ProxyLR: 1.4447  Epoch: 15  Global Step: 88120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:24,420-Speed 3907.54 samples/sec  Loss 7.2540  LearningRate 0.0289  ProxyLR: 1.4442  Epoch: 15  Global Step: 88130   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:27,055-Speed 3887.90 samples/sec  Loss 7.1275  LearningRate 0.0289  ProxyLR: 1.4436  Epoch: 15  Global Step: 88140   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:29,690-Speed 3886.89 samples/sec  Loss 7.1454  LearningRate 0.0289  ProxyLR: 1.4431  Epoch: 15  Global Step: 88150   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:32,324-Speed 3887.70 samples/sec  Loss 7.0844  LearningRate 0.0289  ProxyLR: 1.4426  Epoch: 15  Global Step: 88160   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:34,958-Speed 3889.62 samples/sec  Loss 7.2937  LearningRate 0.0288  ProxyLR: 1.4420  Epoch: 15  Global Step: 88170   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:37,595-Speed 3884.23 samples/sec  Loss 7.0296  LearningRate 0.0288  ProxyLR: 1.4415  Epoch: 15  Global Step: 88180   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:40,226-Speed 3892.75 samples/sec  Loss 7.1476  LearningRate 0.0288  ProxyLR: 1.4410  Epoch: 15  Global Step: 88190   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:42,858-Speed 3891.54 samples/sec  Loss 7.0907  LearningRate 0.0288  ProxyLR: 1.4404  Epoch: 15  Global Step: 88200   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:45,489-Speed 3893.12 samples/sec  Loss 7.1020  LearningRate 0.0288  ProxyLR: 1.4399  Epoch: 15  Global Step: 88210   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:48,120-Speed 3892.52 samples/sec  Loss 7.0498  LearningRate 0.0288  ProxyLR: 1.4394  Epoch: 15  Global Step: 88220   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:16:50,751-Speed 3892.98 samples/sec  Loss 7.2354  LearningRate 0.0288  ProxyLR: 1.4388  Epoch: 15  Global Step: 88230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:53,383-Speed 3891.76 samples/sec  Loss 7.1978  LearningRate 0.0288  ProxyLR: 1.4383  Epoch: 15  Global Step: 88240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:56,013-Speed 3894.98 samples/sec  Loss 7.2031  LearningRate 0.0288  ProxyLR: 1.4378  Epoch: 15  Global Step: 88250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:16:58,644-Speed 3892.32 samples/sec  Loss 7.0634  LearningRate 0.0287  ProxyLR: 1.4372  Epoch: 15  Global Step: 88260   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:01,276-Speed 3891.70 samples/sec  Loss 7.1007  LearningRate 0.0287  ProxyLR: 1.4367  Epoch: 15  Global Step: 88270   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:03,909-Speed 3890.54 samples/sec  Loss 6.9334  LearningRate 0.0287  ProxyLR: 1.4362  Epoch: 15  Global Step: 88280   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:06,542-Speed 3889.82 samples/sec  Loss 7.0444  LearningRate 0.0287  ProxyLR: 1.4356  Epoch: 15  Global Step: 88290   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:09,173-Speed 3893.39 samples/sec  Loss 7.1563  LearningRate 0.0287  ProxyLR: 1.4351  Epoch: 15  Global Step: 88300   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:11,790-Speed 3913.15 samples/sec  Loss 6.9988  LearningRate 0.0287  ProxyLR: 1.4346  Epoch: 15  Global Step: 88310   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:14,423-Speed 3890.39 samples/sec  Loss 6.9718  LearningRate 0.0287  ProxyLR: 1.4340  Epoch: 15  Global Step: 88320   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:17,060-Speed 3884.47 samples/sec  Loss 7.0606  LearningRate 0.0287  ProxyLR: 1.4335  Epoch: 15  Global Step: 88330   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:19,696-Speed 3884.99 samples/sec  Loss 6.8584  LearningRate 0.0287  ProxyLR: 1.4330  Epoch: 15  Global Step: 88340   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:22,333-Speed 3885.22 samples/sec  Loss 7.0224  LearningRate 0.0286  ProxyLR: 1.4324  Epoch: 15  Global Step: 88350   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:24,970-Speed 3883.13 samples/sec  Loss 6.9313  LearningRate 0.0286  ProxyLR: 1.4319  Epoch: 15  Global Step: 88360   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:27,605-Speed 3888.39 samples/sec  Loss 6.9806  LearningRate 0.0286  ProxyLR: 1.4314  Epoch: 15  Global Step: 88370   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:30,243-Speed 3881.66 samples/sec  Loss 6.8764  LearningRate 0.0286  ProxyLR: 1.4308  Epoch: 15  Global Step: 88380   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:32,881-Speed 3882.97 samples/sec  Loss 7.0446  LearningRate 0.0286  ProxyLR: 1.4303  Epoch: 15  Global Step: 88390   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:35,515-Speed 3889.15 samples/sec  Loss 6.7810  LearningRate 0.0286  ProxyLR: 1.4298  Epoch: 15  Global Step: 88400   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:17:38,150-Speed 3886.43 samples/sec  Loss 6.9875  LearningRate 0.0286  ProxyLR: 1.4292  Epoch: 15  Global Step: 88410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:40,784-Speed 3887.82 samples/sec  Loss 6.8668  LearningRate 0.0286  ProxyLR: 1.4287  Epoch: 15  Global Step: 88420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:43,418-Speed 3888.89 samples/sec  Loss 6.9157  LearningRate 0.0286  ProxyLR: 1.4282  Epoch: 15  Global Step: 88430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:46,051-Speed 3890.04 samples/sec  Loss 6.6725  LearningRate 0.0286  ProxyLR: 1.4276  Epoch: 15  Global Step: 88440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:48,686-Speed 3887.74 samples/sec  Loss 6.9390  LearningRate 0.0285  ProxyLR: 1.4271  Epoch: 15  Global Step: 88450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:51,319-Speed 3889.23 samples/sec  Loss 6.8608  LearningRate 0.0285  ProxyLR: 1.4266  Epoch: 15  Global Step: 88460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:53,953-Speed 3889.37 samples/sec  Loss 6.9209  LearningRate 0.0285  ProxyLR: 1.4260  Epoch: 15  Global Step: 88470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:56,587-Speed 3887.69 samples/sec  Loss 6.9179  LearningRate 0.0285  ProxyLR: 1.4255  Epoch: 15  Global Step: 88480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:17:59,221-Speed 3889.13 samples/sec  Loss 6.8209  LearningRate 0.0285  ProxyLR: 1.4250  Epoch: 15  Global Step: 88490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:01,856-Speed 3886.29 samples/sec  Loss 6.8773  LearningRate 0.0285  ProxyLR: 1.4244  Epoch: 15  Global Step: 88500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:04,478-Speed 3907.46 samples/sec  Loss 6.8344  LearningRate 0.0285  ProxyLR: 1.4239  Epoch: 15  Global Step: 88510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:07,111-Speed 3889.01 samples/sec  Loss 6.8818  LearningRate 0.0285  ProxyLR: 1.4234  Epoch: 15  Global Step: 88520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:09,745-Speed 3888.64 samples/sec  Loss 6.8828  LearningRate 0.0285  ProxyLR: 1.4229  Epoch: 15  Global Step: 88530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:12,381-Speed 3886.01 samples/sec  Loss 6.7068  LearningRate 0.0284  ProxyLR: 1.4223  Epoch: 15  Global Step: 88540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:15,015-Speed 3887.84 samples/sec  Loss 6.6835  LearningRate 0.0284  ProxyLR: 1.4218  Epoch: 15  Global Step: 88550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:17,652-Speed 3884.92 samples/sec  Loss 6.7643  LearningRate 0.0284  ProxyLR: 1.4213  Epoch: 15  Global Step: 88560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:20,286-Speed 3887.86 samples/sec  Loss 6.6963  LearningRate 0.0284  ProxyLR: 1.4207  Epoch: 15  Global Step: 88570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:22,920-Speed 3888.10 samples/sec  Loss 6.8119  LearningRate 0.0284  ProxyLR: 1.4202  Epoch: 15  Global Step: 88580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:25,555-Speed 3887.76 samples/sec  Loss 6.7403  LearningRate 0.0284  ProxyLR: 1.4197  Epoch: 15  Global Step: 88590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:28,188-Speed 3889.68 samples/sec  Loss 6.6806  LearningRate 0.0284  ProxyLR: 1.4191  Epoch: 15  Global Step: 88600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:30,807-Speed 3910.82 samples/sec  Loss 6.9861  LearningRate 0.0284  ProxyLR: 1.4186  Epoch: 15  Global Step: 88610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:33,441-Speed 3888.66 samples/sec  Loss 6.6525  LearningRate 0.0284  ProxyLR: 1.4181  Epoch: 15  Global Step: 88620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:36,074-Speed 3890.97 samples/sec  Loss 6.6189  LearningRate 0.0284  ProxyLR: 1.4176  Epoch: 15  Global Step: 88630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:38,706-Speed 3891.41 samples/sec  Loss 6.7431  LearningRate 0.0283  ProxyLR: 1.4170  Epoch: 15  Global Step: 88640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:18:41,324-Speed 3911.88 samples/sec  Loss 6.6255  LearningRate 0.0283  ProxyLR: 1.4165  Epoch: 15  Global Step: 88650   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:43,957-Speed 3889.68 samples/sec  Loss 6.7304  LearningRate 0.0283  ProxyLR: 1.4160  Epoch: 15  Global Step: 88660   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:46,589-Speed 3891.54 samples/sec  Loss 6.5418  LearningRate 0.0283  ProxyLR: 1.4154  Epoch: 15  Global Step: 88670   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:49,222-Speed 3890.64 samples/sec  Loss 6.8210  LearningRate 0.0283  ProxyLR: 1.4149  Epoch: 15  Global Step: 88680   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:51,853-Speed 3893.35 samples/sec  Loss 6.6268  LearningRate 0.0283  ProxyLR: 1.4144  Epoch: 15  Global Step: 88690   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:54,485-Speed 3890.14 samples/sec  Loss 6.7050  LearningRate 0.0283  ProxyLR: 1.4138  Epoch: 15  Global Step: 88700   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:57,118-Speed 3891.51 samples/sec  Loss 6.4560  LearningRate 0.0283  ProxyLR: 1.4133  Epoch: 15  Global Step: 88710   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:18:59,751-Speed 3889.64 samples/sec  Loss 6.5944  LearningRate 0.0283  ProxyLR: 1.4128  Epoch: 15  Global Step: 88720   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:19:02,382-Speed 3892.33 samples/sec  Loss 6.5729  LearningRate 0.0282  ProxyLR: 1.4123  Epoch: 15  Global Step: 88730   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:19:05,015-Speed 3890.42 samples/sec  Loss 6.5326  LearningRate 0.0282  ProxyLR: 1.4117  Epoch: 15  Global Step: 88740   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:19:07,648-Speed 3890.28 samples/sec  Loss 6.5795  LearningRate 0.0282  ProxyLR: 1.4112  Epoch: 15  Global Step: 88750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:10,279-Speed 3891.86 samples/sec  Loss 6.6462  LearningRate 0.0282  ProxyLR: 1.4107  Epoch: 15  Global Step: 88760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:12,913-Speed 3889.00 samples/sec  Loss 6.6161  LearningRate 0.0282  ProxyLR: 1.4101  Epoch: 15  Global Step: 88770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:15,548-Speed 3887.14 samples/sec  Loss 6.5753  LearningRate 0.0282  ProxyLR: 1.4096  Epoch: 15  Global Step: 88780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:18,182-Speed 3887.88 samples/sec  Loss 6.6064  LearningRate 0.0282  ProxyLR: 1.4091  Epoch: 15  Global Step: 88790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:20,817-Speed 3887.00 samples/sec  Loss 6.5814  LearningRate 0.0282  ProxyLR: 1.4086  Epoch: 15  Global Step: 88800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:23,452-Speed 3888.04 samples/sec  Loss 6.4467  LearningRate 0.0282  ProxyLR: 1.4080  Epoch: 15  Global Step: 88810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:26,085-Speed 3890.05 samples/sec  Loss 6.6451  LearningRate 0.0282  ProxyLR: 1.4075  Epoch: 15  Global Step: 88820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:28,718-Speed 3890.19 samples/sec  Loss 6.5962  LearningRate 0.0281  ProxyLR: 1.4070  Epoch: 15  Global Step: 88830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:31,358-Speed 3879.47 samples/sec  Loss 6.4385  LearningRate 0.0281  ProxyLR: 1.4064  Epoch: 15  Global Step: 88840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:33,982-Speed 3903.23 samples/sec  Loss 6.4774  LearningRate 0.0281  ProxyLR: 1.4059  Epoch: 15  Global Step: 88850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:36,616-Speed 3888.51 samples/sec  Loss 6.5988  LearningRate 0.0281  ProxyLR: 1.4054  Epoch: 15  Global Step: 88860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:39,251-Speed 3886.93 samples/sec  Loss 6.5413  LearningRate 0.0281  ProxyLR: 1.4049  Epoch: 15  Global Step: 88870   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:41,888-Speed 3884.76 samples/sec  Loss 6.5885  LearningRate 0.0281  ProxyLR: 1.4043  Epoch: 15  Global Step: 88880   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:44,524-Speed 3884.41 samples/sec  Loss 6.3958  LearningRate 0.0281  ProxyLR: 1.4038  Epoch: 15  Global Step: 88890   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:47,161-Speed 3884.84 samples/sec  Loss 6.3958  LearningRate 0.0281  ProxyLR: 1.4033  Epoch: 15  Global Step: 88900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:49,795-Speed 3888.87 samples/sec  Loss 6.5116  LearningRate 0.0281  ProxyLR: 1.4028  Epoch: 15  Global Step: 88910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:52,431-Speed 3885.71 samples/sec  Loss 6.3190  LearningRate 0.0280  ProxyLR: 1.4022  Epoch: 15  Global Step: 88920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:55,056-Speed 3900.89 samples/sec  Loss 6.4289  LearningRate 0.0280  ProxyLR: 1.4017  Epoch: 15  Global Step: 88930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:19:57,682-Speed 3900.77 samples/sec  Loss 6.4719  LearningRate 0.0280  ProxyLR: 1.4012  Epoch: 15  Global Step: 88940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:00,296-Speed 3917.99 samples/sec  Loss 6.3459  LearningRate 0.0280  ProxyLR: 1.4006  Epoch: 15  Global Step: 88950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:02,923-Speed 3899.63 samples/sec  Loss 6.5945  LearningRate 0.0280  ProxyLR: 1.4001  Epoch: 15  Global Step: 88960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:05,549-Speed 3899.88 samples/sec  Loss 6.3843  LearningRate 0.0280  ProxyLR: 1.3996  Epoch: 15  Global Step: 88970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:08,175-Speed 3900.51 samples/sec  Loss 6.3663  LearningRate 0.0280  ProxyLR: 1.3991  Epoch: 15  Global Step: 88980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:10,801-Speed 3901.48 samples/sec  Loss 6.3432  LearningRate 0.0280  ProxyLR: 1.3985  Epoch: 15  Global Step: 88990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:13,425-Speed 3901.93 samples/sec  Loss 6.3678  LearningRate 0.0280  ProxyLR: 1.3980  Epoch: 15  Global Step: 89000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:16,051-Speed 3901.23 samples/sec  Loss 6.3134  LearningRate 0.0279  ProxyLR: 1.3975  Epoch: 15  Global Step: 89010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:18,678-Speed 3899.15 samples/sec  Loss 6.4462  LearningRate 0.0279  ProxyLR: 1.3970  Epoch: 15  Global Step: 89020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:21,302-Speed 3902.81 samples/sec  Loss 6.3604  LearningRate 0.0279  ProxyLR: 1.3964  Epoch: 15  Global Step: 89030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:23,927-Speed 3901.15 samples/sec  Loss 6.1904  LearningRate 0.0279  ProxyLR: 1.3959  Epoch: 15  Global Step: 89040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:26,538-Speed 3923.00 samples/sec  Loss 6.3559  LearningRate 0.0279  ProxyLR: 1.3954  Epoch: 15  Global Step: 89050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:29,163-Speed 3902.74 samples/sec  Loss 6.3445  LearningRate 0.0279  ProxyLR: 1.3949  Epoch: 15  Global Step: 89060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:31,788-Speed 3901.63 samples/sec  Loss 6.2144  LearningRate 0.0279  ProxyLR: 1.3943  Epoch: 15  Global Step: 89070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:34,413-Speed 3901.85 samples/sec  Loss 6.2993  LearningRate 0.0279  ProxyLR: 1.3938  Epoch: 15  Global Step: 89080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:37,038-Speed 3902.20 samples/sec  Loss 6.4264  LearningRate 0.0279  ProxyLR: 1.3933  Epoch: 15  Global Step: 89090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:39,664-Speed 3900.78 samples/sec  Loss 6.2446  LearningRate 0.0279  ProxyLR: 1.3928  Epoch: 15  Global Step: 89100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:42,290-Speed 3899.77 samples/sec  Loss 6.4136  LearningRate 0.0278  ProxyLR: 1.3922  Epoch: 15  Global Step: 89110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:44,920-Speed 3893.88 samples/sec  Loss 6.2394  LearningRate 0.0278  ProxyLR: 1.3917  Epoch: 15  Global Step: 89120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:47,552-Speed 3892.77 samples/sec  Loss 6.2763  LearningRate 0.0278  ProxyLR: 1.3912  Epoch: 15  Global Step: 89130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:50,180-Speed 3897.31 samples/sec  Loss 6.2774  LearningRate 0.0278  ProxyLR: 1.3907  Epoch: 15  Global Step: 89140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:52,805-Speed 3900.67 samples/sec  Loss 6.3181  LearningRate 0.0278  ProxyLR: 1.3901  Epoch: 15  Global Step: 89150   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:20:55,421-Speed 3916.58 samples/sec  Loss 6.2805  LearningRate 0.0278  ProxyLR: 1.3896  Epoch: 15  Global Step: 89160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:20:58,047-Speed 3900.43 samples/sec  Loss 6.3036  LearningRate 0.0278  ProxyLR: 1.3891  Epoch: 15  Global Step: 89170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:00,671-Speed 3902.03 samples/sec  Loss 6.1486  LearningRate 0.0278  ProxyLR: 1.3886  Epoch: 15  Global Step: 89180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:03,297-Speed 3901.66 samples/sec  Loss 6.2431  LearningRate 0.0278  ProxyLR: 1.3880  Epoch: 15  Global Step: 89190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:05,922-Speed 3901.84 samples/sec  Loss 6.2350  LearningRate 0.0278  ProxyLR: 1.3875  Epoch: 15  Global Step: 89200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:08,547-Speed 3901.28 samples/sec  Loss 6.1801  LearningRate 0.0277  ProxyLR: 1.3870  Epoch: 15  Global Step: 89210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:11,172-Speed 3901.14 samples/sec  Loss 6.1840  LearningRate 0.0277  ProxyLR: 1.3865  Epoch: 15  Global Step: 89220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:13,784-Speed 3921.92 samples/sec  Loss 6.2965  LearningRate 0.0277  ProxyLR: 1.3859  Epoch: 15  Global Step: 89230   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:16,409-Speed 3902.30 samples/sec  Loss 6.1033  LearningRate 0.0277  ProxyLR: 1.3854  Epoch: 15  Global Step: 89240   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:19,036-Speed 3898.73 samples/sec  Loss 6.2072  LearningRate 0.0277  ProxyLR: 1.3849  Epoch: 15  Global Step: 89250   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:21,666-Speed 3894.68 samples/sec  Loss 6.2736  LearningRate 0.0277  ProxyLR: 1.3844  Epoch: 15  Global Step: 89260   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:24,299-Speed 3889.71 samples/sec  Loss 6.2602  LearningRate 0.0277  ProxyLR: 1.3839  Epoch: 15  Global Step: 89270   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:26,934-Speed 3885.94 samples/sec  Loss 6.1650  LearningRate 0.0277  ProxyLR: 1.3833  Epoch: 15  Global Step: 89280   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:29,571-Speed 3885.24 samples/sec  Loss 6.1859  LearningRate 0.0277  ProxyLR: 1.3828  Epoch: 15  Global Step: 89290   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:32,207-Speed 3885.64 samples/sec  Loss 6.0724  LearningRate 0.0276  ProxyLR: 1.3823  Epoch: 15  Global Step: 89300   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:34,841-Speed 3888.30 samples/sec  Loss 6.0921  LearningRate 0.0276  ProxyLR: 1.3818  Epoch: 15  Global Step: 89310   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:37,475-Speed 3888.95 samples/sec  Loss 6.1082  LearningRate 0.0276  ProxyLR: 1.3812  Epoch: 15  Global Step: 89320   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:21:40,111-Speed 3885.76 samples/sec  Loss 6.1977  LearningRate 0.0276  ProxyLR: 1.3807  Epoch: 15  Global Step: 89330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:42,747-Speed 3884.50 samples/sec  Loss 6.1628  LearningRate 0.0276  ProxyLR: 1.3802  Epoch: 15  Global Step: 89340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:45,383-Speed 3886.46 samples/sec  Loss 6.0665  LearningRate 0.0276  ProxyLR: 1.3797  Epoch: 15  Global Step: 89350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:48,019-Speed 3885.91 samples/sec  Loss 6.0386  LearningRate 0.0276  ProxyLR: 1.3791  Epoch: 15  Global Step: 89360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:50,653-Speed 3888.41 samples/sec  Loss 6.0722  LearningRate 0.0276  ProxyLR: 1.3786  Epoch: 15  Global Step: 89370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:53,289-Speed 3884.84 samples/sec  Loss 6.1492  LearningRate 0.0276  ProxyLR: 1.3781  Epoch: 15  Global Step: 89380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:55,924-Speed 3887.13 samples/sec  Loss 6.1121  LearningRate 0.0276  ProxyLR: 1.3776  Epoch: 15  Global Step: 89390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:21:58,560-Speed 3885.75 samples/sec  Loss 5.9795  LearningRate 0.0275  ProxyLR: 1.3771  Epoch: 15  Global Step: 89400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:01,197-Speed 3883.71 samples/sec  Loss 6.1836  LearningRate 0.0275  ProxyLR: 1.3765  Epoch: 15  Global Step: 89410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:03,833-Speed 3885.94 samples/sec  Loss 5.9827  LearningRate 0.0275  ProxyLR: 1.3760  Epoch: 15  Global Step: 89420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:06,453-Speed 3909.66 samples/sec  Loss 6.0107  LearningRate 0.0275  ProxyLR: 1.3755  Epoch: 15  Global Step: 89430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:09,071-Speed 3911.22 samples/sec  Loss 6.2119  LearningRate 0.0275  ProxyLR: 1.3750  Epoch: 15  Global Step: 89440   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:11,704-Speed 3891.08 samples/sec  Loss 6.0226  LearningRate 0.0275  ProxyLR: 1.3744  Epoch: 15  Global Step: 89450   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:14,336-Speed 3891.32 samples/sec  Loss 5.9935  LearningRate 0.0275  ProxyLR: 1.3739  Epoch: 15  Global Step: 89460   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:16,968-Speed 3890.77 samples/sec  Loss 5.9860  LearningRate 0.0275  ProxyLR: 1.3734  Epoch: 15  Global Step: 89470   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:19,600-Speed 3891.99 samples/sec  Loss 6.0474  LearningRate 0.0275  ProxyLR: 1.3729  Epoch: 15  Global Step: 89480   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:22,230-Speed 3894.85 samples/sec  Loss 6.0175  LearningRate 0.0274  ProxyLR: 1.3724  Epoch: 15  Global Step: 89490   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:24,860-Speed 3893.94 samples/sec  Loss 6.0643  LearningRate 0.0274  ProxyLR: 1.3718  Epoch: 15  Global Step: 89500   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:27,490-Speed 3894.32 samples/sec  Loss 5.8878  LearningRate 0.0274  ProxyLR: 1.3713  Epoch: 15  Global Step: 89510   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:30,121-Speed 3894.19 samples/sec  Loss 6.1350  LearningRate 0.0274  ProxyLR: 1.3708  Epoch: 15  Global Step: 89520   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:32,751-Speed 3894.16 samples/sec  Loss 5.9163  LearningRate 0.0274  ProxyLR: 1.3703  Epoch: 15  Global Step: 89530   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:22:35,380-Speed 3895.45 samples/sec  Loss 6.0589  LearningRate 0.0274  ProxyLR: 1.3698  Epoch: 15  Global Step: 89540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:38,010-Speed 3893.89 samples/sec  Loss 5.9660  LearningRate 0.0274  ProxyLR: 1.3692  Epoch: 15  Global Step: 89550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:40,643-Speed 3890.85 samples/sec  Loss 5.8773  LearningRate 0.0274  ProxyLR: 1.3687  Epoch: 15  Global Step: 89560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:43,275-Speed 3891.72 samples/sec  Loss 5.9302  LearningRate 0.0274  ProxyLR: 1.3682  Epoch: 15  Global Step: 89570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:45,906-Speed 3892.48 samples/sec  Loss 5.9994  LearningRate 0.0274  ProxyLR: 1.3677  Epoch: 15  Global Step: 89580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:48,538-Speed 3891.65 samples/sec  Loss 5.8271  LearningRate 0.0273  ProxyLR: 1.3672  Epoch: 15  Global Step: 89590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:51,172-Speed 3888.15 samples/sec  Loss 5.9476  LearningRate 0.0273  ProxyLR: 1.3666  Epoch: 15  Global Step: 89600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:53,803-Speed 3892.88 samples/sec  Loss 5.9715  LearningRate 0.0273  ProxyLR: 1.3661  Epoch: 15  Global Step: 89610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:56,434-Speed 3892.72 samples/sec  Loss 5.7615  LearningRate 0.0273  ProxyLR: 1.3656  Epoch: 15  Global Step: 89620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:22:59,065-Speed 3893.42 samples/sec  Loss 5.8415  LearningRate 0.0273  ProxyLR: 1.3651  Epoch: 15  Global Step: 89630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:01,697-Speed 3892.23 samples/sec  Loss 5.8279  LearningRate 0.0273  ProxyLR: 1.3646  Epoch: 15  Global Step: 89640   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:23:04,314-Speed 3913.58 samples/sec  Loss 6.0802  LearningRate 0.0273  ProxyLR: 1.3640  Epoch: 15  Global Step: 89650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:06,944-Speed 3894.01 samples/sec  Loss 5.9446  LearningRate 0.0273  ProxyLR: 1.3635  Epoch: 15  Global Step: 89660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:09,574-Speed 3894.92 samples/sec  Loss 5.9418  LearningRate 0.0273  ProxyLR: 1.3630  Epoch: 15  Global Step: 89670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:12,205-Speed 3892.16 samples/sec  Loss 5.8830  LearningRate 0.0272  ProxyLR: 1.3625  Epoch: 15  Global Step: 89680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:14,838-Speed 3890.98 samples/sec  Loss 5.8552  LearningRate 0.0272  ProxyLR: 1.3620  Epoch: 15  Global Step: 89690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:17,468-Speed 3893.52 samples/sec  Loss 5.8425  LearningRate 0.0272  ProxyLR: 1.3614  Epoch: 15  Global Step: 89700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:20,100-Speed 3892.22 samples/sec  Loss 5.8359  LearningRate 0.0272  ProxyLR: 1.3609  Epoch: 15  Global Step: 89710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:22,731-Speed 3893.42 samples/sec  Loss 5.8344  LearningRate 0.0272  ProxyLR: 1.3604  Epoch: 15  Global Step: 89720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:25,360-Speed 3895.07 samples/sec  Loss 5.8299  LearningRate 0.0272  ProxyLR: 1.3599  Epoch: 15  Global Step: 89730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:27,991-Speed 3893.88 samples/sec  Loss 5.9499  LearningRate 0.0272  ProxyLR: 1.3594  Epoch: 15  Global Step: 89740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:30,623-Speed 3890.67 samples/sec  Loss 5.6673  LearningRate 0.0272  ProxyLR: 1.3588  Epoch: 15  Global Step: 89750   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:23:33,241-Speed 3911.94 samples/sec  Loss 5.9477  LearningRate 0.0272  ProxyLR: 1.3583  Epoch: 15  Global Step: 89760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:35,872-Speed 3893.21 samples/sec  Loss 5.9017  LearningRate 0.0272  ProxyLR: 1.3578  Epoch: 15  Global Step: 89770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:38,502-Speed 3894.50 samples/sec  Loss 5.7324  LearningRate 0.0271  ProxyLR: 1.3573  Epoch: 15  Global Step: 89780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:41,133-Speed 3893.14 samples/sec  Loss 5.7974  LearningRate 0.0271  ProxyLR: 1.3568  Epoch: 15  Global Step: 89790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:43,764-Speed 3893.47 samples/sec  Loss 5.7649  LearningRate 0.0271  ProxyLR: 1.3562  Epoch: 15  Global Step: 89800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:46,394-Speed 3894.00 samples/sec  Loss 5.7239  LearningRate 0.0271  ProxyLR: 1.3557  Epoch: 15  Global Step: 89810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:49,025-Speed 3893.44 samples/sec  Loss 5.7333  LearningRate 0.0271  ProxyLR: 1.3552  Epoch: 15  Global Step: 89820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:51,656-Speed 3892.80 samples/sec  Loss 5.6966  LearningRate 0.0271  ProxyLR: 1.3547  Epoch: 15  Global Step: 89830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:54,287-Speed 3892.80 samples/sec  Loss 5.8946  LearningRate 0.0271  ProxyLR: 1.3542  Epoch: 15  Global Step: 89840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:56,919-Speed 3891.58 samples/sec  Loss 5.7748  LearningRate 0.0271  ProxyLR: 1.3537  Epoch: 15  Global Step: 89850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:23:59,549-Speed 3893.91 samples/sec  Loss 5.9086  LearningRate 0.0271  ProxyLR: 1.3531  Epoch: 15  Global Step: 89860   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:24:02,166-Speed 3914.45 samples/sec  Loss 5.7005  LearningRate 0.0271  ProxyLR: 1.3526  Epoch: 15  Global Step: 89870   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:24:04,798-Speed 3891.35 samples/sec  Loss 5.7501  LearningRate 0.0270  ProxyLR: 1.3521  Epoch: 15  Global Step: 89880   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:24:07,430-Speed 3891.72 samples/sec  Loss 5.7201  LearningRate 0.0270  ProxyLR: 1.3516  Epoch: 15  Global Step: 89890   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:24:10,061-Speed 3892.08 samples/sec  Loss 5.7318  LearningRate 0.0270  ProxyLR: 1.3511  Epoch: 15  Global Step: 89900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:24:12,678-Speed 3913.80 samples/sec  Loss 5.7160  LearningRate 0.0270  ProxyLR: 1.3506  Epoch: 15  Global Step: 89910   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:15,310-Speed 3892.29 samples/sec  Loss 5.7447  LearningRate 0.0270  ProxyLR: 1.3500  Epoch: 15  Global Step: 89920   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:17,941-Speed 3893.38 samples/sec  Loss 5.7557  LearningRate 0.0270  ProxyLR: 1.3495  Epoch: 15  Global Step: 89930   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:20,573-Speed 3891.41 samples/sec  Loss 5.6233  LearningRate 0.0270  ProxyLR: 1.3490  Epoch: 15  Global Step: 89940   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:23,205-Speed 3891.76 samples/sec  Loss 5.7892  LearningRate 0.0270  ProxyLR: 1.3485  Epoch: 15  Global Step: 89950   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:25,836-Speed 3892.42 samples/sec  Loss 5.7857  LearningRate 0.0270  ProxyLR: 1.3480  Epoch: 15  Global Step: 89960   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:28,468-Speed 3891.25 samples/sec  Loss 5.6766  LearningRate 0.0269  ProxyLR: 1.3475  Epoch: 15  Global Step: 89970   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:31,101-Speed 3890.19 samples/sec  Loss 5.5872  LearningRate 0.0269  ProxyLR: 1.3469  Epoch: 15  Global Step: 89980   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:33,733-Speed 3891.12 samples/sec  Loss 5.7850  LearningRate 0.0269  ProxyLR: 1.3464  Epoch: 15  Global Step: 89990   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:24:36,366-Speed 3890.50 samples/sec  Loss 5.6183  LearningRate 0.0269  ProxyLR: 1.3459  Epoch: 15  Global Step: 90000   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:25:25,899-[lfw][90000]XNorm: 23.168911
Training: 2023-05-04 21:25:25,899-[lfw][90000]Accuracy-Flip: 0.99750+-0.00281
Training: 2023-05-04 21:25:25,899-[lfw][90000]Accuracy-Highest: 0.99750
Training: 2023-05-04 21:25:25,900-[lfw][90000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-04 21:25:25,900-[lfw][90000]Highest TPR@FPR: 0.99600
Training: 2023-05-04 21:26:22,953-[cfp_fp][90000]XNorm: 22.614519
Training: 2023-05-04 21:26:22,953-[cfp_fp][90000]Accuracy-Flip: 0.97971+-0.00731
Training: 2023-05-04 21:26:22,954-[cfp_fp][90000]Accuracy-Highest: 0.97971
Training: 2023-05-04 21:26:22,954-[cfp_fp][90000]TPR@1stNon-Zero-FPR of 0.00029: 0.87086
Training: 2023-05-04 21:26:22,954-[cfp_fp][90000]Highest TPR@FPR: 0.87086
Training: 2023-05-04 21:27:12,593-[agedb_30][90000]XNorm: 23.226563
Training: 2023-05-04 21:27:12,594-[agedb_30][90000]Accuracy-Flip: 0.96750+-0.00914
Training: 2023-05-04 21:27:12,594-[agedb_30][90000]Accuracy-Highest: 0.97067
Training: 2023-05-04 21:27:12,594-[agedb_30][90000]TPR@1stNon-Zero-FPR of 0.00033: 0.79067
Training: 2023-05-04 21:27:12,594-[agedb_30][90000]Highest TPR@FPR: 0.79067
Training: 2023-05-04 21:28:03,532-[calfw][90000]XNorm: 23.214048
Training: 2023-05-04 21:28:03,533-[calfw][90000]Accuracy-Flip: 0.95667+-0.01263
Training: 2023-05-04 21:28:03,533-[calfw][90000]Accuracy-Highest: 0.95817
Training: 2023-05-04 21:28:03,533-[calfw][90000]TPR@1stNon-Zero-FPR of 0.00033: 0.80433
Training: 2023-05-04 21:28:03,533-[calfw][90000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 21:28:54,506-[cplfw][90000]XNorm: 22.314860
Training: 2023-05-04 21:28:54,506-[cplfw][90000]Accuracy-Flip: 0.92683+-0.01555
Training: 2023-05-04 21:28:54,507-[cplfw][90000]Accuracy-Highest: 0.92683
Training: 2023-05-04 21:28:54,507-[cplfw][90000]TPR@1stNon-Zero-FPR of 0.00033: 0.00433
Training: 2023-05-04 21:28:54,507-[cplfw][90000]Highest TPR@FPR: 0.01533
Training: 2023-05-04 21:28:57,147-Speed 39.27 samples/sec  Loss 5.8482  LearningRate 0.0269  ProxyLR: 1.3454  Epoch: 15  Global Step: 90010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:28:59,771-Speed 3903.24 samples/sec  Loss 5.7719  LearningRate 0.0269  ProxyLR: 1.3449  Epoch: 15  Global Step: 90020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:02,395-Speed 3903.73 samples/sec  Loss 5.6502  LearningRate 0.0269  ProxyLR: 1.3444  Epoch: 15  Global Step: 90030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:05,016-Speed 3907.89 samples/sec  Loss 5.5317  LearningRate 0.0269  ProxyLR: 1.3438  Epoch: 15  Global Step: 90040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:07,640-Speed 3903.17 samples/sec  Loss 5.6206  LearningRate 0.0269  ProxyLR: 1.3433  Epoch: 15  Global Step: 90050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:10,258-Speed 3912.05 samples/sec  Loss 5.6734  LearningRate 0.0269  ProxyLR: 1.3428  Epoch: 15  Global Step: 90060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:12,874-Speed 3914.91 samples/sec  Loss 5.8315  LearningRate 0.0268  ProxyLR: 1.3423  Epoch: 15  Global Step: 90070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:15,493-Speed 3911.45 samples/sec  Loss 5.6025  LearningRate 0.0268  ProxyLR: 1.3418  Epoch: 15  Global Step: 90080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:18,114-Speed 3907.85 samples/sec  Loss 5.6817  LearningRate 0.0268  ProxyLR: 1.3413  Epoch: 15  Global Step: 90090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:20,736-Speed 3906.33 samples/sec  Loss 5.6335  LearningRate 0.0268  ProxyLR: 1.3407  Epoch: 15  Global Step: 90100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:23,345-Speed 3926.30 samples/sec  Loss 5.6837  LearningRate 0.0268  ProxyLR: 1.3402  Epoch: 15  Global Step: 90110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:25,965-Speed 3908.20 samples/sec  Loss 5.5833  LearningRate 0.0268  ProxyLR: 1.3397  Epoch: 15  Global Step: 90120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:28,587-Speed 3907.70 samples/sec  Loss 5.6383  LearningRate 0.0268  ProxyLR: 1.3392  Epoch: 15  Global Step: 90130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:31,208-Speed 3907.74 samples/sec  Loss 5.5652  LearningRate 0.0268  ProxyLR: 1.3387  Epoch: 15  Global Step: 90140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:33,829-Speed 3906.76 samples/sec  Loss 5.6435  LearningRate 0.0268  ProxyLR: 1.3382  Epoch: 15  Global Step: 90150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:36,451-Speed 3907.32 samples/sec  Loss 5.4983  LearningRate 0.0268  ProxyLR: 1.3377  Epoch: 15  Global Step: 90160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:39,073-Speed 3905.85 samples/sec  Loss 5.6879  LearningRate 0.0267  ProxyLR: 1.3371  Epoch: 15  Global Step: 90170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:41,696-Speed 3904.87 samples/sec  Loss 5.5108  LearningRate 0.0267  ProxyLR: 1.3366  Epoch: 15  Global Step: 90180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:44,320-Speed 3904.18 samples/sec  Loss 5.5544  LearningRate 0.0267  ProxyLR: 1.3361  Epoch: 15  Global Step: 90190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:46,944-Speed 3903.14 samples/sec  Loss 5.4410  LearningRate 0.0267  ProxyLR: 1.3356  Epoch: 15  Global Step: 90200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:49,553-Speed 3924.59 samples/sec  Loss 5.5740  LearningRate 0.0267  ProxyLR: 1.3351  Epoch: 15  Global Step: 90210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:52,177-Speed 3903.74 samples/sec  Loss 5.4959  LearningRate 0.0267  ProxyLR: 1.3346  Epoch: 15  Global Step: 90220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:54,807-Speed 3894.87 samples/sec  Loss 5.5840  LearningRate 0.0267  ProxyLR: 1.3341  Epoch: 15  Global Step: 90230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:29:57,435-Speed 3897.35 samples/sec  Loss 5.6197  LearningRate 0.0267  ProxyLR: 1.3335  Epoch: 15  Global Step: 90240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:00,060-Speed 3902.31 samples/sec  Loss 5.4360  LearningRate 0.0267  ProxyLR: 1.3330  Epoch: 15  Global Step: 90250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:02,670-Speed 3924.69 samples/sec  Loss 5.4791  LearningRate 0.0267  ProxyLR: 1.3325  Epoch: 15  Global Step: 90260   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:05,294-Speed 3903.29 samples/sec  Loss 5.6225  LearningRate 0.0266  ProxyLR: 1.3320  Epoch: 15  Global Step: 90270   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:07,920-Speed 3900.53 samples/sec  Loss 5.6458  LearningRate 0.0266  ProxyLR: 1.3315  Epoch: 15  Global Step: 90280   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:10,545-Speed 3902.47 samples/sec  Loss 5.4284  LearningRate 0.0266  ProxyLR: 1.3310  Epoch: 15  Global Step: 90290   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:13,169-Speed 3903.34 samples/sec  Loss 5.5566  LearningRate 0.0266  ProxyLR: 1.3305  Epoch: 15  Global Step: 90300   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:15,793-Speed 3902.81 samples/sec  Loss 5.5312  LearningRate 0.0266  ProxyLR: 1.3300  Epoch: 15  Global Step: 90310   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:18,417-Speed 3902.94 samples/sec  Loss 5.5589  LearningRate 0.0266  ProxyLR: 1.3294  Epoch: 15  Global Step: 90320   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:21,044-Speed 3899.86 samples/sec  Loss 5.5041  LearningRate 0.0266  ProxyLR: 1.3289  Epoch: 15  Global Step: 90330   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:23,677-Speed 3888.85 samples/sec  Loss 5.5526  LearningRate 0.0266  ProxyLR: 1.3284  Epoch: 15  Global Step: 90340   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:26,311-Speed 3889.30 samples/sec  Loss 5.5823  LearningRate 0.0266  ProxyLR: 1.3279  Epoch: 15  Global Step: 90350   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:30:28,942-Speed 3892.75 samples/sec  Loss 5.4765  LearningRate 0.0265  ProxyLR: 1.3274  Epoch: 15  Global Step: 90360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:31,573-Speed 3894.07 samples/sec  Loss 5.5242  LearningRate 0.0265  ProxyLR: 1.3269  Epoch: 15  Global Step: 90370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:34,203-Speed 3893.87 samples/sec  Loss 5.5204  LearningRate 0.0265  ProxyLR: 1.3264  Epoch: 15  Global Step: 90380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:36,836-Speed 3889.97 samples/sec  Loss 5.4356  LearningRate 0.0265  ProxyLR: 1.3259  Epoch: 15  Global Step: 90390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:39,466-Speed 3894.75 samples/sec  Loss 5.4825  LearningRate 0.0265  ProxyLR: 1.3253  Epoch: 15  Global Step: 90400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:42,096-Speed 3894.96 samples/sec  Loss 5.4269  LearningRate 0.0265  ProxyLR: 1.3248  Epoch: 15  Global Step: 90410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:44,728-Speed 3891.48 samples/sec  Loss 5.5373  LearningRate 0.0265  ProxyLR: 1.3243  Epoch: 15  Global Step: 90420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:47,359-Speed 3892.68 samples/sec  Loss 5.4719  LearningRate 0.0265  ProxyLR: 1.3238  Epoch: 15  Global Step: 90430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:49,992-Speed 3889.67 samples/sec  Loss 5.4694  LearningRate 0.0265  ProxyLR: 1.3233  Epoch: 15  Global Step: 90440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:52,624-Speed 3891.19 samples/sec  Loss 5.3883  LearningRate 0.0265  ProxyLR: 1.3228  Epoch: 15  Global Step: 90450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:55,243-Speed 3911.94 samples/sec  Loss 5.4719  LearningRate 0.0264  ProxyLR: 1.3223  Epoch: 15  Global Step: 90460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:30:57,874-Speed 3892.48 samples/sec  Loss 5.5198  LearningRate 0.0264  ProxyLR: 1.3218  Epoch: 15  Global Step: 90470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:00,504-Speed 3894.46 samples/sec  Loss 5.4245  LearningRate 0.0264  ProxyLR: 1.3212  Epoch: 15  Global Step: 90480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:03,136-Speed 3891.88 samples/sec  Loss 5.4410  LearningRate 0.0264  ProxyLR: 1.3207  Epoch: 15  Global Step: 90490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:05,769-Speed 3890.94 samples/sec  Loss 5.3944  LearningRate 0.0264  ProxyLR: 1.3202  Epoch: 15  Global Step: 90500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:08,403-Speed 3887.74 samples/sec  Loss 5.3431  LearningRate 0.0264  ProxyLR: 1.3197  Epoch: 15  Global Step: 90510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:11,034-Speed 3892.85 samples/sec  Loss 5.5051  LearningRate 0.0264  ProxyLR: 1.3192  Epoch: 15  Global Step: 90520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:13,666-Speed 3892.10 samples/sec  Loss 5.4352  LearningRate 0.0264  ProxyLR: 1.3187  Epoch: 15  Global Step: 90530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:16,300-Speed 3888.20 samples/sec  Loss 5.4354  LearningRate 0.0264  ProxyLR: 1.3182  Epoch: 15  Global Step: 90540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:18,933-Speed 3889.95 samples/sec  Loss 5.3732  LearningRate 0.0264  ProxyLR: 1.3177  Epoch: 15  Global Step: 90550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:21,564-Speed 3892.47 samples/sec  Loss 5.4496  LearningRate 0.0263  ProxyLR: 1.3172  Epoch: 15  Global Step: 90560   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:31:24,182-Speed 3913.25 samples/sec  Loss 5.3702  LearningRate 0.0263  ProxyLR: 1.3166  Epoch: 15  Global Step: 90570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:26,811-Speed 3895.56 samples/sec  Loss 5.3763  LearningRate 0.0263  ProxyLR: 1.3161  Epoch: 15  Global Step: 90580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:29,442-Speed 3892.38 samples/sec  Loss 5.4385  LearningRate 0.0263  ProxyLR: 1.3156  Epoch: 15  Global Step: 90590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:32,076-Speed 3889.30 samples/sec  Loss 5.5462  LearningRate 0.0263  ProxyLR: 1.3151  Epoch: 15  Global Step: 90600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:34,712-Speed 3886.54 samples/sec  Loss 5.4510  LearningRate 0.0263  ProxyLR: 1.3146  Epoch: 15  Global Step: 90610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:37,344-Speed 3891.36 samples/sec  Loss 5.3280  LearningRate 0.0263  ProxyLR: 1.3141  Epoch: 15  Global Step: 90620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:39,974-Speed 3893.44 samples/sec  Loss 5.2511  LearningRate 0.0263  ProxyLR: 1.3136  Epoch: 15  Global Step: 90630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:42,604-Speed 3894.61 samples/sec  Loss 5.3734  LearningRate 0.0263  ProxyLR: 1.3131  Epoch: 15  Global Step: 90640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:45,235-Speed 3892.89 samples/sec  Loss 5.3358  LearningRate 0.0263  ProxyLR: 1.3126  Epoch: 15  Global Step: 90650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:47,868-Speed 3891.14 samples/sec  Loss 5.3716  LearningRate 0.0262  ProxyLR: 1.3121  Epoch: 15  Global Step: 90660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:50,500-Speed 3891.27 samples/sec  Loss 5.2804  LearningRate 0.0262  ProxyLR: 1.3115  Epoch: 15  Global Step: 90670   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:31:53,118-Speed 3911.84 samples/sec  Loss 5.3870  LearningRate 0.0262  ProxyLR: 1.3110  Epoch: 15  Global Step: 90680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:55,749-Speed 3892.76 samples/sec  Loss 5.3072  LearningRate 0.0262  ProxyLR: 1.3105  Epoch: 15  Global Step: 90690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:31:58,380-Speed 3892.90 samples/sec  Loss 5.3245  LearningRate 0.0262  ProxyLR: 1.3100  Epoch: 15  Global Step: 90700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:01,012-Speed 3892.28 samples/sec  Loss 5.2761  LearningRate 0.0262  ProxyLR: 1.3095  Epoch: 15  Global Step: 90710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:03,645-Speed 3889.68 samples/sec  Loss 5.2871  LearningRate 0.0262  ProxyLR: 1.3090  Epoch: 15  Global Step: 90720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:06,280-Speed 3887.15 samples/sec  Loss 5.4730  LearningRate 0.0262  ProxyLR: 1.3085  Epoch: 15  Global Step: 90730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:08,911-Speed 3892.91 samples/sec  Loss 5.2682  LearningRate 0.0262  ProxyLR: 1.3080  Epoch: 15  Global Step: 90740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:11,543-Speed 3891.83 samples/sec  Loss 5.3574  LearningRate 0.0261  ProxyLR: 1.3075  Epoch: 15  Global Step: 90750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:14,174-Speed 3894.03 samples/sec  Loss 5.3352  LearningRate 0.0261  ProxyLR: 1.3070  Epoch: 15  Global Step: 90760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:16,808-Speed 3888.60 samples/sec  Loss 5.4066  LearningRate 0.0261  ProxyLR: 1.3065  Epoch: 15  Global Step: 90770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:19,438-Speed 3893.20 samples/sec  Loss 5.2713  LearningRate 0.0261  ProxyLR: 1.3059  Epoch: 15  Global Step: 90780   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:32:22,069-Speed 3893.75 samples/sec  Loss 5.2290  LearningRate 0.0261  ProxyLR: 1.3054  Epoch: 15  Global Step: 90790   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:32:24,687-Speed 3912.65 samples/sec  Loss 5.4157  LearningRate 0.0261  ProxyLR: 1.3049  Epoch: 15  Global Step: 90800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:27,306-Speed 3909.47 samples/sec  Loss 5.2622  LearningRate 0.0261  ProxyLR: 1.3044  Epoch: 15  Global Step: 90810   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:29,938-Speed 3892.15 samples/sec  Loss 5.3136  LearningRate 0.0261  ProxyLR: 1.3039  Epoch: 15  Global Step: 90820   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:32,571-Speed 3889.48 samples/sec  Loss 5.3259  LearningRate 0.0261  ProxyLR: 1.3034  Epoch: 15  Global Step: 90830   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:35,204-Speed 3890.28 samples/sec  Loss 5.2280  LearningRate 0.0261  ProxyLR: 1.3029  Epoch: 15  Global Step: 90840   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:37,835-Speed 3893.02 samples/sec  Loss 5.3355  LearningRate 0.0260  ProxyLR: 1.3024  Epoch: 15  Global Step: 90850   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:40,470-Speed 3887.94 samples/sec  Loss 5.4143  LearningRate 0.0260  ProxyLR: 1.3019  Epoch: 15  Global Step: 90860   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:43,104-Speed 3888.71 samples/sec  Loss 5.3242  LearningRate 0.0260  ProxyLR: 1.3014  Epoch: 15  Global Step: 90870   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:45,737-Speed 3889.34 samples/sec  Loss 5.1923  LearningRate 0.0260  ProxyLR: 1.3009  Epoch: 15  Global Step: 90880   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:48,369-Speed 3891.25 samples/sec  Loss 5.3246  LearningRate 0.0260  ProxyLR: 1.3004  Epoch: 15  Global Step: 90890   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:51,000-Speed 3893.44 samples/sec  Loss 5.2904  LearningRate 0.0260  ProxyLR: 1.2999  Epoch: 15  Global Step: 90900   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:32:53,637-Speed 3885.13 samples/sec  Loss 5.1453  LearningRate 0.0260  ProxyLR: 1.2993  Epoch: 15  Global Step: 90910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:56,276-Speed 3880.38 samples/sec  Loss 5.2355  LearningRate 0.0260  ProxyLR: 1.2988  Epoch: 15  Global Step: 90920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:32:58,913-Speed 3885.13 samples/sec  Loss 5.3308  LearningRate 0.0260  ProxyLR: 1.2983  Epoch: 15  Global Step: 90930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:01,550-Speed 3883.99 samples/sec  Loss 5.3337  LearningRate 0.0260  ProxyLR: 1.2978  Epoch: 15  Global Step: 90940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:04,187-Speed 3883.70 samples/sec  Loss 5.2594  LearningRate 0.0259  ProxyLR: 1.2973  Epoch: 15  Global Step: 90950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:06,821-Speed 3888.48 samples/sec  Loss 5.1858  LearningRate 0.0259  ProxyLR: 1.2968  Epoch: 15  Global Step: 90960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:09,514-Speed 3803.24 samples/sec  Loss 5.1856  LearningRate 0.0259  ProxyLR: 1.2963  Epoch: 15  Global Step: 90970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:18,496-Speed 1140.20 samples/sec  Loss 5.0218  LearningRate 0.0259  ProxyLR: 1.2958  Epoch: 16  Global Step: 90980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:21,162-Speed 3841.61 samples/sec  Loss 4.8642  LearningRate 0.0259  ProxyLR: 1.2953  Epoch: 16  Global Step: 90990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:23,794-Speed 3891.93 samples/sec  Loss 4.7381  LearningRate 0.0259  ProxyLR: 1.2948  Epoch: 16  Global Step: 91000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:26,425-Speed 3893.29 samples/sec  Loss 4.8761  LearningRate 0.0259  ProxyLR: 1.2943  Epoch: 16  Global Step: 91010   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:33:29,054-Speed 3895.66 samples/sec  Loss 4.7931  LearningRate 0.0259  ProxyLR: 1.2938  Epoch: 16  Global Step: 91020   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:33:31,674-Speed 3910.37 samples/sec  Loss 4.7319  LearningRate 0.0259  ProxyLR: 1.2933  Epoch: 16  Global Step: 91030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:34,306-Speed 3890.65 samples/sec  Loss 4.8161  LearningRate 0.0259  ProxyLR: 1.2928  Epoch: 16  Global Step: 91040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:36,938-Speed 3892.10 samples/sec  Loss 4.7478  LearningRate 0.0258  ProxyLR: 1.2923  Epoch: 16  Global Step: 91050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:39,571-Speed 3890.59 samples/sec  Loss 4.7292  LearningRate 0.0258  ProxyLR: 1.2917  Epoch: 16  Global Step: 91060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:42,238-Speed 3840.78 samples/sec  Loss 4.7210  LearningRate 0.0258  ProxyLR: 1.2912  Epoch: 16  Global Step: 91070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:44,867-Speed 3896.08 samples/sec  Loss 4.7756  LearningRate 0.0258  ProxyLR: 1.2907  Epoch: 16  Global Step: 91080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:47,517-Speed 3865.18 samples/sec  Loss 4.7166  LearningRate 0.0258  ProxyLR: 1.2902  Epoch: 16  Global Step: 91090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:50,143-Speed 3900.17 samples/sec  Loss 4.7088  LearningRate 0.0258  ProxyLR: 1.2897  Epoch: 16  Global Step: 91100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:52,770-Speed 3898.20 samples/sec  Loss 4.6327  LearningRate 0.0258  ProxyLR: 1.2892  Epoch: 16  Global Step: 91110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:55,398-Speed 3897.51 samples/sec  Loss 4.6711  LearningRate 0.0258  ProxyLR: 1.2887  Epoch: 16  Global Step: 91120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:33:58,013-Speed 3917.96 samples/sec  Loss 4.8213  LearningRate 0.0258  ProxyLR: 1.2882  Epoch: 16  Global Step: 91130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:00,640-Speed 3897.86 samples/sec  Loss 4.6323  LearningRate 0.0258  ProxyLR: 1.2877  Epoch: 16  Global Step: 91140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:03,316-Speed 3827.32 samples/sec  Loss 4.6975  LearningRate 0.0257  ProxyLR: 1.2872  Epoch: 16  Global Step: 91150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:05,947-Speed 3894.38 samples/sec  Loss 4.6284  LearningRate 0.0257  ProxyLR: 1.2867  Epoch: 16  Global Step: 91160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:08,574-Speed 3898.96 samples/sec  Loss 4.6343  LearningRate 0.0257  ProxyLR: 1.2862  Epoch: 16  Global Step: 91170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:11,203-Speed 3895.02 samples/sec  Loss 4.7413  LearningRate 0.0257  ProxyLR: 1.2857  Epoch: 16  Global Step: 91180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:13,830-Speed 3899.31 samples/sec  Loss 4.7313  LearningRate 0.0257  ProxyLR: 1.2852  Epoch: 16  Global Step: 91190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:16,458-Speed 3898.34 samples/sec  Loss 4.6112  LearningRate 0.0257  ProxyLR: 1.2847  Epoch: 16  Global Step: 91200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:19,087-Speed 3895.75 samples/sec  Loss 4.7812  LearningRate 0.0257  ProxyLR: 1.2842  Epoch: 16  Global Step: 91210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:21,715-Speed 3897.23 samples/sec  Loss 4.6230  LearningRate 0.0257  ProxyLR: 1.2837  Epoch: 16  Global Step: 91220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:24,398-Speed 3817.97 samples/sec  Loss 4.6722  LearningRate 0.0257  ProxyLR: 1.2832  Epoch: 16  Global Step: 91230   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:34:27,013-Speed 3917.36 samples/sec  Loss 4.8342  LearningRate 0.0257  ProxyLR: 1.2827  Epoch: 16  Global Step: 91240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:29,643-Speed 3893.58 samples/sec  Loss 4.8355  LearningRate 0.0256  ProxyLR: 1.2822  Epoch: 16  Global Step: 91250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:32,273-Speed 3895.24 samples/sec  Loss 4.6809  LearningRate 0.0256  ProxyLR: 1.2817  Epoch: 16  Global Step: 91260   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:34,905-Speed 3891.50 samples/sec  Loss 4.6505  LearningRate 0.0256  ProxyLR: 1.2812  Epoch: 16  Global Step: 91270   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:37,535-Speed 3894.28 samples/sec  Loss 4.6557  LearningRate 0.0256  ProxyLR: 1.2806  Epoch: 16  Global Step: 91280   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:40,163-Speed 3896.40 samples/sec  Loss 4.6424  LearningRate 0.0256  ProxyLR: 1.2801  Epoch: 16  Global Step: 91290   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:42,794-Speed 3893.79 samples/sec  Loss 4.7176  LearningRate 0.0256  ProxyLR: 1.2796  Epoch: 16  Global Step: 91300   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:45,424-Speed 3893.77 samples/sec  Loss 4.6569  LearningRate 0.0256  ProxyLR: 1.2791  Epoch: 16  Global Step: 91310   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:48,053-Speed 3895.68 samples/sec  Loss 4.7643  LearningRate 0.0256  ProxyLR: 1.2786  Epoch: 16  Global Step: 91320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:50,686-Speed 3890.96 samples/sec  Loss 4.6349  LearningRate 0.0256  ProxyLR: 1.2781  Epoch: 16  Global Step: 91330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:34:53,317-Speed 3893.68 samples/sec  Loss 4.6236  LearningRate 0.0256  ProxyLR: 1.2776  Epoch: 16  Global Step: 91340   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:34:55,947-Speed 3894.48 samples/sec  Loss 4.6682  LearningRate 0.0255  ProxyLR: 1.2771  Epoch: 16  Global Step: 91350   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:34:58,565-Speed 3911.36 samples/sec  Loss 4.6489  LearningRate 0.0255  ProxyLR: 1.2766  Epoch: 16  Global Step: 91360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:01,196-Speed 3893.56 samples/sec  Loss 4.7493  LearningRate 0.0255  ProxyLR: 1.2761  Epoch: 16  Global Step: 91370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:03,827-Speed 3892.11 samples/sec  Loss 4.6332  LearningRate 0.0255  ProxyLR: 1.2756  Epoch: 16  Global Step: 91380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:06,468-Speed 3878.96 samples/sec  Loss 4.6077  LearningRate 0.0255  ProxyLR: 1.2751  Epoch: 16  Global Step: 91390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:09,108-Speed 3879.38 samples/sec  Loss 4.6302  LearningRate 0.0255  ProxyLR: 1.2746  Epoch: 16  Global Step: 91400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:11,742-Speed 3888.12 samples/sec  Loss 4.6793  LearningRate 0.0255  ProxyLR: 1.2741  Epoch: 16  Global Step: 91410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:14,372-Speed 3894.18 samples/sec  Loss 4.6484  LearningRate 0.0255  ProxyLR: 1.2736  Epoch: 16  Global Step: 91420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:17,002-Speed 3894.23 samples/sec  Loss 4.7299  LearningRate 0.0255  ProxyLR: 1.2731  Epoch: 16  Global Step: 91430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:19,633-Speed 3893.46 samples/sec  Loss 4.6858  LearningRate 0.0255  ProxyLR: 1.2726  Epoch: 16  Global Step: 91440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:22,267-Speed 3888.21 samples/sec  Loss 4.6240  LearningRate 0.0254  ProxyLR: 1.2721  Epoch: 16  Global Step: 91450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:24,886-Speed 3912.04 samples/sec  Loss 4.6222  LearningRate 0.0254  ProxyLR: 1.2716  Epoch: 16  Global Step: 91460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:27,517-Speed 3892.23 samples/sec  Loss 4.6144  LearningRate 0.0254  ProxyLR: 1.2711  Epoch: 16  Global Step: 91470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:30,148-Speed 3892.72 samples/sec  Loss 4.6713  LearningRate 0.0254  ProxyLR: 1.2706  Epoch: 16  Global Step: 91480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:32,778-Speed 3895.40 samples/sec  Loss 4.5986  LearningRate 0.0254  ProxyLR: 1.2701  Epoch: 16  Global Step: 91490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:35,409-Speed 3893.13 samples/sec  Loss 4.7526  LearningRate 0.0254  ProxyLR: 1.2696  Epoch: 16  Global Step: 91500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:38,039-Speed 3894.52 samples/sec  Loss 4.6673  LearningRate 0.0254  ProxyLR: 1.2691  Epoch: 16  Global Step: 91510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:40,671-Speed 3890.18 samples/sec  Loss 4.5845  LearningRate 0.0254  ProxyLR: 1.2686  Epoch: 16  Global Step: 91520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:43,302-Speed 3893.86 samples/sec  Loss 4.5845  LearningRate 0.0254  ProxyLR: 1.2681  Epoch: 16  Global Step: 91530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:45,933-Speed 3892.53 samples/sec  Loss 4.6166  LearningRate 0.0254  ProxyLR: 1.2676  Epoch: 16  Global Step: 91540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:48,568-Speed 3887.78 samples/sec  Loss 4.5698  LearningRate 0.0253  ProxyLR: 1.2671  Epoch: 16  Global Step: 91550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:51,197-Speed 3895.36 samples/sec  Loss 4.5760  LearningRate 0.0253  ProxyLR: 1.2666  Epoch: 16  Global Step: 91560   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:35:53,816-Speed 3911.00 samples/sec  Loss 4.5395  LearningRate 0.0253  ProxyLR: 1.2661  Epoch: 16  Global Step: 91570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:56,447-Speed 3893.38 samples/sec  Loss 4.5385  LearningRate 0.0253  ProxyLR: 1.2656  Epoch: 16  Global Step: 91580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:35:59,080-Speed 3890.42 samples/sec  Loss 4.5412  LearningRate 0.0253  ProxyLR: 1.2651  Epoch: 16  Global Step: 91590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:01,714-Speed 3888.53 samples/sec  Loss 4.5794  LearningRate 0.0253  ProxyLR: 1.2646  Epoch: 16  Global Step: 91600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:04,348-Speed 3887.82 samples/sec  Loss 4.6707  LearningRate 0.0253  ProxyLR: 1.2641  Epoch: 16  Global Step: 91610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:06,978-Speed 3894.16 samples/sec  Loss 4.6218  LearningRate 0.0253  ProxyLR: 1.2636  Epoch: 16  Global Step: 91620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:09,608-Speed 3894.52 samples/sec  Loss 4.4902  LearningRate 0.0253  ProxyLR: 1.2631  Epoch: 16  Global Step: 91630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:12,240-Speed 3892.31 samples/sec  Loss 4.6633  LearningRate 0.0253  ProxyLR: 1.2626  Epoch: 16  Global Step: 91640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:14,871-Speed 3891.68 samples/sec  Loss 4.6520  LearningRate 0.0252  ProxyLR: 1.2621  Epoch: 16  Global Step: 91650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:17,502-Speed 3893.64 samples/sec  Loss 4.5280  LearningRate 0.0252  ProxyLR: 1.2616  Epoch: 16  Global Step: 91660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:20,131-Speed 3896.59 samples/sec  Loss 4.6140  LearningRate 0.0252  ProxyLR: 1.2611  Epoch: 16  Global Step: 91670   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:36:22,748-Speed 3912.77 samples/sec  Loss 4.5085  LearningRate 0.0252  ProxyLR: 1.2606  Epoch: 16  Global Step: 91680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:25,377-Speed 3895.73 samples/sec  Loss 4.7376  LearningRate 0.0252  ProxyLR: 1.2601  Epoch: 16  Global Step: 91690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:28,011-Speed 3889.53 samples/sec  Loss 4.6212  LearningRate 0.0252  ProxyLR: 1.2596  Epoch: 16  Global Step: 91700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:30,639-Speed 3897.48 samples/sec  Loss 4.5291  LearningRate 0.0252  ProxyLR: 1.2591  Epoch: 16  Global Step: 91710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:33,268-Speed 3896.02 samples/sec  Loss 4.5323  LearningRate 0.0252  ProxyLR: 1.2586  Epoch: 16  Global Step: 91720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:35,898-Speed 3894.07 samples/sec  Loss 4.5654  LearningRate 0.0252  ProxyLR: 1.2581  Epoch: 16  Global Step: 91730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:38,528-Speed 3895.07 samples/sec  Loss 4.6206  LearningRate 0.0252  ProxyLR: 1.2576  Epoch: 16  Global Step: 91740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:41,156-Speed 3897.26 samples/sec  Loss 4.6093  LearningRate 0.0251  ProxyLR: 1.2571  Epoch: 16  Global Step: 91750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:43,785-Speed 3895.72 samples/sec  Loss 4.5404  LearningRate 0.0251  ProxyLR: 1.2566  Epoch: 16  Global Step: 91760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:46,413-Speed 3897.19 samples/sec  Loss 4.5224  LearningRate 0.0251  ProxyLR: 1.2561  Epoch: 16  Global Step: 91770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:49,042-Speed 3896.88 samples/sec  Loss 4.5231  LearningRate 0.0251  ProxyLR: 1.2556  Epoch: 16  Global Step: 91780   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:36:51,656-Speed 3917.44 samples/sec  Loss 4.5592  LearningRate 0.0251  ProxyLR: 1.2551  Epoch: 16  Global Step: 91790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:54,284-Speed 3897.66 samples/sec  Loss 4.5638  LearningRate 0.0251  ProxyLR: 1.2546  Epoch: 16  Global Step: 91800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:56,911-Speed 3898.78 samples/sec  Loss 4.6105  LearningRate 0.0251  ProxyLR: 1.2541  Epoch: 16  Global Step: 91810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:36:59,540-Speed 3896.85 samples/sec  Loss 4.5962  LearningRate 0.0251  ProxyLR: 1.2536  Epoch: 16  Global Step: 91820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:02,168-Speed 3896.79 samples/sec  Loss 4.4723  LearningRate 0.0251  ProxyLR: 1.2531  Epoch: 16  Global Step: 91830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:04,796-Speed 3897.84 samples/sec  Loss 4.5330  LearningRate 0.0251  ProxyLR: 1.2526  Epoch: 16  Global Step: 91840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:07,426-Speed 3894.73 samples/sec  Loss 4.4712  LearningRate 0.0250  ProxyLR: 1.2521  Epoch: 16  Global Step: 91850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:10,054-Speed 3897.12 samples/sec  Loss 4.5350  LearningRate 0.0250  ProxyLR: 1.2516  Epoch: 16  Global Step: 91860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:12,682-Speed 3897.64 samples/sec  Loss 4.6038  LearningRate 0.0250  ProxyLR: 1.2511  Epoch: 16  Global Step: 91870   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:15,310-Speed 3897.96 samples/sec  Loss 4.4832  LearningRate 0.0250  ProxyLR: 1.2506  Epoch: 16  Global Step: 91880   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:17,938-Speed 3896.81 samples/sec  Loss 4.5108  LearningRate 0.0250  ProxyLR: 1.2501  Epoch: 16  Global Step: 91890   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:37:20,568-Speed 3894.39 samples/sec  Loss 4.5258  LearningRate 0.0250  ProxyLR: 1.2496  Epoch: 16  Global Step: 91900   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:37:23,184-Speed 3915.10 samples/sec  Loss 4.5458  LearningRate 0.0250  ProxyLR: 1.2491  Epoch: 16  Global Step: 91910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:25,809-Speed 3901.50 samples/sec  Loss 4.5120  LearningRate 0.0250  ProxyLR: 1.2486  Epoch: 16  Global Step: 91920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:28,438-Speed 3895.79 samples/sec  Loss 4.4847  LearningRate 0.0250  ProxyLR: 1.2481  Epoch: 16  Global Step: 91930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:31,066-Speed 3898.07 samples/sec  Loss 4.4671  LearningRate 0.0250  ProxyLR: 1.2476  Epoch: 16  Global Step: 91940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:33,694-Speed 3897.37 samples/sec  Loss 4.4977  LearningRate 0.0249  ProxyLR: 1.2471  Epoch: 16  Global Step: 91950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:36,321-Speed 3898.78 samples/sec  Loss 4.5386  LearningRate 0.0249  ProxyLR: 1.2466  Epoch: 16  Global Step: 91960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:38,948-Speed 3899.03 samples/sec  Loss 4.4471  LearningRate 0.0249  ProxyLR: 1.2461  Epoch: 16  Global Step: 91970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:41,576-Speed 3897.75 samples/sec  Loss 4.5497  LearningRate 0.0249  ProxyLR: 1.2456  Epoch: 16  Global Step: 91980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:44,203-Speed 3898.55 samples/sec  Loss 4.4863  LearningRate 0.0249  ProxyLR: 1.2451  Epoch: 16  Global Step: 91990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:46,831-Speed 3897.50 samples/sec  Loss 4.5369  LearningRate 0.0249  ProxyLR: 1.2447  Epoch: 16  Global Step: 92000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:49,460-Speed 3896.65 samples/sec  Loss 4.4305  LearningRate 0.0249  ProxyLR: 1.2442  Epoch: 16  Global Step: 92010   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:37:52,075-Speed 3917.09 samples/sec  Loss 4.4296  LearningRate 0.0249  ProxyLR: 1.2437  Epoch: 16  Global Step: 92020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:54,704-Speed 3895.90 samples/sec  Loss 4.5202  LearningRate 0.0249  ProxyLR: 1.2432  Epoch: 16  Global Step: 92030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:57,333-Speed 3896.18 samples/sec  Loss 4.5596  LearningRate 0.0249  ProxyLR: 1.2427  Epoch: 16  Global Step: 92040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:37:59,961-Speed 3897.15 samples/sec  Loss 4.5614  LearningRate 0.0248  ProxyLR: 1.2422  Epoch: 16  Global Step: 92050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:02,588-Speed 3898.62 samples/sec  Loss 4.4447  LearningRate 0.0248  ProxyLR: 1.2417  Epoch: 16  Global Step: 92060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:05,216-Speed 3897.65 samples/sec  Loss 4.4694  LearningRate 0.0248  ProxyLR: 1.2412  Epoch: 16  Global Step: 92070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:07,845-Speed 3896.20 samples/sec  Loss 4.5070  LearningRate 0.0248  ProxyLR: 1.2407  Epoch: 16  Global Step: 92080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:10,474-Speed 3895.86 samples/sec  Loss 4.4374  LearningRate 0.0248  ProxyLR: 1.2402  Epoch: 16  Global Step: 92090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:13,101-Speed 3898.93 samples/sec  Loss 4.4960  LearningRate 0.0248  ProxyLR: 1.2397  Epoch: 16  Global Step: 92100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:15,728-Speed 3898.93 samples/sec  Loss 4.4534  LearningRate 0.0248  ProxyLR: 1.2392  Epoch: 16  Global Step: 92110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:18,358-Speed 3894.84 samples/sec  Loss 4.4660  LearningRate 0.0248  ProxyLR: 1.2387  Epoch: 16  Global Step: 92120   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:38:20,970-Speed 3920.55 samples/sec  Loss 4.5682  LearningRate 0.0248  ProxyLR: 1.2382  Epoch: 16  Global Step: 92130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:23,597-Speed 3899.72 samples/sec  Loss 4.4868  LearningRate 0.0248  ProxyLR: 1.2377  Epoch: 16  Global Step: 92140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:26,224-Speed 3899.12 samples/sec  Loss 4.4751  LearningRate 0.0247  ProxyLR: 1.2372  Epoch: 16  Global Step: 92150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:28,852-Speed 3897.45 samples/sec  Loss 4.4342  LearningRate 0.0247  ProxyLR: 1.2367  Epoch: 16  Global Step: 92160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:31,478-Speed 3900.38 samples/sec  Loss 4.5625  LearningRate 0.0247  ProxyLR: 1.2362  Epoch: 16  Global Step: 92170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:34,104-Speed 3899.35 samples/sec  Loss 4.3938  LearningRate 0.0247  ProxyLR: 1.2357  Epoch: 16  Global Step: 92180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:36,731-Speed 3899.43 samples/sec  Loss 4.4262  LearningRate 0.0247  ProxyLR: 1.2352  Epoch: 16  Global Step: 92190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:39,359-Speed 3896.82 samples/sec  Loss 4.4415  LearningRate 0.0247  ProxyLR: 1.2347  Epoch: 16  Global Step: 92200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:41,986-Speed 3899.93 samples/sec  Loss 4.4110  LearningRate 0.0247  ProxyLR: 1.2343  Epoch: 16  Global Step: 92210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:44,614-Speed 3897.20 samples/sec  Loss 4.5307  LearningRate 0.0247  ProxyLR: 1.2338  Epoch: 16  Global Step: 92220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:47,229-Speed 3916.77 samples/sec  Loss 4.3977  LearningRate 0.0247  ProxyLR: 1.2333  Epoch: 16  Global Step: 92230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:49,858-Speed 3896.50 samples/sec  Loss 4.5442  LearningRate 0.0247  ProxyLR: 1.2328  Epoch: 16  Global Step: 92240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:52,485-Speed 3898.53 samples/sec  Loss 4.4828  LearningRate 0.0246  ProxyLR: 1.2323  Epoch: 16  Global Step: 92250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:55,111-Speed 3899.68 samples/sec  Loss 4.4061  LearningRate 0.0246  ProxyLR: 1.2318  Epoch: 16  Global Step: 92260   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:38:57,740-Speed 3896.42 samples/sec  Loss 4.4561  LearningRate 0.0246  ProxyLR: 1.2313  Epoch: 16  Global Step: 92270   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:00,366-Speed 3900.75 samples/sec  Loss 4.3904  LearningRate 0.0246  ProxyLR: 1.2308  Epoch: 16  Global Step: 92280   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:02,993-Speed 3898.18 samples/sec  Loss 4.4147  LearningRate 0.0246  ProxyLR: 1.2303  Epoch: 16  Global Step: 92290   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:05,622-Speed 3896.65 samples/sec  Loss 4.4455  LearningRate 0.0246  ProxyLR: 1.2298  Epoch: 16  Global Step: 92300   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:08,252-Speed 3893.39 samples/sec  Loss 4.3890  LearningRate 0.0246  ProxyLR: 1.2293  Epoch: 16  Global Step: 92310   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:10,880-Speed 3898.04 samples/sec  Loss 4.4561  LearningRate 0.0246  ProxyLR: 1.2288  Epoch: 16  Global Step: 92320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:13,497-Speed 3913.70 samples/sec  Loss 4.4730  LearningRate 0.0246  ProxyLR: 1.2283  Epoch: 16  Global Step: 92330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:16,128-Speed 3893.44 samples/sec  Loss 4.4413  LearningRate 0.0246  ProxyLR: 1.2278  Epoch: 16  Global Step: 92340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:18,757-Speed 3895.56 samples/sec  Loss 4.3000  LearningRate 0.0245  ProxyLR: 1.2273  Epoch: 16  Global Step: 92350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:21,386-Speed 3895.15 samples/sec  Loss 4.3203  LearningRate 0.0245  ProxyLR: 1.2268  Epoch: 16  Global Step: 92360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:24,017-Speed 3892.98 samples/sec  Loss 4.4829  LearningRate 0.0245  ProxyLR: 1.2264  Epoch: 16  Global Step: 92370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:26,650-Speed 3890.66 samples/sec  Loss 4.4661  LearningRate 0.0245  ProxyLR: 1.2259  Epoch: 16  Global Step: 92380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:29,284-Speed 3888.63 samples/sec  Loss 4.4938  LearningRate 0.0245  ProxyLR: 1.2254  Epoch: 16  Global Step: 92390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:31,923-Speed 3882.02 samples/sec  Loss 4.4330  LearningRate 0.0245  ProxyLR: 1.2249  Epoch: 16  Global Step: 92400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:34,560-Speed 3883.97 samples/sec  Loss 4.3800  LearningRate 0.0245  ProxyLR: 1.2244  Epoch: 16  Global Step: 92410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:37,196-Speed 3885.62 samples/sec  Loss 4.4310  LearningRate 0.0245  ProxyLR: 1.2239  Epoch: 16  Global Step: 92420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:39,834-Speed 3882.35 samples/sec  Loss 4.3495  LearningRate 0.0245  ProxyLR: 1.2234  Epoch: 16  Global Step: 92430   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:39:42,454-Speed 3908.54 samples/sec  Loss 4.4447  LearningRate 0.0245  ProxyLR: 1.2229  Epoch: 16  Global Step: 92440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:45,089-Speed 3887.02 samples/sec  Loss 4.4140  LearningRate 0.0244  ProxyLR: 1.2224  Epoch: 16  Global Step: 92450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:47,729-Speed 3879.82 samples/sec  Loss 4.4689  LearningRate 0.0244  ProxyLR: 1.2219  Epoch: 16  Global Step: 92460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:50,365-Speed 3885.53 samples/sec  Loss 4.4194  LearningRate 0.0244  ProxyLR: 1.2214  Epoch: 16  Global Step: 92470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:53,000-Speed 3887.43 samples/sec  Loss 4.4217  LearningRate 0.0244  ProxyLR: 1.2209  Epoch: 16  Global Step: 92480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:55,635-Speed 3886.61 samples/sec  Loss 4.3904  LearningRate 0.0244  ProxyLR: 1.2204  Epoch: 16  Global Step: 92490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:39:58,269-Speed 3888.57 samples/sec  Loss 4.4647  LearningRate 0.0244  ProxyLR: 1.2200  Epoch: 16  Global Step: 92500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:00,905-Speed 3886.20 samples/sec  Loss 4.3125  LearningRate 0.0244  ProxyLR: 1.2195  Epoch: 16  Global Step: 92510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:03,541-Speed 3885.31 samples/sec  Loss 4.3126  LearningRate 0.0244  ProxyLR: 1.2190  Epoch: 16  Global Step: 92520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:06,177-Speed 3886.48 samples/sec  Loss 4.3326  LearningRate 0.0244  ProxyLR: 1.2185  Epoch: 16  Global Step: 92530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:08,797-Speed 3909.43 samples/sec  Loss 4.4547  LearningRate 0.0244  ProxyLR: 1.2180  Epoch: 16  Global Step: 92540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:11,433-Speed 3885.69 samples/sec  Loss 4.4752  LearningRate 0.0244  ProxyLR: 1.2175  Epoch: 16  Global Step: 92550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:14,067-Speed 3888.39 samples/sec  Loss 4.3583  LearningRate 0.0243  ProxyLR: 1.2170  Epoch: 16  Global Step: 92560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:16,704-Speed 3883.81 samples/sec  Loss 4.4430  LearningRate 0.0243  ProxyLR: 1.2165  Epoch: 16  Global Step: 92570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:19,337-Speed 3889.16 samples/sec  Loss 4.4745  LearningRate 0.0243  ProxyLR: 1.2160  Epoch: 16  Global Step: 92580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:21,975-Speed 3883.92 samples/sec  Loss 4.3860  LearningRate 0.0243  ProxyLR: 1.2155  Epoch: 16  Global Step: 92590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:24,609-Speed 3887.81 samples/sec  Loss 4.2679  LearningRate 0.0243  ProxyLR: 1.2150  Epoch: 16  Global Step: 92600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:27,247-Speed 3883.96 samples/sec  Loss 4.4012  LearningRate 0.0243  ProxyLR: 1.2146  Epoch: 16  Global Step: 92610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:29,883-Speed 3885.56 samples/sec  Loss 4.3264  LearningRate 0.0243  ProxyLR: 1.2141  Epoch: 16  Global Step: 92620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:32,516-Speed 3888.62 samples/sec  Loss 4.3975  LearningRate 0.0243  ProxyLR: 1.2136  Epoch: 16  Global Step: 92630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:35,147-Speed 3893.33 samples/sec  Loss 4.3099  LearningRate 0.0243  ProxyLR: 1.2131  Epoch: 16  Global Step: 92640   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:40:37,776-Speed 3895.44 samples/sec  Loss 4.3987  LearningRate 0.0243  ProxyLR: 1.2126  Epoch: 16  Global Step: 92650   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:40:40,406-Speed 3895.33 samples/sec  Loss 4.3995  LearningRate 0.0242  ProxyLR: 1.2121  Epoch: 16  Global Step: 92660   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:40:43,022-Speed 3915.15 samples/sec  Loss 4.3428  LearningRate 0.0242  ProxyLR: 1.2116  Epoch: 16  Global Step: 92670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:45,649-Speed 3898.47 samples/sec  Loss 4.3155  LearningRate 0.0242  ProxyLR: 1.2111  Epoch: 16  Global Step: 92680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:48,279-Speed 3894.88 samples/sec  Loss 4.4148  LearningRate 0.0242  ProxyLR: 1.2106  Epoch: 16  Global Step: 92690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:50,908-Speed 3895.52 samples/sec  Loss 4.3561  LearningRate 0.0242  ProxyLR: 1.2101  Epoch: 16  Global Step: 92700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:53,537-Speed 3895.78 samples/sec  Loss 4.3990  LearningRate 0.0242  ProxyLR: 1.2097  Epoch: 16  Global Step: 92710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:56,166-Speed 3896.64 samples/sec  Loss 4.4371  LearningRate 0.0242  ProxyLR: 1.2092  Epoch: 16  Global Step: 92720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:40:58,797-Speed 3892.31 samples/sec  Loss 4.3713  LearningRate 0.0242  ProxyLR: 1.2087  Epoch: 16  Global Step: 92730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:01,429-Speed 3891.09 samples/sec  Loss 4.3651  LearningRate 0.0242  ProxyLR: 1.2082  Epoch: 16  Global Step: 92740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:04,065-Speed 3885.89 samples/sec  Loss 4.2745  LearningRate 0.0242  ProxyLR: 1.2077  Epoch: 16  Global Step: 92750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:06,697-Speed 3891.61 samples/sec  Loss 4.3005  LearningRate 0.0241  ProxyLR: 1.2072  Epoch: 16  Global Step: 92760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:09,331-Speed 3888.92 samples/sec  Loss 4.3586  LearningRate 0.0241  ProxyLR: 1.2067  Epoch: 16  Global Step: 92770   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:41:11,950-Speed 3910.40 samples/sec  Loss 4.3138  LearningRate 0.0241  ProxyLR: 1.2062  Epoch: 16  Global Step: 92780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:14,584-Speed 3888.14 samples/sec  Loss 4.3505  LearningRate 0.0241  ProxyLR: 1.2057  Epoch: 16  Global Step: 92790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:17,220-Speed 3886.97 samples/sec  Loss 4.4061  LearningRate 0.0241  ProxyLR: 1.2053  Epoch: 16  Global Step: 92800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:19,850-Speed 3894.51 samples/sec  Loss 4.3386  LearningRate 0.0241  ProxyLR: 1.2048  Epoch: 16  Global Step: 92810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:22,480-Speed 3893.75 samples/sec  Loss 4.4223  LearningRate 0.0241  ProxyLR: 1.2043  Epoch: 16  Global Step: 92820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:25,113-Speed 3890.61 samples/sec  Loss 4.2920  LearningRate 0.0241  ProxyLR: 1.2038  Epoch: 16  Global Step: 92830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:27,745-Speed 3890.72 samples/sec  Loss 4.3578  LearningRate 0.0241  ProxyLR: 1.2033  Epoch: 16  Global Step: 92840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:30,377-Speed 3891.02 samples/sec  Loss 4.2505  LearningRate 0.0241  ProxyLR: 1.2028  Epoch: 16  Global Step: 92850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:33,016-Speed 3881.77 samples/sec  Loss 4.3312  LearningRate 0.0240  ProxyLR: 1.2023  Epoch: 16  Global Step: 92860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:35,654-Speed 3882.66 samples/sec  Loss 4.3463  LearningRate 0.0240  ProxyLR: 1.2018  Epoch: 16  Global Step: 92870   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:38,290-Speed 3885.05 samples/sec  Loss 4.3474  LearningRate 0.0240  ProxyLR: 1.2014  Epoch: 16  Global Step: 92880   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:41:40,927-Speed 3885.49 samples/sec  Loss 4.2794  LearningRate 0.0240  ProxyLR: 1.2009  Epoch: 16  Global Step: 92890   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:41:43,549-Speed 3906.07 samples/sec  Loss 4.2791  LearningRate 0.0240  ProxyLR: 1.2004  Epoch: 16  Global Step: 92900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:46,186-Speed 3883.97 samples/sec  Loss 4.3598  LearningRate 0.0240  ProxyLR: 1.1999  Epoch: 16  Global Step: 92910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:48,823-Speed 3884.49 samples/sec  Loss 4.2899  LearningRate 0.0240  ProxyLR: 1.1994  Epoch: 16  Global Step: 92920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:41:51,443-Speed 3908.44 samples/sec  Loss 4.3008  LearningRate 0.0240  ProxyLR: 1.1989  Epoch: 16  Global Step: 92930   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:41:54,080-Speed 3884.89 samples/sec  Loss 4.2726  LearningRate 0.0240  ProxyLR: 1.1984  Epoch: 16  Global Step: 92940   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:41:56,711-Speed 3892.76 samples/sec  Loss 4.4022  LearningRate 0.0240  ProxyLR: 1.1979  Epoch: 16  Global Step: 92950   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:41:59,340-Speed 3895.05 samples/sec  Loss 4.3080  LearningRate 0.0239  ProxyLR: 1.1975  Epoch: 16  Global Step: 92960   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:01,972-Speed 3891.88 samples/sec  Loss 4.3947  LearningRate 0.0239  ProxyLR: 1.1970  Epoch: 16  Global Step: 92970   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:04,605-Speed 3890.23 samples/sec  Loss 4.3232  LearningRate 0.0239  ProxyLR: 1.1965  Epoch: 16  Global Step: 92980   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:07,238-Speed 3890.58 samples/sec  Loss 4.3010  LearningRate 0.0239  ProxyLR: 1.1960  Epoch: 16  Global Step: 92990   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:09,869-Speed 3893.09 samples/sec  Loss 4.2612  LearningRate 0.0239  ProxyLR: 1.1955  Epoch: 16  Global Step: 93000   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:12,500-Speed 3891.99 samples/sec  Loss 4.3117  LearningRate 0.0239  ProxyLR: 1.1950  Epoch: 16  Global Step: 93010   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:15,133-Speed 3891.23 samples/sec  Loss 4.2487  LearningRate 0.0239  ProxyLR: 1.1945  Epoch: 16  Global Step: 93020   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:42:17,766-Speed 3890.21 samples/sec  Loss 4.3496  LearningRate 0.0239  ProxyLR: 1.1941  Epoch: 16  Global Step: 93030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:20,397-Speed 3892.98 samples/sec  Loss 4.3008  LearningRate 0.0239  ProxyLR: 1.1936  Epoch: 16  Global Step: 93040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:23,026-Speed 3894.78 samples/sec  Loss 4.3730  LearningRate 0.0239  ProxyLR: 1.1931  Epoch: 16  Global Step: 93050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:25,664-Speed 3883.70 samples/sec  Loss 4.3493  LearningRate 0.0239  ProxyLR: 1.1926  Epoch: 16  Global Step: 93060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:28,301-Speed 3883.27 samples/sec  Loss 4.3371  LearningRate 0.0238  ProxyLR: 1.1921  Epoch: 16  Global Step: 93070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:30,940-Speed 3881.31 samples/sec  Loss 4.2275  LearningRate 0.0238  ProxyLR: 1.1916  Epoch: 16  Global Step: 93080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:33,579-Speed 3881.90 samples/sec  Loss 4.2468  LearningRate 0.0238  ProxyLR: 1.1911  Epoch: 16  Global Step: 93090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:36,212-Speed 3889.37 samples/sec  Loss 4.2712  LearningRate 0.0238  ProxyLR: 1.1907  Epoch: 16  Global Step: 93100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:38,849-Speed 3884.48 samples/sec  Loss 4.2582  LearningRate 0.0238  ProxyLR: 1.1902  Epoch: 16  Global Step: 93110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:41,485-Speed 3885.77 samples/sec  Loss 4.2223  LearningRate 0.0238  ProxyLR: 1.1897  Epoch: 16  Global Step: 93120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:44,122-Speed 3883.93 samples/sec  Loss 4.3285  LearningRate 0.0238  ProxyLR: 1.1892  Epoch: 16  Global Step: 93130   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:42:46,745-Speed 3905.32 samples/sec  Loss 4.3359  LearningRate 0.0238  ProxyLR: 1.1887  Epoch: 16  Global Step: 93140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:49,379-Speed 3887.88 samples/sec  Loss 4.2346  LearningRate 0.0238  ProxyLR: 1.1882  Epoch: 16  Global Step: 93150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:52,019-Speed 3880.63 samples/sec  Loss 4.1284  LearningRate 0.0238  ProxyLR: 1.1877  Epoch: 16  Global Step: 93160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:54,652-Speed 3889.99 samples/sec  Loss 4.2586  LearningRate 0.0237  ProxyLR: 1.1873  Epoch: 16  Global Step: 93170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:57,287-Speed 3886.90 samples/sec  Loss 4.2495  LearningRate 0.0237  ProxyLR: 1.1868  Epoch: 16  Global Step: 93180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:42:59,921-Speed 3887.86 samples/sec  Loss 4.3073  LearningRate 0.0237  ProxyLR: 1.1863  Epoch: 16  Global Step: 93190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:02,557-Speed 3885.86 samples/sec  Loss 4.3569  LearningRate 0.0237  ProxyLR: 1.1858  Epoch: 16  Global Step: 93200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:05,193-Speed 3886.59 samples/sec  Loss 4.2549  LearningRate 0.0237  ProxyLR: 1.1853  Epoch: 16  Global Step: 93210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:07,814-Speed 3907.50 samples/sec  Loss 4.3668  LearningRate 0.0237  ProxyLR: 1.1848  Epoch: 16  Global Step: 93220   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:10,448-Speed 3889.13 samples/sec  Loss 4.2247  LearningRate 0.0237  ProxyLR: 1.1843  Epoch: 16  Global Step: 93230   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:13,081-Speed 3889.16 samples/sec  Loss 4.1957  LearningRate 0.0237  ProxyLR: 1.1839  Epoch: 16  Global Step: 93240   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:15,716-Speed 3887.00 samples/sec  Loss 4.3223  LearningRate 0.0237  ProxyLR: 1.1834  Epoch: 16  Global Step: 93250   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:18,351-Speed 3887.43 samples/sec  Loss 4.3155  LearningRate 0.0237  ProxyLR: 1.1829  Epoch: 16  Global Step: 93260   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:20,985-Speed 3888.79 samples/sec  Loss 4.2677  LearningRate 0.0236  ProxyLR: 1.1824  Epoch: 16  Global Step: 93270   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:23,620-Speed 3886.51 samples/sec  Loss 4.2312  LearningRate 0.0236  ProxyLR: 1.1819  Epoch: 16  Global Step: 93280   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:26,254-Speed 3888.72 samples/sec  Loss 4.2270  LearningRate 0.0236  ProxyLR: 1.1814  Epoch: 16  Global Step: 93290   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:28,886-Speed 3891.42 samples/sec  Loss 4.2626  LearningRate 0.0236  ProxyLR: 1.1810  Epoch: 16  Global Step: 93300   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:31,516-Speed 3895.12 samples/sec  Loss 4.2652  LearningRate 0.0236  ProxyLR: 1.1805  Epoch: 16  Global Step: 93310   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:43:34,146-Speed 3893.78 samples/sec  Loss 4.1506  LearningRate 0.0236  ProxyLR: 1.1800  Epoch: 16  Global Step: 93320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:36,777-Speed 3893.45 samples/sec  Loss 4.2192  LearningRate 0.0236  ProxyLR: 1.1795  Epoch: 16  Global Step: 93330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:39,408-Speed 3892.52 samples/sec  Loss 4.1484  LearningRate 0.0236  ProxyLR: 1.1790  Epoch: 16  Global Step: 93340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:42,036-Speed 3897.78 samples/sec  Loss 4.1921  LearningRate 0.0236  ProxyLR: 1.1785  Epoch: 16  Global Step: 93350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:44,665-Speed 3895.71 samples/sec  Loss 4.1858  LearningRate 0.0236  ProxyLR: 1.1781  Epoch: 16  Global Step: 93360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:47,297-Speed 3892.41 samples/sec  Loss 4.1971  LearningRate 0.0236  ProxyLR: 1.1776  Epoch: 16  Global Step: 93370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:49,927-Speed 3893.63 samples/sec  Loss 4.3268  LearningRate 0.0235  ProxyLR: 1.1771  Epoch: 16  Global Step: 93380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:52,558-Speed 3893.77 samples/sec  Loss 4.2187  LearningRate 0.0235  ProxyLR: 1.1766  Epoch: 16  Global Step: 93390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:55,189-Speed 3892.93 samples/sec  Loss 4.1800  LearningRate 0.0235  ProxyLR: 1.1761  Epoch: 16  Global Step: 93400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:43:57,822-Speed 3889.92 samples/sec  Loss 4.1316  LearningRate 0.0235  ProxyLR: 1.1756  Epoch: 16  Global Step: 93410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:00,439-Speed 3913.36 samples/sec  Loss 4.2436  LearningRate 0.0235  ProxyLR: 1.1752  Epoch: 16  Global Step: 93420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:03,071-Speed 3891.71 samples/sec  Loss 4.1672  LearningRate 0.0235  ProxyLR: 1.1747  Epoch: 16  Global Step: 93430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:05,703-Speed 3891.11 samples/sec  Loss 4.2352  LearningRate 0.0235  ProxyLR: 1.1742  Epoch: 16  Global Step: 93440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:08,337-Speed 3889.22 samples/sec  Loss 4.2194  LearningRate 0.0235  ProxyLR: 1.1737  Epoch: 16  Global Step: 93450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:10,970-Speed 3889.46 samples/sec  Loss 4.2313  LearningRate 0.0235  ProxyLR: 1.1732  Epoch: 16  Global Step: 93460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:13,602-Speed 3892.46 samples/sec  Loss 4.2395  LearningRate 0.0235  ProxyLR: 1.1728  Epoch: 16  Global Step: 93470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:16,233-Speed 3893.02 samples/sec  Loss 4.2104  LearningRate 0.0234  ProxyLR: 1.1723  Epoch: 16  Global Step: 93480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:18,866-Speed 3890.16 samples/sec  Loss 4.1800  LearningRate 0.0234  ProxyLR: 1.1718  Epoch: 16  Global Step: 93490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:21,498-Speed 3891.27 samples/sec  Loss 4.1669  LearningRate 0.0234  ProxyLR: 1.1713  Epoch: 16  Global Step: 93500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:24,129-Speed 3893.28 samples/sec  Loss 4.1102  LearningRate 0.0234  ProxyLR: 1.1708  Epoch: 16  Global Step: 93510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:26,760-Speed 3892.46 samples/sec  Loss 4.2506  LearningRate 0.0234  ProxyLR: 1.1703  Epoch: 16  Global Step: 93520   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:44:29,380-Speed 3909.60 samples/sec  Loss 4.3027  LearningRate 0.0234  ProxyLR: 1.1699  Epoch: 16  Global Step: 93530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:32,012-Speed 3891.85 samples/sec  Loss 4.1067  LearningRate 0.0234  ProxyLR: 1.1694  Epoch: 16  Global Step: 93540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:34,641-Speed 3895.22 samples/sec  Loss 4.1615  LearningRate 0.0234  ProxyLR: 1.1689  Epoch: 16  Global Step: 93550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:37,272-Speed 3892.98 samples/sec  Loss 4.2307  LearningRate 0.0234  ProxyLR: 1.1684  Epoch: 16  Global Step: 93560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:39,902-Speed 3894.42 samples/sec  Loss 4.2803  LearningRate 0.0234  ProxyLR: 1.1679  Epoch: 16  Global Step: 93570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:42,531-Speed 3895.53 samples/sec  Loss 4.1388  LearningRate 0.0233  ProxyLR: 1.1675  Epoch: 16  Global Step: 93580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:45,161-Speed 3895.52 samples/sec  Loss 4.0901  LearningRate 0.0233  ProxyLR: 1.1670  Epoch: 16  Global Step: 93590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:47,793-Speed 3891.00 samples/sec  Loss 4.2100  LearningRate 0.0233  ProxyLR: 1.1665  Epoch: 16  Global Step: 93600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:50,424-Speed 3892.42 samples/sec  Loss 4.1448  LearningRate 0.0233  ProxyLR: 1.1660  Epoch: 16  Global Step: 93610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:53,055-Speed 3893.48 samples/sec  Loss 4.1879  LearningRate 0.0233  ProxyLR: 1.1655  Epoch: 16  Global Step: 93620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:44:55,688-Speed 3890.20 samples/sec  Loss 4.2347  LearningRate 0.0233  ProxyLR: 1.1651  Epoch: 16  Global Step: 93630   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:44:58,319-Speed 3892.26 samples/sec  Loss 4.1362  LearningRate 0.0233  ProxyLR: 1.1646  Epoch: 16  Global Step: 93640   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:45:00,951-Speed 3891.95 samples/sec  Loss 4.1613  LearningRate 0.0233  ProxyLR: 1.1641  Epoch: 16  Global Step: 93650   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:45:03,570-Speed 3912.20 samples/sec  Loss 4.1716  LearningRate 0.0233  ProxyLR: 1.1636  Epoch: 16  Global Step: 93660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:06,202-Speed 3890.53 samples/sec  Loss 4.0942  LearningRate 0.0233  ProxyLR: 1.1631  Epoch: 16  Global Step: 93670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:08,837-Speed 3887.61 samples/sec  Loss 4.2030  LearningRate 0.0233  ProxyLR: 1.1627  Epoch: 16  Global Step: 93680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:11,472-Speed 3886.56 samples/sec  Loss 4.1848  LearningRate 0.0232  ProxyLR: 1.1622  Epoch: 16  Global Step: 93690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:14,106-Speed 3889.71 samples/sec  Loss 4.1469  LearningRate 0.0232  ProxyLR: 1.1617  Epoch: 16  Global Step: 93700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:16,741-Speed 3886.41 samples/sec  Loss 4.1716  LearningRate 0.0232  ProxyLR: 1.1612  Epoch: 16  Global Step: 93710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:19,374-Speed 3889.74 samples/sec  Loss 4.1282  LearningRate 0.0232  ProxyLR: 1.1607  Epoch: 16  Global Step: 93720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:22,009-Speed 3887.27 samples/sec  Loss 4.1348  LearningRate 0.0232  ProxyLR: 1.1603  Epoch: 16  Global Step: 93730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:24,644-Speed 3887.48 samples/sec  Loss 4.1481  LearningRate 0.0232  ProxyLR: 1.1598  Epoch: 16  Global Step: 93740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:27,277-Speed 3889.45 samples/sec  Loss 4.1283  LearningRate 0.0232  ProxyLR: 1.1593  Epoch: 16  Global Step: 93750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:29,916-Speed 3881.86 samples/sec  Loss 4.1916  LearningRate 0.0232  ProxyLR: 1.1588  Epoch: 16  Global Step: 93760   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:45:32,536-Speed 3908.56 samples/sec  Loss 4.1409  LearningRate 0.0232  ProxyLR: 1.1583  Epoch: 16  Global Step: 93770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:35,168-Speed 3892.48 samples/sec  Loss 4.0624  LearningRate 0.0232  ProxyLR: 1.1579  Epoch: 16  Global Step: 93780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:37,797-Speed 3895.73 samples/sec  Loss 4.1658  LearningRate 0.0231  ProxyLR: 1.1574  Epoch: 16  Global Step: 93790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:40,426-Speed 3895.59 samples/sec  Loss 4.1204  LearningRate 0.0231  ProxyLR: 1.1569  Epoch: 16  Global Step: 93800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:43,055-Speed 3895.81 samples/sec  Loss 4.1497  LearningRate 0.0231  ProxyLR: 1.1564  Epoch: 16  Global Step: 93810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:45,684-Speed 3896.02 samples/sec  Loss 4.2197  LearningRate 0.0231  ProxyLR: 1.1560  Epoch: 16  Global Step: 93820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:48,314-Speed 3894.81 samples/sec  Loss 4.1719  LearningRate 0.0231  ProxyLR: 1.1555  Epoch: 16  Global Step: 93830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:50,944-Speed 3894.55 samples/sec  Loss 4.2293  LearningRate 0.0231  ProxyLR: 1.1550  Epoch: 16  Global Step: 93840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:53,574-Speed 3894.89 samples/sec  Loss 4.1710  LearningRate 0.0231  ProxyLR: 1.1545  Epoch: 16  Global Step: 93850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:56,203-Speed 3895.63 samples/sec  Loss 4.0828  LearningRate 0.0231  ProxyLR: 1.1540  Epoch: 16  Global Step: 93860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:45:58,833-Speed 3894.26 samples/sec  Loss 4.0947  LearningRate 0.0231  ProxyLR: 1.1536  Epoch: 16  Global Step: 93870   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:46:01,450-Speed 3915.11 samples/sec  Loss 4.1029  LearningRate 0.0231  ProxyLR: 1.1531  Epoch: 16  Global Step: 93880   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:04,080-Speed 3894.05 samples/sec  Loss 4.2460  LearningRate 0.0231  ProxyLR: 1.1526  Epoch: 16  Global Step: 93890   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:06,708-Speed 3896.92 samples/sec  Loss 4.1386  LearningRate 0.0230  ProxyLR: 1.1521  Epoch: 16  Global Step: 93900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:09,339-Speed 3892.86 samples/sec  Loss 4.1637  LearningRate 0.0230  ProxyLR: 1.1517  Epoch: 16  Global Step: 93910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:11,970-Speed 3894.34 samples/sec  Loss 4.1518  LearningRate 0.0230  ProxyLR: 1.1512  Epoch: 16  Global Step: 93920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:14,586-Speed 3914.27 samples/sec  Loss 4.0635  LearningRate 0.0230  ProxyLR: 1.1507  Epoch: 16  Global Step: 93930   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:17,217-Speed 3893.02 samples/sec  Loss 4.1963  LearningRate 0.0230  ProxyLR: 1.1502  Epoch: 16  Global Step: 93940   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:19,849-Speed 3891.26 samples/sec  Loss 4.1243  LearningRate 0.0230  ProxyLR: 1.1497  Epoch: 16  Global Step: 93950   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:22,480-Speed 3894.15 samples/sec  Loss 4.2038  LearningRate 0.0230  ProxyLR: 1.1493  Epoch: 16  Global Step: 93960   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:25,110-Speed 3893.38 samples/sec  Loss 4.1820  LearningRate 0.0230  ProxyLR: 1.1488  Epoch: 16  Global Step: 93970   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:27,741-Speed 3894.06 samples/sec  Loss 4.0914  LearningRate 0.0230  ProxyLR: 1.1483  Epoch: 16  Global Step: 93980   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:30,371-Speed 3894.30 samples/sec  Loss 4.1962  LearningRate 0.0230  ProxyLR: 1.1478  Epoch: 16  Global Step: 93990   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:33,000-Speed 3895.58 samples/sec  Loss 4.1158  LearningRate 0.0229  ProxyLR: 1.1474  Epoch: 16  Global Step: 94000   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:35,632-Speed 3891.80 samples/sec  Loss 4.0607  LearningRate 0.0229  ProxyLR: 1.1469  Epoch: 16  Global Step: 94010   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:38,263-Speed 3892.97 samples/sec  Loss 4.1570  LearningRate 0.0229  ProxyLR: 1.1464  Epoch: 16  Global Step: 94020   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:40,894-Speed 3892.56 samples/sec  Loss 4.0807  LearningRate 0.0229  ProxyLR: 1.1459  Epoch: 16  Global Step: 94030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:43,524-Speed 3895.27 samples/sec  Loss 4.0125  LearningRate 0.0229  ProxyLR: 1.1455  Epoch: 16  Global Step: 94040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:46,154-Speed 3894.05 samples/sec  Loss 4.1093  LearningRate 0.0229  ProxyLR: 1.1450  Epoch: 16  Global Step: 94050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:48,784-Speed 3895.18 samples/sec  Loss 4.1309  LearningRate 0.0229  ProxyLR: 1.1445  Epoch: 16  Global Step: 94060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:46:51,400-Speed 3915.36 samples/sec  Loss 3.9229  LearningRate 0.0229  ProxyLR: 1.1440  Epoch: 16  Global Step: 94070   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:54,028-Speed 3897.12 samples/sec  Loss 4.0741  LearningRate 0.0229  ProxyLR: 1.1435  Epoch: 16  Global Step: 94080   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:56,658-Speed 3895.04 samples/sec  Loss 4.1291  LearningRate 0.0229  ProxyLR: 1.1431  Epoch: 16  Global Step: 94090   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:46:59,290-Speed 3891.20 samples/sec  Loss 4.0784  LearningRate 0.0229  ProxyLR: 1.1426  Epoch: 16  Global Step: 94100   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:01,919-Speed 3896.05 samples/sec  Loss 4.0309  LearningRate 0.0228  ProxyLR: 1.1421  Epoch: 16  Global Step: 94110   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:04,548-Speed 3895.20 samples/sec  Loss 4.0246  LearningRate 0.0228  ProxyLR: 1.1416  Epoch: 16  Global Step: 94120   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:07,179-Speed 3893.99 samples/sec  Loss 3.9949  LearningRate 0.0228  ProxyLR: 1.1412  Epoch: 16  Global Step: 94130   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:09,808-Speed 3896.00 samples/sec  Loss 4.0741  LearningRate 0.0228  ProxyLR: 1.1407  Epoch: 16  Global Step: 94140   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:12,439-Speed 3893.00 samples/sec  Loss 4.0850  LearningRate 0.0228  ProxyLR: 1.1402  Epoch: 16  Global Step: 94150   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:15,071-Speed 3890.85 samples/sec  Loss 4.0429  LearningRate 0.0228  ProxyLR: 1.1397  Epoch: 16  Global Step: 94160   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:47:17,703-Speed 3892.29 samples/sec  Loss 4.0475  LearningRate 0.0228  ProxyLR: 1.1393  Epoch: 16  Global Step: 94170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:20,333-Speed 3894.40 samples/sec  Loss 4.0231  LearningRate 0.0228  ProxyLR: 1.1388  Epoch: 16  Global Step: 94180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:22,964-Speed 3893.02 samples/sec  Loss 4.0992  LearningRate 0.0228  ProxyLR: 1.1383  Epoch: 16  Global Step: 94190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:25,594-Speed 3894.68 samples/sec  Loss 4.0437  LearningRate 0.0228  ProxyLR: 1.1378  Epoch: 16  Global Step: 94200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:28,225-Speed 3892.04 samples/sec  Loss 3.9611  LearningRate 0.0227  ProxyLR: 1.1374  Epoch: 16  Global Step: 94210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:30,856-Speed 3892.70 samples/sec  Loss 4.1797  LearningRate 0.0227  ProxyLR: 1.1369  Epoch: 16  Global Step: 94220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:33,488-Speed 3892.75 samples/sec  Loss 4.1281  LearningRate 0.0227  ProxyLR: 1.1364  Epoch: 16  Global Step: 94230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:36,122-Speed 3888.34 samples/sec  Loss 4.0863  LearningRate 0.0227  ProxyLR: 1.1359  Epoch: 16  Global Step: 94240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:38,755-Speed 3889.74 samples/sec  Loss 4.1732  LearningRate 0.0227  ProxyLR: 1.1355  Epoch: 16  Global Step: 94250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:41,389-Speed 3888.65 samples/sec  Loss 4.0895  LearningRate 0.0227  ProxyLR: 1.1350  Epoch: 16  Global Step: 94260   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:44,026-Speed 3884.55 samples/sec  Loss 4.1119  LearningRate 0.0227  ProxyLR: 1.1345  Epoch: 16  Global Step: 94270   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:47:46,662-Speed 3884.71 samples/sec  Loss 4.0150  LearningRate 0.0227  ProxyLR: 1.1341  Epoch: 16  Global Step: 94280   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:47:49,298-Speed 3886.47 samples/sec  Loss 4.0588  LearningRate 0.0227  ProxyLR: 1.1336  Epoch: 16  Global Step: 94290   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:47:51,933-Speed 3886.09 samples/sec  Loss 3.9330  LearningRate 0.0227  ProxyLR: 1.1331  Epoch: 16  Global Step: 94300   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:47:54,556-Speed 3905.62 samples/sec  Loss 4.1353  LearningRate 0.0227  ProxyLR: 1.1326  Epoch: 16  Global Step: 94310   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:57,191-Speed 3887.67 samples/sec  Loss 4.1543  LearningRate 0.0226  ProxyLR: 1.1322  Epoch: 16  Global Step: 94320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:47:59,826-Speed 3886.27 samples/sec  Loss 4.0263  LearningRate 0.0226  ProxyLR: 1.1317  Epoch: 16  Global Step: 94330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:02,462-Speed 3886.14 samples/sec  Loss 4.0567  LearningRate 0.0226  ProxyLR: 1.1312  Epoch: 16  Global Step: 94340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:05,099-Speed 3883.46 samples/sec  Loss 3.9582  LearningRate 0.0226  ProxyLR: 1.1307  Epoch: 16  Global Step: 94350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:07,735-Speed 3886.20 samples/sec  Loss 4.0347  LearningRate 0.0226  ProxyLR: 1.1303  Epoch: 16  Global Step: 94360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:10,370-Speed 3886.91 samples/sec  Loss 4.0848  LearningRate 0.0226  ProxyLR: 1.1298  Epoch: 16  Global Step: 94370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:13,005-Speed 3887.13 samples/sec  Loss 4.0662  LearningRate 0.0226  ProxyLR: 1.1293  Epoch: 16  Global Step: 94380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:15,639-Speed 3888.08 samples/sec  Loss 4.0466  LearningRate 0.0226  ProxyLR: 1.1288  Epoch: 16  Global Step: 94390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:18,275-Speed 3885.92 samples/sec  Loss 4.0896  LearningRate 0.0226  ProxyLR: 1.1284  Epoch: 16  Global Step: 94400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:20,896-Speed 3908.05 samples/sec  Loss 4.1327  LearningRate 0.0226  ProxyLR: 1.1279  Epoch: 16  Global Step: 94410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:23,517-Speed 3907.92 samples/sec  Loss 3.9533  LearningRate 0.0225  ProxyLR: 1.1274  Epoch: 16  Global Step: 94420   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:26,153-Speed 3886.03 samples/sec  Loss 4.0666  LearningRate 0.0225  ProxyLR: 1.1270  Epoch: 16  Global Step: 94430   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:28,787-Speed 3887.44 samples/sec  Loss 4.1529  LearningRate 0.0225  ProxyLR: 1.1265  Epoch: 16  Global Step: 94440   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:31,420-Speed 3890.26 samples/sec  Loss 4.0952  LearningRate 0.0225  ProxyLR: 1.1260  Epoch: 16  Global Step: 94450   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:34,054-Speed 3889.39 samples/sec  Loss 4.0421  LearningRate 0.0225  ProxyLR: 1.1255  Epoch: 16  Global Step: 94460   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:36,686-Speed 3891.25 samples/sec  Loss 4.0316  LearningRate 0.0225  ProxyLR: 1.1251  Epoch: 16  Global Step: 94470   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:39,321-Speed 3887.61 samples/sec  Loss 4.1508  LearningRate 0.0225  ProxyLR: 1.1246  Epoch: 16  Global Step: 94480   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:41,956-Speed 3886.44 samples/sec  Loss 4.1764  LearningRate 0.0225  ProxyLR: 1.1241  Epoch: 16  Global Step: 94490   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:44,594-Speed 3882.68 samples/sec  Loss 3.9579  LearningRate 0.0225  ProxyLR: 1.1237  Epoch: 16  Global Step: 94500   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:47,228-Speed 3888.43 samples/sec  Loss 4.0364  LearningRate 0.0225  ProxyLR: 1.1232  Epoch: 16  Global Step: 94510   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:48:49,861-Speed 3890.61 samples/sec  Loss 4.0742  LearningRate 0.0225  ProxyLR: 1.1227  Epoch: 16  Global Step: 94520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:52,496-Speed 3886.49 samples/sec  Loss 4.0124  LearningRate 0.0224  ProxyLR: 1.1222  Epoch: 16  Global Step: 94530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:55,131-Speed 3887.18 samples/sec  Loss 4.0332  LearningRate 0.0224  ProxyLR: 1.1218  Epoch: 16  Global Step: 94540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:48:57,767-Speed 3885.37 samples/sec  Loss 4.0544  LearningRate 0.0224  ProxyLR: 1.1213  Epoch: 16  Global Step: 94550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:00,402-Speed 3888.12 samples/sec  Loss 4.1115  LearningRate 0.0224  ProxyLR: 1.1208  Epoch: 16  Global Step: 94560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:03,034-Speed 3890.76 samples/sec  Loss 3.9608  LearningRate 0.0224  ProxyLR: 1.1204  Epoch: 16  Global Step: 94570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:05,666-Speed 3891.58 samples/sec  Loss 4.0881  LearningRate 0.0224  ProxyLR: 1.1199  Epoch: 16  Global Step: 94580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:08,300-Speed 3888.05 samples/sec  Loss 4.0361  LearningRate 0.0224  ProxyLR: 1.1194  Epoch: 16  Global Step: 94590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:10,936-Speed 3886.54 samples/sec  Loss 4.0605  LearningRate 0.0224  ProxyLR: 1.1189  Epoch: 16  Global Step: 94600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:13,570-Speed 3888.60 samples/sec  Loss 4.0008  LearningRate 0.0224  ProxyLR: 1.1185  Epoch: 16  Global Step: 94610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:16,203-Speed 3889.42 samples/sec  Loss 3.9307  LearningRate 0.0224  ProxyLR: 1.1180  Epoch: 16  Global Step: 94620   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:49:18,820-Speed 3913.50 samples/sec  Loss 3.9988  LearningRate 0.0224  ProxyLR: 1.1175  Epoch: 16  Global Step: 94630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:21,453-Speed 3890.98 samples/sec  Loss 3.9497  LearningRate 0.0223  ProxyLR: 1.1171  Epoch: 16  Global Step: 94640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:24,086-Speed 3890.71 samples/sec  Loss 4.0347  LearningRate 0.0223  ProxyLR: 1.1166  Epoch: 16  Global Step: 94650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:26,718-Speed 3890.72 samples/sec  Loss 4.0211  LearningRate 0.0223  ProxyLR: 1.1161  Epoch: 16  Global Step: 94660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:29,351-Speed 3889.86 samples/sec  Loss 3.9443  LearningRate 0.0223  ProxyLR: 1.1156  Epoch: 16  Global Step: 94670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:49:31,969-Speed 3912.34 samples/sec  Loss 4.0699  LearningRate 0.0223  ProxyLR: 1.1152  Epoch: 16  Global Step: 94680   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:34,602-Speed 3890.19 samples/sec  Loss 4.0517  LearningRate 0.0223  ProxyLR: 1.1147  Epoch: 16  Global Step: 94690   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:37,234-Speed 3892.13 samples/sec  Loss 3.9654  LearningRate 0.0223  ProxyLR: 1.1142  Epoch: 16  Global Step: 94700   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:39,864-Speed 3894.39 samples/sec  Loss 3.9412  LearningRate 0.0223  ProxyLR: 1.1138  Epoch: 16  Global Step: 94710   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:42,493-Speed 3895.96 samples/sec  Loss 3.9708  LearningRate 0.0223  ProxyLR: 1.1133  Epoch: 16  Global Step: 94720   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:45,123-Speed 3894.54 samples/sec  Loss 3.9781  LearningRate 0.0223  ProxyLR: 1.1128  Epoch: 16  Global Step: 94730   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:47,755-Speed 3891.45 samples/sec  Loss 3.9991  LearningRate 0.0222  ProxyLR: 1.1124  Epoch: 16  Global Step: 94740   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:50,385-Speed 3894.01 samples/sec  Loss 3.9594  LearningRate 0.0222  ProxyLR: 1.1119  Epoch: 16  Global Step: 94750   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:53,013-Speed 3897.13 samples/sec  Loss 4.0438  LearningRate 0.0222  ProxyLR: 1.1114  Epoch: 16  Global Step: 94760   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:55,643-Speed 3894.55 samples/sec  Loss 4.0712  LearningRate 0.0222  ProxyLR: 1.1110  Epoch: 16  Global Step: 94770   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:49:58,273-Speed 3894.71 samples/sec  Loss 3.9863  LearningRate 0.0222  ProxyLR: 1.1105  Epoch: 16  Global Step: 94780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:00,904-Speed 3893.40 samples/sec  Loss 4.0234  LearningRate 0.0222  ProxyLR: 1.1100  Epoch: 16  Global Step: 94790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:03,533-Speed 3895.89 samples/sec  Loss 3.9851  LearningRate 0.0222  ProxyLR: 1.1095  Epoch: 16  Global Step: 94800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:06,162-Speed 3896.51 samples/sec  Loss 3.9555  LearningRate 0.0222  ProxyLR: 1.1091  Epoch: 16  Global Step: 94810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:08,790-Speed 3896.71 samples/sec  Loss 3.9747  LearningRate 0.0222  ProxyLR: 1.1086  Epoch: 16  Global Step: 94820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:11,418-Speed 3897.27 samples/sec  Loss 3.9236  LearningRate 0.0222  ProxyLR: 1.1081  Epoch: 16  Global Step: 94830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:14,046-Speed 3899.05 samples/sec  Loss 4.0408  LearningRate 0.0222  ProxyLR: 1.1077  Epoch: 16  Global Step: 94840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:16,674-Speed 3896.85 samples/sec  Loss 3.9290  LearningRate 0.0221  ProxyLR: 1.1072  Epoch: 16  Global Step: 94850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:19,301-Speed 3898.54 samples/sec  Loss 3.9984  LearningRate 0.0221  ProxyLR: 1.1067  Epoch: 16  Global Step: 94860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:21,928-Speed 3899.01 samples/sec  Loss 3.9258  LearningRate 0.0221  ProxyLR: 1.1063  Epoch: 16  Global Step: 94870   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:24,555-Speed 3898.72 samples/sec  Loss 4.0225  LearningRate 0.0221  ProxyLR: 1.1058  Epoch: 16  Global Step: 94880   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:50:27,181-Speed 3900.72 samples/sec  Loss 4.0621  LearningRate 0.0221  ProxyLR: 1.1053  Epoch: 16  Global Step: 94890   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:50:29,793-Speed 3921.11 samples/sec  Loss 3.9774  LearningRate 0.0221  ProxyLR: 1.1049  Epoch: 16  Global Step: 94900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:32,419-Speed 3901.40 samples/sec  Loss 3.9877  LearningRate 0.0221  ProxyLR: 1.1044  Epoch: 16  Global Step: 94910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:35,044-Speed 3901.12 samples/sec  Loss 4.0556  LearningRate 0.0221  ProxyLR: 1.1039  Epoch: 16  Global Step: 94920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:37,671-Speed 3899.22 samples/sec  Loss 3.9316  LearningRate 0.0221  ProxyLR: 1.1035  Epoch: 16  Global Step: 94930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:40,296-Speed 3902.40 samples/sec  Loss 3.9694  LearningRate 0.0221  ProxyLR: 1.1030  Epoch: 16  Global Step: 94940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:42,921-Speed 3901.32 samples/sec  Loss 3.9759  LearningRate 0.0221  ProxyLR: 1.1025  Epoch: 16  Global Step: 94950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:45,547-Speed 3900.18 samples/sec  Loss 3.9418  LearningRate 0.0220  ProxyLR: 1.1021  Epoch: 16  Global Step: 94960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:48,172-Speed 3902.49 samples/sec  Loss 3.8951  LearningRate 0.0220  ProxyLR: 1.1016  Epoch: 16  Global Step: 94970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:50,797-Speed 3901.70 samples/sec  Loss 4.0109  LearningRate 0.0220  ProxyLR: 1.1011  Epoch: 16  Global Step: 94980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:53,423-Speed 3900.35 samples/sec  Loss 3.9267  LearningRate 0.0220  ProxyLR: 1.1007  Epoch: 16  Global Step: 94990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:56,035-Speed 3920.81 samples/sec  Loss 3.9446  LearningRate 0.0220  ProxyLR: 1.1002  Epoch: 16  Global Step: 95000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:50:58,663-Speed 3897.80 samples/sec  Loss 3.9442  LearningRate 0.0220  ProxyLR: 1.0997  Epoch: 16  Global Step: 95010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:01,291-Speed 3897.35 samples/sec  Loss 3.9166  LearningRate 0.0220  ProxyLR: 1.0993  Epoch: 16  Global Step: 95020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:03,919-Speed 3898.50 samples/sec  Loss 3.9477  LearningRate 0.0220  ProxyLR: 1.0988  Epoch: 16  Global Step: 95030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:06,545-Speed 3899.77 samples/sec  Loss 3.8594  LearningRate 0.0220  ProxyLR: 1.0983  Epoch: 16  Global Step: 95040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:09,172-Speed 3899.25 samples/sec  Loss 3.8888  LearningRate 0.0220  ProxyLR: 1.0979  Epoch: 16  Global Step: 95050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:11,798-Speed 3900.47 samples/sec  Loss 3.8477  LearningRate 0.0219  ProxyLR: 1.0974  Epoch: 16  Global Step: 95060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:14,422-Speed 3903.53 samples/sec  Loss 3.9774  LearningRate 0.0219  ProxyLR: 1.0969  Epoch: 16  Global Step: 95070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:17,048-Speed 3900.35 samples/sec  Loss 3.8757  LearningRate 0.0219  ProxyLR: 1.0965  Epoch: 16  Global Step: 95080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:19,674-Speed 3900.08 samples/sec  Loss 4.0367  LearningRate 0.0219  ProxyLR: 1.0960  Epoch: 16  Global Step: 95090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:22,301-Speed 3899.34 samples/sec  Loss 3.9157  LearningRate 0.0219  ProxyLR: 1.0955  Epoch: 16  Global Step: 95100   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:51:24,926-Speed 3901.34 samples/sec  Loss 3.8945  LearningRate 0.0219  ProxyLR: 1.0951  Epoch: 16  Global Step: 95110   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:51:27,539-Speed 3919.82 samples/sec  Loss 3.7221  LearningRate 0.0219  ProxyLR: 1.0946  Epoch: 16  Global Step: 95120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:30,165-Speed 3901.70 samples/sec  Loss 3.9023  LearningRate 0.0219  ProxyLR: 1.0941  Epoch: 16  Global Step: 95130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:32,790-Speed 3900.79 samples/sec  Loss 3.8935  LearningRate 0.0219  ProxyLR: 1.0937  Epoch: 16  Global Step: 95140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:35,416-Speed 3901.31 samples/sec  Loss 4.0446  LearningRate 0.0219  ProxyLR: 1.0932  Epoch: 16  Global Step: 95150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:38,041-Speed 3901.19 samples/sec  Loss 3.8941  LearningRate 0.0219  ProxyLR: 1.0927  Epoch: 16  Global Step: 95160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:40,666-Speed 3902.06 samples/sec  Loss 3.8465  LearningRate 0.0218  ProxyLR: 1.0923  Epoch: 16  Global Step: 95170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:43,294-Speed 3897.90 samples/sec  Loss 3.9918  LearningRate 0.0218  ProxyLR: 1.0918  Epoch: 16  Global Step: 95180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:45,920-Speed 3900.05 samples/sec  Loss 3.9579  LearningRate 0.0218  ProxyLR: 1.0913  Epoch: 16  Global Step: 95190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:48,547-Speed 3899.31 samples/sec  Loss 4.0018  LearningRate 0.0218  ProxyLR: 1.0909  Epoch: 16  Global Step: 95200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:51,172-Speed 3901.28 samples/sec  Loss 3.9381  LearningRate 0.0218  ProxyLR: 1.0904  Epoch: 16  Global Step: 95210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:53,785-Speed 3919.42 samples/sec  Loss 4.0563  LearningRate 0.0218  ProxyLR: 1.0900  Epoch: 16  Global Step: 95220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:56,411-Speed 3901.11 samples/sec  Loss 3.9767  LearningRate 0.0218  ProxyLR: 1.0895  Epoch: 16  Global Step: 95230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:51:59,039-Speed 3898.46 samples/sec  Loss 3.8188  LearningRate 0.0218  ProxyLR: 1.0890  Epoch: 16  Global Step: 95240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:01,649-Speed 3923.67 samples/sec  Loss 4.0429  LearningRate 0.0218  ProxyLR: 1.0886  Epoch: 16  Global Step: 95250   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:04,274-Speed 3901.76 samples/sec  Loss 3.9598  LearningRate 0.0218  ProxyLR: 1.0881  Epoch: 16  Global Step: 95260   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:06,900-Speed 3900.97 samples/sec  Loss 3.9687  LearningRate 0.0218  ProxyLR: 1.0876  Epoch: 16  Global Step: 95270   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:09,526-Speed 3900.15 samples/sec  Loss 3.8505  LearningRate 0.0217  ProxyLR: 1.0872  Epoch: 16  Global Step: 95280   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:12,151-Speed 3902.15 samples/sec  Loss 3.9277  LearningRate 0.0217  ProxyLR: 1.0867  Epoch: 16  Global Step: 95290   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:14,778-Speed 3898.63 samples/sec  Loss 3.8781  LearningRate 0.0217  ProxyLR: 1.0862  Epoch: 16  Global Step: 95300   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:17,404-Speed 3900.56 samples/sec  Loss 3.9478  LearningRate 0.0217  ProxyLR: 1.0858  Epoch: 16  Global Step: 95310   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:20,030-Speed 3901.10 samples/sec  Loss 3.9404  LearningRate 0.0217  ProxyLR: 1.0853  Epoch: 16  Global Step: 95320   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:22,655-Speed 3901.85 samples/sec  Loss 3.8087  LearningRate 0.0217  ProxyLR: 1.0848  Epoch: 16  Global Step: 95330   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:25,280-Speed 3901.71 samples/sec  Loss 3.9428  LearningRate 0.0217  ProxyLR: 1.0844  Epoch: 16  Global Step: 95340   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:52:27,903-Speed 3904.09 samples/sec  Loss 3.9699  LearningRate 0.0217  ProxyLR: 1.0839  Epoch: 16  Global Step: 95350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:30,528-Speed 3901.75 samples/sec  Loss 3.8497  LearningRate 0.0217  ProxyLR: 1.0835  Epoch: 16  Global Step: 95360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:33,154-Speed 3901.05 samples/sec  Loss 3.9997  LearningRate 0.0217  ProxyLR: 1.0830  Epoch: 16  Global Step: 95370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:35,779-Speed 3901.27 samples/sec  Loss 3.8176  LearningRate 0.0217  ProxyLR: 1.0825  Epoch: 16  Global Step: 95380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:38,403-Speed 3904.45 samples/sec  Loss 4.0196  LearningRate 0.0216  ProxyLR: 1.0821  Epoch: 16  Global Step: 95390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:41,028-Speed 3900.97 samples/sec  Loss 3.8212  LearningRate 0.0216  ProxyLR: 1.0816  Epoch: 16  Global Step: 95400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:43,653-Speed 3902.24 samples/sec  Loss 3.8945  LearningRate 0.0216  ProxyLR: 1.0811  Epoch: 16  Global Step: 95410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:46,279-Speed 3900.94 samples/sec  Loss 3.9506  LearningRate 0.0216  ProxyLR: 1.0807  Epoch: 16  Global Step: 95420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:48,902-Speed 3904.36 samples/sec  Loss 3.8727  LearningRate 0.0216  ProxyLR: 1.0802  Epoch: 16  Global Step: 95430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:51,528-Speed 3901.38 samples/sec  Loss 3.8133  LearningRate 0.0216  ProxyLR: 1.0798  Epoch: 16  Global Step: 95440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:52:54,152-Speed 3903.29 samples/sec  Loss 4.0187  LearningRate 0.0216  ProxyLR: 1.0793  Epoch: 16  Global Step: 95450   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:52:56,777-Speed 3901.80 samples/sec  Loss 4.0204  LearningRate 0.0216  ProxyLR: 1.0788  Epoch: 16  Global Step: 95460   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:52:59,402-Speed 3901.55 samples/sec  Loss 3.8742  LearningRate 0.0216  ProxyLR: 1.0784  Epoch: 16  Global Step: 95470   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:53:02,014-Speed 3922.12 samples/sec  Loss 3.9005  LearningRate 0.0216  ProxyLR: 1.0779  Epoch: 16  Global Step: 95480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:04,638-Speed 3902.80 samples/sec  Loss 3.8989  LearningRate 0.0215  ProxyLR: 1.0774  Epoch: 16  Global Step: 95490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:07,265-Speed 3898.92 samples/sec  Loss 4.0261  LearningRate 0.0215  ProxyLR: 1.0770  Epoch: 16  Global Step: 95500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:09,890-Speed 3901.53 samples/sec  Loss 3.7926  LearningRate 0.0215  ProxyLR: 1.0765  Epoch: 16  Global Step: 95510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:12,514-Speed 3903.68 samples/sec  Loss 3.9940  LearningRate 0.0215  ProxyLR: 1.0761  Epoch: 16  Global Step: 95520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:15,140-Speed 3900.60 samples/sec  Loss 3.9673  LearningRate 0.0215  ProxyLR: 1.0756  Epoch: 16  Global Step: 95530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:17,767-Speed 3899.56 samples/sec  Loss 3.9470  LearningRate 0.0215  ProxyLR: 1.0751  Epoch: 16  Global Step: 95540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:20,392-Speed 3901.10 samples/sec  Loss 3.9828  LearningRate 0.0215  ProxyLR: 1.0747  Epoch: 16  Global Step: 95550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:23,018-Speed 3901.09 samples/sec  Loss 3.9478  LearningRate 0.0215  ProxyLR: 1.0742  Epoch: 16  Global Step: 95560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:25,642-Speed 3902.79 samples/sec  Loss 3.8606  LearningRate 0.0215  ProxyLR: 1.0738  Epoch: 16  Global Step: 95570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:28,268-Speed 3900.44 samples/sec  Loss 3.8877  LearningRate 0.0215  ProxyLR: 1.0733  Epoch: 16  Global Step: 95580   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:53:30,893-Speed 3902.84 samples/sec  Loss 3.8016  LearningRate 0.0215  ProxyLR: 1.0728  Epoch: 16  Global Step: 95590   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:53:33,520-Speed 3898.55 samples/sec  Loss 3.8500  LearningRate 0.0214  ProxyLR: 1.0724  Epoch: 16  Global Step: 95600   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:53:36,132-Speed 3920.88 samples/sec  Loss 3.9340  LearningRate 0.0214  ProxyLR: 1.0719  Epoch: 16  Global Step: 95610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:38,756-Speed 3903.79 samples/sec  Loss 3.8733  LearningRate 0.0214  ProxyLR: 1.0715  Epoch: 16  Global Step: 95620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:41,380-Speed 3902.66 samples/sec  Loss 3.9338  LearningRate 0.0214  ProxyLR: 1.0710  Epoch: 16  Global Step: 95630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:44,005-Speed 3903.28 samples/sec  Loss 3.7693  LearningRate 0.0214  ProxyLR: 1.0705  Epoch: 16  Global Step: 95640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:46,629-Speed 3902.58 samples/sec  Loss 3.8227  LearningRate 0.0214  ProxyLR: 1.0701  Epoch: 16  Global Step: 95650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:49,253-Speed 3904.10 samples/sec  Loss 3.8657  LearningRate 0.0214  ProxyLR: 1.0696  Epoch: 16  Global Step: 95660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:53:51,864-Speed 3923.07 samples/sec  Loss 3.9196  LearningRate 0.0214  ProxyLR: 1.0691  Epoch: 16  Global Step: 95670   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:53:54,487-Speed 3904.25 samples/sec  Loss 3.9763  LearningRate 0.0214  ProxyLR: 1.0687  Epoch: 16  Global Step: 95680   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:53:57,112-Speed 3902.02 samples/sec  Loss 3.8223  LearningRate 0.0214  ProxyLR: 1.0682  Epoch: 16  Global Step: 95690   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:53:59,735-Speed 3904.40 samples/sec  Loss 3.9000  LearningRate 0.0214  ProxyLR: 1.0678  Epoch: 16  Global Step: 95700   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:02,360-Speed 3901.80 samples/sec  Loss 3.9696  LearningRate 0.0213  ProxyLR: 1.0673  Epoch: 16  Global Step: 95710   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:04,985-Speed 3902.38 samples/sec  Loss 3.9621  LearningRate 0.0213  ProxyLR: 1.0669  Epoch: 16  Global Step: 95720   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:07,609-Speed 3903.78 samples/sec  Loss 3.8639  LearningRate 0.0213  ProxyLR: 1.0664  Epoch: 16  Global Step: 95730   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:10,236-Speed 3899.18 samples/sec  Loss 3.8200  LearningRate 0.0213  ProxyLR: 1.0659  Epoch: 16  Global Step: 95740   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:12,863-Speed 3898.23 samples/sec  Loss 3.8797  LearningRate 0.0213  ProxyLR: 1.0655  Epoch: 16  Global Step: 95750   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:15,487-Speed 3903.44 samples/sec  Loss 3.8745  LearningRate 0.0213  ProxyLR: 1.0650  Epoch: 16  Global Step: 95760   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:54:18,112-Speed 3902.53 samples/sec  Loss 3.8799  LearningRate 0.0213  ProxyLR: 1.0646  Epoch: 16  Global Step: 95770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:20,740-Speed 3897.63 samples/sec  Loss 3.9104  LearningRate 0.0213  ProxyLR: 1.0641  Epoch: 16  Global Step: 95780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:23,365-Speed 3901.83 samples/sec  Loss 3.8110  LearningRate 0.0213  ProxyLR: 1.0636  Epoch: 16  Global Step: 95790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:25,988-Speed 3903.93 samples/sec  Loss 3.8096  LearningRate 0.0213  ProxyLR: 1.0632  Epoch: 16  Global Step: 95800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:28,615-Speed 3899.45 samples/sec  Loss 3.9399  LearningRate 0.0213  ProxyLR: 1.0627  Epoch: 16  Global Step: 95810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:31,240-Speed 3902.37 samples/sec  Loss 3.8004  LearningRate 0.0212  ProxyLR: 1.0623  Epoch: 16  Global Step: 95820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:33,866-Speed 3899.65 samples/sec  Loss 3.9158  LearningRate 0.0212  ProxyLR: 1.0618  Epoch: 16  Global Step: 95830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:36,490-Speed 3903.55 samples/sec  Loss 3.7894  LearningRate 0.0212  ProxyLR: 1.0613  Epoch: 16  Global Step: 95840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:39,115-Speed 3903.17 samples/sec  Loss 3.9670  LearningRate 0.0212  ProxyLR: 1.0609  Epoch: 16  Global Step: 95850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:41,738-Speed 3904.92 samples/sec  Loss 3.8585  LearningRate 0.0212  ProxyLR: 1.0604  Epoch: 16  Global Step: 95860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:44,363-Speed 3901.24 samples/sec  Loss 3.8370  LearningRate 0.0212  ProxyLR: 1.0600  Epoch: 16  Global Step: 95870   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:54:46,975-Speed 3920.72 samples/sec  Loss 4.0214  LearningRate 0.0212  ProxyLR: 1.0595  Epoch: 16  Global Step: 95880   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:49,600-Speed 3903.07 samples/sec  Loss 3.8071  LearningRate 0.0212  ProxyLR: 1.0591  Epoch: 16  Global Step: 95890   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:52,224-Speed 3903.01 samples/sec  Loss 3.8455  LearningRate 0.0212  ProxyLR: 1.0586  Epoch: 16  Global Step: 95900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:54,850-Speed 3900.77 samples/sec  Loss 3.8895  LearningRate 0.0212  ProxyLR: 1.0581  Epoch: 16  Global Step: 95910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:54:57,473-Speed 3904.93 samples/sec  Loss 3.7663  LearningRate 0.0212  ProxyLR: 1.0577  Epoch: 16  Global Step: 95920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:00,098-Speed 3901.28 samples/sec  Loss 3.9446  LearningRate 0.0211  ProxyLR: 1.0572  Epoch: 16  Global Step: 95930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:02,724-Speed 3900.85 samples/sec  Loss 3.9426  LearningRate 0.0211  ProxyLR: 1.0568  Epoch: 16  Global Step: 95940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:05,349-Speed 3901.69 samples/sec  Loss 3.8352  LearningRate 0.0211  ProxyLR: 1.0563  Epoch: 16  Global Step: 95950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:07,972-Speed 3904.73 samples/sec  Loss 3.8684  LearningRate 0.0211  ProxyLR: 1.0558  Epoch: 16  Global Step: 95960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:10,597-Speed 3901.70 samples/sec  Loss 3.7530  LearningRate 0.0211  ProxyLR: 1.0554  Epoch: 16  Global Step: 95970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:13,222-Speed 3902.53 samples/sec  Loss 3.8761  LearningRate 0.0211  ProxyLR: 1.0549  Epoch: 16  Global Step: 95980   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:55:15,833-Speed 3922.14 samples/sec  Loss 3.7355  LearningRate 0.0211  ProxyLR: 1.0545  Epoch: 16  Global Step: 95990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:18,458-Speed 3901.53 samples/sec  Loss 3.7586  LearningRate 0.0211  ProxyLR: 1.0540  Epoch: 16  Global Step: 96000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:21,087-Speed 3896.48 samples/sec  Loss 3.8779  LearningRate 0.0211  ProxyLR: 1.0536  Epoch: 16  Global Step: 96010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:23,717-Speed 3893.79 samples/sec  Loss 3.8338  LearningRate 0.0211  ProxyLR: 1.0531  Epoch: 16  Global Step: 96020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:26,348-Speed 3893.79 samples/sec  Loss 3.8678  LearningRate 0.0211  ProxyLR: 1.0527  Epoch: 16  Global Step: 96030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:28,977-Speed 3895.65 samples/sec  Loss 3.8073  LearningRate 0.0210  ProxyLR: 1.0522  Epoch: 16  Global Step: 96040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:31,607-Speed 3894.99 samples/sec  Loss 3.8398  LearningRate 0.0210  ProxyLR: 1.0517  Epoch: 16  Global Step: 96050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:55:34,228-Speed 3908.12 samples/sec  Loss 3.7657  LearningRate 0.0210  ProxyLR: 1.0513  Epoch: 16  Global Step: 96060   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:36,863-Speed 3886.24 samples/sec  Loss 3.8989  LearningRate 0.0210  ProxyLR: 1.0508  Epoch: 16  Global Step: 96070   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:39,498-Speed 3888.33 samples/sec  Loss 3.7724  LearningRate 0.0210  ProxyLR: 1.0504  Epoch: 16  Global Step: 96080   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:42,130-Speed 3891.62 samples/sec  Loss 3.8655  LearningRate 0.0210  ProxyLR: 1.0499  Epoch: 16  Global Step: 96090   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:44,762-Speed 3891.58 samples/sec  Loss 3.7875  LearningRate 0.0210  ProxyLR: 1.0495  Epoch: 16  Global Step: 96100   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:47,393-Speed 3892.73 samples/sec  Loss 3.7614  LearningRate 0.0210  ProxyLR: 1.0490  Epoch: 16  Global Step: 96110   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:50,023-Speed 3895.00 samples/sec  Loss 3.9208  LearningRate 0.0210  ProxyLR: 1.0485  Epoch: 16  Global Step: 96120   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:52,653-Speed 3893.79 samples/sec  Loss 3.9295  LearningRate 0.0210  ProxyLR: 1.0481  Epoch: 16  Global Step: 96130   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:55,284-Speed 3893.49 samples/sec  Loss 3.8354  LearningRate 0.0210  ProxyLR: 1.0476  Epoch: 16  Global Step: 96140   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:55:57,917-Speed 3890.16 samples/sec  Loss 3.7817  LearningRate 0.0209  ProxyLR: 1.0472  Epoch: 16  Global Step: 96150   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:00,550-Speed 3890.04 samples/sec  Loss 3.8495  LearningRate 0.0209  ProxyLR: 1.0467  Epoch: 16  Global Step: 96160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:03,167-Speed 3912.54 samples/sec  Loss 3.8765  LearningRate 0.0209  ProxyLR: 1.0463  Epoch: 16  Global Step: 96170   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:05,800-Speed 3890.93 samples/sec  Loss 3.7120  LearningRate 0.0209  ProxyLR: 1.0458  Epoch: 16  Global Step: 96180   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:08,431-Speed 3892.34 samples/sec  Loss 3.8152  LearningRate 0.0209  ProxyLR: 1.0454  Epoch: 16  Global Step: 96190   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:11,062-Speed 3893.07 samples/sec  Loss 3.8731  LearningRate 0.0209  ProxyLR: 1.0449  Epoch: 16  Global Step: 96200   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:13,693-Speed 3893.57 samples/sec  Loss 3.8128  LearningRate 0.0209  ProxyLR: 1.0445  Epoch: 16  Global Step: 96210   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:16,324-Speed 3892.69 samples/sec  Loss 3.8202  LearningRate 0.0209  ProxyLR: 1.0440  Epoch: 16  Global Step: 96220   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:18,955-Speed 3893.82 samples/sec  Loss 3.8492  LearningRate 0.0209  ProxyLR: 1.0435  Epoch: 16  Global Step: 96230   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:21,586-Speed 3892.57 samples/sec  Loss 3.8011  LearningRate 0.0209  ProxyLR: 1.0431  Epoch: 16  Global Step: 96240   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:24,219-Speed 3889.51 samples/sec  Loss 3.8224  LearningRate 0.0209  ProxyLR: 1.0426  Epoch: 16  Global Step: 96250   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:26,850-Speed 3893.15 samples/sec  Loss 3.8089  LearningRate 0.0208  ProxyLR: 1.0422  Epoch: 16  Global Step: 96260   Fp16 Grad Scale: 131072  Required: 4 hours
Training: 2023-05-04 21:56:29,481-Speed 3892.81 samples/sec  Loss 3.8634  LearningRate 0.0208  ProxyLR: 1.0417  Epoch: 16  Global Step: 96270   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:32,112-Speed 3893.86 samples/sec  Loss 3.6938  LearningRate 0.0208  ProxyLR: 1.0413  Epoch: 16  Global Step: 96280   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:34,745-Speed 3890.34 samples/sec  Loss 3.8524  LearningRate 0.0208  ProxyLR: 1.0408  Epoch: 16  Global Step: 96290   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:37,377-Speed 3891.39 samples/sec  Loss 3.8189  LearningRate 0.0208  ProxyLR: 1.0404  Epoch: 16  Global Step: 96300   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:40,007-Speed 3894.78 samples/sec  Loss 3.7368  LearningRate 0.0208  ProxyLR: 1.0399  Epoch: 16  Global Step: 96310   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:42,638-Speed 3891.94 samples/sec  Loss 3.8902  LearningRate 0.0208  ProxyLR: 1.0395  Epoch: 16  Global Step: 96320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:45,269-Speed 3893.69 samples/sec  Loss 3.8665  LearningRate 0.0208  ProxyLR: 1.0390  Epoch: 16  Global Step: 96330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:47,901-Speed 3890.74 samples/sec  Loss 3.8634  LearningRate 0.0208  ProxyLR: 1.0385  Epoch: 16  Global Step: 96340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:50,533-Speed 3892.58 samples/sec  Loss 3.8136  LearningRate 0.0208  ProxyLR: 1.0381  Epoch: 16  Global Step: 96350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:53,163-Speed 3894.40 samples/sec  Loss 3.8987  LearningRate 0.0208  ProxyLR: 1.0376  Epoch: 16  Global Step: 96360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:55,780-Speed 3913.80 samples/sec  Loss 3.8586  LearningRate 0.0207  ProxyLR: 1.0372  Epoch: 16  Global Step: 96370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:56:58,414-Speed 3888.54 samples/sec  Loss 3.7784  LearningRate 0.0207  ProxyLR: 1.0367  Epoch: 16  Global Step: 96380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:01,044-Speed 3895.24 samples/sec  Loss 3.8641  LearningRate 0.0207  ProxyLR: 1.0363  Epoch: 16  Global Step: 96390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:03,675-Speed 3892.75 samples/sec  Loss 3.8537  LearningRate 0.0207  ProxyLR: 1.0358  Epoch: 16  Global Step: 96400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:06,306-Speed 3892.55 samples/sec  Loss 3.7999  LearningRate 0.0207  ProxyLR: 1.0354  Epoch: 16  Global Step: 96410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:08,937-Speed 3893.06 samples/sec  Loss 3.8214  LearningRate 0.0207  ProxyLR: 1.0349  Epoch: 16  Global Step: 96420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:11,568-Speed 3893.19 samples/sec  Loss 3.7378  LearningRate 0.0207  ProxyLR: 1.0345  Epoch: 16  Global Step: 96430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:14,199-Speed 3892.35 samples/sec  Loss 3.8606  LearningRate 0.0207  ProxyLR: 1.0340  Epoch: 16  Global Step: 96440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:16,831-Speed 3891.62 samples/sec  Loss 3.8185  LearningRate 0.0207  ProxyLR: 1.0336  Epoch: 16  Global Step: 96450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:19,462-Speed 3893.47 samples/sec  Loss 3.8219  LearningRate 0.0207  ProxyLR: 1.0331  Epoch: 16  Global Step: 96460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:22,095-Speed 3889.66 samples/sec  Loss 3.7857  LearningRate 0.0207  ProxyLR: 1.0327  Epoch: 16  Global Step: 96470   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:57:24,718-Speed 3905.19 samples/sec  Loss 3.8399  LearningRate 0.0206  ProxyLR: 1.0322  Epoch: 16  Global Step: 96480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:27,362-Speed 3874.37 samples/sec  Loss 3.7393  LearningRate 0.0206  ProxyLR: 1.0318  Epoch: 16  Global Step: 96490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:29,995-Speed 3889.90 samples/sec  Loss 3.8078  LearningRate 0.0206  ProxyLR: 1.0313  Epoch: 16  Global Step: 96500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:32,627-Speed 3890.48 samples/sec  Loss 3.8657  LearningRate 0.0206  ProxyLR: 1.0309  Epoch: 16  Global Step: 96510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:35,256-Speed 3896.17 samples/sec  Loss 3.7686  LearningRate 0.0206  ProxyLR: 1.0304  Epoch: 16  Global Step: 96520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:37,887-Speed 3893.01 samples/sec  Loss 3.8393  LearningRate 0.0206  ProxyLR: 1.0300  Epoch: 16  Global Step: 96530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:40,516-Speed 3895.74 samples/sec  Loss 3.8304  LearningRate 0.0206  ProxyLR: 1.0295  Epoch: 16  Global Step: 96540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:43,146-Speed 3895.55 samples/sec  Loss 3.7600  LearningRate 0.0206  ProxyLR: 1.0290  Epoch: 16  Global Step: 96550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:45,779-Speed 3889.19 samples/sec  Loss 3.7745  LearningRate 0.0206  ProxyLR: 1.0286  Epoch: 16  Global Step: 96560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:48,407-Speed 3898.01 samples/sec  Loss 3.7281  LearningRate 0.0206  ProxyLR: 1.0281  Epoch: 16  Global Step: 96570   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:57:51,033-Speed 3899.68 samples/sec  Loss 3.7474  LearningRate 0.0206  ProxyLR: 1.0277  Epoch: 16  Global Step: 96580   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:57:53,660-Speed 3899.21 samples/sec  Loss 3.7741  LearningRate 0.0205  ProxyLR: 1.0272  Epoch: 16  Global Step: 96590   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:57:56,288-Speed 3897.88 samples/sec  Loss 3.8363  LearningRate 0.0205  ProxyLR: 1.0268  Epoch: 16  Global Step: 96600   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:57:58,916-Speed 3898.30 samples/sec  Loss 3.7470  LearningRate 0.0205  ProxyLR: 1.0263  Epoch: 16  Global Step: 96610   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:58:01,544-Speed 3897.29 samples/sec  Loss 3.7106  LearningRate 0.0205  ProxyLR: 1.0259  Epoch: 16  Global Step: 96620   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:58:04,169-Speed 3901.60 samples/sec  Loss 3.7615  LearningRate 0.0205  ProxyLR: 1.0254  Epoch: 16  Global Step: 96630   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:58:06,780-Speed 3922.69 samples/sec  Loss 3.8002  LearningRate 0.0205  ProxyLR: 1.0250  Epoch: 16  Global Step: 96640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:09,403-Speed 3904.17 samples/sec  Loss 3.7350  LearningRate 0.0205  ProxyLR: 1.0245  Epoch: 16  Global Step: 96650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:12,087-Speed 3816.74 samples/sec  Loss 3.7127  LearningRate 0.0205  ProxyLR: 1.0241  Epoch: 16  Global Step: 96660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:20,940-Speed 1156.78 samples/sec  Loss 3.4511  LearningRate 0.0205  ProxyLR: 1.0236  Epoch: 17  Global Step: 96670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:23,628-Speed 3810.76 samples/sec  Loss 3.4718  LearningRate 0.0205  ProxyLR: 1.0232  Epoch: 17  Global Step: 96680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:26,264-Speed 3886.31 samples/sec  Loss 3.4602  LearningRate 0.0205  ProxyLR: 1.0227  Epoch: 17  Global Step: 96690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:28,895-Speed 3891.94 samples/sec  Loss 3.4311  LearningRate 0.0204  ProxyLR: 1.0223  Epoch: 17  Global Step: 96700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:31,527-Speed 3892.12 samples/sec  Loss 3.4826  LearningRate 0.0204  ProxyLR: 1.0218  Epoch: 17  Global Step: 96710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:34,162-Speed 3886.84 samples/sec  Loss 3.5176  LearningRate 0.0204  ProxyLR: 1.0214  Epoch: 17  Global Step: 96720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:36,799-Speed 3884.20 samples/sec  Loss 3.3718  LearningRate 0.0204  ProxyLR: 1.0209  Epoch: 17  Global Step: 96730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:39,457-Speed 3853.65 samples/sec  Loss 3.4411  LearningRate 0.0204  ProxyLR: 1.0205  Epoch: 17  Global Step: 96740   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:58:42,079-Speed 3907.06 samples/sec  Loss 3.4324  LearningRate 0.0204  ProxyLR: 1.0200  Epoch: 17  Global Step: 96750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:44,714-Speed 3887.28 samples/sec  Loss 3.4336  LearningRate 0.0204  ProxyLR: 1.0196  Epoch: 17  Global Step: 96760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:47,344-Speed 3893.50 samples/sec  Loss 3.4694  LearningRate 0.0204  ProxyLR: 1.0191  Epoch: 17  Global Step: 96770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:50,006-Speed 3848.18 samples/sec  Loss 3.4746  LearningRate 0.0204  ProxyLR: 1.0187  Epoch: 17  Global Step: 96780   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:52,638-Speed 3891.48 samples/sec  Loss 3.5167  LearningRate 0.0204  ProxyLR: 1.0182  Epoch: 17  Global Step: 96790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:55,291-Speed 3860.24 samples/sec  Loss 3.3776  LearningRate 0.0204  ProxyLR: 1.0178  Epoch: 17  Global Step: 96800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:58:57,927-Speed 3886.08 samples/sec  Loss 3.4288  LearningRate 0.0203  ProxyLR: 1.0173  Epoch: 17  Global Step: 96810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:00,560-Speed 3891.21 samples/sec  Loss 3.3951  LearningRate 0.0203  ProxyLR: 1.0169  Epoch: 17  Global Step: 96820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:03,191-Speed 3892.30 samples/sec  Loss 3.4702  LearningRate 0.0203  ProxyLR: 1.0165  Epoch: 17  Global Step: 96830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:05,829-Speed 3882.88 samples/sec  Loss 3.4593  LearningRate 0.0203  ProxyLR: 1.0160  Epoch: 17  Global Step: 96840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:08,459-Speed 3894.85 samples/sec  Loss 3.4611  LearningRate 0.0203  ProxyLR: 1.0156  Epoch: 17  Global Step: 96850   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:59:11,091-Speed 3891.91 samples/sec  Loss 3.4672  LearningRate 0.0203  ProxyLR: 1.0151  Epoch: 17  Global Step: 96860   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:59:13,724-Speed 3890.13 samples/sec  Loss 3.4042  LearningRate 0.0203  ProxyLR: 1.0147  Epoch: 17  Global Step: 96870   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:59:16,354-Speed 3894.43 samples/sec  Loss 3.3693  LearningRate 0.0203  ProxyLR: 1.0142  Epoch: 17  Global Step: 96880   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:59:18,972-Speed 3912.28 samples/sec  Loss 3.3374  LearningRate 0.0203  ProxyLR: 1.0138  Epoch: 17  Global Step: 96890   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:21,608-Speed 3885.70 samples/sec  Loss 3.4358  LearningRate 0.0203  ProxyLR: 1.0133  Epoch: 17  Global Step: 96900   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:24,239-Speed 3892.85 samples/sec  Loss 3.4840  LearningRate 0.0203  ProxyLR: 1.0129  Epoch: 17  Global Step: 96910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:26,868-Speed 3895.22 samples/sec  Loss 3.3743  LearningRate 0.0202  ProxyLR: 1.0124  Epoch: 17  Global Step: 96920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:29,498-Speed 3895.42 samples/sec  Loss 3.4117  LearningRate 0.0202  ProxyLR: 1.0120  Epoch: 17  Global Step: 96930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:32,127-Speed 3896.04 samples/sec  Loss 3.3434  LearningRate 0.0202  ProxyLR: 1.0115  Epoch: 17  Global Step: 96940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:34,755-Speed 3897.31 samples/sec  Loss 3.4510  LearningRate 0.0202  ProxyLR: 1.0111  Epoch: 17  Global Step: 96950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:37,383-Speed 3897.48 samples/sec  Loss 3.3833  LearningRate 0.0202  ProxyLR: 1.0106  Epoch: 17  Global Step: 96960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:40,063-Speed 3823.01 samples/sec  Loss 3.4347  LearningRate 0.0202  ProxyLR: 1.0102  Epoch: 17  Global Step: 96970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:42,690-Speed 3898.48 samples/sec  Loss 3.4942  LearningRate 0.0202  ProxyLR: 1.0097  Epoch: 17  Global Step: 96980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:45,320-Speed 3894.20 samples/sec  Loss 3.3126  LearningRate 0.0202  ProxyLR: 1.0093  Epoch: 17  Global Step: 96990   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 21:59:47,937-Speed 3914.87 samples/sec  Loss 3.5679  LearningRate 0.0202  ProxyLR: 1.0088  Epoch: 17  Global Step: 97000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:50,569-Speed 3891.38 samples/sec  Loss 3.4323  LearningRate 0.0202  ProxyLR: 1.0084  Epoch: 17  Global Step: 97010   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:53,200-Speed 3893.33 samples/sec  Loss 3.4069  LearningRate 0.0202  ProxyLR: 1.0079  Epoch: 17  Global Step: 97020   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:55,831-Speed 3892.23 samples/sec  Loss 3.4939  LearningRate 0.0201  ProxyLR: 1.0075  Epoch: 17  Global Step: 97030   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 21:59:58,463-Speed 3891.93 samples/sec  Loss 3.4151  LearningRate 0.0201  ProxyLR: 1.0071  Epoch: 17  Global Step: 97040   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:01,094-Speed 3893.19 samples/sec  Loss 3.4449  LearningRate 0.0201  ProxyLR: 1.0066  Epoch: 17  Global Step: 97050   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:03,728-Speed 3888.43 samples/sec  Loss 3.4684  LearningRate 0.0201  ProxyLR: 1.0062  Epoch: 17  Global Step: 97060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:06,360-Speed 3891.86 samples/sec  Loss 3.4119  LearningRate 0.0201  ProxyLR: 1.0057  Epoch: 17  Global Step: 97070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:08,993-Speed 3889.68 samples/sec  Loss 3.4423  LearningRate 0.0201  ProxyLR: 1.0053  Epoch: 17  Global Step: 97080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:11,623-Speed 3894.16 samples/sec  Loss 3.4392  LearningRate 0.0201  ProxyLR: 1.0048  Epoch: 17  Global Step: 97090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:14,256-Speed 3891.07 samples/sec  Loss 3.4189  LearningRate 0.0201  ProxyLR: 1.0044  Epoch: 17  Global Step: 97100   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:16,889-Speed 3889.17 samples/sec  Loss 3.3092  LearningRate 0.0201  ProxyLR: 1.0039  Epoch: 17  Global Step: 97110   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:19,521-Speed 3892.08 samples/sec  Loss 3.4158  LearningRate 0.0201  ProxyLR: 1.0035  Epoch: 17  Global Step: 97120   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:22,139-Speed 3912.48 samples/sec  Loss 3.4123  LearningRate 0.0201  ProxyLR: 1.0030  Epoch: 17  Global Step: 97130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:24,770-Speed 3892.19 samples/sec  Loss 3.4936  LearningRate 0.0201  ProxyLR: 1.0026  Epoch: 17  Global Step: 97140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:27,402-Speed 3892.00 samples/sec  Loss 3.4741  LearningRate 0.0200  ProxyLR: 1.0021  Epoch: 17  Global Step: 97150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:30,034-Speed 3892.41 samples/sec  Loss 3.4568  LearningRate 0.0200  ProxyLR: 1.0017  Epoch: 17  Global Step: 97160   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:32,663-Speed 3895.74 samples/sec  Loss 3.4608  LearningRate 0.0200  ProxyLR: 1.0013  Epoch: 17  Global Step: 97170   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:35,294-Speed 3893.53 samples/sec  Loss 3.4659  LearningRate 0.0200  ProxyLR: 1.0008  Epoch: 17  Global Step: 97180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:37,926-Speed 3891.09 samples/sec  Loss 3.4749  LearningRate 0.0200  ProxyLR: 1.0004  Epoch: 17  Global Step: 97190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:40,557-Speed 3893.06 samples/sec  Loss 3.3727  LearningRate 0.0200  ProxyLR: 0.9999  Epoch: 17  Global Step: 97200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:43,191-Speed 3889.44 samples/sec  Loss 3.3950  LearningRate 0.0200  ProxyLR: 0.9995  Epoch: 17  Global Step: 97210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:45,821-Speed 3893.81 samples/sec  Loss 3.3980  LearningRate 0.0200  ProxyLR: 0.9990  Epoch: 17  Global Step: 97220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:00:48,453-Speed 3892.44 samples/sec  Loss 3.4733  LearningRate 0.0200  ProxyLR: 0.9986  Epoch: 17  Global Step: 97230   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:51,084-Speed 3893.20 samples/sec  Loss 3.3911  LearningRate 0.0200  ProxyLR: 0.9981  Epoch: 17  Global Step: 97240   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:53,717-Speed 3889.53 samples/sec  Loss 3.4953  LearningRate 0.0200  ProxyLR: 0.9977  Epoch: 17  Global Step: 97250   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:56,351-Speed 3888.62 samples/sec  Loss 3.3995  LearningRate 0.0199  ProxyLR: 0.9973  Epoch: 17  Global Step: 97260   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:00:58,984-Speed 3890.14 samples/sec  Loss 3.4350  LearningRate 0.0199  ProxyLR: 0.9968  Epoch: 17  Global Step: 97270   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:01,615-Speed 3893.37 samples/sec  Loss 3.3473  LearningRate 0.0199  ProxyLR: 0.9964  Epoch: 17  Global Step: 97280   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:04,249-Speed 3888.33 samples/sec  Loss 3.4405  LearningRate 0.0199  ProxyLR: 0.9959  Epoch: 17  Global Step: 97290   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:06,884-Speed 3887.29 samples/sec  Loss 3.3568  LearningRate 0.0199  ProxyLR: 0.9955  Epoch: 17  Global Step: 97300   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:09,518-Speed 3889.60 samples/sec  Loss 3.4049  LearningRate 0.0199  ProxyLR: 0.9950  Epoch: 17  Global Step: 97310   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:12,148-Speed 3894.43 samples/sec  Loss 3.3824  LearningRate 0.0199  ProxyLR: 0.9946  Epoch: 17  Global Step: 97320   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:14,769-Speed 3907.60 samples/sec  Loss 3.4076  LearningRate 0.0199  ProxyLR: 0.9941  Epoch: 17  Global Step: 97330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:17,403-Speed 3888.77 samples/sec  Loss 3.4754  LearningRate 0.0199  ProxyLR: 0.9937  Epoch: 17  Global Step: 97340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:20,033-Speed 3894.53 samples/sec  Loss 3.3354  LearningRate 0.0199  ProxyLR: 0.9933  Epoch: 17  Global Step: 97350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:22,666-Speed 3889.46 samples/sec  Loss 3.5161  LearningRate 0.0199  ProxyLR: 0.9928  Epoch: 17  Global Step: 97360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:25,298-Speed 3892.51 samples/sec  Loss 3.4509  LearningRate 0.0198  ProxyLR: 0.9924  Epoch: 17  Global Step: 97370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:27,930-Speed 3891.00 samples/sec  Loss 3.4834  LearningRate 0.0198  ProxyLR: 0.9919  Epoch: 17  Global Step: 97380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:30,563-Speed 3890.17 samples/sec  Loss 3.3718  LearningRate 0.0198  ProxyLR: 0.9915  Epoch: 17  Global Step: 97390   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:33,195-Speed 3891.37 samples/sec  Loss 3.4757  LearningRate 0.0198  ProxyLR: 0.9910  Epoch: 17  Global Step: 97400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:35,829-Speed 3889.33 samples/sec  Loss 3.3983  LearningRate 0.0198  ProxyLR: 0.9906  Epoch: 17  Global Step: 97410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:38,463-Speed 3888.06 samples/sec  Loss 3.3740  LearningRate 0.0198  ProxyLR: 0.9902  Epoch: 17  Global Step: 97420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:41,097-Speed 3889.44 samples/sec  Loss 3.4575  LearningRate 0.0198  ProxyLR: 0.9897  Epoch: 17  Global Step: 97430   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:43,730-Speed 3889.86 samples/sec  Loss 3.4714  LearningRate 0.0198  ProxyLR: 0.9893  Epoch: 17  Global Step: 97440   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:46,365-Speed 3887.74 samples/sec  Loss 3.4673  LearningRate 0.0198  ProxyLR: 0.9888  Epoch: 17  Global Step: 97450   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:48,999-Speed 3887.93 samples/sec  Loss 3.4407  LearningRate 0.0198  ProxyLR: 0.9884  Epoch: 17  Global Step: 97460   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:01:51,620-Speed 3908.89 samples/sec  Loss 3.4563  LearningRate 0.0198  ProxyLR: 0.9879  Epoch: 17  Global Step: 97470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:54,259-Speed 3881.64 samples/sec  Loss 3.4997  LearningRate 0.0198  ProxyLR: 0.9875  Epoch: 17  Global Step: 97480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:56,893-Speed 3887.74 samples/sec  Loss 3.4209  LearningRate 0.0197  ProxyLR: 0.9871  Epoch: 17  Global Step: 97490   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:01:59,527-Speed 3889.13 samples/sec  Loss 3.4356  LearningRate 0.0197  ProxyLR: 0.9866  Epoch: 17  Global Step: 97500   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:02,158-Speed 3892.22 samples/sec  Loss 3.3754  LearningRate 0.0197  ProxyLR: 0.9862  Epoch: 17  Global Step: 97510   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:04,794-Speed 3886.45 samples/sec  Loss 3.3315  LearningRate 0.0197  ProxyLR: 0.9857  Epoch: 17  Global Step: 97520   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:07,430-Speed 3885.95 samples/sec  Loss 3.3115  LearningRate 0.0197  ProxyLR: 0.9853  Epoch: 17  Global Step: 97530   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:10,065-Speed 3887.17 samples/sec  Loss 3.3943  LearningRate 0.0197  ProxyLR: 0.9849  Epoch: 17  Global Step: 97540   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:12,698-Speed 3889.77 samples/sec  Loss 3.3839  LearningRate 0.0197  ProxyLR: 0.9844  Epoch: 17  Global Step: 97550   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:15,334-Speed 3886.07 samples/sec  Loss 3.4523  LearningRate 0.0197  ProxyLR: 0.9840  Epoch: 17  Global Step: 97560   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:17,973-Speed 3881.30 samples/sec  Loss 3.4331  LearningRate 0.0197  ProxyLR: 0.9835  Epoch: 17  Global Step: 97570   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:02:20,597-Speed 3904.81 samples/sec  Loss 3.4656  LearningRate 0.0197  ProxyLR: 0.9831  Epoch: 17  Global Step: 97580   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:23,233-Speed 3884.34 samples/sec  Loss 3.3724  LearningRate 0.0197  ProxyLR: 0.9826  Epoch: 17  Global Step: 97590   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:25,872-Speed 3882.22 samples/sec  Loss 3.4292  LearningRate 0.0196  ProxyLR: 0.9822  Epoch: 17  Global Step: 97600   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:28,510-Speed 3882.10 samples/sec  Loss 3.4163  LearningRate 0.0196  ProxyLR: 0.9818  Epoch: 17  Global Step: 97610   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:31,149-Speed 3882.37 samples/sec  Loss 3.3427  LearningRate 0.0196  ProxyLR: 0.9813  Epoch: 17  Global Step: 97620   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:33,788-Speed 3881.13 samples/sec  Loss 3.3820  LearningRate 0.0196  ProxyLR: 0.9809  Epoch: 17  Global Step: 97630   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:36,426-Speed 3882.56 samples/sec  Loss 3.3717  LearningRate 0.0196  ProxyLR: 0.9804  Epoch: 17  Global Step: 97640   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:39,065-Speed 3881.15 samples/sec  Loss 3.4276  LearningRate 0.0196  ProxyLR: 0.9800  Epoch: 17  Global Step: 97650   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:41,703-Speed 3882.11 samples/sec  Loss 3.3913  LearningRate 0.0196  ProxyLR: 0.9796  Epoch: 17  Global Step: 97660   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:44,341-Speed 3882.49 samples/sec  Loss 3.3777  LearningRate 0.0196  ProxyLR: 0.9791  Epoch: 17  Global Step: 97670   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:46,967-Speed 3900.88 samples/sec  Loss 3.4783  LearningRate 0.0196  ProxyLR: 0.9787  Epoch: 17  Global Step: 97680   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:49,606-Speed 3881.66 samples/sec  Loss 3.4339  LearningRate 0.0196  ProxyLR: 0.9782  Epoch: 17  Global Step: 97690   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:52,245-Speed 3881.49 samples/sec  Loss 3.3224  LearningRate 0.0196  ProxyLR: 0.9778  Epoch: 17  Global Step: 97700   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:54,881-Speed 3885.16 samples/sec  Loss 3.4589  LearningRate 0.0195  ProxyLR: 0.9774  Epoch: 17  Global Step: 97710   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:02:57,517-Speed 3886.24 samples/sec  Loss 3.4307  LearningRate 0.0195  ProxyLR: 0.9769  Epoch: 17  Global Step: 97720   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:00,153-Speed 3885.67 samples/sec  Loss 3.4127  LearningRate 0.0195  ProxyLR: 0.9765  Epoch: 17  Global Step: 97730   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:02,790-Speed 3883.90 samples/sec  Loss 3.2889  LearningRate 0.0195  ProxyLR: 0.9760  Epoch: 17  Global Step: 97740   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:05,429-Speed 3881.90 samples/sec  Loss 3.4355  LearningRate 0.0195  ProxyLR: 0.9756  Epoch: 17  Global Step: 97750   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:08,068-Speed 3880.97 samples/sec  Loss 3.3667  LearningRate 0.0195  ProxyLR: 0.9752  Epoch: 17  Global Step: 97760   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:10,709-Speed 3879.02 samples/sec  Loss 3.4175  LearningRate 0.0195  ProxyLR: 0.9747  Epoch: 17  Global Step: 97770   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:13,348-Speed 3880.90 samples/sec  Loss 3.4657  LearningRate 0.0195  ProxyLR: 0.9743  Epoch: 17  Global Step: 97780   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:03:15,976-Speed 3897.32 samples/sec  Loss 3.3938  LearningRate 0.0195  ProxyLR: 0.9738  Epoch: 17  Global Step: 97790   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:18,615-Speed 3882.00 samples/sec  Loss 3.4027  LearningRate 0.0195  ProxyLR: 0.9734  Epoch: 17  Global Step: 97800   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:21,255-Speed 3879.71 samples/sec  Loss 3.3971  LearningRate 0.0195  ProxyLR: 0.9730  Epoch: 17  Global Step: 97810   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:23,896-Speed 3878.17 samples/sec  Loss 3.4094  LearningRate 0.0195  ProxyLR: 0.9725  Epoch: 17  Global Step: 97820   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:26,536-Speed 3878.83 samples/sec  Loss 3.3060  LearningRate 0.0194  ProxyLR: 0.9721  Epoch: 17  Global Step: 97830   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:29,177-Speed 3878.86 samples/sec  Loss 3.3492  LearningRate 0.0194  ProxyLR: 0.9716  Epoch: 17  Global Step: 97840   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:31,816-Speed 3880.91 samples/sec  Loss 3.4518  LearningRate 0.0194  ProxyLR: 0.9712  Epoch: 17  Global Step: 97850   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:34,456-Speed 3880.54 samples/sec  Loss 3.4138  LearningRate 0.0194  ProxyLR: 0.9708  Epoch: 17  Global Step: 97860   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:37,093-Speed 3884.60 samples/sec  Loss 3.3785  LearningRate 0.0194  ProxyLR: 0.9703  Epoch: 17  Global Step: 97870   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:39,725-Speed 3891.65 samples/sec  Loss 3.3994  LearningRate 0.0194  ProxyLR: 0.9699  Epoch: 17  Global Step: 97880   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:42,358-Speed 3889.71 samples/sec  Loss 3.3496  LearningRate 0.0194  ProxyLR: 0.9695  Epoch: 17  Global Step: 97890   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:03:44,989-Speed 3893.78 samples/sec  Loss 3.3438  LearningRate 0.0194  ProxyLR: 0.9690  Epoch: 17  Global Step: 97900   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:03:47,609-Speed 3908.84 samples/sec  Loss 3.3540  LearningRate 0.0194  ProxyLR: 0.9686  Epoch: 17  Global Step: 97910   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:50,241-Speed 3891.07 samples/sec  Loss 3.3770  LearningRate 0.0194  ProxyLR: 0.9681  Epoch: 17  Global Step: 97920   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:52,875-Speed 3889.13 samples/sec  Loss 3.4050  LearningRate 0.0194  ProxyLR: 0.9677  Epoch: 17  Global Step: 97930   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:55,510-Speed 3887.67 samples/sec  Loss 3.4262  LearningRate 0.0193  ProxyLR: 0.9673  Epoch: 17  Global Step: 97940   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:03:58,143-Speed 3889.26 samples/sec  Loss 3.3960  LearningRate 0.0193  ProxyLR: 0.9668  Epoch: 17  Global Step: 97950   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:00,778-Speed 3887.81 samples/sec  Loss 3.3755  LearningRate 0.0193  ProxyLR: 0.9664  Epoch: 17  Global Step: 97960   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:03,410-Speed 3891.80 samples/sec  Loss 3.4307  LearningRate 0.0193  ProxyLR: 0.9660  Epoch: 17  Global Step: 97970   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:06,044-Speed 3888.82 samples/sec  Loss 3.4282  LearningRate 0.0193  ProxyLR: 0.9655  Epoch: 17  Global Step: 97980   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:08,677-Speed 3890.48 samples/sec  Loss 3.3574  LearningRate 0.0193  ProxyLR: 0.9651  Epoch: 17  Global Step: 97990   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:11,310-Speed 3888.91 samples/sec  Loss 3.3522  LearningRate 0.0193  ProxyLR: 0.9646  Epoch: 17  Global Step: 98000   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:13,946-Speed 3887.15 samples/sec  Loss 3.3874  LearningRate 0.0193  ProxyLR: 0.9642  Epoch: 17  Global Step: 98010   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:16,579-Speed 3890.00 samples/sec  Loss 3.3865  LearningRate 0.0193  ProxyLR: 0.9638  Epoch: 17  Global Step: 98020   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:19,211-Speed 3891.07 samples/sec  Loss 3.3418  LearningRate 0.0193  ProxyLR: 0.9633  Epoch: 17  Global Step: 98030   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:21,842-Speed 3893.68 samples/sec  Loss 3.3557  LearningRate 0.0193  ProxyLR: 0.9629  Epoch: 17  Global Step: 98040   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:24,474-Speed 3891.80 samples/sec  Loss 3.4145  LearningRate 0.0192  ProxyLR: 0.9625  Epoch: 17  Global Step: 98050   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:27,092-Speed 3911.86 samples/sec  Loss 3.3785  LearningRate 0.0192  ProxyLR: 0.9620  Epoch: 17  Global Step: 98060   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:29,722-Speed 3894.20 samples/sec  Loss 3.4839  LearningRate 0.0192  ProxyLR: 0.9616  Epoch: 17  Global Step: 98070   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:32,352-Speed 3895.23 samples/sec  Loss 3.3953  LearningRate 0.0192  ProxyLR: 0.9612  Epoch: 17  Global Step: 98080   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:34,984-Speed 3890.93 samples/sec  Loss 3.4783  LearningRate 0.0192  ProxyLR: 0.9607  Epoch: 17  Global Step: 98090   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:37,615-Speed 3893.59 samples/sec  Loss 3.3745  LearningRate 0.0192  ProxyLR: 0.9603  Epoch: 17  Global Step: 98100   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:40,245-Speed 3894.40 samples/sec  Loss 3.4078  LearningRate 0.0192  ProxyLR: 0.9598  Epoch: 17  Global Step: 98110   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:42,875-Speed 3895.10 samples/sec  Loss 3.3476  LearningRate 0.0192  ProxyLR: 0.9594  Epoch: 17  Global Step: 98120   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:45,505-Speed 3893.88 samples/sec  Loss 3.3969  LearningRate 0.0192  ProxyLR: 0.9590  Epoch: 17  Global Step: 98130   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:48,136-Speed 3893.58 samples/sec  Loss 3.3059  LearningRate 0.0192  ProxyLR: 0.9585  Epoch: 17  Global Step: 98140   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:50,767-Speed 3892.92 samples/sec  Loss 3.3493  LearningRate 0.0192  ProxyLR: 0.9581  Epoch: 17  Global Step: 98150   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:04:53,398-Speed 3892.84 samples/sec  Loss 3.4793  LearningRate 0.0192  ProxyLR: 0.9577  Epoch: 17  Global Step: 98160   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:56,028-Speed 3894.55 samples/sec  Loss 3.3926  LearningRate 0.0191  ProxyLR: 0.9572  Epoch: 17  Global Step: 98170   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:04:58,648-Speed 3910.50 samples/sec  Loss 3.2484  LearningRate 0.0191  ProxyLR: 0.9568  Epoch: 17  Global Step: 98180   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:01,279-Speed 3892.12 samples/sec  Loss 3.3422  LearningRate 0.0191  ProxyLR: 0.9564  Epoch: 17  Global Step: 98190   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:03,909-Speed 3894.23 samples/sec  Loss 3.3718  LearningRate 0.0191  ProxyLR: 0.9559  Epoch: 17  Global Step: 98200   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:06,539-Speed 3894.66 samples/sec  Loss 3.3459  LearningRate 0.0191  ProxyLR: 0.9555  Epoch: 17  Global Step: 98210   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:09,169-Speed 3895.15 samples/sec  Loss 3.3765  LearningRate 0.0191  ProxyLR: 0.9551  Epoch: 17  Global Step: 98220   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:11,799-Speed 3894.80 samples/sec  Loss 3.2541  LearningRate 0.0191  ProxyLR: 0.9546  Epoch: 17  Global Step: 98230   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:14,429-Speed 3895.01 samples/sec  Loss 3.4244  LearningRate 0.0191  ProxyLR: 0.9542  Epoch: 17  Global Step: 98240   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:17,061-Speed 3891.16 samples/sec  Loss 3.3705  LearningRate 0.0191  ProxyLR: 0.9538  Epoch: 17  Global Step: 98250   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:19,690-Speed 3895.40 samples/sec  Loss 3.3834  LearningRate 0.0191  ProxyLR: 0.9533  Epoch: 17  Global Step: 98260   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:22,320-Speed 3895.30 samples/sec  Loss 3.3536  LearningRate 0.0191  ProxyLR: 0.9529  Epoch: 17  Global Step: 98270   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:24,951-Speed 3893.56 samples/sec  Loss 3.3442  LearningRate 0.0190  ProxyLR: 0.9524  Epoch: 17  Global Step: 98280   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:05:27,570-Speed 3910.85 samples/sec  Loss 3.4621  LearningRate 0.0190  ProxyLR: 0.9520  Epoch: 17  Global Step: 98290   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:30,203-Speed 3889.54 samples/sec  Loss 3.3490  LearningRate 0.0190  ProxyLR: 0.9516  Epoch: 17  Global Step: 98300   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:32,833-Speed 3894.26 samples/sec  Loss 3.3341  LearningRate 0.0190  ProxyLR: 0.9511  Epoch: 17  Global Step: 98310   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:35,464-Speed 3893.47 samples/sec  Loss 3.4052  LearningRate 0.0190  ProxyLR: 0.9507  Epoch: 17  Global Step: 98320   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:38,095-Speed 3893.48 samples/sec  Loss 3.4140  LearningRate 0.0190  ProxyLR: 0.9503  Epoch: 17  Global Step: 98330   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:40,726-Speed 3893.46 samples/sec  Loss 3.4033  LearningRate 0.0190  ProxyLR: 0.9498  Epoch: 17  Global Step: 98340   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:43,354-Speed 3896.89 samples/sec  Loss 3.3767  LearningRate 0.0190  ProxyLR: 0.9494  Epoch: 17  Global Step: 98350   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:45,983-Speed 3895.86 samples/sec  Loss 3.3620  LearningRate 0.0190  ProxyLR: 0.9490  Epoch: 17  Global Step: 98360   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:48,615-Speed 3891.99 samples/sec  Loss 3.4042  LearningRate 0.0190  ProxyLR: 0.9485  Epoch: 17  Global Step: 98370   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:51,246-Speed 3893.74 samples/sec  Loss 3.4073  LearningRate 0.0190  ProxyLR: 0.9481  Epoch: 17  Global Step: 98380   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:53,875-Speed 3896.31 samples/sec  Loss 3.4141  LearningRate 0.0190  ProxyLR: 0.9477  Epoch: 17  Global Step: 98390   Fp16 Grad Scale: 524288  Required: 4 hours
Training: 2023-05-04 22:05:56,491-Speed 3915.34 samples/sec  Loss 3.4426  LearningRate 0.0189  ProxyLR: 0.9472  Epoch: 17  Global Step: 98400   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:05:59,123-Speed 3891.35 samples/sec  Loss 3.4160  LearningRate 0.0189  ProxyLR: 0.9468  Epoch: 17  Global Step: 98410   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:01,752-Speed 3895.69 samples/sec  Loss 3.3931  LearningRate 0.0189  ProxyLR: 0.9464  Epoch: 17  Global Step: 98420   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:04,381-Speed 3895.83 samples/sec  Loss 3.2437  LearningRate 0.0189  ProxyLR: 0.9459  Epoch: 17  Global Step: 98430   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:07,013-Speed 3891.68 samples/sec  Loss 3.4450  LearningRate 0.0189  ProxyLR: 0.9455  Epoch: 17  Global Step: 98440   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:09,644-Speed 3892.53 samples/sec  Loss 3.2680  LearningRate 0.0189  ProxyLR: 0.9451  Epoch: 17  Global Step: 98450   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:12,275-Speed 3893.70 samples/sec  Loss 3.4194  LearningRate 0.0189  ProxyLR: 0.9446  Epoch: 17  Global Step: 98460   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:14,907-Speed 3891.58 samples/sec  Loss 3.4012  LearningRate 0.0189  ProxyLR: 0.9442  Epoch: 17  Global Step: 98470   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:17,541-Speed 3888.79 samples/sec  Loss 3.3709  LearningRate 0.0189  ProxyLR: 0.9438  Epoch: 17  Global Step: 98480   Fp16 Grad Scale: 262144  Required: 4 hours
Training: 2023-05-04 22:06:20,176-Speed 3887.64 samples/sec  Loss 3.3084  LearningRate 0.0189  ProxyLR: 0.9434  Epoch: 17  Global Step: 98490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:22,812-Speed 3884.69 samples/sec  Loss 3.4258  LearningRate 0.0189  ProxyLR: 0.9429  Epoch: 17  Global Step: 98500   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:06:25,435-Speed 3905.16 samples/sec  Loss 3.3968  LearningRate 0.0188  ProxyLR: 0.9425  Epoch: 17  Global Step: 98510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:28,071-Speed 3886.07 samples/sec  Loss 3.3804  LearningRate 0.0188  ProxyLR: 0.9421  Epoch: 17  Global Step: 98520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:30,708-Speed 3884.22 samples/sec  Loss 3.3713  LearningRate 0.0188  ProxyLR: 0.9416  Epoch: 17  Global Step: 98530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:33,345-Speed 3884.81 samples/sec  Loss 3.2876  LearningRate 0.0188  ProxyLR: 0.9412  Epoch: 17  Global Step: 98540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:35,982-Speed 3883.48 samples/sec  Loss 3.3584  LearningRate 0.0188  ProxyLR: 0.9408  Epoch: 17  Global Step: 98550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:38,621-Speed 3881.47 samples/sec  Loss 3.4402  LearningRate 0.0188  ProxyLR: 0.9403  Epoch: 17  Global Step: 98560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:41,492-Speed 3567.42 samples/sec  Loss 3.4390  LearningRate 0.0188  ProxyLR: 0.9399  Epoch: 17  Global Step: 98570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:44,126-Speed 3889.81 samples/sec  Loss 3.3613  LearningRate 0.0188  ProxyLR: 0.9395  Epoch: 17  Global Step: 98580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:46,755-Speed 3894.87 samples/sec  Loss 3.3473  LearningRate 0.0188  ProxyLR: 0.9390  Epoch: 17  Global Step: 98590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:49,391-Speed 3886.96 samples/sec  Loss 3.3924  LearningRate 0.0188  ProxyLR: 0.9386  Epoch: 17  Global Step: 98600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:06:52,025-Speed 3888.27 samples/sec  Loss 3.4159  LearningRate 0.0188  ProxyLR: 0.9382  Epoch: 17  Global Step: 98610   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:06:54,656-Speed 3891.96 samples/sec  Loss 3.3272  LearningRate 0.0188  ProxyLR: 0.9377  Epoch: 17  Global Step: 98620   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:06:57,286-Speed 3894.58 samples/sec  Loss 3.3532  LearningRate 0.0187  ProxyLR: 0.9373  Epoch: 17  Global Step: 98630   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:06:59,915-Speed 3896.13 samples/sec  Loss 3.2398  LearningRate 0.0187  ProxyLR: 0.9369  Epoch: 17  Global Step: 98640   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:07:02,532-Speed 3914.24 samples/sec  Loss 3.3672  LearningRate 0.0187  ProxyLR: 0.9365  Epoch: 17  Global Step: 98650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:05,162-Speed 3895.59 samples/sec  Loss 3.4397  LearningRate 0.0187  ProxyLR: 0.9360  Epoch: 17  Global Step: 98660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:07,792-Speed 3894.43 samples/sec  Loss 3.3566  LearningRate 0.0187  ProxyLR: 0.9356  Epoch: 17  Global Step: 98670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:10,420-Speed 3896.65 samples/sec  Loss 3.3707  LearningRate 0.0187  ProxyLR: 0.9352  Epoch: 17  Global Step: 98680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:13,050-Speed 3895.95 samples/sec  Loss 3.3497  LearningRate 0.0187  ProxyLR: 0.9347  Epoch: 17  Global Step: 98690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:15,680-Speed 3894.42 samples/sec  Loss 3.3488  LearningRate 0.0187  ProxyLR: 0.9343  Epoch: 17  Global Step: 98700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:18,308-Speed 3896.93 samples/sec  Loss 3.3999  LearningRate 0.0187  ProxyLR: 0.9339  Epoch: 17  Global Step: 98710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:20,938-Speed 3894.28 samples/sec  Loss 3.3455  LearningRate 0.0187  ProxyLR: 0.9334  Epoch: 17  Global Step: 98720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:23,568-Speed 3894.92 samples/sec  Loss 3.3994  LearningRate 0.0187  ProxyLR: 0.9330  Epoch: 17  Global Step: 98730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:26,197-Speed 3895.79 samples/sec  Loss 3.3145  LearningRate 0.0187  ProxyLR: 0.9326  Epoch: 17  Global Step: 98740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:28,826-Speed 3896.08 samples/sec  Loss 3.3329  LearningRate 0.0186  ProxyLR: 0.9321  Epoch: 17  Global Step: 98750   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:07:31,443-Speed 3914.31 samples/sec  Loss 3.3696  LearningRate 0.0186  ProxyLR: 0.9317  Epoch: 17  Global Step: 98760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:34,075-Speed 3891.25 samples/sec  Loss 3.4078  LearningRate 0.0186  ProxyLR: 0.9313  Epoch: 17  Global Step: 98770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:36,707-Speed 3892.49 samples/sec  Loss 3.3280  LearningRate 0.0186  ProxyLR: 0.9309  Epoch: 17  Global Step: 98780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:39,336-Speed 3895.59 samples/sec  Loss 3.3508  LearningRate 0.0186  ProxyLR: 0.9304  Epoch: 17  Global Step: 98790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:41,965-Speed 3895.53 samples/sec  Loss 3.3182  LearningRate 0.0186  ProxyLR: 0.9300  Epoch: 17  Global Step: 98800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:44,597-Speed 3892.81 samples/sec  Loss 3.4901  LearningRate 0.0186  ProxyLR: 0.9296  Epoch: 17  Global Step: 98810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:47,229-Speed 3890.81 samples/sec  Loss 3.3974  LearningRate 0.0186  ProxyLR: 0.9291  Epoch: 17  Global Step: 98820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:50,101-Speed 3566.46 samples/sec  Loss 3.3813  LearningRate 0.0186  ProxyLR: 0.9287  Epoch: 17  Global Step: 98830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:52,734-Speed 3890.21 samples/sec  Loss 3.2990  LearningRate 0.0186  ProxyLR: 0.9283  Epoch: 17  Global Step: 98840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:55,367-Speed 3889.89 samples/sec  Loss 3.4396  LearningRate 0.0186  ProxyLR: 0.9279  Epoch: 17  Global Step: 98850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:07:57,999-Speed 3892.07 samples/sec  Loss 3.3182  LearningRate 0.0185  ProxyLR: 0.9274  Epoch: 17  Global Step: 98860   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:08:00,630-Speed 3892.27 samples/sec  Loss 3.4637  LearningRate 0.0185  ProxyLR: 0.9270  Epoch: 17  Global Step: 98870   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:08:03,262-Speed 3892.53 samples/sec  Loss 3.2677  LearningRate 0.0185  ProxyLR: 0.9266  Epoch: 17  Global Step: 98880   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:08:05,879-Speed 3914.11 samples/sec  Loss 3.3163  LearningRate 0.0185  ProxyLR: 0.9261  Epoch: 17  Global Step: 98890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:08,510-Speed 3893.67 samples/sec  Loss 3.3405  LearningRate 0.0185  ProxyLR: 0.9257  Epoch: 17  Global Step: 98900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:11,141-Speed 3892.37 samples/sec  Loss 3.3863  LearningRate 0.0185  ProxyLR: 0.9253  Epoch: 17  Global Step: 98910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:13,772-Speed 3892.73 samples/sec  Loss 3.3795  LearningRate 0.0185  ProxyLR: 0.9249  Epoch: 17  Global Step: 98920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:16,403-Speed 3893.78 samples/sec  Loss 3.3794  LearningRate 0.0185  ProxyLR: 0.9244  Epoch: 17  Global Step: 98930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:19,036-Speed 3889.73 samples/sec  Loss 3.3842  LearningRate 0.0185  ProxyLR: 0.9240  Epoch: 17  Global Step: 98940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:21,668-Speed 3890.97 samples/sec  Loss 3.3110  LearningRate 0.0185  ProxyLR: 0.9236  Epoch: 17  Global Step: 98950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:24,301-Speed 3890.07 samples/sec  Loss 3.3430  LearningRate 0.0185  ProxyLR: 0.9232  Epoch: 17  Global Step: 98960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:26,934-Speed 3891.17 samples/sec  Loss 3.3404  LearningRate 0.0185  ProxyLR: 0.9227  Epoch: 17  Global Step: 98970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:29,562-Speed 3896.81 samples/sec  Loss 3.3049  LearningRate 0.0184  ProxyLR: 0.9223  Epoch: 17  Global Step: 98980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:32,187-Speed 3902.04 samples/sec  Loss 3.3528  LearningRate 0.0184  ProxyLR: 0.9219  Epoch: 17  Global Step: 98990   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:08:34,813-Speed 3900.23 samples/sec  Loss 3.3248  LearningRate 0.0184  ProxyLR: 0.9214  Epoch: 17  Global Step: 99000   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:08:37,425-Speed 3922.42 samples/sec  Loss 3.3566  LearningRate 0.0184  ProxyLR: 0.9210  Epoch: 17  Global Step: 99010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:40,049-Speed 3903.19 samples/sec  Loss 3.3190  LearningRate 0.0184  ProxyLR: 0.9206  Epoch: 17  Global Step: 99020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:42,674-Speed 3902.24 samples/sec  Loss 3.3655  LearningRate 0.0184  ProxyLR: 0.9202  Epoch: 17  Global Step: 99030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:45,299-Speed 3901.49 samples/sec  Loss 3.2533  LearningRate 0.0184  ProxyLR: 0.9197  Epoch: 17  Global Step: 99040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:47,924-Speed 3902.25 samples/sec  Loss 3.4095  LearningRate 0.0184  ProxyLR: 0.9193  Epoch: 17  Global Step: 99050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:50,552-Speed 3897.57 samples/sec  Loss 3.3189  LearningRate 0.0184  ProxyLR: 0.9189  Epoch: 17  Global Step: 99060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:53,178-Speed 3900.13 samples/sec  Loss 3.3556  LearningRate 0.0184  ProxyLR: 0.9185  Epoch: 17  Global Step: 99070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:55,807-Speed 3896.97 samples/sec  Loss 3.2916  LearningRate 0.0184  ProxyLR: 0.9180  Epoch: 17  Global Step: 99080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:08:58,430-Speed 3904.14 samples/sec  Loss 3.3019  LearningRate 0.0184  ProxyLR: 0.9176  Epoch: 17  Global Step: 99090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:01,056-Speed 3900.38 samples/sec  Loss 3.3036  LearningRate 0.0183  ProxyLR: 0.9172  Epoch: 17  Global Step: 99100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:03,682-Speed 3900.97 samples/sec  Loss 3.3032  LearningRate 0.0183  ProxyLR: 0.9167  Epoch: 17  Global Step: 99110   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:09:06,296-Speed 3918.54 samples/sec  Loss 3.4236  LearningRate 0.0183  ProxyLR: 0.9163  Epoch: 17  Global Step: 99120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:08,921-Speed 3901.74 samples/sec  Loss 3.4190  LearningRate 0.0183  ProxyLR: 0.9159  Epoch: 17  Global Step: 99130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:11,545-Speed 3902.45 samples/sec  Loss 3.3174  LearningRate 0.0183  ProxyLR: 0.9155  Epoch: 17  Global Step: 99140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:14,170-Speed 3902.95 samples/sec  Loss 3.3402  LearningRate 0.0183  ProxyLR: 0.9150  Epoch: 17  Global Step: 99150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:16,795-Speed 3902.04 samples/sec  Loss 3.4024  LearningRate 0.0183  ProxyLR: 0.9146  Epoch: 17  Global Step: 99160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:19,419-Speed 3903.50 samples/sec  Loss 3.4147  LearningRate 0.0183  ProxyLR: 0.9142  Epoch: 17  Global Step: 99170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:22,043-Speed 3902.70 samples/sec  Loss 3.3218  LearningRate 0.0183  ProxyLR: 0.9138  Epoch: 17  Global Step: 99180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:24,667-Speed 3903.38 samples/sec  Loss 3.3451  LearningRate 0.0183  ProxyLR: 0.9133  Epoch: 17  Global Step: 99190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:27,292-Speed 3901.88 samples/sec  Loss 3.3022  LearningRate 0.0183  ProxyLR: 0.9129  Epoch: 17  Global Step: 99200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:29,917-Speed 3902.05 samples/sec  Loss 3.2996  LearningRate 0.0182  ProxyLR: 0.9125  Epoch: 17  Global Step: 99210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:32,542-Speed 3902.47 samples/sec  Loss 3.3553  LearningRate 0.0182  ProxyLR: 0.9121  Epoch: 17  Global Step: 99220   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:09:35,154-Speed 3921.63 samples/sec  Loss 3.4327  LearningRate 0.0182  ProxyLR: 0.9116  Epoch: 17  Global Step: 99230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:37,778-Speed 3902.69 samples/sec  Loss 3.4326  LearningRate 0.0182  ProxyLR: 0.9112  Epoch: 17  Global Step: 99240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:40,402-Speed 3903.85 samples/sec  Loss 3.3535  LearningRate 0.0182  ProxyLR: 0.9108  Epoch: 17  Global Step: 99250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:43,028-Speed 3901.28 samples/sec  Loss 3.3336  LearningRate 0.0182  ProxyLR: 0.9104  Epoch: 17  Global Step: 99260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:45,652-Speed 3902.75 samples/sec  Loss 3.3286  LearningRate 0.0182  ProxyLR: 0.9099  Epoch: 17  Global Step: 99270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:48,276-Speed 3903.74 samples/sec  Loss 3.4044  LearningRate 0.0182  ProxyLR: 0.9095  Epoch: 17  Global Step: 99280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:50,901-Speed 3901.38 samples/sec  Loss 3.3988  LearningRate 0.0182  ProxyLR: 0.9091  Epoch: 17  Global Step: 99290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:53,527-Speed 3901.36 samples/sec  Loss 3.3435  LearningRate 0.0182  ProxyLR: 0.9087  Epoch: 17  Global Step: 99300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:56,151-Speed 3903.46 samples/sec  Loss 3.2963  LearningRate 0.0182  ProxyLR: 0.9082  Epoch: 17  Global Step: 99310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:09:58,773-Speed 3906.59 samples/sec  Loss 3.3389  LearningRate 0.0182  ProxyLR: 0.9078  Epoch: 17  Global Step: 99320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:01,398-Speed 3901.64 samples/sec  Loss 3.3102  LearningRate 0.0181  ProxyLR: 0.9074  Epoch: 17  Global Step: 99330   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:04,023-Speed 3901.72 samples/sec  Loss 3.4128  LearningRate 0.0181  ProxyLR: 0.9070  Epoch: 17  Global Step: 99340   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:06,649-Speed 3900.96 samples/sec  Loss 3.4767  LearningRate 0.0181  ProxyLR: 0.9066  Epoch: 17  Global Step: 99350   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:09,271-Speed 3905.26 samples/sec  Loss 3.2524  LearningRate 0.0181  ProxyLR: 0.9061  Epoch: 17  Global Step: 99360   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:11,883-Speed 3921.57 samples/sec  Loss 3.3910  LearningRate 0.0181  ProxyLR: 0.9057  Epoch: 17  Global Step: 99370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:14,508-Speed 3902.36 samples/sec  Loss 3.2598  LearningRate 0.0181  ProxyLR: 0.9053  Epoch: 17  Global Step: 99380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:17,132-Speed 3903.37 samples/sec  Loss 3.2962  LearningRate 0.0181  ProxyLR: 0.9049  Epoch: 17  Global Step: 99390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:19,757-Speed 3902.25 samples/sec  Loss 3.3585  LearningRate 0.0181  ProxyLR: 0.9044  Epoch: 17  Global Step: 99400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:22,383-Speed 3900.10 samples/sec  Loss 3.4283  LearningRate 0.0181  ProxyLR: 0.9040  Epoch: 17  Global Step: 99410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:25,008-Speed 3901.94 samples/sec  Loss 3.3389  LearningRate 0.0181  ProxyLR: 0.9036  Epoch: 17  Global Step: 99420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:27,632-Speed 3903.16 samples/sec  Loss 3.3495  LearningRate 0.0181  ProxyLR: 0.9032  Epoch: 17  Global Step: 99430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:30,257-Speed 3903.12 samples/sec  Loss 3.3226  LearningRate 0.0181  ProxyLR: 0.9027  Epoch: 17  Global Step: 99440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:32,881-Speed 3902.79 samples/sec  Loss 3.3316  LearningRate 0.0180  ProxyLR: 0.9023  Epoch: 17  Global Step: 99450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:35,509-Speed 3897.83 samples/sec  Loss 3.3765  LearningRate 0.0180  ProxyLR: 0.9019  Epoch: 17  Global Step: 99460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:10:38,138-Speed 3894.93 samples/sec  Loss 3.2280  LearningRate 0.0180  ProxyLR: 0.9015  Epoch: 17  Global Step: 99470   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:40,772-Speed 3889.02 samples/sec  Loss 3.3729  LearningRate 0.0180  ProxyLR: 0.9011  Epoch: 17  Global Step: 99480   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:43,404-Speed 3892.13 samples/sec  Loss 3.3860  LearningRate 0.0180  ProxyLR: 0.9006  Epoch: 17  Global Step: 99490   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:46,036-Speed 3890.38 samples/sec  Loss 3.3789  LearningRate 0.0180  ProxyLR: 0.9002  Epoch: 17  Global Step: 99500   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:48,670-Speed 3889.80 samples/sec  Loss 3.3576  LearningRate 0.0180  ProxyLR: 0.8998  Epoch: 17  Global Step: 99510   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:51,302-Speed 3892.01 samples/sec  Loss 3.3161  LearningRate 0.0180  ProxyLR: 0.8994  Epoch: 17  Global Step: 99520   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:53,935-Speed 3889.76 samples/sec  Loss 3.3929  LearningRate 0.0180  ProxyLR: 0.8989  Epoch: 17  Global Step: 99530   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:56,569-Speed 3888.19 samples/sec  Loss 3.3560  LearningRate 0.0180  ProxyLR: 0.8985  Epoch: 17  Global Step: 99540   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:10:59,204-Speed 3887.17 samples/sec  Loss 3.3045  LearningRate 0.0180  ProxyLR: 0.8981  Epoch: 17  Global Step: 99550   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:11:01,837-Speed 3890.15 samples/sec  Loss 3.3027  LearningRate 0.0180  ProxyLR: 0.8977  Epoch: 17  Global Step: 99560   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:11:04,460-Speed 3904.60 samples/sec  Loss 3.3136  LearningRate 0.0179  ProxyLR: 0.8973  Epoch: 17  Global Step: 99570   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:11:07,084-Speed 3904.17 samples/sec  Loss 3.3393  LearningRate 0.0179  ProxyLR: 0.8968  Epoch: 17  Global Step: 99580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:09,719-Speed 3886.05 samples/sec  Loss 3.3535  LearningRate 0.0179  ProxyLR: 0.8964  Epoch: 17  Global Step: 99590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:12,354-Speed 3887.53 samples/sec  Loss 3.4389  LearningRate 0.0179  ProxyLR: 0.8960  Epoch: 17  Global Step: 99600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:14,990-Speed 3886.75 samples/sec  Loss 3.3629  LearningRate 0.0179  ProxyLR: 0.8956  Epoch: 17  Global Step: 99610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:17,626-Speed 3885.43 samples/sec  Loss 3.2615  LearningRate 0.0179  ProxyLR: 0.8952  Epoch: 17  Global Step: 99620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:20,261-Speed 3886.81 samples/sec  Loss 3.2959  LearningRate 0.0179  ProxyLR: 0.8947  Epoch: 17  Global Step: 99630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:22,894-Speed 3890.74 samples/sec  Loss 3.3441  LearningRate 0.0179  ProxyLR: 0.8943  Epoch: 17  Global Step: 99640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:25,525-Speed 3893.17 samples/sec  Loss 3.4129  LearningRate 0.0179  ProxyLR: 0.8939  Epoch: 17  Global Step: 99650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:28,155-Speed 3893.27 samples/sec  Loss 3.2455  LearningRate 0.0179  ProxyLR: 0.8935  Epoch: 17  Global Step: 99660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:30,787-Speed 3892.62 samples/sec  Loss 3.3655  LearningRate 0.0179  ProxyLR: 0.8930  Epoch: 17  Global Step: 99670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:33,420-Speed 3890.24 samples/sec  Loss 3.3562  LearningRate 0.0179  ProxyLR: 0.8926  Epoch: 17  Global Step: 99680   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:11:36,037-Speed 3912.92 samples/sec  Loss 3.2603  LearningRate 0.0178  ProxyLR: 0.8922  Epoch: 17  Global Step: 99690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:38,667-Speed 3895.40 samples/sec  Loss 3.2497  LearningRate 0.0178  ProxyLR: 0.8918  Epoch: 17  Global Step: 99700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:41,297-Speed 3895.20 samples/sec  Loss 3.3478  LearningRate 0.0178  ProxyLR: 0.8914  Epoch: 17  Global Step: 99710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:43,926-Speed 3894.57 samples/sec  Loss 3.2422  LearningRate 0.0178  ProxyLR: 0.8909  Epoch: 17  Global Step: 99720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:46,557-Speed 3894.29 samples/sec  Loss 3.1611  LearningRate 0.0178  ProxyLR: 0.8905  Epoch: 17  Global Step: 99730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:49,189-Speed 3890.69 samples/sec  Loss 3.2770  LearningRate 0.0178  ProxyLR: 0.8901  Epoch: 17  Global Step: 99740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:51,821-Speed 3892.08 samples/sec  Loss 3.3348  LearningRate 0.0178  ProxyLR: 0.8897  Epoch: 17  Global Step: 99750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:54,450-Speed 3895.49 samples/sec  Loss 3.4112  LearningRate 0.0178  ProxyLR: 0.8893  Epoch: 17  Global Step: 99760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:57,079-Speed 3896.36 samples/sec  Loss 3.2435  LearningRate 0.0178  ProxyLR: 0.8888  Epoch: 17  Global Step: 99770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:11:59,711-Speed 3891.48 samples/sec  Loss 3.3732  LearningRate 0.0178  ProxyLR: 0.8884  Epoch: 17  Global Step: 99780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:02,327-Speed 3915.08 samples/sec  Loss 3.2937  LearningRate 0.0178  ProxyLR: 0.8880  Epoch: 17  Global Step: 99790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:04,957-Speed 3894.78 samples/sec  Loss 3.2852  LearningRate 0.0178  ProxyLR: 0.8876  Epoch: 17  Global Step: 99800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:07,588-Speed 3893.41 samples/sec  Loss 3.3844  LearningRate 0.0177  ProxyLR: 0.8872  Epoch: 17  Global Step: 99810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:10,219-Speed 3892.59 samples/sec  Loss 3.2696  LearningRate 0.0177  ProxyLR: 0.8868  Epoch: 17  Global Step: 99820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:12,849-Speed 3894.18 samples/sec  Loss 3.4154  LearningRate 0.0177  ProxyLR: 0.8863  Epoch: 17  Global Step: 99830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:15,479-Speed 3894.08 samples/sec  Loss 3.3510  LearningRate 0.0177  ProxyLR: 0.8859  Epoch: 17  Global Step: 99840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:18,112-Speed 3891.14 samples/sec  Loss 3.2753  LearningRate 0.0177  ProxyLR: 0.8855  Epoch: 17  Global Step: 99850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:20,742-Speed 3894.63 samples/sec  Loss 3.2740  LearningRate 0.0177  ProxyLR: 0.8851  Epoch: 17  Global Step: 99860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:23,372-Speed 3893.54 samples/sec  Loss 3.3300  LearningRate 0.0177  ProxyLR: 0.8847  Epoch: 17  Global Step: 99870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:25,990-Speed 3913.00 samples/sec  Loss 3.3932  LearningRate 0.0177  ProxyLR: 0.8842  Epoch: 17  Global Step: 99880   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:28,620-Speed 3894.05 samples/sec  Loss 3.2689  LearningRate 0.0177  ProxyLR: 0.8838  Epoch: 17  Global Step: 99890   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:31,252-Speed 3891.76 samples/sec  Loss 3.3137  LearningRate 0.0177  ProxyLR: 0.8834  Epoch: 17  Global Step: 99900   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:33,882-Speed 3895.49 samples/sec  Loss 3.3422  LearningRate 0.0177  ProxyLR: 0.8830  Epoch: 17  Global Step: 99910   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:36,513-Speed 3892.65 samples/sec  Loss 3.2779  LearningRate 0.0177  ProxyLR: 0.8826  Epoch: 17  Global Step: 99920   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:39,143-Speed 3894.42 samples/sec  Loss 3.2996  LearningRate 0.0176  ProxyLR: 0.8822  Epoch: 17  Global Step: 99930   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:41,773-Speed 3894.79 samples/sec  Loss 3.2886  LearningRate 0.0176  ProxyLR: 0.8817  Epoch: 17  Global Step: 99940   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:44,402-Speed 3895.47 samples/sec  Loss 3.4297  LearningRate 0.0176  ProxyLR: 0.8813  Epoch: 17  Global Step: 99950   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:47,035-Speed 3890.79 samples/sec  Loss 3.2731  LearningRate 0.0176  ProxyLR: 0.8809  Epoch: 17  Global Step: 99960   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:49,666-Speed 3893.02 samples/sec  Loss 3.3151  LearningRate 0.0176  ProxyLR: 0.8805  Epoch: 17  Global Step: 99970   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:12:52,296-Speed 3893.91 samples/sec  Loss 3.2987  LearningRate 0.0176  ProxyLR: 0.8801  Epoch: 17  Global Step: 99980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:54,924-Speed 3897.23 samples/sec  Loss 3.3199  LearningRate 0.0176  ProxyLR: 0.8796  Epoch: 17  Global Step: 99990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:12:57,556-Speed 3892.05 samples/sec  Loss 3.1885  LearningRate 0.0176  ProxyLR: 0.8792  Epoch: 17  Global Step: 100000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:13:46,778-[lfw][100000]XNorm: 23.780164
Training: 2023-05-04 22:13:46,778-[lfw][100000]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-04 22:13:46,779-[lfw][100000]Accuracy-Highest: 0.99750
Training: 2023-05-04 22:13:46,779-[lfw][100000]TPR@1stNon-Zero-FPR of 0.00033: 0.99467
Training: 2023-05-04 22:13:46,779-[lfw][100000]Highest TPR@FPR: 0.99600
Training: 2023-05-04 22:14:43,450-[cfp_fp][100000]XNorm: 23.498529
Training: 2023-05-04 22:14:43,451-[cfp_fp][100000]Accuracy-Flip: 0.98129+-0.00640
Training: 2023-05-04 22:14:43,451-[cfp_fp][100000]Accuracy-Highest: 0.98129
Training: 2023-05-04 22:14:43,451-[cfp_fp][100000]TPR@1stNon-Zero-FPR of 0.00029: 0.74629
Training: 2023-05-04 22:14:43,451-[cfp_fp][100000]Highest TPR@FPR: 0.87086
Training: 2023-05-04 22:15:32,652-[agedb_30][100000]XNorm: 23.861115
Training: 2023-05-04 22:15:32,653-[agedb_30][100000]Accuracy-Flip: 0.96883+-0.01070
Training: 2023-05-04 22:15:32,653-[agedb_30][100000]Accuracy-Highest: 0.97067
Training: 2023-05-04 22:15:32,653-[agedb_30][100000]TPR@1stNon-Zero-FPR of 0.00033: 0.85600
Training: 2023-05-04 22:15:32,653-[agedb_30][100000]Highest TPR@FPR: 0.85600
Training: 2023-05-04 22:16:23,322-[calfw][100000]XNorm: 23.837854
Training: 2023-05-04 22:16:23,322-[calfw][100000]Accuracy-Flip: 0.95833+-0.01247
Training: 2023-05-04 22:16:23,322-[calfw][100000]Accuracy-Highest: 0.95833
Training: 2023-05-04 22:16:23,323-[calfw][100000]TPR@1stNon-Zero-FPR of 0.00033: 0.80500
Training: 2023-05-04 22:16:23,323-[calfw][100000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 22:17:13,954-[cplfw][100000]XNorm: 23.114273
Training: 2023-05-04 22:17:13,954-[cplfw][100000]Accuracy-Flip: 0.92650+-0.01334
Training: 2023-05-04 22:17:13,955-[cplfw][100000]Accuracy-Highest: 0.92683
Training: 2023-05-04 22:17:13,955-[cplfw][100000]TPR@1stNon-Zero-FPR of 0.00033: 0.04200
Training: 2023-05-04 22:17:13,955-[cplfw][100000]Highest TPR@FPR: 0.04200
Training: 2023-05-04 22:17:16,607-Speed 39.53 samples/sec  Loss 3.3803  LearningRate 0.0176  ProxyLR: 0.8788  Epoch: 17  Global Step: 100010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:19,227-Speed 3909.59 samples/sec  Loss 3.2708  LearningRate 0.0176  ProxyLR: 0.8784  Epoch: 17  Global Step: 100020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:21,849-Speed 3904.95 samples/sec  Loss 3.2533  LearningRate 0.0176  ProxyLR: 0.8780  Epoch: 17  Global Step: 100030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:24,470-Speed 3909.16 samples/sec  Loss 3.3152  LearningRate 0.0176  ProxyLR: 0.8776  Epoch: 17  Global Step: 100040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:27,091-Speed 3907.90 samples/sec  Loss 3.3396  LearningRate 0.0175  ProxyLR: 0.8771  Epoch: 17  Global Step: 100050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:29,710-Speed 3909.97 samples/sec  Loss 3.2993  LearningRate 0.0175  ProxyLR: 0.8767  Epoch: 17  Global Step: 100060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:32,332-Speed 3906.46 samples/sec  Loss 3.2586  LearningRate 0.0175  ProxyLR: 0.8763  Epoch: 17  Global Step: 100070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:34,942-Speed 3925.94 samples/sec  Loss 3.3556  LearningRate 0.0175  ProxyLR: 0.8759  Epoch: 17  Global Step: 100080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:37,563-Speed 3907.32 samples/sec  Loss 3.3168  LearningRate 0.0175  ProxyLR: 0.8755  Epoch: 17  Global Step: 100090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:40,187-Speed 3903.73 samples/sec  Loss 3.4090  LearningRate 0.0175  ProxyLR: 0.8751  Epoch: 17  Global Step: 100100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:43,052-Speed 3574.28 samples/sec  Loss 3.2693  LearningRate 0.0175  ProxyLR: 0.8746  Epoch: 17  Global Step: 100110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:45,675-Speed 3905.87 samples/sec  Loss 3.3071  LearningRate 0.0175  ProxyLR: 0.8742  Epoch: 17  Global Step: 100120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:48,295-Speed 3909.67 samples/sec  Loss 3.2566  LearningRate 0.0175  ProxyLR: 0.8738  Epoch: 17  Global Step: 100130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:50,916-Speed 3907.84 samples/sec  Loss 3.3017  LearningRate 0.0175  ProxyLR: 0.8734  Epoch: 17  Global Step: 100140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:53,537-Speed 3907.72 samples/sec  Loss 3.3563  LearningRate 0.0175  ProxyLR: 0.8730  Epoch: 17  Global Step: 100150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:56,162-Speed 3901.66 samples/sec  Loss 3.3432  LearningRate 0.0175  ProxyLR: 0.8726  Epoch: 17  Global Step: 100160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:17:58,786-Speed 3903.52 samples/sec  Loss 3.3166  LearningRate 0.0174  ProxyLR: 0.8721  Epoch: 17  Global Step: 100170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:01,410-Speed 3902.68 samples/sec  Loss 3.3042  LearningRate 0.0174  ProxyLR: 0.8717  Epoch: 17  Global Step: 100180   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:18:04,022-Speed 3921.80 samples/sec  Loss 3.3480  LearningRate 0.0174  ProxyLR: 0.8713  Epoch: 17  Global Step: 100190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:06,648-Speed 3900.69 samples/sec  Loss 3.4025  LearningRate 0.0174  ProxyLR: 0.8709  Epoch: 17  Global Step: 100200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:09,271-Speed 3905.54 samples/sec  Loss 3.3413  LearningRate 0.0174  ProxyLR: 0.8705  Epoch: 17  Global Step: 100210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:11,895-Speed 3903.47 samples/sec  Loss 3.3238  LearningRate 0.0174  ProxyLR: 0.8701  Epoch: 17  Global Step: 100220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:14,518-Speed 3903.73 samples/sec  Loss 3.3731  LearningRate 0.0174  ProxyLR: 0.8697  Epoch: 17  Global Step: 100230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:17,143-Speed 3902.90 samples/sec  Loss 3.2018  LearningRate 0.0174  ProxyLR: 0.8692  Epoch: 17  Global Step: 100240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:19,767-Speed 3903.55 samples/sec  Loss 3.4015  LearningRate 0.0174  ProxyLR: 0.8688  Epoch: 17  Global Step: 100250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:22,392-Speed 3901.13 samples/sec  Loss 3.2286  LearningRate 0.0174  ProxyLR: 0.8684  Epoch: 17  Global Step: 100260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:25,017-Speed 3902.33 samples/sec  Loss 3.2981  LearningRate 0.0174  ProxyLR: 0.8680  Epoch: 17  Global Step: 100270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:27,642-Speed 3901.82 samples/sec  Loss 3.2106  LearningRate 0.0174  ProxyLR: 0.8676  Epoch: 17  Global Step: 100280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:30,268-Speed 3899.76 samples/sec  Loss 3.4193  LearningRate 0.0173  ProxyLR: 0.8672  Epoch: 17  Global Step: 100290   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:18:32,880-Speed 3922.53 samples/sec  Loss 3.3828  LearningRate 0.0173  ProxyLR: 0.8668  Epoch: 17  Global Step: 100300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:35,503-Speed 3904.40 samples/sec  Loss 3.3104  LearningRate 0.0173  ProxyLR: 0.8663  Epoch: 17  Global Step: 100310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:38,126-Speed 3904.53 samples/sec  Loss 3.2224  LearningRate 0.0173  ProxyLR: 0.8659  Epoch: 17  Global Step: 100320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:40,752-Speed 3901.57 samples/sec  Loss 3.3474  LearningRate 0.0173  ProxyLR: 0.8655  Epoch: 17  Global Step: 100330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:43,378-Speed 3900.37 samples/sec  Loss 3.3714  LearningRate 0.0173  ProxyLR: 0.8651  Epoch: 17  Global Step: 100340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:46,005-Speed 3898.53 samples/sec  Loss 3.3120  LearningRate 0.0173  ProxyLR: 0.8647  Epoch: 17  Global Step: 100350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:48,637-Speed 3891.30 samples/sec  Loss 3.3954  LearningRate 0.0173  ProxyLR: 0.8643  Epoch: 17  Global Step: 100360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:51,269-Speed 3892.52 samples/sec  Loss 3.3541  LearningRate 0.0173  ProxyLR: 0.8639  Epoch: 17  Global Step: 100370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:53,901-Speed 3890.36 samples/sec  Loss 3.3662  LearningRate 0.0173  ProxyLR: 0.8634  Epoch: 17  Global Step: 100380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:56,532-Speed 3893.81 samples/sec  Loss 3.3867  LearningRate 0.0173  ProxyLR: 0.8630  Epoch: 17  Global Step: 100390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:18:59,162-Speed 3894.89 samples/sec  Loss 3.3394  LearningRate 0.0173  ProxyLR: 0.8626  Epoch: 17  Global Step: 100400   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:19:01,796-Speed 3888.34 samples/sec  Loss 3.4183  LearningRate 0.0172  ProxyLR: 0.8622  Epoch: 17  Global Step: 100410   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:19:04,414-Speed 3912.16 samples/sec  Loss 3.2701  LearningRate 0.0172  ProxyLR: 0.8618  Epoch: 17  Global Step: 100420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:07,046-Speed 3891.43 samples/sec  Loss 3.3850  LearningRate 0.0172  ProxyLR: 0.8614  Epoch: 17  Global Step: 100430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:09,677-Speed 3893.42 samples/sec  Loss 3.2817  LearningRate 0.0172  ProxyLR: 0.8610  Epoch: 17  Global Step: 100440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:12,307-Speed 3893.77 samples/sec  Loss 3.3658  LearningRate 0.0172  ProxyLR: 0.8606  Epoch: 17  Global Step: 100450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:14,939-Speed 3891.78 samples/sec  Loss 3.3437  LearningRate 0.0172  ProxyLR: 0.8601  Epoch: 17  Global Step: 100460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:17,571-Speed 3892.13 samples/sec  Loss 3.2750  LearningRate 0.0172  ProxyLR: 0.8597  Epoch: 17  Global Step: 100470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:20,203-Speed 3890.99 samples/sec  Loss 3.3527  LearningRate 0.0172  ProxyLR: 0.8593  Epoch: 17  Global Step: 100480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:22,836-Speed 3891.28 samples/sec  Loss 3.2486  LearningRate 0.0172  ProxyLR: 0.8589  Epoch: 17  Global Step: 100490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:25,468-Speed 3890.95 samples/sec  Loss 3.3265  LearningRate 0.0172  ProxyLR: 0.8585  Epoch: 17  Global Step: 100500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:28,099-Speed 3893.31 samples/sec  Loss 3.2181  LearningRate 0.0172  ProxyLR: 0.8581  Epoch: 17  Global Step: 100510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:30,730-Speed 3893.31 samples/sec  Loss 3.2908  LearningRate 0.0172  ProxyLR: 0.8577  Epoch: 17  Global Step: 100520   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:19:33,349-Speed 3910.96 samples/sec  Loss 3.3184  LearningRate 0.0171  ProxyLR: 0.8573  Epoch: 17  Global Step: 100530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:35,975-Speed 3899.69 samples/sec  Loss 3.3163  LearningRate 0.0171  ProxyLR: 0.8568  Epoch: 17  Global Step: 100540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:38,603-Speed 3898.54 samples/sec  Loss 3.2177  LearningRate 0.0171  ProxyLR: 0.8564  Epoch: 17  Global Step: 100550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:41,229-Speed 3899.43 samples/sec  Loss 3.2915  LearningRate 0.0171  ProxyLR: 0.8560  Epoch: 17  Global Step: 100560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:43,856-Speed 3899.98 samples/sec  Loss 3.3087  LearningRate 0.0171  ProxyLR: 0.8556  Epoch: 17  Global Step: 100570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:46,480-Speed 3902.49 samples/sec  Loss 3.3487  LearningRate 0.0171  ProxyLR: 0.8552  Epoch: 17  Global Step: 100580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:49,106-Speed 3900.65 samples/sec  Loss 3.2370  LearningRate 0.0171  ProxyLR: 0.8548  Epoch: 17  Global Step: 100590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:51,732-Speed 3901.14 samples/sec  Loss 3.2513  LearningRate 0.0171  ProxyLR: 0.8544  Epoch: 17  Global Step: 100600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:54,358-Speed 3899.19 samples/sec  Loss 3.3439  LearningRate 0.0171  ProxyLR: 0.8540  Epoch: 17  Global Step: 100610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:56,985-Speed 3899.93 samples/sec  Loss 3.2658  LearningRate 0.0171  ProxyLR: 0.8536  Epoch: 17  Global Step: 100620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:19:59,609-Speed 3903.32 samples/sec  Loss 3.2362  LearningRate 0.0171  ProxyLR: 0.8531  Epoch: 17  Global Step: 100630   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:02,238-Speed 3896.35 samples/sec  Loss 3.2698  LearningRate 0.0171  ProxyLR: 0.8527  Epoch: 17  Global Step: 100640   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:04,866-Speed 3896.37 samples/sec  Loss 3.2979  LearningRate 0.0170  ProxyLR: 0.8523  Epoch: 17  Global Step: 100650   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:07,492-Speed 3900.81 samples/sec  Loss 3.3457  LearningRate 0.0170  ProxyLR: 0.8519  Epoch: 17  Global Step: 100660   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:10,106-Speed 3919.17 samples/sec  Loss 3.2960  LearningRate 0.0170  ProxyLR: 0.8515  Epoch: 17  Global Step: 100670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:12,731-Speed 3902.13 samples/sec  Loss 3.1566  LearningRate 0.0170  ProxyLR: 0.8511  Epoch: 17  Global Step: 100680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:15,358-Speed 3898.60 samples/sec  Loss 3.2383  LearningRate 0.0170  ProxyLR: 0.8507  Epoch: 17  Global Step: 100690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:17,986-Speed 3898.46 samples/sec  Loss 3.2324  LearningRate 0.0170  ProxyLR: 0.8503  Epoch: 17  Global Step: 100700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:20,610-Speed 3902.36 samples/sec  Loss 3.3340  LearningRate 0.0170  ProxyLR: 0.8499  Epoch: 17  Global Step: 100710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:23,239-Speed 3897.13 samples/sec  Loss 3.3226  LearningRate 0.0170  ProxyLR: 0.8494  Epoch: 17  Global Step: 100720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:25,865-Speed 3899.83 samples/sec  Loss 3.3072  LearningRate 0.0170  ProxyLR: 0.8490  Epoch: 17  Global Step: 100730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:28,491-Speed 3900.90 samples/sec  Loss 3.2695  LearningRate 0.0170  ProxyLR: 0.8486  Epoch: 17  Global Step: 100740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:31,116-Speed 3902.39 samples/sec  Loss 3.2703  LearningRate 0.0170  ProxyLR: 0.8482  Epoch: 17  Global Step: 100750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:33,743-Speed 3898.99 samples/sec  Loss 3.3210  LearningRate 0.0170  ProxyLR: 0.8478  Epoch: 17  Global Step: 100760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:36,369-Speed 3900.51 samples/sec  Loss 3.3544  LearningRate 0.0169  ProxyLR: 0.8474  Epoch: 17  Global Step: 100770   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:38,994-Speed 3901.40 samples/sec  Loss 3.3419  LearningRate 0.0169  ProxyLR: 0.8470  Epoch: 17  Global Step: 100780   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:41,618-Speed 3903.00 samples/sec  Loss 3.3984  LearningRate 0.0169  ProxyLR: 0.8466  Epoch: 17  Global Step: 100790   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:20:44,239-Speed 3909.43 samples/sec  Loss 3.2684  LearningRate 0.0169  ProxyLR: 0.8462  Epoch: 17  Global Step: 100800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:46,872-Speed 3889.12 samples/sec  Loss 3.2821  LearningRate 0.0169  ProxyLR: 0.8458  Epoch: 17  Global Step: 100810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:49,504-Speed 3892.80 samples/sec  Loss 3.2849  LearningRate 0.0169  ProxyLR: 0.8454  Epoch: 17  Global Step: 100820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:52,130-Speed 3899.95 samples/sec  Loss 3.2725  LearningRate 0.0169  ProxyLR: 0.8449  Epoch: 17  Global Step: 100830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:54,755-Speed 3901.62 samples/sec  Loss 3.2692  LearningRate 0.0169  ProxyLR: 0.8445  Epoch: 17  Global Step: 100840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:20:57,379-Speed 3903.21 samples/sec  Loss 3.3101  LearningRate 0.0169  ProxyLR: 0.8441  Epoch: 17  Global Step: 100850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:00,005-Speed 3901.07 samples/sec  Loss 3.2801  LearningRate 0.0169  ProxyLR: 0.8437  Epoch: 17  Global Step: 100860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:02,630-Speed 3901.63 samples/sec  Loss 3.2959  LearningRate 0.0169  ProxyLR: 0.8433  Epoch: 17  Global Step: 100870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:05,256-Speed 3901.08 samples/sec  Loss 3.3358  LearningRate 0.0169  ProxyLR: 0.8429  Epoch: 17  Global Step: 100880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:07,882-Speed 3899.25 samples/sec  Loss 3.3490  LearningRate 0.0168  ProxyLR: 0.8425  Epoch: 17  Global Step: 100890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:10,509-Speed 3899.91 samples/sec  Loss 3.3065  LearningRate 0.0168  ProxyLR: 0.8421  Epoch: 17  Global Step: 100900   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:13,135-Speed 3899.63 samples/sec  Loss 3.2299  LearningRate 0.0168  ProxyLR: 0.8417  Epoch: 17  Global Step: 100910   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:15,765-Speed 3894.68 samples/sec  Loss 3.2488  LearningRate 0.0168  ProxyLR: 0.8413  Epoch: 17  Global Step: 100920   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:18,388-Speed 3905.27 samples/sec  Loss 3.3125  LearningRate 0.0168  ProxyLR: 0.8409  Epoch: 17  Global Step: 100930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:21,018-Speed 3894.42 samples/sec  Loss 3.3264  LearningRate 0.0168  ProxyLR: 0.8404  Epoch: 17  Global Step: 100940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:23,645-Speed 3899.22 samples/sec  Loss 3.3377  LearningRate 0.0168  ProxyLR: 0.8400  Epoch: 17  Global Step: 100950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:26,269-Speed 3903.58 samples/sec  Loss 3.3225  LearningRate 0.0168  ProxyLR: 0.8396  Epoch: 17  Global Step: 100960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:28,897-Speed 3897.20 samples/sec  Loss 3.3084  LearningRate 0.0168  ProxyLR: 0.8392  Epoch: 17  Global Step: 100970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:31,527-Speed 3894.75 samples/sec  Loss 3.1816  LearningRate 0.0168  ProxyLR: 0.8388  Epoch: 17  Global Step: 100980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:34,152-Speed 3902.82 samples/sec  Loss 3.2654  LearningRate 0.0168  ProxyLR: 0.8384  Epoch: 17  Global Step: 100990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:36,778-Speed 3899.38 samples/sec  Loss 3.2492  LearningRate 0.0168  ProxyLR: 0.8380  Epoch: 17  Global Step: 101000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:39,405-Speed 3900.06 samples/sec  Loss 3.3090  LearningRate 0.0168  ProxyLR: 0.8376  Epoch: 17  Global Step: 101010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:42,030-Speed 3900.77 samples/sec  Loss 3.2949  LearningRate 0.0167  ProxyLR: 0.8372  Epoch: 17  Global Step: 101020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:44,656-Speed 3900.84 samples/sec  Loss 3.3150  LearningRate 0.0167  ProxyLR: 0.8368  Epoch: 17  Global Step: 101030   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:47,283-Speed 3898.77 samples/sec  Loss 3.2843  LearningRate 0.0167  ProxyLR: 0.8364  Epoch: 17  Global Step: 101040   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:49,912-Speed 3896.81 samples/sec  Loss 3.3416  LearningRate 0.0167  ProxyLR: 0.8360  Epoch: 17  Global Step: 101050   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:52,537-Speed 3900.66 samples/sec  Loss 3.2460  LearningRate 0.0167  ProxyLR: 0.8356  Epoch: 17  Global Step: 101060   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:21:55,149-Speed 3921.40 samples/sec  Loss 3.1692  LearningRate 0.0167  ProxyLR: 0.8352  Epoch: 17  Global Step: 101070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:21:57,774-Speed 3901.84 samples/sec  Loss 3.2231  LearningRate 0.0167  ProxyLR: 0.8347  Epoch: 17  Global Step: 101080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:00,400-Speed 3901.43 samples/sec  Loss 3.2360  LearningRate 0.0167  ProxyLR: 0.8343  Epoch: 17  Global Step: 101090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:03,026-Speed 3900.52 samples/sec  Loss 3.2846  LearningRate 0.0167  ProxyLR: 0.8339  Epoch: 17  Global Step: 101100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:05,651-Speed 3901.00 samples/sec  Loss 3.2519  LearningRate 0.0167  ProxyLR: 0.8335  Epoch: 17  Global Step: 101110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:08,279-Speed 3898.62 samples/sec  Loss 3.2309  LearningRate 0.0167  ProxyLR: 0.8331  Epoch: 17  Global Step: 101120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:10,903-Speed 3902.60 samples/sec  Loss 3.2958  LearningRate 0.0167  ProxyLR: 0.8327  Epoch: 17  Global Step: 101130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:13,529-Speed 3900.80 samples/sec  Loss 3.2736  LearningRate 0.0166  ProxyLR: 0.8323  Epoch: 17  Global Step: 101140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:16,153-Speed 3903.11 samples/sec  Loss 3.3438  LearningRate 0.0166  ProxyLR: 0.8319  Epoch: 17  Global Step: 101150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:18,780-Speed 3899.50 samples/sec  Loss 3.2882  LearningRate 0.0166  ProxyLR: 0.8315  Epoch: 17  Global Step: 101160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:21,393-Speed 3920.35 samples/sec  Loss 3.2843  LearningRate 0.0166  ProxyLR: 0.8311  Epoch: 17  Global Step: 101170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:24,018-Speed 3901.50 samples/sec  Loss 3.2778  LearningRate 0.0166  ProxyLR: 0.8307  Epoch: 17  Global Step: 101180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:26,643-Speed 3901.89 samples/sec  Loss 3.3229  LearningRate 0.0166  ProxyLR: 0.8303  Epoch: 17  Global Step: 101190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:29,268-Speed 3902.03 samples/sec  Loss 3.2580  LearningRate 0.0166  ProxyLR: 0.8299  Epoch: 17  Global Step: 101200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:31,893-Speed 3901.19 samples/sec  Loss 3.2019  LearningRate 0.0166  ProxyLR: 0.8295  Epoch: 17  Global Step: 101210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:34,518-Speed 3901.92 samples/sec  Loss 3.2249  LearningRate 0.0166  ProxyLR: 0.8291  Epoch: 17  Global Step: 101220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:37,144-Speed 3901.59 samples/sec  Loss 3.1669  LearningRate 0.0166  ProxyLR: 0.8287  Epoch: 17  Global Step: 101230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:39,769-Speed 3901.53 samples/sec  Loss 3.2759  LearningRate 0.0166  ProxyLR: 0.8283  Epoch: 17  Global Step: 101240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:42,395-Speed 3900.65 samples/sec  Loss 3.2709  LearningRate 0.0166  ProxyLR: 0.8279  Epoch: 17  Global Step: 101250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:45,020-Speed 3901.77 samples/sec  Loss 3.2875  LearningRate 0.0165  ProxyLR: 0.8274  Epoch: 17  Global Step: 101260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:47,646-Speed 3899.95 samples/sec  Loss 3.1638  LearningRate 0.0165  ProxyLR: 0.8270  Epoch: 17  Global Step: 101270   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:22:50,272-Speed 3900.78 samples/sec  Loss 3.2997  LearningRate 0.0165  ProxyLR: 0.8266  Epoch: 17  Global Step: 101280   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:22:52,898-Speed 3900.56 samples/sec  Loss 3.2635  LearningRate 0.0165  ProxyLR: 0.8262  Epoch: 17  Global Step: 101290   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:22:55,510-Speed 3920.63 samples/sec  Loss 3.2815  LearningRate 0.0165  ProxyLR: 0.8258  Epoch: 17  Global Step: 101300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:22:58,134-Speed 3903.57 samples/sec  Loss 3.2929  LearningRate 0.0165  ProxyLR: 0.8254  Epoch: 17  Global Step: 101310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:00,746-Speed 3921.84 samples/sec  Loss 3.2165  LearningRate 0.0165  ProxyLR: 0.8250  Epoch: 17  Global Step: 101320   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:03,368-Speed 3905.83 samples/sec  Loss 3.2722  LearningRate 0.0165  ProxyLR: 0.8246  Epoch: 17  Global Step: 101330   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:05,992-Speed 3903.70 samples/sec  Loss 3.2268  LearningRate 0.0165  ProxyLR: 0.8242  Epoch: 17  Global Step: 101340   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:08,618-Speed 3901.20 samples/sec  Loss 3.2870  LearningRate 0.0165  ProxyLR: 0.8238  Epoch: 17  Global Step: 101350   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:11,243-Speed 3901.62 samples/sec  Loss 3.2580  LearningRate 0.0165  ProxyLR: 0.8234  Epoch: 17  Global Step: 101360   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:13,868-Speed 3901.29 samples/sec  Loss 3.1927  LearningRate 0.0165  ProxyLR: 0.8230  Epoch: 17  Global Step: 101370   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:16,497-Speed 3897.29 samples/sec  Loss 3.2163  LearningRate 0.0165  ProxyLR: 0.8226  Epoch: 17  Global Step: 101380   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:19,126-Speed 3895.04 samples/sec  Loss 3.2993  LearningRate 0.0164  ProxyLR: 0.8222  Epoch: 17  Global Step: 101390   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:21,759-Speed 3891.11 samples/sec  Loss 3.2748  LearningRate 0.0164  ProxyLR: 0.8218  Epoch: 17  Global Step: 101400   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:24,389-Speed 3893.86 samples/sec  Loss 3.2057  LearningRate 0.0164  ProxyLR: 0.8214  Epoch: 17  Global Step: 101410   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:23:27,020-Speed 3893.62 samples/sec  Loss 3.2408  LearningRate 0.0164  ProxyLR: 0.8210  Epoch: 17  Global Step: 101420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:29,650-Speed 3893.81 samples/sec  Loss 3.2104  LearningRate 0.0164  ProxyLR: 0.8206  Epoch: 17  Global Step: 101430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:32,280-Speed 3894.12 samples/sec  Loss 3.2089  LearningRate 0.0164  ProxyLR: 0.8202  Epoch: 17  Global Step: 101440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:34,912-Speed 3892.66 samples/sec  Loss 3.3186  LearningRate 0.0164  ProxyLR: 0.8198  Epoch: 17  Global Step: 101450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:37,543-Speed 3892.90 samples/sec  Loss 3.2725  LearningRate 0.0164  ProxyLR: 0.8194  Epoch: 17  Global Step: 101460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:40,173-Speed 3894.06 samples/sec  Loss 3.2435  LearningRate 0.0164  ProxyLR: 0.8190  Epoch: 17  Global Step: 101470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:42,804-Speed 3892.63 samples/sec  Loss 3.3192  LearningRate 0.0164  ProxyLR: 0.8186  Epoch: 17  Global Step: 101480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:45,436-Speed 3892.79 samples/sec  Loss 3.2584  LearningRate 0.0164  ProxyLR: 0.8182  Epoch: 17  Global Step: 101490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:48,067-Speed 3892.38 samples/sec  Loss 3.2896  LearningRate 0.0164  ProxyLR: 0.8178  Epoch: 17  Global Step: 101500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:50,698-Speed 3893.63 samples/sec  Loss 3.3679  LearningRate 0.0163  ProxyLR: 0.8174  Epoch: 17  Global Step: 101510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:23:53,329-Speed 3893.53 samples/sec  Loss 3.2351  LearningRate 0.0163  ProxyLR: 0.8170  Epoch: 17  Global Step: 101520   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:23:55,958-Speed 3895.25 samples/sec  Loss 3.2943  LearningRate 0.0163  ProxyLR: 0.8166  Epoch: 17  Global Step: 101530   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:23:58,588-Speed 3895.02 samples/sec  Loss 3.2810  LearningRate 0.0163  ProxyLR: 0.8162  Epoch: 17  Global Step: 101540   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:24:01,219-Speed 3892.24 samples/sec  Loss 3.3009  LearningRate 0.0163  ProxyLR: 0.8158  Epoch: 17  Global Step: 101550   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:24:03,850-Speed 3893.09 samples/sec  Loss 3.3331  LearningRate 0.0163  ProxyLR: 0.8154  Epoch: 17  Global Step: 101560   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:24:06,481-Speed 3893.34 samples/sec  Loss 3.1631  LearningRate 0.0163  ProxyLR: 0.8149  Epoch: 17  Global Step: 101570   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:24:09,086-Speed 3931.79 samples/sec  Loss 3.2359  LearningRate 0.0163  ProxyLR: 0.8145  Epoch: 17  Global Step: 101580   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:11,718-Speed 3892.39 samples/sec  Loss 3.2894  LearningRate 0.0163  ProxyLR: 0.8141  Epoch: 17  Global Step: 101590   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:14,350-Speed 3890.50 samples/sec  Loss 3.2022  LearningRate 0.0163  ProxyLR: 0.8137  Epoch: 17  Global Step: 101600   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:16,980-Speed 3894.11 samples/sec  Loss 3.2917  LearningRate 0.0163  ProxyLR: 0.8133  Epoch: 17  Global Step: 101610   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:19,614-Speed 3888.48 samples/sec  Loss 3.3037  LearningRate 0.0163  ProxyLR: 0.8129  Epoch: 17  Global Step: 101620   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:22,246-Speed 3891.91 samples/sec  Loss 3.2911  LearningRate 0.0163  ProxyLR: 0.8125  Epoch: 17  Global Step: 101630   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:24,873-Speed 3899.41 samples/sec  Loss 3.3378  LearningRate 0.0162  ProxyLR: 0.8121  Epoch: 17  Global Step: 101640   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:27,502-Speed 3895.72 samples/sec  Loss 3.2368  LearningRate 0.0162  ProxyLR: 0.8117  Epoch: 17  Global Step: 101650   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:30,140-Speed 3883.18 samples/sec  Loss 3.2547  LearningRate 0.0162  ProxyLR: 0.8113  Epoch: 17  Global Step: 101660   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:32,781-Speed 3878.12 samples/sec  Loss 3.2477  LearningRate 0.0162  ProxyLR: 0.8109  Epoch: 17  Global Step: 101670   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:24:35,419-Speed 3882.36 samples/sec  Loss 3.1851  LearningRate 0.0162  ProxyLR: 0.8105  Epoch: 17  Global Step: 101680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:38,045-Speed 3900.73 samples/sec  Loss 3.2353  LearningRate 0.0162  ProxyLR: 0.8101  Epoch: 17  Global Step: 101690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:40,672-Speed 3898.99 samples/sec  Loss 3.2349  LearningRate 0.0162  ProxyLR: 0.8097  Epoch: 17  Global Step: 101700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:43,297-Speed 3902.48 samples/sec  Loss 3.2257  LearningRate 0.0162  ProxyLR: 0.8093  Epoch: 17  Global Step: 101710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:45,922-Speed 3901.01 samples/sec  Loss 3.3684  LearningRate 0.0162  ProxyLR: 0.8089  Epoch: 17  Global Step: 101720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:48,548-Speed 3900.57 samples/sec  Loss 3.2795  LearningRate 0.0162  ProxyLR: 0.8085  Epoch: 17  Global Step: 101730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:51,173-Speed 3902.85 samples/sec  Loss 3.2402  LearningRate 0.0162  ProxyLR: 0.8081  Epoch: 17  Global Step: 101740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:53,799-Speed 3899.83 samples/sec  Loss 3.2496  LearningRate 0.0162  ProxyLR: 0.8077  Epoch: 17  Global Step: 101750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:56,424-Speed 3902.10 samples/sec  Loss 3.1856  LearningRate 0.0161  ProxyLR: 0.8073  Epoch: 17  Global Step: 101760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:24:59,051-Speed 3899.62 samples/sec  Loss 3.2754  LearningRate 0.0161  ProxyLR: 0.8069  Epoch: 17  Global Step: 101770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:01,676-Speed 3900.99 samples/sec  Loss 3.2770  LearningRate 0.0161  ProxyLR: 0.8065  Epoch: 17  Global Step: 101780   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:25:04,289-Speed 3920.56 samples/sec  Loss 3.2746  LearningRate 0.0161  ProxyLR: 0.8061  Epoch: 17  Global Step: 101790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:06,915-Speed 3900.32 samples/sec  Loss 3.2513  LearningRate 0.0161  ProxyLR: 0.8057  Epoch: 17  Global Step: 101800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:09,540-Speed 3901.54 samples/sec  Loss 3.2054  LearningRate 0.0161  ProxyLR: 0.8053  Epoch: 17  Global Step: 101810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:12,165-Speed 3901.53 samples/sec  Loss 3.3167  LearningRate 0.0161  ProxyLR: 0.8049  Epoch: 17  Global Step: 101820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:14,791-Speed 3900.78 samples/sec  Loss 3.3777  LearningRate 0.0161  ProxyLR: 0.8045  Epoch: 17  Global Step: 101830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:17,419-Speed 3897.35 samples/sec  Loss 3.2435  LearningRate 0.0161  ProxyLR: 0.8041  Epoch: 17  Global Step: 101840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:20,045-Speed 3900.19 samples/sec  Loss 3.2782  LearningRate 0.0161  ProxyLR: 0.8037  Epoch: 17  Global Step: 101850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:22,680-Speed 3886.80 samples/sec  Loss 3.1563  LearningRate 0.0161  ProxyLR: 0.8033  Epoch: 17  Global Step: 101860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:25,307-Speed 3899.02 samples/sec  Loss 3.2311  LearningRate 0.0161  ProxyLR: 0.8029  Epoch: 17  Global Step: 101870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:27,933-Speed 3902.09 samples/sec  Loss 3.1197  LearningRate 0.0161  ProxyLR: 0.8025  Epoch: 17  Global Step: 101880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:30,560-Speed 3899.03 samples/sec  Loss 3.2676  LearningRate 0.0160  ProxyLR: 0.8021  Epoch: 17  Global Step: 101890   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:25:33,187-Speed 3898.70 samples/sec  Loss 3.2728  LearningRate 0.0160  ProxyLR: 0.8017  Epoch: 17  Global Step: 101900   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:25:35,813-Speed 3899.28 samples/sec  Loss 3.2476  LearningRate 0.0160  ProxyLR: 0.8013  Epoch: 17  Global Step: 101910   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:25:38,438-Speed 3902.15 samples/sec  Loss 3.2449  LearningRate 0.0160  ProxyLR: 0.8010  Epoch: 17  Global Step: 101920   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:25:41,064-Speed 3900.85 samples/sec  Loss 3.2008  LearningRate 0.0160  ProxyLR: 0.8006  Epoch: 17  Global Step: 101930   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:25:43,678-Speed 3918.78 samples/sec  Loss 3.2313  LearningRate 0.0160  ProxyLR: 0.8002  Epoch: 17  Global Step: 101940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:46,306-Speed 3897.56 samples/sec  Loss 3.2662  LearningRate 0.0160  ProxyLR: 0.7998  Epoch: 17  Global Step: 101950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:48,931-Speed 3901.92 samples/sec  Loss 3.2836  LearningRate 0.0160  ProxyLR: 0.7994  Epoch: 17  Global Step: 101960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:51,557-Speed 3900.70 samples/sec  Loss 3.2209  LearningRate 0.0160  ProxyLR: 0.7990  Epoch: 17  Global Step: 101970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:54,182-Speed 3901.55 samples/sec  Loss 3.2411  LearningRate 0.0160  ProxyLR: 0.7986  Epoch: 17  Global Step: 101980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:56,807-Speed 3901.50 samples/sec  Loss 3.2177  LearningRate 0.0160  ProxyLR: 0.7982  Epoch: 17  Global Step: 101990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:25:59,434-Speed 3898.90 samples/sec  Loss 3.2842  LearningRate 0.0160  ProxyLR: 0.7978  Epoch: 17  Global Step: 102000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:02,063-Speed 3895.75 samples/sec  Loss 3.1581  LearningRate 0.0159  ProxyLR: 0.7974  Epoch: 17  Global Step: 102010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:04,689-Speed 3900.39 samples/sec  Loss 3.2108  LearningRate 0.0159  ProxyLR: 0.7970  Epoch: 17  Global Step: 102020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:07,315-Speed 3901.62 samples/sec  Loss 3.2440  LearningRate 0.0159  ProxyLR: 0.7966  Epoch: 17  Global Step: 102030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:09,941-Speed 3900.22 samples/sec  Loss 3.2360  LearningRate 0.0159  ProxyLR: 0.7962  Epoch: 17  Global Step: 102040   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:26:12,566-Speed 3902.23 samples/sec  Loss 3.1485  LearningRate 0.0159  ProxyLR: 0.7958  Epoch: 17  Global Step: 102050   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:26:15,193-Speed 3898.37 samples/sec  Loss 3.2961  LearningRate 0.0159  ProxyLR: 0.7954  Epoch: 17  Global Step: 102060   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:26:17,820-Speed 3899.09 samples/sec  Loss 3.2012  LearningRate 0.0159  ProxyLR: 0.7950  Epoch: 17  Global Step: 102070   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:26:20,432-Speed 3922.20 samples/sec  Loss 3.2288  LearningRate 0.0159  ProxyLR: 0.7946  Epoch: 17  Global Step: 102080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:23,058-Speed 3900.65 samples/sec  Loss 3.2976  LearningRate 0.0159  ProxyLR: 0.7942  Epoch: 17  Global Step: 102090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:25,683-Speed 3900.96 samples/sec  Loss 3.2185  LearningRate 0.0159  ProxyLR: 0.7938  Epoch: 17  Global Step: 102100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:28,309-Speed 3900.47 samples/sec  Loss 3.2122  LearningRate 0.0159  ProxyLR: 0.7934  Epoch: 17  Global Step: 102110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:30,936-Speed 3899.53 samples/sec  Loss 3.1338  LearningRate 0.0159  ProxyLR: 0.7930  Epoch: 17  Global Step: 102120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:33,560-Speed 3902.63 samples/sec  Loss 3.2102  LearningRate 0.0159  ProxyLR: 0.7926  Epoch: 17  Global Step: 102130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:36,187-Speed 3899.30 samples/sec  Loss 3.3166  LearningRate 0.0158  ProxyLR: 0.7922  Epoch: 17  Global Step: 102140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:38,813-Speed 3900.22 samples/sec  Loss 3.2067  LearningRate 0.0158  ProxyLR: 0.7918  Epoch: 17  Global Step: 102150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:41,438-Speed 3901.69 samples/sec  Loss 3.2519  LearningRate 0.0158  ProxyLR: 0.7914  Epoch: 17  Global Step: 102160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:44,069-Speed 3893.74 samples/sec  Loss 3.2087  LearningRate 0.0158  ProxyLR: 0.7910  Epoch: 17  Global Step: 102170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:46,700-Speed 3893.02 samples/sec  Loss 3.1644  LearningRate 0.0158  ProxyLR: 0.7906  Epoch: 17  Global Step: 102180   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:26:49,318-Speed 3912.70 samples/sec  Loss 3.3704  LearningRate 0.0158  ProxyLR: 0.7902  Epoch: 17  Global Step: 102190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:51,948-Speed 3894.29 samples/sec  Loss 3.2291  LearningRate 0.0158  ProxyLR: 0.7898  Epoch: 17  Global Step: 102200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:54,578-Speed 3894.54 samples/sec  Loss 3.2240  LearningRate 0.0158  ProxyLR: 0.7894  Epoch: 17  Global Step: 102210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:57,209-Speed 3892.87 samples/sec  Loss 3.2016  LearningRate 0.0158  ProxyLR: 0.7891  Epoch: 17  Global Step: 102220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:26:59,837-Speed 3897.95 samples/sec  Loss 3.1858  LearningRate 0.0158  ProxyLR: 0.7887  Epoch: 17  Global Step: 102230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:02,468-Speed 3892.75 samples/sec  Loss 3.2751  LearningRate 0.0158  ProxyLR: 0.7883  Epoch: 17  Global Step: 102240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:05,096-Speed 3897.12 samples/sec  Loss 3.2934  LearningRate 0.0158  ProxyLR: 0.7879  Epoch: 17  Global Step: 102250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:07,725-Speed 3895.32 samples/sec  Loss 3.2463  LearningRate 0.0157  ProxyLR: 0.7875  Epoch: 17  Global Step: 102260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:10,356-Speed 3894.14 samples/sec  Loss 3.2515  LearningRate 0.0157  ProxyLR: 0.7871  Epoch: 17  Global Step: 102270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:12,984-Speed 3897.02 samples/sec  Loss 3.2453  LearningRate 0.0157  ProxyLR: 0.7867  Epoch: 17  Global Step: 102280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:15,612-Speed 3897.53 samples/sec  Loss 3.2055  LearningRate 0.0157  ProxyLR: 0.7863  Epoch: 17  Global Step: 102290   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:27:18,227-Speed 3916.63 samples/sec  Loss 3.2743  LearningRate 0.0157  ProxyLR: 0.7859  Epoch: 17  Global Step: 102300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:20,855-Speed 3897.42 samples/sec  Loss 3.1551  LearningRate 0.0157  ProxyLR: 0.7855  Epoch: 17  Global Step: 102310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:23,483-Speed 3898.60 samples/sec  Loss 3.2198  LearningRate 0.0157  ProxyLR: 0.7851  Epoch: 17  Global Step: 102320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:26,112-Speed 3895.62 samples/sec  Loss 3.2942  LearningRate 0.0157  ProxyLR: 0.7847  Epoch: 17  Global Step: 102330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:28,799-Speed 3811.44 samples/sec  Loss 3.3128  LearningRate 0.0157  ProxyLR: 0.7843  Epoch: 17  Global Step: 102340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:37,729-Speed 1146.92 samples/sec  Loss 3.0795  LearningRate 0.0157  ProxyLR: 0.7839  Epoch: 18  Global Step: 102350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:40,362-Speed 3890.12 samples/sec  Loss 2.9076  LearningRate 0.0157  ProxyLR: 0.7835  Epoch: 18  Global Step: 102360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:43,015-Speed 3860.13 samples/sec  Loss 2.9587  LearningRate 0.0157  ProxyLR: 0.7831  Epoch: 18  Global Step: 102370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:45,690-Speed 3829.62 samples/sec  Loss 2.9147  LearningRate 0.0157  ProxyLR: 0.7827  Epoch: 18  Global Step: 102380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:48,316-Speed 3900.19 samples/sec  Loss 2.9819  LearningRate 0.0156  ProxyLR: 0.7823  Epoch: 18  Global Step: 102390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:50,930-Speed 3918.77 samples/sec  Loss 2.9896  LearningRate 0.0156  ProxyLR: 0.7820  Epoch: 18  Global Step: 102400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:53,567-Speed 3884.20 samples/sec  Loss 2.9295  LearningRate 0.0156  ProxyLR: 0.7816  Epoch: 18  Global Step: 102410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:56,195-Speed 3896.60 samples/sec  Loss 2.9602  LearningRate 0.0156  ProxyLR: 0.7812  Epoch: 18  Global Step: 102420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:27:58,824-Speed 3895.91 samples/sec  Loss 2.9801  LearningRate 0.0156  ProxyLR: 0.7808  Epoch: 18  Global Step: 102430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:01,453-Speed 3896.13 samples/sec  Loss 2.9403  LearningRate 0.0156  ProxyLR: 0.7804  Epoch: 18  Global Step: 102440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:04,082-Speed 3896.84 samples/sec  Loss 2.9084  LearningRate 0.0156  ProxyLR: 0.7800  Epoch: 18  Global Step: 102450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:06,735-Speed 3859.44 samples/sec  Loss 2.9372  LearningRate 0.0156  ProxyLR: 0.7796  Epoch: 18  Global Step: 102460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:09,364-Speed 3896.24 samples/sec  Loss 3.0394  LearningRate 0.0156  ProxyLR: 0.7792  Epoch: 18  Global Step: 102470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:11,995-Speed 3894.24 samples/sec  Loss 2.9793  LearningRate 0.0156  ProxyLR: 0.7788  Epoch: 18  Global Step: 102480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:14,624-Speed 3896.23 samples/sec  Loss 2.9388  LearningRate 0.0156  ProxyLR: 0.7784  Epoch: 18  Global Step: 102490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:17,253-Speed 3895.01 samples/sec  Loss 2.9316  LearningRate 0.0156  ProxyLR: 0.7780  Epoch: 18  Global Step: 102500   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:28:19,870-Speed 3914.13 samples/sec  Loss 2.9676  LearningRate 0.0156  ProxyLR: 0.7776  Epoch: 18  Global Step: 102510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:22,501-Speed 3892.70 samples/sec  Loss 2.9441  LearningRate 0.0155  ProxyLR: 0.7772  Epoch: 18  Global Step: 102520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:25,129-Speed 3897.15 samples/sec  Loss 2.9688  LearningRate 0.0155  ProxyLR: 0.7768  Epoch: 18  Global Step: 102530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:27,813-Speed 3817.00 samples/sec  Loss 2.9181  LearningRate 0.0155  ProxyLR: 0.7765  Epoch: 18  Global Step: 102540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:30,441-Speed 3897.52 samples/sec  Loss 2.9587  LearningRate 0.0155  ProxyLR: 0.7761  Epoch: 18  Global Step: 102550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:33,071-Speed 3893.47 samples/sec  Loss 3.0294  LearningRate 0.0155  ProxyLR: 0.7757  Epoch: 18  Global Step: 102560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:35,700-Speed 3896.34 samples/sec  Loss 2.8910  LearningRate 0.0155  ProxyLR: 0.7753  Epoch: 18  Global Step: 102570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:38,329-Speed 3896.50 samples/sec  Loss 2.9550  LearningRate 0.0155  ProxyLR: 0.7749  Epoch: 18  Global Step: 102580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:40,958-Speed 3895.27 samples/sec  Loss 2.9870  LearningRate 0.0155  ProxyLR: 0.7745  Epoch: 18  Global Step: 102590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:43,588-Speed 3894.62 samples/sec  Loss 2.8858  LearningRate 0.0155  ProxyLR: 0.7741  Epoch: 18  Global Step: 102600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:46,219-Speed 3893.26 samples/sec  Loss 2.9491  LearningRate 0.0155  ProxyLR: 0.7737  Epoch: 18  Global Step: 102610   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:28:48,850-Speed 3892.74 samples/sec  Loss 2.9763  LearningRate 0.0155  ProxyLR: 0.7733  Epoch: 18  Global Step: 102620   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:28:51,468-Speed 3913.43 samples/sec  Loss 2.9717  LearningRate 0.0155  ProxyLR: 0.7729  Epoch: 18  Global Step: 102630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:54,098-Speed 3894.65 samples/sec  Loss 2.9190  LearningRate 0.0155  ProxyLR: 0.7725  Epoch: 18  Global Step: 102640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:56,730-Speed 3890.64 samples/sec  Loss 2.9199  LearningRate 0.0154  ProxyLR: 0.7721  Epoch: 18  Global Step: 102650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:28:59,362-Speed 3891.19 samples/sec  Loss 2.9593  LearningRate 0.0154  ProxyLR: 0.7718  Epoch: 18  Global Step: 102660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:01,995-Speed 3890.12 samples/sec  Loss 3.0188  LearningRate 0.0154  ProxyLR: 0.7714  Epoch: 18  Global Step: 102670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:04,628-Speed 3890.61 samples/sec  Loss 3.0138  LearningRate 0.0154  ProxyLR: 0.7710  Epoch: 18  Global Step: 102680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:07,260-Speed 3891.09 samples/sec  Loss 2.9013  LearningRate 0.0154  ProxyLR: 0.7706  Epoch: 18  Global Step: 102690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:09,896-Speed 3885.52 samples/sec  Loss 2.9491  LearningRate 0.0154  ProxyLR: 0.7702  Epoch: 18  Global Step: 102700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:12,532-Speed 3886.84 samples/sec  Loss 2.9557  LearningRate 0.0154  ProxyLR: 0.7698  Epoch: 18  Global Step: 102710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:15,167-Speed 3886.66 samples/sec  Loss 2.9454  LearningRate 0.0154  ProxyLR: 0.7694  Epoch: 18  Global Step: 102720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:17,788-Speed 3907.65 samples/sec  Loss 3.0039  LearningRate 0.0154  ProxyLR: 0.7690  Epoch: 18  Global Step: 102730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:20,422-Speed 3889.46 samples/sec  Loss 2.9485  LearningRate 0.0154  ProxyLR: 0.7686  Epoch: 18  Global Step: 102740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:23,058-Speed 3885.89 samples/sec  Loss 2.9572  LearningRate 0.0154  ProxyLR: 0.7682  Epoch: 18  Global Step: 102750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:25,693-Speed 3886.84 samples/sec  Loss 3.0235  LearningRate 0.0154  ProxyLR: 0.7679  Epoch: 18  Global Step: 102760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:28,327-Speed 3888.96 samples/sec  Loss 2.9404  LearningRate 0.0153  ProxyLR: 0.7675  Epoch: 18  Global Step: 102770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:30,959-Speed 3890.79 samples/sec  Loss 3.0126  LearningRate 0.0153  ProxyLR: 0.7671  Epoch: 18  Global Step: 102780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:33,592-Speed 3889.63 samples/sec  Loss 2.8828  LearningRate 0.0153  ProxyLR: 0.7667  Epoch: 18  Global Step: 102790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:36,226-Speed 3888.77 samples/sec  Loss 2.9447  LearningRate 0.0153  ProxyLR: 0.7663  Epoch: 18  Global Step: 102800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:38,859-Speed 3889.78 samples/sec  Loss 3.0051  LearningRate 0.0153  ProxyLR: 0.7659  Epoch: 18  Global Step: 102810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:41,491-Speed 3891.53 samples/sec  Loss 2.9361  LearningRate 0.0153  ProxyLR: 0.7655  Epoch: 18  Global Step: 102820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:44,125-Speed 3889.67 samples/sec  Loss 3.0273  LearningRate 0.0153  ProxyLR: 0.7651  Epoch: 18  Global Step: 102830   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:29:46,747-Speed 3906.38 samples/sec  Loss 3.0210  LearningRate 0.0153  ProxyLR: 0.7647  Epoch: 18  Global Step: 102840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:49,383-Speed 3885.63 samples/sec  Loss 2.9403  LearningRate 0.0153  ProxyLR: 0.7643  Epoch: 18  Global Step: 102850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:52,019-Speed 3886.13 samples/sec  Loss 2.9206  LearningRate 0.0153  ProxyLR: 0.7640  Epoch: 18  Global Step: 102860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:54,658-Speed 3880.75 samples/sec  Loss 3.0180  LearningRate 0.0153  ProxyLR: 0.7636  Epoch: 18  Global Step: 102870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:57,294-Speed 3885.30 samples/sec  Loss 2.9975  LearningRate 0.0153  ProxyLR: 0.7632  Epoch: 18  Global Step: 102880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:29:59,929-Speed 3887.30 samples/sec  Loss 2.9505  LearningRate 0.0153  ProxyLR: 0.7628  Epoch: 18  Global Step: 102890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:02,566-Speed 3884.00 samples/sec  Loss 3.0070  LearningRate 0.0152  ProxyLR: 0.7624  Epoch: 18  Global Step: 102900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:05,200-Speed 3889.03 samples/sec  Loss 2.9249  LearningRate 0.0152  ProxyLR: 0.7620  Epoch: 18  Global Step: 102910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:07,831-Speed 3893.33 samples/sec  Loss 2.9380  LearningRate 0.0152  ProxyLR: 0.7616  Epoch: 18  Global Step: 102920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:10,463-Speed 3891.28 samples/sec  Loss 2.9232  LearningRate 0.0152  ProxyLR: 0.7612  Epoch: 18  Global Step: 102930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:13,097-Speed 3887.96 samples/sec  Loss 2.9120  LearningRate 0.0152  ProxyLR: 0.7609  Epoch: 18  Global Step: 102940   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:30:15,719-Speed 3907.41 samples/sec  Loss 2.9368  LearningRate 0.0152  ProxyLR: 0.7605  Epoch: 18  Global Step: 102950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:18,356-Speed 3883.27 samples/sec  Loss 2.9563  LearningRate 0.0152  ProxyLR: 0.7601  Epoch: 18  Global Step: 102960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:20,994-Speed 3883.22 samples/sec  Loss 2.9409  LearningRate 0.0152  ProxyLR: 0.7597  Epoch: 18  Global Step: 102970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:23,632-Speed 3882.15 samples/sec  Loss 2.9417  LearningRate 0.0152  ProxyLR: 0.7593  Epoch: 18  Global Step: 102980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:26,270-Speed 3883.89 samples/sec  Loss 2.9973  LearningRate 0.0152  ProxyLR: 0.7589  Epoch: 18  Global Step: 102990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:28,907-Speed 3883.86 samples/sec  Loss 2.9480  LearningRate 0.0152  ProxyLR: 0.7585  Epoch: 18  Global Step: 103000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:31,543-Speed 3884.80 samples/sec  Loss 2.9094  LearningRate 0.0152  ProxyLR: 0.7581  Epoch: 18  Global Step: 103010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:34,179-Speed 3887.00 samples/sec  Loss 3.0609  LearningRate 0.0152  ProxyLR: 0.7578  Epoch: 18  Global Step: 103020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:36,815-Speed 3884.33 samples/sec  Loss 2.9693  LearningRate 0.0151  ProxyLR: 0.7574  Epoch: 18  Global Step: 103030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:39,451-Speed 3887.01 samples/sec  Loss 2.9913  LearningRate 0.0151  ProxyLR: 0.7570  Epoch: 18  Global Step: 103040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:42,086-Speed 3886.54 samples/sec  Loss 2.8960  LearningRate 0.0151  ProxyLR: 0.7566  Epoch: 18  Global Step: 103050   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:30:44,722-Speed 3884.72 samples/sec  Loss 3.0093  LearningRate 0.0151  ProxyLR: 0.7562  Epoch: 18  Global Step: 103060   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:30:47,345-Speed 3905.29 samples/sec  Loss 2.9445  LearningRate 0.0151  ProxyLR: 0.7558  Epoch: 18  Global Step: 103070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:49,982-Speed 3884.79 samples/sec  Loss 2.9021  LearningRate 0.0151  ProxyLR: 0.7554  Epoch: 18  Global Step: 103080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:52,617-Speed 3887.01 samples/sec  Loss 2.9209  LearningRate 0.0151  ProxyLR: 0.7550  Epoch: 18  Global Step: 103090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:55,252-Speed 3887.09 samples/sec  Loss 2.9889  LearningRate 0.0151  ProxyLR: 0.7547  Epoch: 18  Global Step: 103100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:30:57,889-Speed 3884.28 samples/sec  Loss 2.9628  LearningRate 0.0151  ProxyLR: 0.7543  Epoch: 18  Global Step: 103110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:00,526-Speed 3883.58 samples/sec  Loss 2.9510  LearningRate 0.0151  ProxyLR: 0.7539  Epoch: 18  Global Step: 103120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:03,163-Speed 3884.70 samples/sec  Loss 3.0095  LearningRate 0.0151  ProxyLR: 0.7535  Epoch: 18  Global Step: 103130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:05,800-Speed 3884.68 samples/sec  Loss 2.9894  LearningRate 0.0151  ProxyLR: 0.7531  Epoch: 18  Global Step: 103140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:08,437-Speed 3884.08 samples/sec  Loss 2.9541  LearningRate 0.0151  ProxyLR: 0.7527  Epoch: 18  Global Step: 103150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:11,073-Speed 3885.24 samples/sec  Loss 2.9630  LearningRate 0.0150  ProxyLR: 0.7523  Epoch: 18  Global Step: 103160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:13,709-Speed 3886.09 samples/sec  Loss 2.9380  LearningRate 0.0150  ProxyLR: 0.7520  Epoch: 18  Global Step: 103170   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:31:16,331-Speed 3905.53 samples/sec  Loss 2.9594  LearningRate 0.0150  ProxyLR: 0.7516  Epoch: 18  Global Step: 103180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:18,968-Speed 3884.99 samples/sec  Loss 2.9729  LearningRate 0.0150  ProxyLR: 0.7512  Epoch: 18  Global Step: 103190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:21,603-Speed 3886.49 samples/sec  Loss 2.9700  LearningRate 0.0150  ProxyLR: 0.7508  Epoch: 18  Global Step: 103200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:24,240-Speed 3884.13 samples/sec  Loss 2.9111  LearningRate 0.0150  ProxyLR: 0.7504  Epoch: 18  Global Step: 103210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:26,872-Speed 3891.75 samples/sec  Loss 2.9533  LearningRate 0.0150  ProxyLR: 0.7500  Epoch: 18  Global Step: 103220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:29,500-Speed 3898.21 samples/sec  Loss 2.9942  LearningRate 0.0150  ProxyLR: 0.7496  Epoch: 18  Global Step: 103230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:32,127-Speed 3898.22 samples/sec  Loss 2.9712  LearningRate 0.0150  ProxyLR: 0.7493  Epoch: 18  Global Step: 103240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:34,756-Speed 3895.91 samples/sec  Loss 2.9209  LearningRate 0.0150  ProxyLR: 0.7489  Epoch: 18  Global Step: 103250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:37,383-Speed 3898.82 samples/sec  Loss 2.9160  LearningRate 0.0150  ProxyLR: 0.7485  Epoch: 18  Global Step: 103260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:40,012-Speed 3896.24 samples/sec  Loss 3.0061  LearningRate 0.0150  ProxyLR: 0.7481  Epoch: 18  Global Step: 103270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:42,641-Speed 3896.61 samples/sec  Loss 2.9580  LearningRate 0.0150  ProxyLR: 0.7477  Epoch: 18  Global Step: 103280   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:31:45,257-Speed 3916.12 samples/sec  Loss 2.9674  LearningRate 0.0149  ProxyLR: 0.7473  Epoch: 18  Global Step: 103290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:47,883-Speed 3900.27 samples/sec  Loss 2.9902  LearningRate 0.0149  ProxyLR: 0.7469  Epoch: 18  Global Step: 103300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:50,513-Speed 3893.62 samples/sec  Loss 2.9510  LearningRate 0.0149  ProxyLR: 0.7466  Epoch: 18  Global Step: 103310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:53,142-Speed 3896.20 samples/sec  Loss 2.9841  LearningRate 0.0149  ProxyLR: 0.7462  Epoch: 18  Global Step: 103320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:55,769-Speed 3899.41 samples/sec  Loss 2.9462  LearningRate 0.0149  ProxyLR: 0.7458  Epoch: 18  Global Step: 103330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:31:58,396-Speed 3898.54 samples/sec  Loss 2.9544  LearningRate 0.0149  ProxyLR: 0.7454  Epoch: 18  Global Step: 103340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:01,023-Speed 3899.22 samples/sec  Loss 2.9826  LearningRate 0.0149  ProxyLR: 0.7450  Epoch: 18  Global Step: 103350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:03,651-Speed 3898.07 samples/sec  Loss 3.0332  LearningRate 0.0149  ProxyLR: 0.7446  Epoch: 18  Global Step: 103360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:06,279-Speed 3897.56 samples/sec  Loss 2.9306  LearningRate 0.0149  ProxyLR: 0.7443  Epoch: 18  Global Step: 103370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:08,906-Speed 3898.90 samples/sec  Loss 2.9830  LearningRate 0.0149  ProxyLR: 0.7439  Epoch: 18  Global Step: 103380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:11,533-Speed 3898.52 samples/sec  Loss 2.9893  LearningRate 0.0149  ProxyLR: 0.7435  Epoch: 18  Global Step: 103390   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:14,161-Speed 3898.01 samples/sec  Loss 3.0188  LearningRate 0.0149  ProxyLR: 0.7431  Epoch: 18  Global Step: 103400   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:16,789-Speed 3896.75 samples/sec  Loss 2.9596  LearningRate 0.0149  ProxyLR: 0.7427  Epoch: 18  Global Step: 103410   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:19,416-Speed 3899.90 samples/sec  Loss 2.9223  LearningRate 0.0148  ProxyLR: 0.7423  Epoch: 18  Global Step: 103420   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:22,045-Speed 3895.90 samples/sec  Loss 2.9063  LearningRate 0.0148  ProxyLR: 0.7420  Epoch: 18  Global Step: 103430   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:24,671-Speed 3899.22 samples/sec  Loss 2.9018  LearningRate 0.0148  ProxyLR: 0.7416  Epoch: 18  Global Step: 103440   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:27,301-Speed 3895.81 samples/sec  Loss 3.0366  LearningRate 0.0148  ProxyLR: 0.7412  Epoch: 18  Global Step: 103450   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:29,930-Speed 3894.82 samples/sec  Loss 2.9370  LearningRate 0.0148  ProxyLR: 0.7408  Epoch: 18  Global Step: 103460   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:32,559-Speed 3897.50 samples/sec  Loss 2.9196  LearningRate 0.0148  ProxyLR: 0.7404  Epoch: 18  Global Step: 103470   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:35,188-Speed 3895.50 samples/sec  Loss 2.9286  LearningRate 0.0148  ProxyLR: 0.7400  Epoch: 18  Global Step: 103480   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:32:37,787-Speed 3940.38 samples/sec  Loss 2.9824  LearningRate 0.0148  ProxyLR: 0.7397  Epoch: 18  Global Step: 103490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:40,415-Speed 3897.79 samples/sec  Loss 2.9540  LearningRate 0.0148  ProxyLR: 0.7393  Epoch: 18  Global Step: 103500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:43,046-Speed 3893.11 samples/sec  Loss 2.9714  LearningRate 0.0148  ProxyLR: 0.7389  Epoch: 18  Global Step: 103510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:45,679-Speed 3890.84 samples/sec  Loss 2.9944  LearningRate 0.0148  ProxyLR: 0.7385  Epoch: 18  Global Step: 103520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:48,313-Speed 3888.17 samples/sec  Loss 2.9745  LearningRate 0.0148  ProxyLR: 0.7381  Epoch: 18  Global Step: 103530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:50,946-Speed 3890.61 samples/sec  Loss 3.0001  LearningRate 0.0148  ProxyLR: 0.7377  Epoch: 18  Global Step: 103540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:53,582-Speed 3884.89 samples/sec  Loss 2.9606  LearningRate 0.0147  ProxyLR: 0.7374  Epoch: 18  Global Step: 103550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:56,216-Speed 3888.99 samples/sec  Loss 2.9518  LearningRate 0.0147  ProxyLR: 0.7370  Epoch: 18  Global Step: 103560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:32:58,849-Speed 3889.92 samples/sec  Loss 2.9913  LearningRate 0.0147  ProxyLR: 0.7366  Epoch: 18  Global Step: 103570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:01,480-Speed 3892.55 samples/sec  Loss 2.8707  LearningRate 0.0147  ProxyLR: 0.7362  Epoch: 18  Global Step: 103580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:04,115-Speed 3887.63 samples/sec  Loss 2.9717  LearningRate 0.0147  ProxyLR: 0.7358  Epoch: 18  Global Step: 103590   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:06,753-Speed 3883.07 samples/sec  Loss 2.9213  LearningRate 0.0147  ProxyLR: 0.7355  Epoch: 18  Global Step: 103600   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:09,390-Speed 3883.06 samples/sec  Loss 2.8648  LearningRate 0.0147  ProxyLR: 0.7351  Epoch: 18  Global Step: 103610   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:12,025-Speed 3888.06 samples/sec  Loss 3.0451  LearningRate 0.0147  ProxyLR: 0.7347  Epoch: 18  Global Step: 103620   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:14,662-Speed 3884.22 samples/sec  Loss 2.9449  LearningRate 0.0147  ProxyLR: 0.7343  Epoch: 18  Global Step: 103630   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:17,282-Speed 3909.04 samples/sec  Loss 3.0191  LearningRate 0.0147  ProxyLR: 0.7339  Epoch: 18  Global Step: 103640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:19,915-Speed 3890.47 samples/sec  Loss 2.9665  LearningRate 0.0147  ProxyLR: 0.7335  Epoch: 18  Global Step: 103650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:22,548-Speed 3890.22 samples/sec  Loss 3.0580  LearningRate 0.0147  ProxyLR: 0.7332  Epoch: 18  Global Step: 103660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:25,182-Speed 3888.69 samples/sec  Loss 2.9616  LearningRate 0.0147  ProxyLR: 0.7328  Epoch: 18  Global Step: 103670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:27,815-Speed 3889.67 samples/sec  Loss 2.8523  LearningRate 0.0146  ProxyLR: 0.7324  Epoch: 18  Global Step: 103680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:30,447-Speed 3891.98 samples/sec  Loss 2.9797  LearningRate 0.0146  ProxyLR: 0.7320  Epoch: 18  Global Step: 103690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:33,081-Speed 3888.62 samples/sec  Loss 2.9218  LearningRate 0.0146  ProxyLR: 0.7316  Epoch: 18  Global Step: 103700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:35,714-Speed 3888.76 samples/sec  Loss 2.9448  LearningRate 0.0146  ProxyLR: 0.7313  Epoch: 18  Global Step: 103710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:38,346-Speed 3892.75 samples/sec  Loss 2.9115  LearningRate 0.0146  ProxyLR: 0.7309  Epoch: 18  Global Step: 103720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:40,980-Speed 3888.67 samples/sec  Loss 2.9828  LearningRate 0.0146  ProxyLR: 0.7305  Epoch: 18  Global Step: 103730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:43,612-Speed 3891.48 samples/sec  Loss 3.0414  LearningRate 0.0146  ProxyLR: 0.7301  Epoch: 18  Global Step: 103740   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:46,243-Speed 3891.78 samples/sec  Loss 2.9864  LearningRate 0.0146  ProxyLR: 0.7297  Epoch: 18  Global Step: 103750   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:33:48,864-Speed 3908.54 samples/sec  Loss 2.9166  LearningRate 0.0146  ProxyLR: 0.7294  Epoch: 18  Global Step: 103760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:51,495-Speed 3892.51 samples/sec  Loss 2.9467  LearningRate 0.0146  ProxyLR: 0.7290  Epoch: 18  Global Step: 103770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:54,129-Speed 3888.90 samples/sec  Loss 2.9840  LearningRate 0.0146  ProxyLR: 0.7286  Epoch: 18  Global Step: 103780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:56,762-Speed 3890.08 samples/sec  Loss 2.9686  LearningRate 0.0146  ProxyLR: 0.7282  Epoch: 18  Global Step: 103790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:33:59,397-Speed 3887.26 samples/sec  Loss 3.0863  LearningRate 0.0146  ProxyLR: 0.7278  Epoch: 18  Global Step: 103800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:02,028-Speed 3892.96 samples/sec  Loss 2.9049  LearningRate 0.0145  ProxyLR: 0.7275  Epoch: 18  Global Step: 103810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:04,665-Speed 3884.69 samples/sec  Loss 2.9753  LearningRate 0.0145  ProxyLR: 0.7271  Epoch: 18  Global Step: 103820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:07,302-Speed 3884.59 samples/sec  Loss 2.9642  LearningRate 0.0145  ProxyLR: 0.7267  Epoch: 18  Global Step: 103830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:09,938-Speed 3884.56 samples/sec  Loss 2.9216  LearningRate 0.0145  ProxyLR: 0.7263  Epoch: 18  Global Step: 103840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:12,575-Speed 3883.99 samples/sec  Loss 2.9387  LearningRate 0.0145  ProxyLR: 0.7259  Epoch: 18  Global Step: 103850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:15,207-Speed 3892.07 samples/sec  Loss 2.9909  LearningRate 0.0145  ProxyLR: 0.7256  Epoch: 18  Global Step: 103860   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:34:17,828-Speed 3908.32 samples/sec  Loss 2.9487  LearningRate 0.0145  ProxyLR: 0.7252  Epoch: 18  Global Step: 103870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:20,457-Speed 3895.11 samples/sec  Loss 2.9277  LearningRate 0.0145  ProxyLR: 0.7248  Epoch: 18  Global Step: 103880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:23,083-Speed 3901.06 samples/sec  Loss 3.0238  LearningRate 0.0145  ProxyLR: 0.7244  Epoch: 18  Global Step: 103890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:25,713-Speed 3894.49 samples/sec  Loss 3.0338  LearningRate 0.0145  ProxyLR: 0.7241  Epoch: 18  Global Step: 103900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:28,340-Speed 3898.94 samples/sec  Loss 2.9518  LearningRate 0.0145  ProxyLR: 0.7237  Epoch: 18  Global Step: 103910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:30,967-Speed 3899.26 samples/sec  Loss 2.8648  LearningRate 0.0145  ProxyLR: 0.7233  Epoch: 18  Global Step: 103920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:33,593-Speed 3900.30 samples/sec  Loss 2.9661  LearningRate 0.0145  ProxyLR: 0.7229  Epoch: 18  Global Step: 103930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:36,219-Speed 3900.28 samples/sec  Loss 2.8965  LearningRate 0.0145  ProxyLR: 0.7225  Epoch: 18  Global Step: 103940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:38,845-Speed 3900.95 samples/sec  Loss 3.0387  LearningRate 0.0144  ProxyLR: 0.7222  Epoch: 18  Global Step: 103950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:41,471-Speed 3900.17 samples/sec  Loss 2.9630  LearningRate 0.0144  ProxyLR: 0.7218  Epoch: 18  Global Step: 103960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:44,097-Speed 3900.09 samples/sec  Loss 2.9418  LearningRate 0.0144  ProxyLR: 0.7214  Epoch: 18  Global Step: 103970   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:34:46,722-Speed 3901.77 samples/sec  Loss 3.0467  LearningRate 0.0144  ProxyLR: 0.7210  Epoch: 18  Global Step: 103980   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:34:49,347-Speed 3901.90 samples/sec  Loss 3.0002  LearningRate 0.0144  ProxyLR: 0.7206  Epoch: 18  Global Step: 103990   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:34:51,962-Speed 3916.64 samples/sec  Loss 2.9440  LearningRate 0.0144  ProxyLR: 0.7203  Epoch: 18  Global Step: 104000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:54,590-Speed 3897.05 samples/sec  Loss 2.9042  LearningRate 0.0144  ProxyLR: 0.7199  Epoch: 18  Global Step: 104010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:57,221-Speed 3893.22 samples/sec  Loss 2.9543  LearningRate 0.0144  ProxyLR: 0.7195  Epoch: 18  Global Step: 104020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:34:59,853-Speed 3892.61 samples/sec  Loss 2.8504  LearningRate 0.0144  ProxyLR: 0.7191  Epoch: 18  Global Step: 104030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:02,483-Speed 3893.90 samples/sec  Loss 3.0113  LearningRate 0.0144  ProxyLR: 0.7188  Epoch: 18  Global Step: 104040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:05,115-Speed 3891.97 samples/sec  Loss 2.9400  LearningRate 0.0144  ProxyLR: 0.7184  Epoch: 18  Global Step: 104050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:07,745-Speed 3894.91 samples/sec  Loss 2.9971  LearningRate 0.0144  ProxyLR: 0.7180  Epoch: 18  Global Step: 104060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:10,377-Speed 3891.55 samples/sec  Loss 2.9786  LearningRate 0.0144  ProxyLR: 0.7176  Epoch: 18  Global Step: 104070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:13,008-Speed 3891.83 samples/sec  Loss 2.9380  LearningRate 0.0143  ProxyLR: 0.7173  Epoch: 18  Global Step: 104080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:15,639-Speed 3893.47 samples/sec  Loss 2.9077  LearningRate 0.0143  ProxyLR: 0.7169  Epoch: 18  Global Step: 104090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:18,269-Speed 3894.79 samples/sec  Loss 2.8985  LearningRate 0.0143  ProxyLR: 0.7165  Epoch: 18  Global Step: 104100   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:35:20,886-Speed 3913.33 samples/sec  Loss 2.9305  LearningRate 0.0143  ProxyLR: 0.7161  Epoch: 18  Global Step: 104110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:23,518-Speed 3892.29 samples/sec  Loss 2.9262  LearningRate 0.0143  ProxyLR: 0.7157  Epoch: 18  Global Step: 104120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:26,147-Speed 3895.01 samples/sec  Loss 2.9487  LearningRate 0.0143  ProxyLR: 0.7154  Epoch: 18  Global Step: 104130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:28,781-Speed 3888.52 samples/sec  Loss 2.9453  LearningRate 0.0143  ProxyLR: 0.7150  Epoch: 18  Global Step: 104140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:31,412-Speed 3892.62 samples/sec  Loss 2.9881  LearningRate 0.0143  ProxyLR: 0.7146  Epoch: 18  Global Step: 104150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:34,042-Speed 3894.65 samples/sec  Loss 2.9559  LearningRate 0.0143  ProxyLR: 0.7142  Epoch: 18  Global Step: 104160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:36,674-Speed 3891.82 samples/sec  Loss 2.9417  LearningRate 0.0143  ProxyLR: 0.7139  Epoch: 18  Global Step: 104170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:39,305-Speed 3893.08 samples/sec  Loss 2.9743  LearningRate 0.0143  ProxyLR: 0.7135  Epoch: 18  Global Step: 104180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:41,937-Speed 3891.41 samples/sec  Loss 2.9879  LearningRate 0.0143  ProxyLR: 0.7131  Epoch: 18  Global Step: 104190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:44,570-Speed 3890.32 samples/sec  Loss 3.0097  LearningRate 0.0143  ProxyLR: 0.7127  Epoch: 18  Global Step: 104200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:47,204-Speed 3889.18 samples/sec  Loss 2.9553  LearningRate 0.0142  ProxyLR: 0.7124  Epoch: 18  Global Step: 104210   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:35:49,837-Speed 3890.42 samples/sec  Loss 2.9705  LearningRate 0.0142  ProxyLR: 0.7120  Epoch: 18  Global Step: 104220   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:35:52,454-Speed 3913.23 samples/sec  Loss 2.9547  LearningRate 0.0142  ProxyLR: 0.7116  Epoch: 18  Global Step: 104230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:55,085-Speed 3893.03 samples/sec  Loss 2.9791  LearningRate 0.0142  ProxyLR: 0.7112  Epoch: 18  Global Step: 104240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:35:57,717-Speed 3891.97 samples/sec  Loss 2.9898  LearningRate 0.0142  ProxyLR: 0.7109  Epoch: 18  Global Step: 104250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:00,345-Speed 3896.48 samples/sec  Loss 3.0072  LearningRate 0.0142  ProxyLR: 0.7105  Epoch: 18  Global Step: 104260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:02,972-Speed 3899.07 samples/sec  Loss 2.9603  LearningRate 0.0142  ProxyLR: 0.7101  Epoch: 18  Global Step: 104270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:05,599-Speed 3899.50 samples/sec  Loss 2.9969  LearningRate 0.0142  ProxyLR: 0.7097  Epoch: 18  Global Step: 104280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:08,226-Speed 3899.64 samples/sec  Loss 3.0339  LearningRate 0.0142  ProxyLR: 0.7094  Epoch: 18  Global Step: 104290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:10,850-Speed 3903.07 samples/sec  Loss 2.9049  LearningRate 0.0142  ProxyLR: 0.7090  Epoch: 18  Global Step: 104300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:13,477-Speed 3899.03 samples/sec  Loss 3.0425  LearningRate 0.0142  ProxyLR: 0.7086  Epoch: 18  Global Step: 104310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:16,106-Speed 3896.30 samples/sec  Loss 2.8776  LearningRate 0.0142  ProxyLR: 0.7082  Epoch: 18  Global Step: 104320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:18,733-Speed 3898.38 samples/sec  Loss 2.9365  LearningRate 0.0142  ProxyLR: 0.7079  Epoch: 18  Global Step: 104330   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:36:21,352-Speed 3909.91 samples/sec  Loss 2.9296  LearningRate 0.0141  ProxyLR: 0.7075  Epoch: 18  Global Step: 104340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:23,981-Speed 3897.23 samples/sec  Loss 2.9692  LearningRate 0.0141  ProxyLR: 0.7071  Epoch: 18  Global Step: 104350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:26,611-Speed 3894.12 samples/sec  Loss 2.9518  LearningRate 0.0141  ProxyLR: 0.7067  Epoch: 18  Global Step: 104360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:29,242-Speed 3892.47 samples/sec  Loss 2.9145  LearningRate 0.0141  ProxyLR: 0.7064  Epoch: 18  Global Step: 104370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:31,873-Speed 3893.59 samples/sec  Loss 3.0005  LearningRate 0.0141  ProxyLR: 0.7060  Epoch: 18  Global Step: 104380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:34,500-Speed 3898.51 samples/sec  Loss 2.9136  LearningRate 0.0141  ProxyLR: 0.7056  Epoch: 18  Global Step: 104390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:37,131-Speed 3893.18 samples/sec  Loss 2.9426  LearningRate 0.0141  ProxyLR: 0.7052  Epoch: 18  Global Step: 104400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:39,762-Speed 3893.06 samples/sec  Loss 2.8823  LearningRate 0.0141  ProxyLR: 0.7049  Epoch: 18  Global Step: 104410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:42,392-Speed 3894.66 samples/sec  Loss 2.9419  LearningRate 0.0141  ProxyLR: 0.7045  Epoch: 18  Global Step: 104420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:45,022-Speed 3895.41 samples/sec  Loss 2.9530  LearningRate 0.0141  ProxyLR: 0.7041  Epoch: 18  Global Step: 104430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:47,651-Speed 3894.66 samples/sec  Loss 2.9504  LearningRate 0.0141  ProxyLR: 0.7038  Epoch: 18  Global Step: 104440   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:36:50,282-Speed 3894.12 samples/sec  Loss 3.0031  LearningRate 0.0141  ProxyLR: 0.7034  Epoch: 18  Global Step: 104450   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:36:52,912-Speed 3893.75 samples/sec  Loss 3.0115  LearningRate 0.0141  ProxyLR: 0.7030  Epoch: 18  Global Step: 104460   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:36:55,528-Speed 3916.00 samples/sec  Loss 2.9105  LearningRate 0.0141  ProxyLR: 0.7026  Epoch: 18  Global Step: 104470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:36:58,157-Speed 3895.03 samples/sec  Loss 2.9434  LearningRate 0.0140  ProxyLR: 0.7023  Epoch: 18  Global Step: 104480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:00,787-Speed 3895.36 samples/sec  Loss 3.0097  LearningRate 0.0140  ProxyLR: 0.7019  Epoch: 18  Global Step: 104490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:03,415-Speed 3897.35 samples/sec  Loss 2.8857  LearningRate 0.0140  ProxyLR: 0.7015  Epoch: 18  Global Step: 104500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:06,045-Speed 3894.70 samples/sec  Loss 2.9964  LearningRate 0.0140  ProxyLR: 0.7011  Epoch: 18  Global Step: 104510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:08,675-Speed 3893.75 samples/sec  Loss 2.8811  LearningRate 0.0140  ProxyLR: 0.7008  Epoch: 18  Global Step: 104520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:11,305-Speed 3894.89 samples/sec  Loss 2.9206  LearningRate 0.0140  ProxyLR: 0.7004  Epoch: 18  Global Step: 104530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:13,937-Speed 3891.94 samples/sec  Loss 2.9828  LearningRate 0.0140  ProxyLR: 0.7000  Epoch: 18  Global Step: 104540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:16,568-Speed 3893.00 samples/sec  Loss 2.9617  LearningRate 0.0140  ProxyLR: 0.6997  Epoch: 18  Global Step: 104550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:19,198-Speed 3893.97 samples/sec  Loss 2.9601  LearningRate 0.0140  ProxyLR: 0.6993  Epoch: 18  Global Step: 104560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:21,815-Speed 3914.32 samples/sec  Loss 2.9961  LearningRate 0.0140  ProxyLR: 0.6989  Epoch: 18  Global Step: 104570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:24,446-Speed 3892.11 samples/sec  Loss 2.9105  LearningRate 0.0140  ProxyLR: 0.6985  Epoch: 18  Global Step: 104580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:27,077-Speed 3893.93 samples/sec  Loss 2.8847  LearningRate 0.0140  ProxyLR: 0.6982  Epoch: 18  Global Step: 104590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:29,707-Speed 3894.24 samples/sec  Loss 2.9218  LearningRate 0.0140  ProxyLR: 0.6978  Epoch: 18  Global Step: 104600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:32,339-Speed 3891.95 samples/sec  Loss 2.9980  LearningRate 0.0139  ProxyLR: 0.6974  Epoch: 18  Global Step: 104610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:34,969-Speed 3894.65 samples/sec  Loss 3.0024  LearningRate 0.0139  ProxyLR: 0.6970  Epoch: 18  Global Step: 104620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:37,599-Speed 3894.37 samples/sec  Loss 2.8854  LearningRate 0.0139  ProxyLR: 0.6967  Epoch: 18  Global Step: 104630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:40,229-Speed 3894.68 samples/sec  Loss 2.9716  LearningRate 0.0139  ProxyLR: 0.6963  Epoch: 18  Global Step: 104640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:42,859-Speed 3893.82 samples/sec  Loss 2.9421  LearningRate 0.0139  ProxyLR: 0.6959  Epoch: 18  Global Step: 104650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:45,490-Speed 3893.14 samples/sec  Loss 2.9639  LearningRate 0.0139  ProxyLR: 0.6956  Epoch: 18  Global Step: 104660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:48,122-Speed 3891.74 samples/sec  Loss 3.0026  LearningRate 0.0139  ProxyLR: 0.6952  Epoch: 18  Global Step: 104670   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:37:50,740-Speed 3911.97 samples/sec  Loss 3.0025  LearningRate 0.0139  ProxyLR: 0.6948  Epoch: 18  Global Step: 104680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:53,370-Speed 3895.51 samples/sec  Loss 2.9879  LearningRate 0.0139  ProxyLR: 0.6945  Epoch: 18  Global Step: 104690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:56,001-Speed 3892.78 samples/sec  Loss 2.9134  LearningRate 0.0139  ProxyLR: 0.6941  Epoch: 18  Global Step: 104700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:37:58,630-Speed 3896.40 samples/sec  Loss 3.0671  LearningRate 0.0139  ProxyLR: 0.6937  Epoch: 18  Global Step: 104710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:01,257-Speed 3898.48 samples/sec  Loss 2.9393  LearningRate 0.0139  ProxyLR: 0.6933  Epoch: 18  Global Step: 104720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:03,887-Speed 3894.00 samples/sec  Loss 3.0724  LearningRate 0.0139  ProxyLR: 0.6930  Epoch: 18  Global Step: 104730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:06,519-Speed 3892.00 samples/sec  Loss 2.9044  LearningRate 0.0139  ProxyLR: 0.6926  Epoch: 18  Global Step: 104740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:09,151-Speed 3890.85 samples/sec  Loss 2.9889  LearningRate 0.0138  ProxyLR: 0.6922  Epoch: 18  Global Step: 104750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:11,784-Speed 3891.06 samples/sec  Loss 2.9538  LearningRate 0.0138  ProxyLR: 0.6919  Epoch: 18  Global Step: 104760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:14,414-Speed 3893.36 samples/sec  Loss 3.0418  LearningRate 0.0138  ProxyLR: 0.6915  Epoch: 18  Global Step: 104770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:17,045-Speed 3892.96 samples/sec  Loss 2.9263  LearningRate 0.0138  ProxyLR: 0.6911  Epoch: 18  Global Step: 104780   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:38:19,662-Speed 3914.33 samples/sec  Loss 2.9436  LearningRate 0.0138  ProxyLR: 0.6907  Epoch: 18  Global Step: 104790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:22,294-Speed 3892.06 samples/sec  Loss 2.9378  LearningRate 0.0138  ProxyLR: 0.6904  Epoch: 18  Global Step: 104800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:24,923-Speed 3896.41 samples/sec  Loss 3.0273  LearningRate 0.0138  ProxyLR: 0.6900  Epoch: 18  Global Step: 104810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:27,554-Speed 3892.91 samples/sec  Loss 2.8992  LearningRate 0.0138  ProxyLR: 0.6896  Epoch: 18  Global Step: 104820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:30,182-Speed 3897.74 samples/sec  Loss 2.9635  LearningRate 0.0138  ProxyLR: 0.6893  Epoch: 18  Global Step: 104830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:32,811-Speed 3896.16 samples/sec  Loss 2.9905  LearningRate 0.0138  ProxyLR: 0.6889  Epoch: 18  Global Step: 104840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:35,441-Speed 3894.27 samples/sec  Loss 3.0081  LearningRate 0.0138  ProxyLR: 0.6885  Epoch: 18  Global Step: 104850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:38,070-Speed 3895.16 samples/sec  Loss 2.9918  LearningRate 0.0138  ProxyLR: 0.6882  Epoch: 18  Global Step: 104860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:40,700-Speed 3894.41 samples/sec  Loss 2.9278  LearningRate 0.0138  ProxyLR: 0.6878  Epoch: 18  Global Step: 104870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:43,330-Speed 3894.64 samples/sec  Loss 3.0039  LearningRate 0.0137  ProxyLR: 0.6874  Epoch: 18  Global Step: 104880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:45,948-Speed 3912.69 samples/sec  Loss 2.9512  LearningRate 0.0137  ProxyLR: 0.6871  Epoch: 18  Global Step: 104890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:48,577-Speed 3895.70 samples/sec  Loss 2.9413  LearningRate 0.0137  ProxyLR: 0.6867  Epoch: 18  Global Step: 104900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:51,207-Speed 3895.37 samples/sec  Loss 2.9688  LearningRate 0.0137  ProxyLR: 0.6863  Epoch: 18  Global Step: 104910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:53,835-Speed 3896.32 samples/sec  Loss 2.9537  LearningRate 0.0137  ProxyLR: 0.6859  Epoch: 18  Global Step: 104920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:56,466-Speed 3892.82 samples/sec  Loss 2.9318  LearningRate 0.0137  ProxyLR: 0.6856  Epoch: 18  Global Step: 104930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:38:59,098-Speed 3892.02 samples/sec  Loss 2.8872  LearningRate 0.0137  ProxyLR: 0.6852  Epoch: 18  Global Step: 104940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:01,728-Speed 3894.98 samples/sec  Loss 2.9661  LearningRate 0.0137  ProxyLR: 0.6848  Epoch: 18  Global Step: 104950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:04,357-Speed 3895.78 samples/sec  Loss 2.9795  LearningRate 0.0137  ProxyLR: 0.6845  Epoch: 18  Global Step: 104960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:06,988-Speed 3892.66 samples/sec  Loss 2.9223  LearningRate 0.0137  ProxyLR: 0.6841  Epoch: 18  Global Step: 104970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:09,621-Speed 3891.02 samples/sec  Loss 2.9135  LearningRate 0.0137  ProxyLR: 0.6837  Epoch: 18  Global Step: 104980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:12,251-Speed 3893.77 samples/sec  Loss 2.9821  LearningRate 0.0137  ProxyLR: 0.6834  Epoch: 18  Global Step: 104990   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:39:14,869-Speed 3912.36 samples/sec  Loss 2.8906  LearningRate 0.0137  ProxyLR: 0.6830  Epoch: 18  Global Step: 105000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:17,500-Speed 3892.80 samples/sec  Loss 2.9976  LearningRate 0.0137  ProxyLR: 0.6826  Epoch: 18  Global Step: 105010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:20,129-Speed 3896.02 samples/sec  Loss 2.9900  LearningRate 0.0136  ProxyLR: 0.6823  Epoch: 18  Global Step: 105020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:22,761-Speed 3892.35 samples/sec  Loss 2.9884  LearningRate 0.0136  ProxyLR: 0.6819  Epoch: 18  Global Step: 105030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:25,394-Speed 3890.40 samples/sec  Loss 2.9226  LearningRate 0.0136  ProxyLR: 0.6815  Epoch: 18  Global Step: 105040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:28,023-Speed 3894.86 samples/sec  Loss 2.8954  LearningRate 0.0136  ProxyLR: 0.6812  Epoch: 18  Global Step: 105050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:30,654-Speed 3894.04 samples/sec  Loss 2.9428  LearningRate 0.0136  ProxyLR: 0.6808  Epoch: 18  Global Step: 105060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:33,286-Speed 3890.83 samples/sec  Loss 2.9642  LearningRate 0.0136  ProxyLR: 0.6804  Epoch: 18  Global Step: 105070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:35,916-Speed 3894.30 samples/sec  Loss 2.9400  LearningRate 0.0136  ProxyLR: 0.6801  Epoch: 18  Global Step: 105080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:38,544-Speed 3898.22 samples/sec  Loss 2.9318  LearningRate 0.0136  ProxyLR: 0.6797  Epoch: 18  Global Step: 105090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:41,174-Speed 3893.94 samples/sec  Loss 2.9886  LearningRate 0.0136  ProxyLR: 0.6793  Epoch: 18  Global Step: 105100   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:39:43,804-Speed 3894.68 samples/sec  Loss 2.9603  LearningRate 0.0136  ProxyLR: 0.6790  Epoch: 18  Global Step: 105110   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:39:46,420-Speed 3916.15 samples/sec  Loss 2.9397  LearningRate 0.0136  ProxyLR: 0.6786  Epoch: 18  Global Step: 105120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:49,047-Speed 3899.26 samples/sec  Loss 2.9270  LearningRate 0.0136  ProxyLR: 0.6782  Epoch: 18  Global Step: 105130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:51,672-Speed 3901.90 samples/sec  Loss 2.9551  LearningRate 0.0136  ProxyLR: 0.6779  Epoch: 18  Global Step: 105140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:54,299-Speed 3898.88 samples/sec  Loss 3.0477  LearningRate 0.0136  ProxyLR: 0.6775  Epoch: 18  Global Step: 105150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:56,925-Speed 3899.16 samples/sec  Loss 2.9093  LearningRate 0.0135  ProxyLR: 0.6771  Epoch: 18  Global Step: 105160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:39:59,550-Speed 3902.07 samples/sec  Loss 2.9335  LearningRate 0.0135  ProxyLR: 0.6768  Epoch: 18  Global Step: 105170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:02,177-Speed 3899.41 samples/sec  Loss 2.9047  LearningRate 0.0135  ProxyLR: 0.6764  Epoch: 18  Global Step: 105180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:04,804-Speed 3898.60 samples/sec  Loss 2.9204  LearningRate 0.0135  ProxyLR: 0.6760  Epoch: 18  Global Step: 105190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:07,429-Speed 3901.88 samples/sec  Loss 3.0192  LearningRate 0.0135  ProxyLR: 0.6757  Epoch: 18  Global Step: 105200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:10,055-Speed 3900.94 samples/sec  Loss 3.0643  LearningRate 0.0135  ProxyLR: 0.6753  Epoch: 18  Global Step: 105210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:12,665-Speed 3923.53 samples/sec  Loss 2.9515  LearningRate 0.0135  ProxyLR: 0.6749  Epoch: 18  Global Step: 105220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:15,290-Speed 3902.41 samples/sec  Loss 2.9372  LearningRate 0.0135  ProxyLR: 0.6746  Epoch: 18  Global Step: 105230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:17,917-Speed 3899.59 samples/sec  Loss 2.9186  LearningRate 0.0135  ProxyLR: 0.6742  Epoch: 18  Global Step: 105240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:20,543-Speed 3899.70 samples/sec  Loss 2.8934  LearningRate 0.0135  ProxyLR: 0.6738  Epoch: 18  Global Step: 105250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:23,169-Speed 3900.64 samples/sec  Loss 2.9587  LearningRate 0.0135  ProxyLR: 0.6735  Epoch: 18  Global Step: 105260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:25,794-Speed 3902.32 samples/sec  Loss 2.9832  LearningRate 0.0135  ProxyLR: 0.6731  Epoch: 18  Global Step: 105270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:28,421-Speed 3899.86 samples/sec  Loss 2.9984  LearningRate 0.0135  ProxyLR: 0.6727  Epoch: 18  Global Step: 105280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:31,048-Speed 3898.94 samples/sec  Loss 2.9935  LearningRate 0.0134  ProxyLR: 0.6724  Epoch: 18  Global Step: 105290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:33,674-Speed 3899.20 samples/sec  Loss 2.9289  LearningRate 0.0134  ProxyLR: 0.6720  Epoch: 18  Global Step: 105300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:36,302-Speed 3898.40 samples/sec  Loss 2.8549  LearningRate 0.0134  ProxyLR: 0.6717  Epoch: 18  Global Step: 105310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:38,928-Speed 3900.39 samples/sec  Loss 2.9508  LearningRate 0.0134  ProxyLR: 0.6713  Epoch: 18  Global Step: 105320   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:40:41,540-Speed 3921.33 samples/sec  Loss 2.9646  LearningRate 0.0134  ProxyLR: 0.6709  Epoch: 18  Global Step: 105330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:44,167-Speed 3899.31 samples/sec  Loss 2.8836  LearningRate 0.0134  ProxyLR: 0.6706  Epoch: 18  Global Step: 105340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:46,792-Speed 3900.73 samples/sec  Loss 3.0339  LearningRate 0.0134  ProxyLR: 0.6702  Epoch: 18  Global Step: 105350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:49,419-Speed 3899.86 samples/sec  Loss 2.9972  LearningRate 0.0134  ProxyLR: 0.6698  Epoch: 18  Global Step: 105360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:52,045-Speed 3900.20 samples/sec  Loss 2.9849  LearningRate 0.0134  ProxyLR: 0.6695  Epoch: 18  Global Step: 105370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:54,670-Speed 3901.53 samples/sec  Loss 2.9042  LearningRate 0.0134  ProxyLR: 0.6691  Epoch: 18  Global Step: 105380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:57,297-Speed 3899.73 samples/sec  Loss 2.8189  LearningRate 0.0134  ProxyLR: 0.6687  Epoch: 18  Global Step: 105390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:40:59,923-Speed 3900.30 samples/sec  Loss 2.9619  LearningRate 0.0134  ProxyLR: 0.6684  Epoch: 18  Global Step: 105400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:02,548-Speed 3901.85 samples/sec  Loss 2.8734  LearningRate 0.0134  ProxyLR: 0.6680  Epoch: 18  Global Step: 105410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:05,174-Speed 3900.03 samples/sec  Loss 2.9355  LearningRate 0.0134  ProxyLR: 0.6676  Epoch: 18  Global Step: 105420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:07,785-Speed 3922.11 samples/sec  Loss 2.9670  LearningRate 0.0133  ProxyLR: 0.6673  Epoch: 18  Global Step: 105430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:10,411-Speed 3900.74 samples/sec  Loss 2.8996  LearningRate 0.0133  ProxyLR: 0.6669  Epoch: 18  Global Step: 105440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:13,035-Speed 3903.49 samples/sec  Loss 2.9144  LearningRate 0.0133  ProxyLR: 0.6666  Epoch: 18  Global Step: 105450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:15,660-Speed 3901.87 samples/sec  Loss 2.9494  LearningRate 0.0133  ProxyLR: 0.6662  Epoch: 18  Global Step: 105460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:18,288-Speed 3897.61 samples/sec  Loss 2.9874  LearningRate 0.0133  ProxyLR: 0.6658  Epoch: 18  Global Step: 105470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:20,914-Speed 3900.91 samples/sec  Loss 2.9887  LearningRate 0.0133  ProxyLR: 0.6655  Epoch: 18  Global Step: 105480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:23,540-Speed 3900.81 samples/sec  Loss 2.9917  LearningRate 0.0133  ProxyLR: 0.6651  Epoch: 18  Global Step: 105490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:26,165-Speed 3901.31 samples/sec  Loss 2.8285  LearningRate 0.0133  ProxyLR: 0.6647  Epoch: 18  Global Step: 105500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:28,792-Speed 3899.63 samples/sec  Loss 2.8842  LearningRate 0.0133  ProxyLR: 0.6644  Epoch: 18  Global Step: 105510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:31,417-Speed 3901.49 samples/sec  Loss 2.9587  LearningRate 0.0133  ProxyLR: 0.6640  Epoch: 18  Global Step: 105520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:34,031-Speed 3919.33 samples/sec  Loss 3.0506  LearningRate 0.0133  ProxyLR: 0.6637  Epoch: 18  Global Step: 105530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:36,657-Speed 3899.09 samples/sec  Loss 3.0303  LearningRate 0.0133  ProxyLR: 0.6633  Epoch: 18  Global Step: 105540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:39,284-Speed 3900.09 samples/sec  Loss 2.9527  LearningRate 0.0133  ProxyLR: 0.6629  Epoch: 18  Global Step: 105550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:41,909-Speed 3901.01 samples/sec  Loss 2.9395  LearningRate 0.0133  ProxyLR: 0.6626  Epoch: 18  Global Step: 105560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:44,535-Speed 3900.89 samples/sec  Loss 2.9489  LearningRate 0.0132  ProxyLR: 0.6622  Epoch: 18  Global Step: 105570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:47,160-Speed 3901.88 samples/sec  Loss 2.8893  LearningRate 0.0132  ProxyLR: 0.6618  Epoch: 18  Global Step: 105580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:49,786-Speed 3900.33 samples/sec  Loss 2.9906  LearningRate 0.0132  ProxyLR: 0.6615  Epoch: 18  Global Step: 105590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:52,410-Speed 3903.33 samples/sec  Loss 2.9196  LearningRate 0.0132  ProxyLR: 0.6611  Epoch: 18  Global Step: 105600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:55,036-Speed 3901.10 samples/sec  Loss 2.9814  LearningRate 0.0132  ProxyLR: 0.6608  Epoch: 18  Global Step: 105610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:41:57,661-Speed 3901.09 samples/sec  Loss 2.9032  LearningRate 0.0132  ProxyLR: 0.6604  Epoch: 18  Global Step: 105620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:00,275-Speed 3919.14 samples/sec  Loss 2.9421  LearningRate 0.0132  ProxyLR: 0.6600  Epoch: 18  Global Step: 105630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:02,901-Speed 3900.53 samples/sec  Loss 2.9333  LearningRate 0.0132  ProxyLR: 0.6597  Epoch: 18  Global Step: 105640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:05,527-Speed 3900.63 samples/sec  Loss 2.9406  LearningRate 0.0132  ProxyLR: 0.6593  Epoch: 18  Global Step: 105650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:08,152-Speed 3901.79 samples/sec  Loss 2.8778  LearningRate 0.0132  ProxyLR: 0.6590  Epoch: 18  Global Step: 105660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:10,776-Speed 3903.41 samples/sec  Loss 2.9687  LearningRate 0.0132  ProxyLR: 0.6586  Epoch: 18  Global Step: 105670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:13,401-Speed 3901.06 samples/sec  Loss 2.9397  LearningRate 0.0132  ProxyLR: 0.6582  Epoch: 18  Global Step: 105680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:16,026-Speed 3902.79 samples/sec  Loss 2.9708  LearningRate 0.0132  ProxyLR: 0.6579  Epoch: 18  Global Step: 105690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:18,652-Speed 3900.29 samples/sec  Loss 2.9540  LearningRate 0.0132  ProxyLR: 0.6575  Epoch: 18  Global Step: 105700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:21,277-Speed 3901.56 samples/sec  Loss 2.9660  LearningRate 0.0131  ProxyLR: 0.6571  Epoch: 18  Global Step: 105710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:23,902-Speed 3902.79 samples/sec  Loss 2.9028  LearningRate 0.0131  ProxyLR: 0.6568  Epoch: 18  Global Step: 105720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:26,528-Speed 3900.28 samples/sec  Loss 2.8878  LearningRate 0.0131  ProxyLR: 0.6564  Epoch: 18  Global Step: 105730   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:29,153-Speed 3901.65 samples/sec  Loss 2.9338  LearningRate 0.0131  ProxyLR: 0.6561  Epoch: 18  Global Step: 105740   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:31,777-Speed 3902.90 samples/sec  Loss 2.9427  LearningRate 0.0131  ProxyLR: 0.6557  Epoch: 18  Global Step: 105750   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:34,404-Speed 3899.12 samples/sec  Loss 2.9987  LearningRate 0.0131  ProxyLR: 0.6553  Epoch: 18  Global Step: 105760   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:37,030-Speed 3900.61 samples/sec  Loss 2.9456  LearningRate 0.0131  ProxyLR: 0.6550  Epoch: 18  Global Step: 105770   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:39,656-Speed 3900.62 samples/sec  Loss 2.9356  LearningRate 0.0131  ProxyLR: 0.6546  Epoch: 18  Global Step: 105780   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:42,281-Speed 3901.62 samples/sec  Loss 2.9627  LearningRate 0.0131  ProxyLR: 0.6543  Epoch: 18  Global Step: 105790   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:44,907-Speed 3899.91 samples/sec  Loss 2.9717  LearningRate 0.0131  ProxyLR: 0.6539  Epoch: 18  Global Step: 105800   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:47,534-Speed 3900.20 samples/sec  Loss 2.9086  LearningRate 0.0131  ProxyLR: 0.6535  Epoch: 18  Global Step: 105810   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:50,159-Speed 3901.33 samples/sec  Loss 2.9999  LearningRate 0.0131  ProxyLR: 0.6532  Epoch: 18  Global Step: 105820   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:42:52,771-Speed 3921.19 samples/sec  Loss 3.0021  LearningRate 0.0131  ProxyLR: 0.6528  Epoch: 18  Global Step: 105830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:55,397-Speed 3900.28 samples/sec  Loss 3.0694  LearningRate 0.0130  ProxyLR: 0.6525  Epoch: 18  Global Step: 105840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:42:58,021-Speed 3903.69 samples/sec  Loss 2.9669  LearningRate 0.0130  ProxyLR: 0.6521  Epoch: 18  Global Step: 105850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:00,648-Speed 3898.98 samples/sec  Loss 2.9909  LearningRate 0.0130  ProxyLR: 0.6517  Epoch: 18  Global Step: 105860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:03,274-Speed 3900.38 samples/sec  Loss 2.9598  LearningRate 0.0130  ProxyLR: 0.6514  Epoch: 18  Global Step: 105870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:05,900-Speed 3899.86 samples/sec  Loss 2.9102  LearningRate 0.0130  ProxyLR: 0.6510  Epoch: 18  Global Step: 105880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:08,527-Speed 3899.55 samples/sec  Loss 2.8533  LearningRate 0.0130  ProxyLR: 0.6507  Epoch: 18  Global Step: 105890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:11,153-Speed 3900.27 samples/sec  Loss 2.9841  LearningRate 0.0130  ProxyLR: 0.6503  Epoch: 18  Global Step: 105900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:13,779-Speed 3900.63 samples/sec  Loss 2.9117  LearningRate 0.0130  ProxyLR: 0.6500  Epoch: 18  Global Step: 105910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:16,405-Speed 3899.86 samples/sec  Loss 2.9493  LearningRate 0.0130  ProxyLR: 0.6496  Epoch: 18  Global Step: 105920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:19,030-Speed 3902.37 samples/sec  Loss 2.9421  LearningRate 0.0130  ProxyLR: 0.6492  Epoch: 18  Global Step: 105930   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:43:21,643-Speed 3919.84 samples/sec  Loss 2.9820  LearningRate 0.0130  ProxyLR: 0.6489  Epoch: 18  Global Step: 105940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:24,269-Speed 3901.33 samples/sec  Loss 2.9990  LearningRate 0.0130  ProxyLR: 0.6485  Epoch: 18  Global Step: 105950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:26,894-Speed 3900.79 samples/sec  Loss 3.0106  LearningRate 0.0130  ProxyLR: 0.6482  Epoch: 18  Global Step: 105960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:29,520-Speed 3901.66 samples/sec  Loss 3.0050  LearningRate 0.0130  ProxyLR: 0.6478  Epoch: 18  Global Step: 105970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:32,145-Speed 3901.78 samples/sec  Loss 2.9764  LearningRate 0.0129  ProxyLR: 0.6474  Epoch: 18  Global Step: 105980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:34,772-Speed 3899.40 samples/sec  Loss 2.9782  LearningRate 0.0129  ProxyLR: 0.6471  Epoch: 18  Global Step: 105990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:37,397-Speed 3901.33 samples/sec  Loss 2.9828  LearningRate 0.0129  ProxyLR: 0.6467  Epoch: 18  Global Step: 106000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:40,023-Speed 3900.14 samples/sec  Loss 2.9047  LearningRate 0.0129  ProxyLR: 0.6464  Epoch: 18  Global Step: 106010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:42,648-Speed 3901.95 samples/sec  Loss 2.9540  LearningRate 0.0129  ProxyLR: 0.6460  Epoch: 18  Global Step: 106020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:45,274-Speed 3901.17 samples/sec  Loss 2.9638  LearningRate 0.0129  ProxyLR: 0.6457  Epoch: 18  Global Step: 106030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:43:47,899-Speed 3901.55 samples/sec  Loss 2.9870  LearningRate 0.0129  ProxyLR: 0.6453  Epoch: 18  Global Step: 106040   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:43:50,524-Speed 3901.69 samples/sec  Loss 2.8343  LearningRate 0.0129  ProxyLR: 0.6449  Epoch: 18  Global Step: 106050   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:43:53,151-Speed 3898.62 samples/sec  Loss 2.9755  LearningRate 0.0129  ProxyLR: 0.6446  Epoch: 18  Global Step: 106060   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:43:55,776-Speed 3901.35 samples/sec  Loss 2.9946  LearningRate 0.0129  ProxyLR: 0.6442  Epoch: 18  Global Step: 106070   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:43:58,402-Speed 3900.91 samples/sec  Loss 2.8742  LearningRate 0.0129  ProxyLR: 0.6439  Epoch: 18  Global Step: 106080   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:44:01,028-Speed 3900.03 samples/sec  Loss 3.0069  LearningRate 0.0129  ProxyLR: 0.6435  Epoch: 18  Global Step: 106090   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:44:03,648-Speed 3910.28 samples/sec  Loss 3.0037  LearningRate 0.0129  ProxyLR: 0.6432  Epoch: 18  Global Step: 106100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:06,285-Speed 3883.91 samples/sec  Loss 2.9563  LearningRate 0.0129  ProxyLR: 0.6428  Epoch: 18  Global Step: 106110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:08,921-Speed 3884.85 samples/sec  Loss 2.8909  LearningRate 0.0128  ProxyLR: 0.6424  Epoch: 18  Global Step: 106120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:11,556-Speed 3887.75 samples/sec  Loss 2.9434  LearningRate 0.0128  ProxyLR: 0.6421  Epoch: 18  Global Step: 106130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:14,191-Speed 3886.95 samples/sec  Loss 2.9699  LearningRate 0.0128  ProxyLR: 0.6417  Epoch: 18  Global Step: 106140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:16,826-Speed 3887.47 samples/sec  Loss 2.9272  LearningRate 0.0128  ProxyLR: 0.6414  Epoch: 18  Global Step: 106150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:19,462-Speed 3884.93 samples/sec  Loss 2.9128  LearningRate 0.0128  ProxyLR: 0.6410  Epoch: 18  Global Step: 106160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:22,099-Speed 3884.75 samples/sec  Loss 2.9341  LearningRate 0.0128  ProxyLR: 0.6407  Epoch: 18  Global Step: 106170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:24,736-Speed 3884.36 samples/sec  Loss 2.8697  LearningRate 0.0128  ProxyLR: 0.6403  Epoch: 18  Global Step: 106180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:27,371-Speed 3887.22 samples/sec  Loss 2.9849  LearningRate 0.0128  ProxyLR: 0.6399  Epoch: 18  Global Step: 106190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:29,993-Speed 3905.30 samples/sec  Loss 2.9739  LearningRate 0.0128  ProxyLR: 0.6396  Epoch: 18  Global Step: 106200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:32,632-Speed 3882.63 samples/sec  Loss 2.9302  LearningRate 0.0128  ProxyLR: 0.6392  Epoch: 18  Global Step: 106210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:35,268-Speed 3884.37 samples/sec  Loss 2.9566  LearningRate 0.0128  ProxyLR: 0.6389  Epoch: 18  Global Step: 106220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:37,905-Speed 3884.87 samples/sec  Loss 2.9954  LearningRate 0.0128  ProxyLR: 0.6385  Epoch: 18  Global Step: 106230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:44:40,525-Speed 3908.80 samples/sec  Loss 2.9671  LearningRate 0.0128  ProxyLR: 0.6382  Epoch: 18  Global Step: 106240   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:43,161-Speed 3885.88 samples/sec  Loss 2.8976  LearningRate 0.0128  ProxyLR: 0.6378  Epoch: 18  Global Step: 106250   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:45,796-Speed 3886.76 samples/sec  Loss 3.0353  LearningRate 0.0127  ProxyLR: 0.6375  Epoch: 18  Global Step: 106260   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:48,432-Speed 3885.82 samples/sec  Loss 2.9816  LearningRate 0.0127  ProxyLR: 0.6371  Epoch: 18  Global Step: 106270   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:51,066-Speed 3888.76 samples/sec  Loss 2.9163  LearningRate 0.0127  ProxyLR: 0.6368  Epoch: 18  Global Step: 106280   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:53,700-Speed 3888.50 samples/sec  Loss 2.9853  LearningRate 0.0127  ProxyLR: 0.6364  Epoch: 18  Global Step: 106290   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:56,336-Speed 3885.27 samples/sec  Loss 2.9037  LearningRate 0.0127  ProxyLR: 0.6360  Epoch: 18  Global Step: 106300   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:44:58,970-Speed 3889.27 samples/sec  Loss 2.9676  LearningRate 0.0127  ProxyLR: 0.6357  Epoch: 18  Global Step: 106310   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:45:01,602-Speed 3890.84 samples/sec  Loss 2.9286  LearningRate 0.0127  ProxyLR: 0.6353  Epoch: 18  Global Step: 106320   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:45:04,234-Speed 3891.61 samples/sec  Loss 2.9040  LearningRate 0.0127  ProxyLR: 0.6350  Epoch: 18  Global Step: 106330   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 22:45:06,868-Speed 3889.54 samples/sec  Loss 3.0240  LearningRate 0.0127  ProxyLR: 0.6346  Epoch: 18  Global Step: 106340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:09,502-Speed 3888.63 samples/sec  Loss 2.8888  LearningRate 0.0127  ProxyLR: 0.6343  Epoch: 18  Global Step: 106350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:12,135-Speed 3889.77 samples/sec  Loss 3.0158  LearningRate 0.0127  ProxyLR: 0.6339  Epoch: 18  Global Step: 106360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:14,768-Speed 3889.52 samples/sec  Loss 2.8832  LearningRate 0.0127  ProxyLR: 0.6336  Epoch: 18  Global Step: 106370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:17,401-Speed 3890.38 samples/sec  Loss 2.9830  LearningRate 0.0127  ProxyLR: 0.6332  Epoch: 18  Global Step: 106380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:20,033-Speed 3892.44 samples/sec  Loss 2.9297  LearningRate 0.0127  ProxyLR: 0.6329  Epoch: 18  Global Step: 106390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:22,665-Speed 3891.06 samples/sec  Loss 2.8839  LearningRate 0.0126  ProxyLR: 0.6325  Epoch: 18  Global Step: 106400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:25,297-Speed 3891.21 samples/sec  Loss 2.9689  LearningRate 0.0126  ProxyLR: 0.6321  Epoch: 18  Global Step: 106410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:27,928-Speed 3892.78 samples/sec  Loss 2.9717  LearningRate 0.0126  ProxyLR: 0.6318  Epoch: 18  Global Step: 106420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:30,559-Speed 3893.13 samples/sec  Loss 2.9774  LearningRate 0.0126  ProxyLR: 0.6314  Epoch: 18  Global Step: 106430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:33,191-Speed 3891.80 samples/sec  Loss 2.9208  LearningRate 0.0126  ProxyLR: 0.6311  Epoch: 18  Global Step: 106440   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:45:35,824-Speed 3889.60 samples/sec  Loss 3.0093  LearningRate 0.0126  ProxyLR: 0.6307  Epoch: 18  Global Step: 106450   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:45:38,442-Speed 3912.45 samples/sec  Loss 2.9018  LearningRate 0.0126  ProxyLR: 0.6304  Epoch: 18  Global Step: 106460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:41,073-Speed 3893.32 samples/sec  Loss 2.9435  LearningRate 0.0126  ProxyLR: 0.6300  Epoch: 18  Global Step: 106470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:43,703-Speed 3894.68 samples/sec  Loss 2.9646  LearningRate 0.0126  ProxyLR: 0.6297  Epoch: 18  Global Step: 106480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:46,333-Speed 3894.21 samples/sec  Loss 2.9696  LearningRate 0.0126  ProxyLR: 0.6293  Epoch: 18  Global Step: 106490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:48,963-Speed 3894.99 samples/sec  Loss 2.9274  LearningRate 0.0126  ProxyLR: 0.6290  Epoch: 18  Global Step: 106500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:51,592-Speed 3895.30 samples/sec  Loss 2.9668  LearningRate 0.0126  ProxyLR: 0.6286  Epoch: 18  Global Step: 106510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:54,219-Speed 3898.43 samples/sec  Loss 3.0119  LearningRate 0.0126  ProxyLR: 0.6283  Epoch: 18  Global Step: 106520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:56,850-Speed 3894.14 samples/sec  Loss 2.9663  LearningRate 0.0126  ProxyLR: 0.6279  Epoch: 18  Global Step: 106530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:45:59,481-Speed 3893.15 samples/sec  Loss 2.9586  LearningRate 0.0126  ProxyLR: 0.6276  Epoch: 18  Global Step: 106540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:02,114-Speed 3888.92 samples/sec  Loss 2.8288  LearningRate 0.0125  ProxyLR: 0.6272  Epoch: 18  Global Step: 106550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:04,744-Speed 3895.08 samples/sec  Loss 2.8967  LearningRate 0.0125  ProxyLR: 0.6268  Epoch: 18  Global Step: 106560   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:07,373-Speed 3895.51 samples/sec  Loss 2.9314  LearningRate 0.0125  ProxyLR: 0.6265  Epoch: 18  Global Step: 106570   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:10,005-Speed 3892.12 samples/sec  Loss 2.8524  LearningRate 0.0125  ProxyLR: 0.6261  Epoch: 18  Global Step: 106580   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:12,637-Speed 3890.68 samples/sec  Loss 2.8826  LearningRate 0.0125  ProxyLR: 0.6258  Epoch: 18  Global Step: 106590   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:15,265-Speed 3898.02 samples/sec  Loss 2.9531  LearningRate 0.0125  ProxyLR: 0.6254  Epoch: 18  Global Step: 106600   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:17,893-Speed 3898.22 samples/sec  Loss 2.9657  LearningRate 0.0125  ProxyLR: 0.6251  Epoch: 18  Global Step: 106610   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:20,520-Speed 3897.70 samples/sec  Loss 2.9704  LearningRate 0.0125  ProxyLR: 0.6247  Epoch: 18  Global Step: 106620   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:23,146-Speed 3900.65 samples/sec  Loss 2.9626  LearningRate 0.0125  ProxyLR: 0.6244  Epoch: 18  Global Step: 106630   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:25,759-Speed 3919.59 samples/sec  Loss 3.0083  LearningRate 0.0125  ProxyLR: 0.6240  Epoch: 18  Global Step: 106640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:28,386-Speed 3899.62 samples/sec  Loss 2.9002  LearningRate 0.0125  ProxyLR: 0.6237  Epoch: 18  Global Step: 106650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:31,013-Speed 3898.90 samples/sec  Loss 2.9570  LearningRate 0.0125  ProxyLR: 0.6233  Epoch: 18  Global Step: 106660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:33,640-Speed 3899.55 samples/sec  Loss 3.0210  LearningRate 0.0125  ProxyLR: 0.6230  Epoch: 18  Global Step: 106670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:36,266-Speed 3900.21 samples/sec  Loss 3.0004  LearningRate 0.0125  ProxyLR: 0.6226  Epoch: 18  Global Step: 106680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:38,892-Speed 3899.95 samples/sec  Loss 2.9647  LearningRate 0.0124  ProxyLR: 0.6223  Epoch: 18  Global Step: 106690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:41,519-Speed 3899.90 samples/sec  Loss 2.9289  LearningRate 0.0124  ProxyLR: 0.6219  Epoch: 18  Global Step: 106700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:44,145-Speed 3899.88 samples/sec  Loss 2.9683  LearningRate 0.0124  ProxyLR: 0.6216  Epoch: 18  Global Step: 106710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:46,773-Speed 3898.02 samples/sec  Loss 2.9444  LearningRate 0.0124  ProxyLR: 0.6212  Epoch: 18  Global Step: 106720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:49,400-Speed 3898.27 samples/sec  Loss 2.9564  LearningRate 0.0124  ProxyLR: 0.6209  Epoch: 18  Global Step: 106730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:52,028-Speed 3897.36 samples/sec  Loss 2.8798  LearningRate 0.0124  ProxyLR: 0.6205  Epoch: 18  Global Step: 106740   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:54,655-Speed 3898.84 samples/sec  Loss 2.9316  LearningRate 0.0124  ProxyLR: 0.6202  Epoch: 18  Global Step: 106750   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:46:57,269-Speed 3918.85 samples/sec  Loss 2.9271  LearningRate 0.0124  ProxyLR: 0.6198  Epoch: 18  Global Step: 106760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:46:59,898-Speed 3896.24 samples/sec  Loss 2.9379  LearningRate 0.0124  ProxyLR: 0.6195  Epoch: 18  Global Step: 106770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:02,524-Speed 3899.99 samples/sec  Loss 3.0138  LearningRate 0.0124  ProxyLR: 0.6191  Epoch: 18  Global Step: 106780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:05,151-Speed 3899.13 samples/sec  Loss 2.8784  LearningRate 0.0124  ProxyLR: 0.6188  Epoch: 18  Global Step: 106790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:07,778-Speed 3898.90 samples/sec  Loss 2.9354  LearningRate 0.0124  ProxyLR: 0.6184  Epoch: 18  Global Step: 106800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:10,404-Speed 3900.52 samples/sec  Loss 2.9348  LearningRate 0.0124  ProxyLR: 0.6181  Epoch: 18  Global Step: 106810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:13,029-Speed 3901.13 samples/sec  Loss 2.8808  LearningRate 0.0124  ProxyLR: 0.6177  Epoch: 18  Global Step: 106820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:15,655-Speed 3900.67 samples/sec  Loss 2.9314  LearningRate 0.0123  ProxyLR: 0.6174  Epoch: 18  Global Step: 106830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:18,282-Speed 3899.31 samples/sec  Loss 2.9712  LearningRate 0.0123  ProxyLR: 0.6170  Epoch: 18  Global Step: 106840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:20,909-Speed 3899.02 samples/sec  Loss 2.9270  LearningRate 0.0123  ProxyLR: 0.6167  Epoch: 18  Global Step: 106850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:23,523-Speed 3919.04 samples/sec  Loss 2.9306  LearningRate 0.0123  ProxyLR: 0.6163  Epoch: 18  Global Step: 106860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:26,149-Speed 3899.98 samples/sec  Loss 2.8850  LearningRate 0.0123  ProxyLR: 0.6160  Epoch: 18  Global Step: 106870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:28,778-Speed 3896.72 samples/sec  Loss 2.9596  LearningRate 0.0123  ProxyLR: 0.6156  Epoch: 18  Global Step: 106880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:31,406-Speed 3896.53 samples/sec  Loss 2.9430  LearningRate 0.0123  ProxyLR: 0.6153  Epoch: 18  Global Step: 106890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:34,033-Speed 3899.25 samples/sec  Loss 2.9007  LearningRate 0.0123  ProxyLR: 0.6149  Epoch: 18  Global Step: 106900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:36,661-Speed 3897.72 samples/sec  Loss 2.8976  LearningRate 0.0123  ProxyLR: 0.6146  Epoch: 18  Global Step: 106910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:39,286-Speed 3902.02 samples/sec  Loss 3.0025  LearningRate 0.0123  ProxyLR: 0.6142  Epoch: 18  Global Step: 106920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:41,912-Speed 3899.37 samples/sec  Loss 2.9879  LearningRate 0.0123  ProxyLR: 0.6139  Epoch: 18  Global Step: 106930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:44,538-Speed 3900.34 samples/sec  Loss 2.9774  LearningRate 0.0123  ProxyLR: 0.6135  Epoch: 18  Global Step: 106940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:47,165-Speed 3899.77 samples/sec  Loss 3.0164  LearningRate 0.0123  ProxyLR: 0.6132  Epoch: 18  Global Step: 106950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:47:49,793-Speed 3897.62 samples/sec  Loss 2.8602  LearningRate 0.0123  ProxyLR: 0.6128  Epoch: 18  Global Step: 106960   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:47:52,420-Speed 3898.69 samples/sec  Loss 2.9570  LearningRate 0.0122  ProxyLR: 0.6125  Epoch: 18  Global Step: 106970   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:47:55,047-Speed 3899.47 samples/sec  Loss 2.9025  LearningRate 0.0122  ProxyLR: 0.6121  Epoch: 18  Global Step: 106980   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:47:57,660-Speed 3920.01 samples/sec  Loss 3.0807  LearningRate 0.0122  ProxyLR: 0.6118  Epoch: 18  Global Step: 106990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:00,286-Speed 3899.83 samples/sec  Loss 2.9098  LearningRate 0.0122  ProxyLR: 0.6114  Epoch: 18  Global Step: 107000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:02,914-Speed 3897.85 samples/sec  Loss 2.9200  LearningRate 0.0122  ProxyLR: 0.6111  Epoch: 18  Global Step: 107010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:05,540-Speed 3899.37 samples/sec  Loss 2.9734  LearningRate 0.0122  ProxyLR: 0.6107  Epoch: 18  Global Step: 107020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:08,166-Speed 3901.09 samples/sec  Loss 2.9416  LearningRate 0.0122  ProxyLR: 0.6104  Epoch: 18  Global Step: 107030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:10,797-Speed 3893.58 samples/sec  Loss 2.9980  LearningRate 0.0122  ProxyLR: 0.6101  Epoch: 18  Global Step: 107040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:13,430-Speed 3890.44 samples/sec  Loss 2.9256  LearningRate 0.0122  ProxyLR: 0.6097  Epoch: 18  Global Step: 107050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:16,060-Speed 3894.07 samples/sec  Loss 2.9440  LearningRate 0.0122  ProxyLR: 0.6094  Epoch: 18  Global Step: 107060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:18,687-Speed 3898.27 samples/sec  Loss 2.8917  LearningRate 0.0122  ProxyLR: 0.6090  Epoch: 18  Global Step: 107070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:21,314-Speed 3899.11 samples/sec  Loss 2.9858  LearningRate 0.0122  ProxyLR: 0.6087  Epoch: 18  Global Step: 107080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:23,927-Speed 3920.62 samples/sec  Loss 2.9813  LearningRate 0.0122  ProxyLR: 0.6083  Epoch: 18  Global Step: 107090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:26,553-Speed 3899.29 samples/sec  Loss 2.9792  LearningRate 0.0122  ProxyLR: 0.6080  Epoch: 18  Global Step: 107100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:29,182-Speed 3896.29 samples/sec  Loss 2.9188  LearningRate 0.0122  ProxyLR: 0.6076  Epoch: 18  Global Step: 107110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:31,808-Speed 3901.04 samples/sec  Loss 2.9604  LearningRate 0.0121  ProxyLR: 0.6073  Epoch: 18  Global Step: 107120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:34,434-Speed 3899.86 samples/sec  Loss 2.9724  LearningRate 0.0121  ProxyLR: 0.6069  Epoch: 18  Global Step: 107130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:37,061-Speed 3899.10 samples/sec  Loss 2.9799  LearningRate 0.0121  ProxyLR: 0.6066  Epoch: 18  Global Step: 107140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:39,688-Speed 3899.32 samples/sec  Loss 2.9347  LearningRate 0.0121  ProxyLR: 0.6062  Epoch: 18  Global Step: 107150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:42,314-Speed 3900.45 samples/sec  Loss 2.9815  LearningRate 0.0121  ProxyLR: 0.6059  Epoch: 18  Global Step: 107160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:44,940-Speed 3899.59 samples/sec  Loss 2.9631  LearningRate 0.0121  ProxyLR: 0.6055  Epoch: 18  Global Step: 107170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:47,566-Speed 3901.29 samples/sec  Loss 2.9469  LearningRate 0.0121  ProxyLR: 0.6052  Epoch: 18  Global Step: 107180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:50,193-Speed 3897.99 samples/sec  Loss 2.9789  LearningRate 0.0121  ProxyLR: 0.6049  Epoch: 18  Global Step: 107190   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:48:52,805-Speed 3921.76 samples/sec  Loss 2.9921  LearningRate 0.0121  ProxyLR: 0.6045  Epoch: 18  Global Step: 107200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:55,432-Speed 3899.41 samples/sec  Loss 2.9354  LearningRate 0.0121  ProxyLR: 0.6042  Epoch: 18  Global Step: 107210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:48:58,057-Speed 3902.19 samples/sec  Loss 2.9131  LearningRate 0.0121  ProxyLR: 0.6038  Epoch: 18  Global Step: 107220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:00,683-Speed 3899.84 samples/sec  Loss 2.8965  LearningRate 0.0121  ProxyLR: 0.6035  Epoch: 18  Global Step: 107230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:03,310-Speed 3899.43 samples/sec  Loss 2.9017  LearningRate 0.0121  ProxyLR: 0.6031  Epoch: 18  Global Step: 107240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:05,934-Speed 3902.83 samples/sec  Loss 3.0190  LearningRate 0.0121  ProxyLR: 0.6028  Epoch: 18  Global Step: 107250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:08,559-Speed 3902.47 samples/sec  Loss 2.8847  LearningRate 0.0120  ProxyLR: 0.6024  Epoch: 18  Global Step: 107260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:11,185-Speed 3900.51 samples/sec  Loss 2.9159  LearningRate 0.0120  ProxyLR: 0.6021  Epoch: 18  Global Step: 107270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:13,810-Speed 3902.56 samples/sec  Loss 2.9721  LearningRate 0.0120  ProxyLR: 0.6017  Epoch: 18  Global Step: 107280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:16,435-Speed 3901.08 samples/sec  Loss 2.9045  LearningRate 0.0120  ProxyLR: 0.6014  Epoch: 18  Global Step: 107290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:19,064-Speed 3895.70 samples/sec  Loss 2.9989  LearningRate 0.0120  ProxyLR: 0.6011  Epoch: 18  Global Step: 107300   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:49:21,698-Speed 3889.56 samples/sec  Loss 2.8058  LearningRate 0.0120  ProxyLR: 0.6007  Epoch: 18  Global Step: 107310   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:49:24,330-Speed 3890.87 samples/sec  Loss 2.8919  LearningRate 0.0120  ProxyLR: 0.6004  Epoch: 18  Global Step: 107320   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:49:26,950-Speed 3908.75 samples/sec  Loss 2.9383  LearningRate 0.0120  ProxyLR: 0.6000  Epoch: 18  Global Step: 107330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:29,583-Speed 3889.77 samples/sec  Loss 2.9714  LearningRate 0.0120  ProxyLR: 0.5997  Epoch: 18  Global Step: 107340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:32,217-Speed 3888.59 samples/sec  Loss 2.9422  LearningRate 0.0120  ProxyLR: 0.5993  Epoch: 18  Global Step: 107350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:34,851-Speed 3889.43 samples/sec  Loss 2.8931  LearningRate 0.0120  ProxyLR: 0.5990  Epoch: 18  Global Step: 107360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:37,485-Speed 3888.18 samples/sec  Loss 2.8907  LearningRate 0.0120  ProxyLR: 0.5986  Epoch: 18  Global Step: 107370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:40,119-Speed 3888.25 samples/sec  Loss 2.9197  LearningRate 0.0120  ProxyLR: 0.5983  Epoch: 18  Global Step: 107380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:42,751-Speed 3891.99 samples/sec  Loss 2.8731  LearningRate 0.0120  ProxyLR: 0.5980  Epoch: 18  Global Step: 107390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:45,386-Speed 3886.88 samples/sec  Loss 3.0034  LearningRate 0.0120  ProxyLR: 0.5976  Epoch: 18  Global Step: 107400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:48,019-Speed 3889.72 samples/sec  Loss 2.8889  LearningRate 0.0119  ProxyLR: 0.5973  Epoch: 18  Global Step: 107410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:50,651-Speed 3891.42 samples/sec  Loss 2.9570  LearningRate 0.0119  ProxyLR: 0.5969  Epoch: 18  Global Step: 107420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:49:53,284-Speed 3890.72 samples/sec  Loss 2.9831  LearningRate 0.0119  ProxyLR: 0.5966  Epoch: 18  Global Step: 107430   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:49:55,916-Speed 3892.14 samples/sec  Loss 2.8759  LearningRate 0.0119  ProxyLR: 0.5962  Epoch: 18  Global Step: 107440   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:49:58,546-Speed 3893.69 samples/sec  Loss 2.9589  LearningRate 0.0119  ProxyLR: 0.5959  Epoch: 18  Global Step: 107450   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:01,179-Speed 3890.51 samples/sec  Loss 2.8996  LearningRate 0.0119  ProxyLR: 0.5955  Epoch: 18  Global Step: 107460   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:03,810-Speed 3892.94 samples/sec  Loss 2.9990  LearningRate 0.0119  ProxyLR: 0.5952  Epoch: 18  Global Step: 107470   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:06,444-Speed 3889.91 samples/sec  Loss 2.8706  LearningRate 0.0119  ProxyLR: 0.5949  Epoch: 18  Global Step: 107480   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:09,075-Speed 3891.74 samples/sec  Loss 2.9605  LearningRate 0.0119  ProxyLR: 0.5945  Epoch: 18  Global Step: 107490   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:11,708-Speed 3891.34 samples/sec  Loss 2.8718  LearningRate 0.0119  ProxyLR: 0.5942  Epoch: 18  Global Step: 107500   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:14,341-Speed 3888.81 samples/sec  Loss 2.9459  LearningRate 0.0119  ProxyLR: 0.5938  Epoch: 18  Global Step: 107510   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:16,960-Speed 3910.99 samples/sec  Loss 2.9787  LearningRate 0.0119  ProxyLR: 0.5935  Epoch: 18  Global Step: 107520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:19,593-Speed 3890.56 samples/sec  Loss 2.9571  LearningRate 0.0119  ProxyLR: 0.5931  Epoch: 18  Global Step: 107530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:22,226-Speed 3889.58 samples/sec  Loss 2.9210  LearningRate 0.0119  ProxyLR: 0.5928  Epoch: 18  Global Step: 107540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:24,859-Speed 3889.91 samples/sec  Loss 2.9457  LearningRate 0.0118  ProxyLR: 0.5925  Epoch: 18  Global Step: 107550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:27,491-Speed 3891.67 samples/sec  Loss 2.9811  LearningRate 0.0118  ProxyLR: 0.5921  Epoch: 18  Global Step: 107560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:30,123-Speed 3891.70 samples/sec  Loss 2.9315  LearningRate 0.0118  ProxyLR: 0.5918  Epoch: 18  Global Step: 107570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:32,755-Speed 3891.73 samples/sec  Loss 2.9655  LearningRate 0.0118  ProxyLR: 0.5914  Epoch: 18  Global Step: 107580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:35,387-Speed 3891.10 samples/sec  Loss 2.9448  LearningRate 0.0118  ProxyLR: 0.5911  Epoch: 18  Global Step: 107590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:38,022-Speed 3887.26 samples/sec  Loss 2.9833  LearningRate 0.0118  ProxyLR: 0.5907  Epoch: 18  Global Step: 107600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:40,654-Speed 3891.29 samples/sec  Loss 2.8984  LearningRate 0.0118  ProxyLR: 0.5904  Epoch: 18  Global Step: 107610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:43,287-Speed 3890.77 samples/sec  Loss 3.0166  LearningRate 0.0118  ProxyLR: 0.5901  Epoch: 18  Global Step: 107620   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:45,919-Speed 3891.57 samples/sec  Loss 2.8517  LearningRate 0.0118  ProxyLR: 0.5897  Epoch: 18  Global Step: 107630   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:48,551-Speed 3890.79 samples/sec  Loss 2.9544  LearningRate 0.0118  ProxyLR: 0.5894  Epoch: 18  Global Step: 107640   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:50:51,171-Speed 3909.57 samples/sec  Loss 2.9712  LearningRate 0.0118  ProxyLR: 0.5890  Epoch: 18  Global Step: 107650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:53,802-Speed 3893.11 samples/sec  Loss 2.9940  LearningRate 0.0118  ProxyLR: 0.5887  Epoch: 18  Global Step: 107660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:56,433-Speed 3893.69 samples/sec  Loss 2.9133  LearningRate 0.0118  ProxyLR: 0.5884  Epoch: 18  Global Step: 107670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:50:59,062-Speed 3895.74 samples/sec  Loss 2.8982  LearningRate 0.0118  ProxyLR: 0.5880  Epoch: 18  Global Step: 107680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:01,698-Speed 3886.32 samples/sec  Loss 2.9908  LearningRate 0.0118  ProxyLR: 0.5877  Epoch: 18  Global Step: 107690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:04,332-Speed 3887.99 samples/sec  Loss 2.8998  LearningRate 0.0117  ProxyLR: 0.5873  Epoch: 18  Global Step: 107700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:06,967-Speed 3887.69 samples/sec  Loss 2.9704  LearningRate 0.0117  ProxyLR: 0.5870  Epoch: 18  Global Step: 107710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:09,600-Speed 3889.37 samples/sec  Loss 2.9040  LearningRate 0.0117  ProxyLR: 0.5867  Epoch: 18  Global Step: 107720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:12,233-Speed 3890.51 samples/sec  Loss 2.9260  LearningRate 0.0117  ProxyLR: 0.5863  Epoch: 18  Global Step: 107730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:14,867-Speed 3888.81 samples/sec  Loss 2.9173  LearningRate 0.0117  ProxyLR: 0.5860  Epoch: 18  Global Step: 107740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:17,502-Speed 3886.32 samples/sec  Loss 2.8690  LearningRate 0.0117  ProxyLR: 0.5856  Epoch: 18  Global Step: 107750   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:51:20,119-Speed 3914.65 samples/sec  Loss 2.8776  LearningRate 0.0117  ProxyLR: 0.5853  Epoch: 18  Global Step: 107760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:22,749-Speed 3894.57 samples/sec  Loss 2.9627  LearningRate 0.0117  ProxyLR: 0.5849  Epoch: 18  Global Step: 107770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:25,378-Speed 3895.98 samples/sec  Loss 2.9840  LearningRate 0.0117  ProxyLR: 0.5846  Epoch: 18  Global Step: 107780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:28,008-Speed 3894.01 samples/sec  Loss 2.9753  LearningRate 0.0117  ProxyLR: 0.5843  Epoch: 18  Global Step: 107790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:30,641-Speed 3889.99 samples/sec  Loss 2.9439  LearningRate 0.0117  ProxyLR: 0.5839  Epoch: 18  Global Step: 107800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:33,272-Speed 3892.48 samples/sec  Loss 2.9453  LearningRate 0.0117  ProxyLR: 0.5836  Epoch: 18  Global Step: 107810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:35,901-Speed 3896.25 samples/sec  Loss 2.9672  LearningRate 0.0117  ProxyLR: 0.5832  Epoch: 18  Global Step: 107820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:38,530-Speed 3896.59 samples/sec  Loss 2.9550  LearningRate 0.0117  ProxyLR: 0.5829  Epoch: 18  Global Step: 107830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:41,157-Speed 3898.71 samples/sec  Loss 2.9140  LearningRate 0.0117  ProxyLR: 0.5826  Epoch: 18  Global Step: 107840   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:43,785-Speed 3897.70 samples/sec  Loss 2.9425  LearningRate 0.0116  ProxyLR: 0.5822  Epoch: 18  Global Step: 107850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:46,416-Speed 3892.79 samples/sec  Loss 2.9201  LearningRate 0.0116  ProxyLR: 0.5819  Epoch: 18  Global Step: 107860   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:51:49,032-Speed 3914.71 samples/sec  Loss 2.9225  LearningRate 0.0116  ProxyLR: 0.5816  Epoch: 18  Global Step: 107870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:51,661-Speed 3896.92 samples/sec  Loss 2.9603  LearningRate 0.0116  ProxyLR: 0.5812  Epoch: 18  Global Step: 107880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:54,293-Speed 3891.25 samples/sec  Loss 2.8856  LearningRate 0.0116  ProxyLR: 0.5809  Epoch: 18  Global Step: 107890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:56,925-Speed 3891.34 samples/sec  Loss 2.9001  LearningRate 0.0116  ProxyLR: 0.5805  Epoch: 18  Global Step: 107900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:51:59,560-Speed 3887.19 samples/sec  Loss 2.9173  LearningRate 0.0116  ProxyLR: 0.5802  Epoch: 18  Global Step: 107910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:02,193-Speed 3890.46 samples/sec  Loss 3.0401  LearningRate 0.0116  ProxyLR: 0.5799  Epoch: 18  Global Step: 107920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:04,826-Speed 3889.99 samples/sec  Loss 2.9181  LearningRate 0.0116  ProxyLR: 0.5795  Epoch: 18  Global Step: 107930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:07,460-Speed 3888.15 samples/sec  Loss 2.8240  LearningRate 0.0116  ProxyLR: 0.5792  Epoch: 18  Global Step: 107940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:10,095-Speed 3887.39 samples/sec  Loss 2.9235  LearningRate 0.0116  ProxyLR: 0.5788  Epoch: 18  Global Step: 107950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:12,729-Speed 3889.10 samples/sec  Loss 2.9463  LearningRate 0.0116  ProxyLR: 0.5785  Epoch: 18  Global Step: 107960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:15,361-Speed 3890.92 samples/sec  Loss 2.8811  LearningRate 0.0116  ProxyLR: 0.5782  Epoch: 18  Global Step: 107970   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:52:17,979-Speed 3913.15 samples/sec  Loss 2.9750  LearningRate 0.0116  ProxyLR: 0.5778  Epoch: 18  Global Step: 107980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:20,609-Speed 3894.45 samples/sec  Loss 2.9603  LearningRate 0.0115  ProxyLR: 0.5775  Epoch: 18  Global Step: 107990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:23,237-Speed 3896.78 samples/sec  Loss 2.9215  LearningRate 0.0115  ProxyLR: 0.5771  Epoch: 18  Global Step: 108000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:25,868-Speed 3892.55 samples/sec  Loss 2.9226  LearningRate 0.0115  ProxyLR: 0.5768  Epoch: 18  Global Step: 108010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:28,502-Speed 3888.88 samples/sec  Loss 2.8538  LearningRate 0.0115  ProxyLR: 0.5765  Epoch: 18  Global Step: 108020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:31,191-Speed 3809.48 samples/sec  Loss 2.9471  LearningRate 0.0115  ProxyLR: 0.5761  Epoch: 18  Global Step: 108030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:40,065-Speed 1154.01 samples/sec  Loss 2.7710  LearningRate 0.0115  ProxyLR: 0.5758  Epoch: 19  Global Step: 108040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:42,761-Speed 3800.62 samples/sec  Loss 2.5915  LearningRate 0.0115  ProxyLR: 0.5755  Epoch: 19  Global Step: 108050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:45,393-Speed 3891.83 samples/sec  Loss 2.6917  LearningRate 0.0115  ProxyLR: 0.5751  Epoch: 19  Global Step: 108060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:48,087-Speed 3801.51 samples/sec  Loss 2.6648  LearningRate 0.0115  ProxyLR: 0.5748  Epoch: 19  Global Step: 108070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:52:50,717-Speed 3894.73 samples/sec  Loss 2.6483  LearningRate 0.0115  ProxyLR: 0.5744  Epoch: 19  Global Step: 108080   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:52:53,353-Speed 3885.83 samples/sec  Loss 2.6161  LearningRate 0.0115  ProxyLR: 0.5741  Epoch: 19  Global Step: 108090   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:52:55,988-Speed 3886.50 samples/sec  Loss 2.6956  LearningRate 0.0115  ProxyLR: 0.5738  Epoch: 19  Global Step: 108100   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:52:58,623-Speed 3888.07 samples/sec  Loss 2.7074  LearningRate 0.0115  ProxyLR: 0.5734  Epoch: 19  Global Step: 108110   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:01,254-Speed 3893.22 samples/sec  Loss 2.6712  LearningRate 0.0115  ProxyLR: 0.5731  Epoch: 19  Global Step: 108120   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:03,906-Speed 3861.53 samples/sec  Loss 2.7295  LearningRate 0.0115  ProxyLR: 0.5728  Epoch: 19  Global Step: 108130   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:06,544-Speed 3883.96 samples/sec  Loss 2.6805  LearningRate 0.0114  ProxyLR: 0.5724  Epoch: 19  Global Step: 108140   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:09,178-Speed 3887.40 samples/sec  Loss 2.6878  LearningRate 0.0114  ProxyLR: 0.5721  Epoch: 19  Global Step: 108150   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:11,816-Speed 3883.89 samples/sec  Loss 2.7564  LearningRate 0.0114  ProxyLR: 0.5718  Epoch: 19  Global Step: 108160   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:14,453-Speed 3884.32 samples/sec  Loss 2.6789  LearningRate 0.0114  ProxyLR: 0.5714  Epoch: 19  Global Step: 108170   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:53:17,077-Speed 3902.60 samples/sec  Loss 2.6752  LearningRate 0.0114  ProxyLR: 0.5711  Epoch: 19  Global Step: 108180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:19,715-Speed 3882.54 samples/sec  Loss 2.6840  LearningRate 0.0114  ProxyLR: 0.5707  Epoch: 19  Global Step: 108190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:22,351-Speed 3886.64 samples/sec  Loss 2.8069  LearningRate 0.0114  ProxyLR: 0.5704  Epoch: 19  Global Step: 108200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:24,990-Speed 3881.83 samples/sec  Loss 2.7303  LearningRate 0.0114  ProxyLR: 0.5701  Epoch: 19  Global Step: 108210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:27,626-Speed 3885.84 samples/sec  Loss 2.6681  LearningRate 0.0114  ProxyLR: 0.5697  Epoch: 19  Global Step: 108220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:30,261-Speed 3886.21 samples/sec  Loss 2.6071  LearningRate 0.0114  ProxyLR: 0.5694  Epoch: 19  Global Step: 108230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:32,940-Speed 3823.57 samples/sec  Loss 2.6776  LearningRate 0.0114  ProxyLR: 0.5691  Epoch: 19  Global Step: 108240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:35,575-Speed 3887.56 samples/sec  Loss 2.7085  LearningRate 0.0114  ProxyLR: 0.5687  Epoch: 19  Global Step: 108250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:38,212-Speed 3885.28 samples/sec  Loss 2.7190  LearningRate 0.0114  ProxyLR: 0.5684  Epoch: 19  Global Step: 108260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:40,849-Speed 3884.41 samples/sec  Loss 2.7014  LearningRate 0.0114  ProxyLR: 0.5681  Epoch: 19  Global Step: 108270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:43,470-Speed 3907.57 samples/sec  Loss 2.7178  LearningRate 0.0114  ProxyLR: 0.5677  Epoch: 19  Global Step: 108280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:46,106-Speed 3886.08 samples/sec  Loss 2.7432  LearningRate 0.0113  ProxyLR: 0.5674  Epoch: 19  Global Step: 108290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:48,740-Speed 3888.30 samples/sec  Loss 2.7465  LearningRate 0.0113  ProxyLR: 0.5671  Epoch: 19  Global Step: 108300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:51,377-Speed 3884.86 samples/sec  Loss 2.6103  LearningRate 0.0113  ProxyLR: 0.5667  Epoch: 19  Global Step: 108310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:54,011-Speed 3888.32 samples/sec  Loss 2.7594  LearningRate 0.0113  ProxyLR: 0.5664  Epoch: 19  Global Step: 108320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:56,645-Speed 3889.31 samples/sec  Loss 2.7788  LearningRate 0.0113  ProxyLR: 0.5660  Epoch: 19  Global Step: 108330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:53:59,279-Speed 3887.88 samples/sec  Loss 2.6161  LearningRate 0.0113  ProxyLR: 0.5657  Epoch: 19  Global Step: 108340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:01,912-Speed 3891.52 samples/sec  Loss 2.6846  LearningRate 0.0113  ProxyLR: 0.5654  Epoch: 19  Global Step: 108350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:04,548-Speed 3885.68 samples/sec  Loss 2.7004  LearningRate 0.0113  ProxyLR: 0.5650  Epoch: 19  Global Step: 108360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:07,184-Speed 3885.73 samples/sec  Loss 2.6386  LearningRate 0.0113  ProxyLR: 0.5647  Epoch: 19  Global Step: 108370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:09,817-Speed 3889.24 samples/sec  Loss 2.6557  LearningRate 0.0113  ProxyLR: 0.5644  Epoch: 19  Global Step: 108380   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:54:12,450-Speed 3890.44 samples/sec  Loss 2.6848  LearningRate 0.0113  ProxyLR: 0.5640  Epoch: 19  Global Step: 108390   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:54:15,086-Speed 3886.81 samples/sec  Loss 2.7538  LearningRate 0.0113  ProxyLR: 0.5637  Epoch: 19  Global Step: 108400   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:54:17,718-Speed 3891.01 samples/sec  Loss 2.6665  LearningRate 0.0113  ProxyLR: 0.5634  Epoch: 19  Global Step: 108410   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:54:20,337-Speed 3911.04 samples/sec  Loss 2.7924  LearningRate 0.0113  ProxyLR: 0.5630  Epoch: 19  Global Step: 108420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:22,971-Speed 3889.10 samples/sec  Loss 2.6868  LearningRate 0.0113  ProxyLR: 0.5627  Epoch: 19  Global Step: 108430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:25,605-Speed 3888.61 samples/sec  Loss 2.6490  LearningRate 0.0112  ProxyLR: 0.5624  Epoch: 19  Global Step: 108440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:28,238-Speed 3890.60 samples/sec  Loss 2.7171  LearningRate 0.0112  ProxyLR: 0.5620  Epoch: 19  Global Step: 108450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:30,872-Speed 3888.29 samples/sec  Loss 2.6211  LearningRate 0.0112  ProxyLR: 0.5617  Epoch: 19  Global Step: 108460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:33,509-Speed 3884.16 samples/sec  Loss 2.7312  LearningRate 0.0112  ProxyLR: 0.5614  Epoch: 19  Global Step: 108470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:36,141-Speed 3891.71 samples/sec  Loss 2.6815  LearningRate 0.0112  ProxyLR: 0.5610  Epoch: 19  Global Step: 108480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:38,774-Speed 3891.34 samples/sec  Loss 2.6997  LearningRate 0.0112  ProxyLR: 0.5607  Epoch: 19  Global Step: 108490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:41,405-Speed 3892.00 samples/sec  Loss 2.7020  LearningRate 0.0112  ProxyLR: 0.5604  Epoch: 19  Global Step: 108500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:44,042-Speed 3885.34 samples/sec  Loss 2.7888  LearningRate 0.0112  ProxyLR: 0.5600  Epoch: 19  Global Step: 108510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:46,675-Speed 3889.64 samples/sec  Loss 2.6894  LearningRate 0.0112  ProxyLR: 0.5597  Epoch: 19  Global Step: 108520   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:54:49,309-Speed 3889.45 samples/sec  Loss 2.6987  LearningRate 0.0112  ProxyLR: 0.5594  Epoch: 19  Global Step: 108530   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:54:51,929-Speed 3909.40 samples/sec  Loss 2.7030  LearningRate 0.0112  ProxyLR: 0.5590  Epoch: 19  Global Step: 108540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:54,563-Speed 3888.50 samples/sec  Loss 2.6581  LearningRate 0.0112  ProxyLR: 0.5587  Epoch: 19  Global Step: 108550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:57,198-Speed 3886.97 samples/sec  Loss 2.6897  LearningRate 0.0112  ProxyLR: 0.5584  Epoch: 19  Global Step: 108560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:54:59,833-Speed 3888.00 samples/sec  Loss 2.6896  LearningRate 0.0112  ProxyLR: 0.5580  Epoch: 19  Global Step: 108570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:02,465-Speed 3891.18 samples/sec  Loss 2.6771  LearningRate 0.0112  ProxyLR: 0.5577  Epoch: 19  Global Step: 108580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:05,098-Speed 3891.12 samples/sec  Loss 2.7403  LearningRate 0.0111  ProxyLR: 0.5574  Epoch: 19  Global Step: 108590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:07,730-Speed 3891.22 samples/sec  Loss 2.6794  LearningRate 0.0111  ProxyLR: 0.5570  Epoch: 19  Global Step: 108600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:10,362-Speed 3892.10 samples/sec  Loss 2.7080  LearningRate 0.0111  ProxyLR: 0.5567  Epoch: 19  Global Step: 108610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:12,993-Speed 3892.81 samples/sec  Loss 2.7308  LearningRate 0.0111  ProxyLR: 0.5564  Epoch: 19  Global Step: 108620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:15,628-Speed 3886.85 samples/sec  Loss 2.7517  LearningRate 0.0111  ProxyLR: 0.5561  Epoch: 19  Global Step: 108630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:18,263-Speed 3887.19 samples/sec  Loss 2.6967  LearningRate 0.0111  ProxyLR: 0.5557  Epoch: 19  Global Step: 108640   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:55:20,897-Speed 3889.16 samples/sec  Loss 2.6569  LearningRate 0.0111  ProxyLR: 0.5554  Epoch: 19  Global Step: 108650   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:55:23,533-Speed 3885.42 samples/sec  Loss 2.7325  LearningRate 0.0111  ProxyLR: 0.5551  Epoch: 19  Global Step: 108660   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:55:26,167-Speed 3888.89 samples/sec  Loss 2.7283  LearningRate 0.0111  ProxyLR: 0.5547  Epoch: 19  Global Step: 108670   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:55:28,802-Speed 3887.69 samples/sec  Loss 2.7629  LearningRate 0.0111  ProxyLR: 0.5544  Epoch: 19  Global Step: 108680   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:55:31,425-Speed 3905.10 samples/sec  Loss 2.7057  LearningRate 0.0111  ProxyLR: 0.5541  Epoch: 19  Global Step: 108690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:34,058-Speed 3889.88 samples/sec  Loss 2.7413  LearningRate 0.0111  ProxyLR: 0.5537  Epoch: 19  Global Step: 108700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:36,694-Speed 3885.89 samples/sec  Loss 2.7182  LearningRate 0.0111  ProxyLR: 0.5534  Epoch: 19  Global Step: 108710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:39,329-Speed 3887.11 samples/sec  Loss 2.7148  LearningRate 0.0111  ProxyLR: 0.5531  Epoch: 19  Global Step: 108720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:41,962-Speed 3890.27 samples/sec  Loss 2.6265  LearningRate 0.0111  ProxyLR: 0.5527  Epoch: 19  Global Step: 108730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:44,596-Speed 3889.20 samples/sec  Loss 2.6730  LearningRate 0.0110  ProxyLR: 0.5524  Epoch: 19  Global Step: 108740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:47,230-Speed 3889.32 samples/sec  Loss 2.7420  LearningRate 0.0110  ProxyLR: 0.5521  Epoch: 19  Global Step: 108750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:49,867-Speed 3883.78 samples/sec  Loss 2.6453  LearningRate 0.0110  ProxyLR: 0.5517  Epoch: 19  Global Step: 108760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:52,504-Speed 3884.11 samples/sec  Loss 2.6510  LearningRate 0.0110  ProxyLR: 0.5514  Epoch: 19  Global Step: 108770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:55,139-Speed 3886.40 samples/sec  Loss 2.8010  LearningRate 0.0110  ProxyLR: 0.5511  Epoch: 19  Global Step: 108780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:55:57,777-Speed 3883.04 samples/sec  Loss 2.7361  LearningRate 0.0110  ProxyLR: 0.5508  Epoch: 19  Global Step: 108790   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:00,415-Speed 3883.33 samples/sec  Loss 2.6201  LearningRate 0.0110  ProxyLR: 0.5504  Epoch: 19  Global Step: 108800   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:03,053-Speed 3883.47 samples/sec  Loss 2.6998  LearningRate 0.0110  ProxyLR: 0.5501  Epoch: 19  Global Step: 108810   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:05,691-Speed 3881.76 samples/sec  Loss 2.6507  LearningRate 0.0110  ProxyLR: 0.5498  Epoch: 19  Global Step: 108820   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:08,330-Speed 3881.49 samples/sec  Loss 2.6881  LearningRate 0.0110  ProxyLR: 0.5494  Epoch: 19  Global Step: 108830   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:10,965-Speed 3887.41 samples/sec  Loss 2.6767  LearningRate 0.0110  ProxyLR: 0.5491  Epoch: 19  Global Step: 108840   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:13,597-Speed 3892.29 samples/sec  Loss 2.7217  LearningRate 0.0110  ProxyLR: 0.5488  Epoch: 19  Global Step: 108850   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:16,228-Speed 3893.13 samples/sec  Loss 2.6909  LearningRate 0.0110  ProxyLR: 0.5484  Epoch: 19  Global Step: 108860   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:18,858-Speed 3894.09 samples/sec  Loss 2.7086  LearningRate 0.0110  ProxyLR: 0.5481  Epoch: 19  Global Step: 108870   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:21,487-Speed 3895.47 samples/sec  Loss 2.7203  LearningRate 0.0110  ProxyLR: 0.5478  Epoch: 19  Global Step: 108880   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:24,104-Speed 3915.15 samples/sec  Loss 2.7453  LearningRate 0.0109  ProxyLR: 0.5475  Epoch: 19  Global Step: 108890   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:26,718-Speed 3918.64 samples/sec  Loss 2.7645  LearningRate 0.0109  ProxyLR: 0.5471  Epoch: 19  Global Step: 108900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:29,347-Speed 3896.23 samples/sec  Loss 2.7249  LearningRate 0.0109  ProxyLR: 0.5468  Epoch: 19  Global Step: 108910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:31,974-Speed 3898.33 samples/sec  Loss 2.7258  LearningRate 0.0109  ProxyLR: 0.5465  Epoch: 19  Global Step: 108920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:34,603-Speed 3896.81 samples/sec  Loss 2.7435  LearningRate 0.0109  ProxyLR: 0.5461  Epoch: 19  Global Step: 108930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:37,230-Speed 3898.79 samples/sec  Loss 2.7669  LearningRate 0.0109  ProxyLR: 0.5458  Epoch: 19  Global Step: 108940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:39,857-Speed 3898.98 samples/sec  Loss 2.7329  LearningRate 0.0109  ProxyLR: 0.5455  Epoch: 19  Global Step: 108950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:42,484-Speed 3898.22 samples/sec  Loss 2.7882  LearningRate 0.0109  ProxyLR: 0.5452  Epoch: 19  Global Step: 108960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:45,112-Speed 3897.27 samples/sec  Loss 2.7125  LearningRate 0.0109  ProxyLR: 0.5448  Epoch: 19  Global Step: 108970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:47,743-Speed 3894.13 samples/sec  Loss 2.6898  LearningRate 0.0109  ProxyLR: 0.5445  Epoch: 19  Global Step: 108980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:50,370-Speed 3899.32 samples/sec  Loss 2.6314  LearningRate 0.0109  ProxyLR: 0.5442  Epoch: 19  Global Step: 108990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:56:52,997-Speed 3898.26 samples/sec  Loss 2.7411  LearningRate 0.0109  ProxyLR: 0.5438  Epoch: 19  Global Step: 109000   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:55,624-Speed 3898.51 samples/sec  Loss 2.6575  LearningRate 0.0109  ProxyLR: 0.5435  Epoch: 19  Global Step: 109010   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:56:58,250-Speed 3901.80 samples/sec  Loss 2.7877  LearningRate 0.0109  ProxyLR: 0.5432  Epoch: 19  Global Step: 109020   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:57:00,879-Speed 3896.15 samples/sec  Loss 2.7608  LearningRate 0.0109  ProxyLR: 0.5429  Epoch: 19  Global Step: 109030   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:57:03,492-Speed 3919.77 samples/sec  Loss 2.6837  LearningRate 0.0109  ProxyLR: 0.5425  Epoch: 19  Global Step: 109040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:06,119-Speed 3898.85 samples/sec  Loss 2.6649  LearningRate 0.0108  ProxyLR: 0.5422  Epoch: 19  Global Step: 109050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:08,747-Speed 3897.32 samples/sec  Loss 2.6945  LearningRate 0.0108  ProxyLR: 0.5419  Epoch: 19  Global Step: 109060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:11,375-Speed 3898.09 samples/sec  Loss 2.6977  LearningRate 0.0108  ProxyLR: 0.5415  Epoch: 19  Global Step: 109070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:14,002-Speed 3899.18 samples/sec  Loss 2.7594  LearningRate 0.0108  ProxyLR: 0.5412  Epoch: 19  Global Step: 109080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:16,630-Speed 3897.25 samples/sec  Loss 2.6243  LearningRate 0.0108  ProxyLR: 0.5409  Epoch: 19  Global Step: 109090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:19,255-Speed 3901.85 samples/sec  Loss 2.7272  LearningRate 0.0108  ProxyLR: 0.5406  Epoch: 19  Global Step: 109100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:21,883-Speed 3897.80 samples/sec  Loss 2.6592  LearningRate 0.0108  ProxyLR: 0.5402  Epoch: 19  Global Step: 109110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:24,511-Speed 3897.76 samples/sec  Loss 2.7472  LearningRate 0.0108  ProxyLR: 0.5399  Epoch: 19  Global Step: 109120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:27,138-Speed 3899.61 samples/sec  Loss 2.7255  LearningRate 0.0108  ProxyLR: 0.5396  Epoch: 19  Global Step: 109130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:29,765-Speed 3898.75 samples/sec  Loss 2.7248  LearningRate 0.0108  ProxyLR: 0.5393  Epoch: 19  Global Step: 109140   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:57:32,394-Speed 3895.31 samples/sec  Loss 2.7212  LearningRate 0.0108  ProxyLR: 0.5389  Epoch: 19  Global Step: 109150   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:57:35,023-Speed 3896.48 samples/sec  Loss 2.7367  LearningRate 0.0108  ProxyLR: 0.5386  Epoch: 19  Global Step: 109160   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:57:37,651-Speed 3898.15 samples/sec  Loss 2.7430  LearningRate 0.0108  ProxyLR: 0.5383  Epoch: 19  Global Step: 109170   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:57:40,264-Speed 3920.24 samples/sec  Loss 2.6898  LearningRate 0.0108  ProxyLR: 0.5380  Epoch: 19  Global Step: 109180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:42,890-Speed 3900.24 samples/sec  Loss 2.7038  LearningRate 0.0108  ProxyLR: 0.5376  Epoch: 19  Global Step: 109190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:45,517-Speed 3899.30 samples/sec  Loss 2.6659  LearningRate 0.0107  ProxyLR: 0.5373  Epoch: 19  Global Step: 109200   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:48,143-Speed 3900.45 samples/sec  Loss 2.6295  LearningRate 0.0107  ProxyLR: 0.5370  Epoch: 19  Global Step: 109210   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:50,769-Speed 3900.22 samples/sec  Loss 2.7055  LearningRate 0.0107  ProxyLR: 0.5366  Epoch: 19  Global Step: 109220   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:53,396-Speed 3900.08 samples/sec  Loss 2.6761  LearningRate 0.0107  ProxyLR: 0.5363  Epoch: 19  Global Step: 109230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:56,022-Speed 3899.47 samples/sec  Loss 2.6893  LearningRate 0.0107  ProxyLR: 0.5360  Epoch: 19  Global Step: 109240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:57:58,647-Speed 3902.17 samples/sec  Loss 2.6797  LearningRate 0.0107  ProxyLR: 0.5357  Epoch: 19  Global Step: 109250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:01,273-Speed 3900.82 samples/sec  Loss 2.7202  LearningRate 0.0107  ProxyLR: 0.5353  Epoch: 19  Global Step: 109260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:03,899-Speed 3900.78 samples/sec  Loss 2.7168  LearningRate 0.0107  ProxyLR: 0.5350  Epoch: 19  Global Step: 109270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:06,527-Speed 3897.11 samples/sec  Loss 2.7764  LearningRate 0.0107  ProxyLR: 0.5347  Epoch: 19  Global Step: 109280   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:58:09,153-Speed 3900.37 samples/sec  Loss 2.6892  LearningRate 0.0107  ProxyLR: 0.5344  Epoch: 19  Global Step: 109290   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:58:11,782-Speed 3897.02 samples/sec  Loss 2.6406  LearningRate 0.0107  ProxyLR: 0.5340  Epoch: 19  Global Step: 109300   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:58:14,409-Speed 3898.20 samples/sec  Loss 2.6390  LearningRate 0.0107  ProxyLR: 0.5337  Epoch: 19  Global Step: 109310   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:58:17,038-Speed 3897.01 samples/sec  Loss 2.7031  LearningRate 0.0107  ProxyLR: 0.5334  Epoch: 19  Global Step: 109320   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:58:19,652-Speed 3918.45 samples/sec  Loss 2.6811  LearningRate 0.0107  ProxyLR: 0.5331  Epoch: 19  Global Step: 109330   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:22,279-Speed 3899.24 samples/sec  Loss 2.7387  LearningRate 0.0107  ProxyLR: 0.5327  Epoch: 19  Global Step: 109340   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:24,906-Speed 3899.17 samples/sec  Loss 2.7682  LearningRate 0.0106  ProxyLR: 0.5324  Epoch: 19  Global Step: 109350   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:27,532-Speed 3900.35 samples/sec  Loss 2.7249  LearningRate 0.0106  ProxyLR: 0.5321  Epoch: 19  Global Step: 109360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:30,158-Speed 3899.70 samples/sec  Loss 2.7242  LearningRate 0.0106  ProxyLR: 0.5318  Epoch: 19  Global Step: 109370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:32,786-Speed 3898.07 samples/sec  Loss 2.7241  LearningRate 0.0106  ProxyLR: 0.5314  Epoch: 19  Global Step: 109380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:35,419-Speed 3890.87 samples/sec  Loss 2.7076  LearningRate 0.0106  ProxyLR: 0.5311  Epoch: 19  Global Step: 109390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:38,051-Speed 3890.46 samples/sec  Loss 2.6945  LearningRate 0.0106  ProxyLR: 0.5308  Epoch: 19  Global Step: 109400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:40,683-Speed 3892.74 samples/sec  Loss 2.7301  LearningRate 0.0106  ProxyLR: 0.5305  Epoch: 19  Global Step: 109410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:43,316-Speed 3889.44 samples/sec  Loss 2.7783  LearningRate 0.0106  ProxyLR: 0.5301  Epoch: 19  Global Step: 109420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:45,952-Speed 3886.29 samples/sec  Loss 2.6589  LearningRate 0.0106  ProxyLR: 0.5298  Epoch: 19  Global Step: 109430   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:58:48,577-Speed 3902.63 samples/sec  Loss 2.7555  LearningRate 0.0106  ProxyLR: 0.5295  Epoch: 19  Global Step: 109440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:51,213-Speed 3885.36 samples/sec  Loss 2.6993  LearningRate 0.0106  ProxyLR: 0.5292  Epoch: 19  Global Step: 109450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:53,849-Speed 3885.74 samples/sec  Loss 2.6977  LearningRate 0.0106  ProxyLR: 0.5289  Epoch: 19  Global Step: 109460   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:56,484-Speed 3887.04 samples/sec  Loss 2.6820  LearningRate 0.0106  ProxyLR: 0.5285  Epoch: 19  Global Step: 109470   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:58:59,118-Speed 3888.69 samples/sec  Loss 2.7750  LearningRate 0.0106  ProxyLR: 0.5282  Epoch: 19  Global Step: 109480   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:01,750-Speed 3891.56 samples/sec  Loss 2.6623  LearningRate 0.0106  ProxyLR: 0.5279  Epoch: 19  Global Step: 109490   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:04,374-Speed 3904.01 samples/sec  Loss 2.7143  LearningRate 0.0106  ProxyLR: 0.5276  Epoch: 19  Global Step: 109500   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:06,998-Speed 3902.85 samples/sec  Loss 2.7177  LearningRate 0.0105  ProxyLR: 0.5272  Epoch: 19  Global Step: 109510   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:09,624-Speed 3901.09 samples/sec  Loss 2.6773  LearningRate 0.0105  ProxyLR: 0.5269  Epoch: 19  Global Step: 109520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:12,249-Speed 3901.58 samples/sec  Loss 2.7070  LearningRate 0.0105  ProxyLR: 0.5266  Epoch: 19  Global Step: 109530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:14,874-Speed 3902.40 samples/sec  Loss 2.6835  LearningRate 0.0105  ProxyLR: 0.5263  Epoch: 19  Global Step: 109540   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:59:17,488-Speed 3918.84 samples/sec  Loss 2.6869  LearningRate 0.0105  ProxyLR: 0.5259  Epoch: 19  Global Step: 109550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:20,113-Speed 3901.59 samples/sec  Loss 2.7155  LearningRate 0.0105  ProxyLR: 0.5256  Epoch: 19  Global Step: 109560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:22,738-Speed 3903.37 samples/sec  Loss 2.7091  LearningRate 0.0105  ProxyLR: 0.5253  Epoch: 19  Global Step: 109570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:25,363-Speed 3901.28 samples/sec  Loss 2.7252  LearningRate 0.0105  ProxyLR: 0.5250  Epoch: 19  Global Step: 109580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:27,988-Speed 3902.61 samples/sec  Loss 2.6882  LearningRate 0.0105  ProxyLR: 0.5247  Epoch: 19  Global Step: 109590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:30,613-Speed 3901.13 samples/sec  Loss 2.7151  LearningRate 0.0105  ProxyLR: 0.5243  Epoch: 19  Global Step: 109600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:33,239-Speed 3901.43 samples/sec  Loss 2.7211  LearningRate 0.0105  ProxyLR: 0.5240  Epoch: 19  Global Step: 109610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:35,864-Speed 3902.66 samples/sec  Loss 2.7485  LearningRate 0.0105  ProxyLR: 0.5237  Epoch: 19  Global Step: 109620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:38,488-Speed 3902.59 samples/sec  Loss 2.6887  LearningRate 0.0105  ProxyLR: 0.5234  Epoch: 19  Global Step: 109630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:41,114-Speed 3900.77 samples/sec  Loss 2.7382  LearningRate 0.0105  ProxyLR: 0.5230  Epoch: 19  Global Step: 109640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:43,739-Speed 3902.04 samples/sec  Loss 2.7413  LearningRate 0.0105  ProxyLR: 0.5227  Epoch: 19  Global Step: 109650   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:59:46,365-Speed 3900.79 samples/sec  Loss 2.6703  LearningRate 0.0104  ProxyLR: 0.5224  Epoch: 19  Global Step: 109660   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:59:48,991-Speed 3899.98 samples/sec  Loss 2.6611  LearningRate 0.0104  ProxyLR: 0.5221  Epoch: 19  Global Step: 109670   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 22:59:51,604-Speed 3921.03 samples/sec  Loss 2.7428  LearningRate 0.0104  ProxyLR: 0.5218  Epoch: 19  Global Step: 109680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:54,229-Speed 3900.87 samples/sec  Loss 2.7244  LearningRate 0.0104  ProxyLR: 0.5214  Epoch: 19  Global Step: 109690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:56,856-Speed 3899.43 samples/sec  Loss 2.6486  LearningRate 0.0104  ProxyLR: 0.5211  Epoch: 19  Global Step: 109700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 22:59:59,483-Speed 3899.06 samples/sec  Loss 2.7170  LearningRate 0.0104  ProxyLR: 0.5208  Epoch: 19  Global Step: 109710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:02,109-Speed 3900.37 samples/sec  Loss 2.6831  LearningRate 0.0104  ProxyLR: 0.5205  Epoch: 19  Global Step: 109720   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:04,740-Speed 3893.22 samples/sec  Loss 2.7282  LearningRate 0.0104  ProxyLR: 0.5202  Epoch: 19  Global Step: 109730   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:07,370-Speed 3895.25 samples/sec  Loss 2.7672  LearningRate 0.0104  ProxyLR: 0.5198  Epoch: 19  Global Step: 109740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:10,001-Speed 3892.99 samples/sec  Loss 2.6566  LearningRate 0.0104  ProxyLR: 0.5195  Epoch: 19  Global Step: 109750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:12,631-Speed 3894.79 samples/sec  Loss 2.7387  LearningRate 0.0104  ProxyLR: 0.5192  Epoch: 19  Global Step: 109760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:15,249-Speed 3912.77 samples/sec  Loss 2.7568  LearningRate 0.0104  ProxyLR: 0.5189  Epoch: 19  Global Step: 109770   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:17,878-Speed 3895.46 samples/sec  Loss 2.7430  LearningRate 0.0104  ProxyLR: 0.5186  Epoch: 19  Global Step: 109780   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:20,508-Speed 3895.19 samples/sec  Loss 2.7644  LearningRate 0.0104  ProxyLR: 0.5182  Epoch: 19  Global Step: 109790   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:23,137-Speed 3895.43 samples/sec  Loss 2.7036  LearningRate 0.0104  ProxyLR: 0.5179  Epoch: 19  Global Step: 109800   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:25,767-Speed 3894.75 samples/sec  Loss 2.6803  LearningRate 0.0104  ProxyLR: 0.5176  Epoch: 19  Global Step: 109810   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:28,395-Speed 3898.79 samples/sec  Loss 2.6667  LearningRate 0.0103  ProxyLR: 0.5173  Epoch: 19  Global Step: 109820   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:31,020-Speed 3901.29 samples/sec  Loss 2.7239  LearningRate 0.0103  ProxyLR: 0.5170  Epoch: 19  Global Step: 109830   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:33,647-Speed 3898.83 samples/sec  Loss 2.7180  LearningRate 0.0103  ProxyLR: 0.5166  Epoch: 19  Global Step: 109840   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:36,278-Speed 3894.16 samples/sec  Loss 2.7784  LearningRate 0.0103  ProxyLR: 0.5163  Epoch: 19  Global Step: 109850   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:38,907-Speed 3895.32 samples/sec  Loss 2.6833  LearningRate 0.0103  ProxyLR: 0.5160  Epoch: 19  Global Step: 109860   Fp16 Grad Scale: 131072  Required: 3 hours
Training: 2023-05-04 23:00:41,535-Speed 3897.22 samples/sec  Loss 2.7058  LearningRate 0.0103  ProxyLR: 0.5157  Epoch: 19  Global Step: 109870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:44,166-Speed 3893.50 samples/sec  Loss 2.7189  LearningRate 0.0103  ProxyLR: 0.5154  Epoch: 19  Global Step: 109880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:46,798-Speed 3892.46 samples/sec  Loss 2.6507  LearningRate 0.0103  ProxyLR: 0.5150  Epoch: 19  Global Step: 109890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:49,428-Speed 3894.29 samples/sec  Loss 2.7553  LearningRate 0.0103  ProxyLR: 0.5147  Epoch: 19  Global Step: 109900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:52,058-Speed 3893.62 samples/sec  Loss 2.7313  LearningRate 0.0103  ProxyLR: 0.5144  Epoch: 19  Global Step: 109910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:54,690-Speed 3892.17 samples/sec  Loss 2.6872  LearningRate 0.0103  ProxyLR: 0.5141  Epoch: 19  Global Step: 109920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:57,323-Speed 3889.68 samples/sec  Loss 2.7849  LearningRate 0.0103  ProxyLR: 0.5138  Epoch: 19  Global Step: 109930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:00:59,953-Speed 3894.67 samples/sec  Loss 2.7017  LearningRate 0.0103  ProxyLR: 0.5134  Epoch: 19  Global Step: 109940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:01:02,583-Speed 3894.88 samples/sec  Loss 2.8070  LearningRate 0.0103  ProxyLR: 0.5131  Epoch: 19  Global Step: 109950   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:01:05,213-Speed 3895.41 samples/sec  Loss 2.7085  LearningRate 0.0103  ProxyLR: 0.5128  Epoch: 19  Global Step: 109960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:01:07,841-Speed 3896.69 samples/sec  Loss 2.7297  LearningRate 0.0102  ProxyLR: 0.5125  Epoch: 19  Global Step: 109970   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:01:10,469-Speed 3896.97 samples/sec  Loss 2.7894  LearningRate 0.0102  ProxyLR: 0.5122  Epoch: 19  Global Step: 109980   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:01:13,084-Speed 3916.76 samples/sec  Loss 2.7356  LearningRate 0.0102  ProxyLR: 0.5118  Epoch: 19  Global Step: 109990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:01:15,713-Speed 3895.90 samples/sec  Loss 2.7144  LearningRate 0.0102  ProxyLR: 0.5115  Epoch: 19  Global Step: 110000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:02:05,248-[lfw][110000]XNorm: 23.849442
Training: 2023-05-04 23:02:05,249-[lfw][110000]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-04 23:02:05,249-[lfw][110000]Accuracy-Highest: 0.99750
Training: 2023-05-04 23:02:05,249-[lfw][110000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-04 23:02:05,249-[lfw][110000]Highest TPR@FPR: 0.99600
Training: 2023-05-04 23:03:02,190-[cfp_fp][110000]XNorm: 23.681357
Training: 2023-05-04 23:03:02,190-[cfp_fp][110000]Accuracy-Flip: 0.98114+-0.00648
Training: 2023-05-04 23:03:02,190-[cfp_fp][110000]Accuracy-Highest: 0.98129
Training: 2023-05-04 23:03:02,190-[cfp_fp][110000]TPR@1stNon-Zero-FPR of 0.00029: 0.82657
Training: 2023-05-04 23:03:02,190-[cfp_fp][110000]Highest TPR@FPR: 0.87086
Training: 2023-05-04 23:03:51,681-[agedb_30][110000]XNorm: 23.901844
Training: 2023-05-04 23:03:51,681-[agedb_30][110000]Accuracy-Flip: 0.97300+-0.00971
Training: 2023-05-04 23:03:51,681-[agedb_30][110000]Accuracy-Highest: 0.97300
Training: 2023-05-04 23:03:51,681-[agedb_30][110000]TPR@1stNon-Zero-FPR of 0.00033: 0.83633
Training: 2023-05-04 23:03:51,681-[agedb_30][110000]Highest TPR@FPR: 0.85600
Training: 2023-05-04 23:04:42,589-[calfw][110000]XNorm: 23.861240
Training: 2023-05-04 23:04:42,590-[calfw][110000]Accuracy-Flip: 0.95767+-0.01179
Training: 2023-05-04 23:04:42,590-[calfw][110000]Accuracy-Highest: 0.95833
Training: 2023-05-04 23:04:42,590-[calfw][110000]TPR@1stNon-Zero-FPR of 0.00033: 0.82700
Training: 2023-05-04 23:04:42,590-[calfw][110000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 23:05:33,564-[cplfw][110000]XNorm: 23.265339
Training: 2023-05-04 23:05:33,564-[cplfw][110000]Accuracy-Flip: 0.92700+-0.01396
Training: 2023-05-04 23:05:33,564-[cplfw][110000]Accuracy-Highest: 0.92700
Training: 2023-05-04 23:05:33,565-[cplfw][110000]TPR@1stNon-Zero-FPR of 0.00033: 0.07633
Training: 2023-05-04 23:05:33,565-[cplfw][110000]Highest TPR@FPR: 0.07633
Training: 2023-05-04 23:05:36,230-Speed 39.31 samples/sec  Loss 2.6865  LearningRate 0.0102  ProxyLR: 0.5112  Epoch: 19  Global Step: 110010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:38,847-Speed 3913.10 samples/sec  Loss 2.6506  LearningRate 0.0102  ProxyLR: 0.5109  Epoch: 19  Global Step: 110020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:41,467-Speed 3910.08 samples/sec  Loss 2.6956  LearningRate 0.0102  ProxyLR: 0.5106  Epoch: 19  Global Step: 110030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:44,086-Speed 3910.88 samples/sec  Loss 2.7545  LearningRate 0.0102  ProxyLR: 0.5103  Epoch: 19  Global Step: 110040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:46,705-Speed 3910.21 samples/sec  Loss 2.7210  LearningRate 0.0102  ProxyLR: 0.5099  Epoch: 19  Global Step: 110050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:49,324-Speed 3911.60 samples/sec  Loss 2.7407  LearningRate 0.0102  ProxyLR: 0.5096  Epoch: 19  Global Step: 110060   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:51,947-Speed 3905.04 samples/sec  Loss 2.6577  LearningRate 0.0102  ProxyLR: 0.5093  Epoch: 19  Global Step: 110070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:54,566-Speed 3909.93 samples/sec  Loss 2.7349  LearningRate 0.0102  ProxyLR: 0.5090  Epoch: 19  Global Step: 110080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:05:57,188-Speed 3906.28 samples/sec  Loss 2.7257  LearningRate 0.0102  ProxyLR: 0.5087  Epoch: 19  Global Step: 110090   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:05:59,797-Speed 3927.32 samples/sec  Loss 2.7366  LearningRate 0.0102  ProxyLR: 0.5083  Epoch: 19  Global Step: 110100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:02,419-Speed 3906.34 samples/sec  Loss 2.7564  LearningRate 0.0102  ProxyLR: 0.5080  Epoch: 19  Global Step: 110110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:05,041-Speed 3906.41 samples/sec  Loss 2.7730  LearningRate 0.0102  ProxyLR: 0.5077  Epoch: 19  Global Step: 110120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:07,663-Speed 3906.16 samples/sec  Loss 2.6838  LearningRate 0.0101  ProxyLR: 0.5074  Epoch: 19  Global Step: 110130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:10,285-Speed 3906.04 samples/sec  Loss 2.7772  LearningRate 0.0101  ProxyLR: 0.5071  Epoch: 19  Global Step: 110140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:12,908-Speed 3904.41 samples/sec  Loss 2.6520  LearningRate 0.0101  ProxyLR: 0.5068  Epoch: 19  Global Step: 110150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:15,532-Speed 3903.54 samples/sec  Loss 2.7154  LearningRate 0.0101  ProxyLR: 0.5064  Epoch: 19  Global Step: 110160   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:18,153-Speed 3907.82 samples/sec  Loss 2.6713  LearningRate 0.0101  ProxyLR: 0.5061  Epoch: 19  Global Step: 110170   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:20,777-Speed 3903.45 samples/sec  Loss 2.7125  LearningRate 0.0101  ProxyLR: 0.5058  Epoch: 19  Global Step: 110180   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:23,401-Speed 3904.42 samples/sec  Loss 2.6541  LearningRate 0.0101  ProxyLR: 0.5055  Epoch: 19  Global Step: 110190   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:26,026-Speed 3901.43 samples/sec  Loss 2.6305  LearningRate 0.0101  ProxyLR: 0.5052  Epoch: 19  Global Step: 110200   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:06:28,652-Speed 3900.41 samples/sec  Loss 2.8314  LearningRate 0.0101  ProxyLR: 0.5049  Epoch: 19  Global Step: 110210   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:06:31,277-Speed 3901.99 samples/sec  Loss 2.7219  LearningRate 0.0101  ProxyLR: 0.5045  Epoch: 19  Global Step: 110220   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:06:33,888-Speed 3923.58 samples/sec  Loss 2.7202  LearningRate 0.0101  ProxyLR: 0.5042  Epoch: 19  Global Step: 110230   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:36,513-Speed 3900.80 samples/sec  Loss 2.6853  LearningRate 0.0101  ProxyLR: 0.5039  Epoch: 19  Global Step: 110240   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:39,137-Speed 3904.04 samples/sec  Loss 2.7673  LearningRate 0.0101  ProxyLR: 0.5036  Epoch: 19  Global Step: 110250   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:41,760-Speed 3905.38 samples/sec  Loss 2.7198  LearningRate 0.0101  ProxyLR: 0.5033  Epoch: 19  Global Step: 110260   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:44,385-Speed 3902.11 samples/sec  Loss 2.6631  LearningRate 0.0101  ProxyLR: 0.5030  Epoch: 19  Global Step: 110270   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:47,007-Speed 3904.97 samples/sec  Loss 2.7181  LearningRate 0.0101  ProxyLR: 0.5027  Epoch: 19  Global Step: 110280   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:49,632-Speed 3902.19 samples/sec  Loss 2.7138  LearningRate 0.0100  ProxyLR: 0.5023  Epoch: 19  Global Step: 110290   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:52,257-Speed 3902.61 samples/sec  Loss 2.7786  LearningRate 0.0100  ProxyLR: 0.5020  Epoch: 19  Global Step: 110300   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:54,881-Speed 3903.46 samples/sec  Loss 2.6909  LearningRate 0.0100  ProxyLR: 0.5017  Epoch: 19  Global Step: 110310   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:06:57,505-Speed 3903.79 samples/sec  Loss 2.6750  LearningRate 0.0100  ProxyLR: 0.5014  Epoch: 19  Global Step: 110320   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:00,130-Speed 3902.41 samples/sec  Loss 2.7246  LearningRate 0.0100  ProxyLR: 0.5011  Epoch: 19  Global Step: 110330   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:02,754-Speed 3903.22 samples/sec  Loss 2.7741  LearningRate 0.0100  ProxyLR: 0.5008  Epoch: 19  Global Step: 110340   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:05,378-Speed 3902.78 samples/sec  Loss 2.7232  LearningRate 0.0100  ProxyLR: 0.5004  Epoch: 19  Global Step: 110350   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:07,989-Speed 3922.56 samples/sec  Loss 2.7223  LearningRate 0.0100  ProxyLR: 0.5001  Epoch: 19  Global Step: 110360   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:10,613-Speed 3904.29 samples/sec  Loss 2.7644  LearningRate 0.0100  ProxyLR: 0.4998  Epoch: 19  Global Step: 110370   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:13,236-Speed 3903.77 samples/sec  Loss 2.7035  LearningRate 0.0100  ProxyLR: 0.4995  Epoch: 19  Global Step: 110380   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:15,860-Speed 3903.73 samples/sec  Loss 2.7927  LearningRate 0.0100  ProxyLR: 0.4992  Epoch: 19  Global Step: 110390   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:18,485-Speed 3902.11 samples/sec  Loss 2.7087  LearningRate 0.0100  ProxyLR: 0.4989  Epoch: 19  Global Step: 110400   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:21,109-Speed 3903.46 samples/sec  Loss 2.7090  LearningRate 0.0100  ProxyLR: 0.4986  Epoch: 19  Global Step: 110410   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:23,736-Speed 3898.59 samples/sec  Loss 2.7193  LearningRate 0.0100  ProxyLR: 0.4982  Epoch: 19  Global Step: 110420   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:26,360-Speed 3903.61 samples/sec  Loss 2.7287  LearningRate 0.0100  ProxyLR: 0.4979  Epoch: 19  Global Step: 110430   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:28,986-Speed 3900.38 samples/sec  Loss 2.7508  LearningRate 0.0100  ProxyLR: 0.4976  Epoch: 19  Global Step: 110440   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:31,611-Speed 3903.21 samples/sec  Loss 2.7087  LearningRate 0.0099  ProxyLR: 0.4973  Epoch: 19  Global Step: 110450   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:34,235-Speed 3902.32 samples/sec  Loss 2.7354  LearningRate 0.0099  ProxyLR: 0.4970  Epoch: 19  Global Step: 110460   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:36,862-Speed 3899.01 samples/sec  Loss 2.7279  LearningRate 0.0099  ProxyLR: 0.4967  Epoch: 19  Global Step: 110470   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:39,488-Speed 3901.19 samples/sec  Loss 2.6296  LearningRate 0.0099  ProxyLR: 0.4964  Epoch: 19  Global Step: 110480   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:42,114-Speed 3899.69 samples/sec  Loss 2.7261  LearningRate 0.0099  ProxyLR: 0.4961  Epoch: 19  Global Step: 110490   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:44,741-Speed 3899.78 samples/sec  Loss 2.6836  LearningRate 0.0099  ProxyLR: 0.4957  Epoch: 19  Global Step: 110500   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:47,366-Speed 3901.68 samples/sec  Loss 2.7329  LearningRate 0.0099  ProxyLR: 0.4954  Epoch: 19  Global Step: 110510   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:07:49,978-Speed 3921.49 samples/sec  Loss 2.7059  LearningRate 0.0099  ProxyLR: 0.4951  Epoch: 19  Global Step: 110520   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:52,601-Speed 3903.81 samples/sec  Loss 2.7737  LearningRate 0.0099  ProxyLR: 0.4948  Epoch: 19  Global Step: 110530   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:55,225-Speed 3904.05 samples/sec  Loss 2.7242  LearningRate 0.0099  ProxyLR: 0.4945  Epoch: 19  Global Step: 110540   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:07:57,851-Speed 3901.21 samples/sec  Loss 2.8412  LearningRate 0.0099  ProxyLR: 0.4942  Epoch: 19  Global Step: 110550   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:00,475-Speed 3903.21 samples/sec  Loss 2.6158  LearningRate 0.0099  ProxyLR: 0.4939  Epoch: 19  Global Step: 110560   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:03,100-Speed 3901.65 samples/sec  Loss 2.7667  LearningRate 0.0099  ProxyLR: 0.4935  Epoch: 19  Global Step: 110570   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:05,725-Speed 3902.56 samples/sec  Loss 2.7452  LearningRate 0.0099  ProxyLR: 0.4932  Epoch: 19  Global Step: 110580   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:08,349-Speed 3903.52 samples/sec  Loss 2.7326  LearningRate 0.0099  ProxyLR: 0.4929  Epoch: 19  Global Step: 110590   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:10,974-Speed 3900.86 samples/sec  Loss 2.7153  LearningRate 0.0099  ProxyLR: 0.4926  Epoch: 19  Global Step: 110600   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:13,600-Speed 3900.98 samples/sec  Loss 2.7255  LearningRate 0.0098  ProxyLR: 0.4923  Epoch: 19  Global Step: 110610   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:16,211-Speed 3922.76 samples/sec  Loss 2.7319  LearningRate 0.0098  ProxyLR: 0.4920  Epoch: 19  Global Step: 110620   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:18,835-Speed 3903.55 samples/sec  Loss 2.7245  LearningRate 0.0098  ProxyLR: 0.4917  Epoch: 19  Global Step: 110630   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:21,461-Speed 3899.87 samples/sec  Loss 2.7116  LearningRate 0.0098  ProxyLR: 0.4914  Epoch: 19  Global Step: 110640   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:24,087-Speed 3901.65 samples/sec  Loss 2.6913  LearningRate 0.0098  ProxyLR: 0.4911  Epoch: 19  Global Step: 110650   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:26,712-Speed 3900.52 samples/sec  Loss 2.7463  LearningRate 0.0098  ProxyLR: 0.4907  Epoch: 19  Global Step: 110660   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:29,338-Speed 3900.82 samples/sec  Loss 2.7184  LearningRate 0.0098  ProxyLR: 0.4904  Epoch: 19  Global Step: 110670   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:31,963-Speed 3902.95 samples/sec  Loss 2.6964  LearningRate 0.0098  ProxyLR: 0.4901  Epoch: 19  Global Step: 110680   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:34,588-Speed 3901.55 samples/sec  Loss 2.7128  LearningRate 0.0098  ProxyLR: 0.4898  Epoch: 19  Global Step: 110690   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:37,213-Speed 3901.74 samples/sec  Loss 2.7364  LearningRate 0.0098  ProxyLR: 0.4895  Epoch: 19  Global Step: 110700   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:39,840-Speed 3899.19 samples/sec  Loss 2.7422  LearningRate 0.0098  ProxyLR: 0.4892  Epoch: 19  Global Step: 110710   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:42,467-Speed 3898.59 samples/sec  Loss 2.8065  LearningRate 0.0098  ProxyLR: 0.4889  Epoch: 19  Global Step: 110720   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:08:45,096-Speed 3897.05 samples/sec  Loss 2.6473  LearningRate 0.0098  ProxyLR: 0.4886  Epoch: 19  Global Step: 110730   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:08:47,711-Speed 3916.39 samples/sec  Loss 2.7804  LearningRate 0.0098  ProxyLR: 0.4882  Epoch: 19  Global Step: 110740   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:50,343-Speed 3891.10 samples/sec  Loss 2.7063  LearningRate 0.0098  ProxyLR: 0.4879  Epoch: 19  Global Step: 110750   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:52,975-Speed 3892.52 samples/sec  Loss 2.7740  LearningRate 0.0098  ProxyLR: 0.4876  Epoch: 19  Global Step: 110760   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:55,603-Speed 3897.24 samples/sec  Loss 2.7578  LearningRate 0.0097  ProxyLR: 0.4873  Epoch: 19  Global Step: 110770   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:08:58,231-Speed 3897.77 samples/sec  Loss 2.7212  LearningRate 0.0097  ProxyLR: 0.4870  Epoch: 19  Global Step: 110780   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:00,860-Speed 3896.47 samples/sec  Loss 2.7148  LearningRate 0.0097  ProxyLR: 0.4867  Epoch: 19  Global Step: 110790   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:03,488-Speed 3896.94 samples/sec  Loss 2.7466  LearningRate 0.0097  ProxyLR: 0.4864  Epoch: 19  Global Step: 110800   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:06,118-Speed 3895.05 samples/sec  Loss 2.7054  LearningRate 0.0097  ProxyLR: 0.4861  Epoch: 19  Global Step: 110810   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:08,745-Speed 3897.76 samples/sec  Loss 2.6735  LearningRate 0.0097  ProxyLR: 0.4858  Epoch: 19  Global Step: 110820   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:11,374-Speed 3896.99 samples/sec  Loss 2.6767  LearningRate 0.0097  ProxyLR: 0.4855  Epoch: 19  Global Step: 110830   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:14,001-Speed 3899.07 samples/sec  Loss 2.7479  LearningRate 0.0097  ProxyLR: 0.4851  Epoch: 19  Global Step: 110840   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:09:16,618-Speed 3913.61 samples/sec  Loss 2.7292  LearningRate 0.0097  ProxyLR: 0.4848  Epoch: 19  Global Step: 110850   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:19,247-Speed 3896.49 samples/sec  Loss 2.7610  LearningRate 0.0097  ProxyLR: 0.4845  Epoch: 19  Global Step: 110860   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:21,873-Speed 3900.00 samples/sec  Loss 2.6863  LearningRate 0.0097  ProxyLR: 0.4842  Epoch: 19  Global Step: 110870   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:24,501-Speed 3897.79 samples/sec  Loss 2.7270  LearningRate 0.0097  ProxyLR: 0.4839  Epoch: 19  Global Step: 110880   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:27,127-Speed 3900.00 samples/sec  Loss 2.7353  LearningRate 0.0097  ProxyLR: 0.4836  Epoch: 19  Global Step: 110890   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:29,755-Speed 3897.17 samples/sec  Loss 2.7720  LearningRate 0.0097  ProxyLR: 0.4833  Epoch: 19  Global Step: 110900   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:32,381-Speed 3900.00 samples/sec  Loss 2.7199  LearningRate 0.0097  ProxyLR: 0.4830  Epoch: 19  Global Step: 110910   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:35,006-Speed 3902.22 samples/sec  Loss 2.8163  LearningRate 0.0097  ProxyLR: 0.4827  Epoch: 19  Global Step: 110920   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:37,632-Speed 3901.52 samples/sec  Loss 2.7399  LearningRate 0.0096  ProxyLR: 0.4824  Epoch: 19  Global Step: 110930   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:40,261-Speed 3895.59 samples/sec  Loss 2.6582  LearningRate 0.0096  ProxyLR: 0.4821  Epoch: 19  Global Step: 110940   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:42,891-Speed 3894.79 samples/sec  Loss 2.7051  LearningRate 0.0096  ProxyLR: 0.4817  Epoch: 19  Global Step: 110950   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:09:45,508-Speed 3913.56 samples/sec  Loss 2.6978  LearningRate 0.0096  ProxyLR: 0.4814  Epoch: 19  Global Step: 110960   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:48,137-Speed 3895.19 samples/sec  Loss 2.7793  LearningRate 0.0096  ProxyLR: 0.4811  Epoch: 19  Global Step: 110970   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:50,769-Speed 3891.63 samples/sec  Loss 2.7410  LearningRate 0.0096  ProxyLR: 0.4808  Epoch: 19  Global Step: 110980   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:53,401-Speed 3892.38 samples/sec  Loss 2.7371  LearningRate 0.0096  ProxyLR: 0.4805  Epoch: 19  Global Step: 110990   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:56,031-Speed 3895.01 samples/sec  Loss 2.7538  LearningRate 0.0096  ProxyLR: 0.4802  Epoch: 19  Global Step: 111000   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:09:58,663-Speed 3890.73 samples/sec  Loss 2.7000  LearningRate 0.0096  ProxyLR: 0.4799  Epoch: 19  Global Step: 111010   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:01,294-Speed 3893.58 samples/sec  Loss 2.6702  LearningRate 0.0096  ProxyLR: 0.4796  Epoch: 19  Global Step: 111020   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:03,923-Speed 3895.88 samples/sec  Loss 2.6460  LearningRate 0.0096  ProxyLR: 0.4793  Epoch: 19  Global Step: 111030   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:06,558-Speed 3886.67 samples/sec  Loss 2.7229  LearningRate 0.0096  ProxyLR: 0.4790  Epoch: 19  Global Step: 111040   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:09,196-Speed 3882.50 samples/sec  Loss 2.7452  LearningRate 0.0096  ProxyLR: 0.4787  Epoch: 19  Global Step: 111050   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:11,835-Speed 3881.63 samples/sec  Loss 2.7430  LearningRate 0.0096  ProxyLR: 0.4784  Epoch: 19  Global Step: 111060   Fp16 Grad Scale: 524288  Required: 3 hours
Training: 2023-05-04 23:10:14,458-Speed 3905.61 samples/sec  Loss 2.7206  LearningRate 0.0096  ProxyLR: 0.4780  Epoch: 19  Global Step: 111070   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:17,095-Speed 3883.37 samples/sec  Loss 2.8016  LearningRate 0.0096  ProxyLR: 0.4777  Epoch: 19  Global Step: 111080   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:19,735-Speed 3880.69 samples/sec  Loss 2.7308  LearningRate 0.0095  ProxyLR: 0.4774  Epoch: 19  Global Step: 111090   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:22,368-Speed 3889.36 samples/sec  Loss 2.7332  LearningRate 0.0095  ProxyLR: 0.4771  Epoch: 19  Global Step: 111100   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:25,006-Speed 3882.81 samples/sec  Loss 2.7174  LearningRate 0.0095  ProxyLR: 0.4768  Epoch: 19  Global Step: 111110   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:27,640-Speed 3889.07 samples/sec  Loss 2.7059  LearningRate 0.0095  ProxyLR: 0.4765  Epoch: 19  Global Step: 111120   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:30,271-Speed 3892.53 samples/sec  Loss 2.6950  LearningRate 0.0095  ProxyLR: 0.4762  Epoch: 19  Global Step: 111130   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:32,905-Speed 3889.54 samples/sec  Loss 2.6392  LearningRate 0.0095  ProxyLR: 0.4759  Epoch: 19  Global Step: 111140   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:35,535-Speed 3893.12 samples/sec  Loss 2.7070  LearningRate 0.0095  ProxyLR: 0.4756  Epoch: 19  Global Step: 111150   Fp16 Grad Scale: 262144  Required: 3 hours
Training: 2023-05-04 23:10:38,167-Speed 3892.30 samples/sec  Loss 2.7450  LearningRate 0.0095  ProxyLR: 0.4753  Epoch: 19  Global Step: 111160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:40,798-Speed 3893.50 samples/sec  Loss 2.7262  LearningRate 0.0095  ProxyLR: 0.4750  Epoch: 19  Global Step: 111170   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:10:43,417-Speed 3910.32 samples/sec  Loss 2.6988  LearningRate 0.0095  ProxyLR: 0.4747  Epoch: 19  Global Step: 111180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:46,048-Speed 3892.30 samples/sec  Loss 2.7062  LearningRate 0.0095  ProxyLR: 0.4744  Epoch: 19  Global Step: 111190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:48,681-Speed 3890.16 samples/sec  Loss 2.7241  LearningRate 0.0095  ProxyLR: 0.4741  Epoch: 19  Global Step: 111200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:51,314-Speed 3890.32 samples/sec  Loss 2.8120  LearningRate 0.0095  ProxyLR: 0.4737  Epoch: 19  Global Step: 111210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:53,944-Speed 3894.55 samples/sec  Loss 2.6989  LearningRate 0.0095  ProxyLR: 0.4734  Epoch: 19  Global Step: 111220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:56,575-Speed 3894.00 samples/sec  Loss 2.7487  LearningRate 0.0095  ProxyLR: 0.4731  Epoch: 19  Global Step: 111230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:10:59,206-Speed 3892.06 samples/sec  Loss 2.8239  LearningRate 0.0095  ProxyLR: 0.4728  Epoch: 19  Global Step: 111240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:01,839-Speed 3890.38 samples/sec  Loss 2.6533  LearningRate 0.0095  ProxyLR: 0.4725  Epoch: 19  Global Step: 111250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:04,470-Speed 3893.05 samples/sec  Loss 2.7114  LearningRate 0.0094  ProxyLR: 0.4722  Epoch: 19  Global Step: 111260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:07,102-Speed 3892.18 samples/sec  Loss 2.6676  LearningRate 0.0094  ProxyLR: 0.4719  Epoch: 19  Global Step: 111270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:09,735-Speed 3889.91 samples/sec  Loss 2.7813  LearningRate 0.0094  ProxyLR: 0.4716  Epoch: 19  Global Step: 111280   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:11:12,369-Speed 3888.85 samples/sec  Loss 2.7138  LearningRate 0.0094  ProxyLR: 0.4713  Epoch: 19  Global Step: 111290   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:11:14,988-Speed 3910.10 samples/sec  Loss 2.6536  LearningRate 0.0094  ProxyLR: 0.4710  Epoch: 19  Global Step: 111300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:17,621-Speed 3890.24 samples/sec  Loss 2.7321  LearningRate 0.0094  ProxyLR: 0.4707  Epoch: 19  Global Step: 111310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:20,252-Speed 3893.21 samples/sec  Loss 2.7283  LearningRate 0.0094  ProxyLR: 0.4704  Epoch: 19  Global Step: 111320   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:22,883-Speed 3892.22 samples/sec  Loss 2.6697  LearningRate 0.0094  ProxyLR: 0.4701  Epoch: 19  Global Step: 111330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:25,515-Speed 3892.05 samples/sec  Loss 2.7533  LearningRate 0.0094  ProxyLR: 0.4698  Epoch: 19  Global Step: 111340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:28,146-Speed 3893.40 samples/sec  Loss 2.7284  LearningRate 0.0094  ProxyLR: 0.4695  Epoch: 19  Global Step: 111350   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:30,778-Speed 3891.52 samples/sec  Loss 2.7014  LearningRate 0.0094  ProxyLR: 0.4692  Epoch: 19  Global Step: 111360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:33,410-Speed 3891.78 samples/sec  Loss 2.7614  LearningRate 0.0094  ProxyLR: 0.4689  Epoch: 19  Global Step: 111370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:36,039-Speed 3895.64 samples/sec  Loss 2.7547  LearningRate 0.0094  ProxyLR: 0.4686  Epoch: 19  Global Step: 111380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:38,672-Speed 3890.68 samples/sec  Loss 2.7626  LearningRate 0.0094  ProxyLR: 0.4683  Epoch: 19  Global Step: 111390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:41,303-Speed 3893.05 samples/sec  Loss 2.7517  LearningRate 0.0094  ProxyLR: 0.4679  Epoch: 19  Global Step: 111400   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:11:43,934-Speed 3892.25 samples/sec  Loss 2.6911  LearningRate 0.0094  ProxyLR: 0.4676  Epoch: 19  Global Step: 111410   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:11:46,566-Speed 3892.25 samples/sec  Loss 2.6525  LearningRate 0.0093  ProxyLR: 0.4673  Epoch: 19  Global Step: 111420   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:11:49,200-Speed 3888.25 samples/sec  Loss 2.7397  LearningRate 0.0093  ProxyLR: 0.4670  Epoch: 19  Global Step: 111430   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:11:51,820-Speed 3909.88 samples/sec  Loss 2.6857  LearningRate 0.0093  ProxyLR: 0.4667  Epoch: 19  Global Step: 111440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:54,451-Speed 3892.14 samples/sec  Loss 2.7739  LearningRate 0.0093  ProxyLR: 0.4664  Epoch: 19  Global Step: 111450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:57,084-Speed 3890.72 samples/sec  Loss 2.7037  LearningRate 0.0093  ProxyLR: 0.4661  Epoch: 19  Global Step: 111460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:11:59,716-Speed 3891.99 samples/sec  Loss 2.7053  LearningRate 0.0093  ProxyLR: 0.4658  Epoch: 19  Global Step: 111470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:02,352-Speed 3885.23 samples/sec  Loss 2.7657  LearningRate 0.0093  ProxyLR: 0.4655  Epoch: 19  Global Step: 111480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:04,987-Speed 3886.79 samples/sec  Loss 2.6735  LearningRate 0.0093  ProxyLR: 0.4652  Epoch: 19  Global Step: 111490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:07,622-Speed 3887.89 samples/sec  Loss 2.6406  LearningRate 0.0093  ProxyLR: 0.4649  Epoch: 19  Global Step: 111500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:10,255-Speed 3889.40 samples/sec  Loss 2.7535  LearningRate 0.0093  ProxyLR: 0.4646  Epoch: 19  Global Step: 111510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:12,887-Speed 3891.68 samples/sec  Loss 2.7340  LearningRate 0.0093  ProxyLR: 0.4643  Epoch: 19  Global Step: 111520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:15,522-Speed 3886.87 samples/sec  Loss 2.7008  LearningRate 0.0093  ProxyLR: 0.4640  Epoch: 19  Global Step: 111530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:18,157-Speed 3887.72 samples/sec  Loss 2.7412  LearningRate 0.0093  ProxyLR: 0.4637  Epoch: 19  Global Step: 111540   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:12:20,779-Speed 3906.37 samples/sec  Loss 2.7144  LearningRate 0.0093  ProxyLR: 0.4634  Epoch: 19  Global Step: 111550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:23,413-Speed 3888.90 samples/sec  Loss 2.7209  LearningRate 0.0093  ProxyLR: 0.4631  Epoch: 19  Global Step: 111560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:26,047-Speed 3887.40 samples/sec  Loss 2.7080  LearningRate 0.0093  ProxyLR: 0.4628  Epoch: 19  Global Step: 111570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:28,682-Speed 3887.54 samples/sec  Loss 2.6800  LearningRate 0.0092  ProxyLR: 0.4625  Epoch: 19  Global Step: 111580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:31,319-Speed 3885.12 samples/sec  Loss 2.7188  LearningRate 0.0092  ProxyLR: 0.4622  Epoch: 19  Global Step: 111590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:33,954-Speed 3886.10 samples/sec  Loss 2.7533  LearningRate 0.0092  ProxyLR: 0.4619  Epoch: 19  Global Step: 111600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:36,588-Speed 3888.23 samples/sec  Loss 2.6734  LearningRate 0.0092  ProxyLR: 0.4616  Epoch: 19  Global Step: 111610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:39,225-Speed 3885.28 samples/sec  Loss 2.7185  LearningRate 0.0092  ProxyLR: 0.4613  Epoch: 19  Global Step: 111620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:41,861-Speed 3885.30 samples/sec  Loss 2.7735  LearningRate 0.0092  ProxyLR: 0.4610  Epoch: 19  Global Step: 111630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:44,498-Speed 3884.02 samples/sec  Loss 2.6818  LearningRate 0.0092  ProxyLR: 0.4607  Epoch: 19  Global Step: 111640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:47,134-Speed 3885.25 samples/sec  Loss 2.7916  LearningRate 0.0092  ProxyLR: 0.4604  Epoch: 19  Global Step: 111650   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:12:49,758-Speed 3903.60 samples/sec  Loss 2.7307  LearningRate 0.0092  ProxyLR: 0.4601  Epoch: 19  Global Step: 111660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:52,391-Speed 3890.30 samples/sec  Loss 2.7334  LearningRate 0.0092  ProxyLR: 0.4598  Epoch: 19  Global Step: 111670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:55,022-Speed 3893.26 samples/sec  Loss 2.6799  LearningRate 0.0092  ProxyLR: 0.4595  Epoch: 19  Global Step: 111680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:12:57,654-Speed 3890.90 samples/sec  Loss 2.7040  LearningRate 0.0092  ProxyLR: 0.4592  Epoch: 19  Global Step: 111690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:00,290-Speed 3886.17 samples/sec  Loss 2.7632  LearningRate 0.0092  ProxyLR: 0.4589  Epoch: 19  Global Step: 111700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:02,925-Speed 3886.65 samples/sec  Loss 2.6929  LearningRate 0.0092  ProxyLR: 0.4586  Epoch: 19  Global Step: 111710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:05,556-Speed 3893.04 samples/sec  Loss 2.7071  LearningRate 0.0092  ProxyLR: 0.4583  Epoch: 19  Global Step: 111720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:08,190-Speed 3889.78 samples/sec  Loss 2.7033  LearningRate 0.0092  ProxyLR: 0.4580  Epoch: 19  Global Step: 111730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:10,824-Speed 3888.40 samples/sec  Loss 2.7646  LearningRate 0.0092  ProxyLR: 0.4577  Epoch: 19  Global Step: 111740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:13,456-Speed 3890.34 samples/sec  Loss 2.6933  LearningRate 0.0091  ProxyLR: 0.4574  Epoch: 19  Global Step: 111750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:16,090-Speed 3889.40 samples/sec  Loss 2.7684  LearningRate 0.0091  ProxyLR: 0.4571  Epoch: 19  Global Step: 111760   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:13:18,725-Speed 3886.59 samples/sec  Loss 2.7469  LearningRate 0.0091  ProxyLR: 0.4568  Epoch: 19  Global Step: 111770   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:13:21,346-Speed 3907.91 samples/sec  Loss 2.7109  LearningRate 0.0091  ProxyLR: 0.4565  Epoch: 19  Global Step: 111780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:23,984-Speed 3883.69 samples/sec  Loss 2.6641  LearningRate 0.0091  ProxyLR: 0.4562  Epoch: 19  Global Step: 111790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:26,620-Speed 3884.39 samples/sec  Loss 2.7783  LearningRate 0.0091  ProxyLR: 0.4559  Epoch: 19  Global Step: 111800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:29,255-Speed 3887.42 samples/sec  Loss 2.7394  LearningRate 0.0091  ProxyLR: 0.4556  Epoch: 19  Global Step: 111810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:31,891-Speed 3886.59 samples/sec  Loss 2.7293  LearningRate 0.0091  ProxyLR: 0.4553  Epoch: 19  Global Step: 111820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:34,526-Speed 3885.83 samples/sec  Loss 2.7194  LearningRate 0.0091  ProxyLR: 0.4550  Epoch: 19  Global Step: 111830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:37,162-Speed 3885.80 samples/sec  Loss 2.7745  LearningRate 0.0091  ProxyLR: 0.4547  Epoch: 19  Global Step: 111840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:39,800-Speed 3883.52 samples/sec  Loss 2.7149  LearningRate 0.0091  ProxyLR: 0.4544  Epoch: 19  Global Step: 111850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:42,440-Speed 3879.38 samples/sec  Loss 2.7182  LearningRate 0.0091  ProxyLR: 0.4541  Epoch: 19  Global Step: 111860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:45,078-Speed 3882.10 samples/sec  Loss 2.6931  LearningRate 0.0091  ProxyLR: 0.4538  Epoch: 19  Global Step: 111870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:47,713-Speed 3887.14 samples/sec  Loss 2.7318  LearningRate 0.0091  ProxyLR: 0.4535  Epoch: 19  Global Step: 111880   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:13:50,352-Speed 3882.14 samples/sec  Loss 2.7708  LearningRate 0.0091  ProxyLR: 0.4532  Epoch: 19  Global Step: 111890   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:13:52,974-Speed 3905.61 samples/sec  Loss 2.7662  LearningRate 0.0091  ProxyLR: 0.4529  Epoch: 19  Global Step: 111900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:55,612-Speed 3883.61 samples/sec  Loss 2.7495  LearningRate 0.0091  ProxyLR: 0.4526  Epoch: 19  Global Step: 111910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:13:58,249-Speed 3883.79 samples/sec  Loss 2.6718  LearningRate 0.0090  ProxyLR: 0.4523  Epoch: 19  Global Step: 111920   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:00,884-Speed 3887.68 samples/sec  Loss 2.6747  LearningRate 0.0090  ProxyLR: 0.4520  Epoch: 19  Global Step: 111930   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:03,520-Speed 3885.31 samples/sec  Loss 2.7049  LearningRate 0.0090  ProxyLR: 0.4517  Epoch: 19  Global Step: 111940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:06,159-Speed 3882.02 samples/sec  Loss 2.7793  LearningRate 0.0090  ProxyLR: 0.4514  Epoch: 19  Global Step: 111950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:08,796-Speed 3883.64 samples/sec  Loss 2.7348  LearningRate 0.0090  ProxyLR: 0.4511  Epoch: 19  Global Step: 111960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:11,434-Speed 3883.55 samples/sec  Loss 2.6818  LearningRate 0.0090  ProxyLR: 0.4508  Epoch: 19  Global Step: 111970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:14,071-Speed 3883.13 samples/sec  Loss 2.7178  LearningRate 0.0090  ProxyLR: 0.4505  Epoch: 19  Global Step: 111980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:16,707-Speed 3885.98 samples/sec  Loss 2.7715  LearningRate 0.0090  ProxyLR: 0.4502  Epoch: 19  Global Step: 111990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:19,343-Speed 3886.51 samples/sec  Loss 2.7475  LearningRate 0.0090  ProxyLR: 0.4499  Epoch: 19  Global Step: 112000   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:21,974-Speed 3892.10 samples/sec  Loss 2.6826  LearningRate 0.0090  ProxyLR: 0.4496  Epoch: 19  Global Step: 112010   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:24,606-Speed 3891.46 samples/sec  Loss 2.7654  LearningRate 0.0090  ProxyLR: 0.4493  Epoch: 19  Global Step: 112020   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:27,245-Speed 3881.35 samples/sec  Loss 2.7064  LearningRate 0.0090  ProxyLR: 0.4490  Epoch: 19  Global Step: 112030   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:29,881-Speed 3885.60 samples/sec  Loss 2.6811  LearningRate 0.0090  ProxyLR: 0.4487  Epoch: 19  Global Step: 112040   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:32,518-Speed 3885.24 samples/sec  Loss 2.7330  LearningRate 0.0090  ProxyLR: 0.4484  Epoch: 19  Global Step: 112050   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:35,154-Speed 3885.99 samples/sec  Loss 2.6647  LearningRate 0.0090  ProxyLR: 0.4481  Epoch: 19  Global Step: 112060   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:37,789-Speed 3886.75 samples/sec  Loss 2.6673  LearningRate 0.0090  ProxyLR: 0.4478  Epoch: 19  Global Step: 112070   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:40,423-Speed 3888.78 samples/sec  Loss 2.7270  LearningRate 0.0089  ProxyLR: 0.4475  Epoch: 19  Global Step: 112080   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:14:43,044-Speed 3906.93 samples/sec  Loss 2.7331  LearningRate 0.0089  ProxyLR: 0.4472  Epoch: 19  Global Step: 112090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:45,682-Speed 3883.41 samples/sec  Loss 2.7080  LearningRate 0.0089  ProxyLR: 0.4469  Epoch: 19  Global Step: 112100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:48,316-Speed 3889.25 samples/sec  Loss 2.7224  LearningRate 0.0089  ProxyLR: 0.4466  Epoch: 19  Global Step: 112110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:50,948-Speed 3891.63 samples/sec  Loss 2.6535  LearningRate 0.0089  ProxyLR: 0.4463  Epoch: 19  Global Step: 112120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:53,578-Speed 3894.39 samples/sec  Loss 2.7041  LearningRate 0.0089  ProxyLR: 0.4460  Epoch: 19  Global Step: 112130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:56,207-Speed 3896.00 samples/sec  Loss 2.7308  LearningRate 0.0089  ProxyLR: 0.4457  Epoch: 19  Global Step: 112140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:14:58,836-Speed 3896.18 samples/sec  Loss 2.7371  LearningRate 0.0089  ProxyLR: 0.4454  Epoch: 19  Global Step: 112150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:01,465-Speed 3895.39 samples/sec  Loss 2.7698  LearningRate 0.0089  ProxyLR: 0.4451  Epoch: 19  Global Step: 112160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:04,096-Speed 3894.17 samples/sec  Loss 2.7912  LearningRate 0.0089  ProxyLR: 0.4448  Epoch: 19  Global Step: 112170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:06,724-Speed 3896.74 samples/sec  Loss 2.7039  LearningRate 0.0089  ProxyLR: 0.4445  Epoch: 19  Global Step: 112180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:09,355-Speed 3892.58 samples/sec  Loss 2.6771  LearningRate 0.0089  ProxyLR: 0.4442  Epoch: 19  Global Step: 112190   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:15:11,985-Speed 3894.66 samples/sec  Loss 2.7022  LearningRate 0.0089  ProxyLR: 0.4439  Epoch: 19  Global Step: 112200   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:15:14,618-Speed 3891.08 samples/sec  Loss 2.7817  LearningRate 0.0089  ProxyLR: 0.4436  Epoch: 19  Global Step: 112210   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:15:17,237-Speed 3910.26 samples/sec  Loss 2.6833  LearningRate 0.0089  ProxyLR: 0.4433  Epoch: 19  Global Step: 112220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:19,867-Speed 3894.50 samples/sec  Loss 2.7055  LearningRate 0.0089  ProxyLR: 0.4430  Epoch: 19  Global Step: 112230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:22,498-Speed 3893.81 samples/sec  Loss 2.6652  LearningRate 0.0089  ProxyLR: 0.4427  Epoch: 19  Global Step: 112240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:25,127-Speed 3894.94 samples/sec  Loss 2.7634  LearningRate 0.0088  ProxyLR: 0.4424  Epoch: 19  Global Step: 112250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:27,758-Speed 3893.10 samples/sec  Loss 2.6541  LearningRate 0.0088  ProxyLR: 0.4421  Epoch: 19  Global Step: 112260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:30,388-Speed 3895.31 samples/sec  Loss 2.7155  LearningRate 0.0088  ProxyLR: 0.4418  Epoch: 19  Global Step: 112270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:33,019-Speed 3892.50 samples/sec  Loss 2.7083  LearningRate 0.0088  ProxyLR: 0.4415  Epoch: 19  Global Step: 112280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:35,649-Speed 3894.76 samples/sec  Loss 2.6803  LearningRate 0.0088  ProxyLR: 0.4413  Epoch: 19  Global Step: 112290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:38,280-Speed 3893.40 samples/sec  Loss 2.6479  LearningRate 0.0088  ProxyLR: 0.4410  Epoch: 19  Global Step: 112300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:40,911-Speed 3893.03 samples/sec  Loss 2.7323  LearningRate 0.0088  ProxyLR: 0.4407  Epoch: 19  Global Step: 112310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:43,543-Speed 3891.14 samples/sec  Loss 2.8147  LearningRate 0.0088  ProxyLR: 0.4404  Epoch: 19  Global Step: 112320   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:15:46,162-Speed 3911.08 samples/sec  Loss 2.7475  LearningRate 0.0088  ProxyLR: 0.4401  Epoch: 19  Global Step: 112330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:48,794-Speed 3891.73 samples/sec  Loss 2.7969  LearningRate 0.0088  ProxyLR: 0.4398  Epoch: 19  Global Step: 112340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:51,425-Speed 3894.27 samples/sec  Loss 2.6942  LearningRate 0.0088  ProxyLR: 0.4395  Epoch: 19  Global Step: 112350   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:54,053-Speed 3896.79 samples/sec  Loss 2.7409  LearningRate 0.0088  ProxyLR: 0.4392  Epoch: 19  Global Step: 112360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:56,681-Speed 3897.28 samples/sec  Loss 2.7923  LearningRate 0.0088  ProxyLR: 0.4389  Epoch: 19  Global Step: 112370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:15:59,309-Speed 3897.43 samples/sec  Loss 2.7351  LearningRate 0.0088  ProxyLR: 0.4386  Epoch: 19  Global Step: 112380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:01,937-Speed 3897.31 samples/sec  Loss 2.7342  LearningRate 0.0088  ProxyLR: 0.4383  Epoch: 19  Global Step: 112390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:04,567-Speed 3894.76 samples/sec  Loss 2.7123  LearningRate 0.0088  ProxyLR: 0.4380  Epoch: 19  Global Step: 112400   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:07,195-Speed 3898.22 samples/sec  Loss 2.7193  LearningRate 0.0088  ProxyLR: 0.4377  Epoch: 19  Global Step: 112410   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:09,821-Speed 3899.31 samples/sec  Loss 2.7740  LearningRate 0.0087  ProxyLR: 0.4374  Epoch: 19  Global Step: 112420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:12,436-Speed 3917.53 samples/sec  Loss 2.6310  LearningRate 0.0087  ProxyLR: 0.4371  Epoch: 19  Global Step: 112430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:15,064-Speed 3897.85 samples/sec  Loss 2.7207  LearningRate 0.0087  ProxyLR: 0.4368  Epoch: 19  Global Step: 112440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:17,692-Speed 3897.87 samples/sec  Loss 2.7311  LearningRate 0.0087  ProxyLR: 0.4365  Epoch: 19  Global Step: 112450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:20,320-Speed 3897.50 samples/sec  Loss 2.6753  LearningRate 0.0087  ProxyLR: 0.4362  Epoch: 19  Global Step: 112460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:22,948-Speed 3896.70 samples/sec  Loss 2.7637  LearningRate 0.0087  ProxyLR: 0.4359  Epoch: 19  Global Step: 112470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:25,577-Speed 3896.47 samples/sec  Loss 2.6898  LearningRate 0.0087  ProxyLR: 0.4357  Epoch: 19  Global Step: 112480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:28,207-Speed 3894.44 samples/sec  Loss 2.7682  LearningRate 0.0087  ProxyLR: 0.4354  Epoch: 19  Global Step: 112490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:30,836-Speed 3897.17 samples/sec  Loss 2.7508  LearningRate 0.0087  ProxyLR: 0.4351  Epoch: 19  Global Step: 112500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:33,465-Speed 3896.09 samples/sec  Loss 2.6974  LearningRate 0.0087  ProxyLR: 0.4348  Epoch: 19  Global Step: 112510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:36,096-Speed 3892.44 samples/sec  Loss 2.7912  LearningRate 0.0087  ProxyLR: 0.4345  Epoch: 19  Global Step: 112520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:16:38,725-Speed 3895.47 samples/sec  Loss 2.6897  LearningRate 0.0087  ProxyLR: 0.4342  Epoch: 19  Global Step: 112530   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:41,354-Speed 3897.03 samples/sec  Loss 2.7345  LearningRate 0.0087  ProxyLR: 0.4339  Epoch: 19  Global Step: 112540   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:43,981-Speed 3899.21 samples/sec  Loss 2.7040  LearningRate 0.0087  ProxyLR: 0.4336  Epoch: 19  Global Step: 112550   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:46,610-Speed 3895.72 samples/sec  Loss 2.7792  LearningRate 0.0087  ProxyLR: 0.4333  Epoch: 19  Global Step: 112560   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:49,238-Speed 3897.87 samples/sec  Loss 2.6774  LearningRate 0.0087  ProxyLR: 0.4330  Epoch: 19  Global Step: 112570   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:51,865-Speed 3898.36 samples/sec  Loss 2.6809  LearningRate 0.0087  ProxyLR: 0.4327  Epoch: 19  Global Step: 112580   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:54,495-Speed 3894.92 samples/sec  Loss 2.7325  LearningRate 0.0086  ProxyLR: 0.4324  Epoch: 19  Global Step: 112590   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:57,123-Speed 3896.77 samples/sec  Loss 2.7432  LearningRate 0.0086  ProxyLR: 0.4321  Epoch: 19  Global Step: 112600   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:16:59,753-Speed 3895.19 samples/sec  Loss 2.7549  LearningRate 0.0086  ProxyLR: 0.4318  Epoch: 19  Global Step: 112610   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:17:02,382-Speed 3895.79 samples/sec  Loss 2.6055  LearningRate 0.0086  ProxyLR: 0.4316  Epoch: 19  Global Step: 112620   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:17:05,012-Speed 3895.29 samples/sec  Loss 2.7244  LearningRate 0.0086  ProxyLR: 0.4313  Epoch: 19  Global Step: 112630   Fp16 Grad Scale: 1048576  Required: 2 hours
Training: 2023-05-04 23:17:07,625-Speed 3919.22 samples/sec  Loss 2.7418  LearningRate 0.0086  ProxyLR: 0.4310  Epoch: 19  Global Step: 112640   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:17:10,253-Speed 3897.33 samples/sec  Loss 2.7533  LearningRate 0.0086  ProxyLR: 0.4307  Epoch: 19  Global Step: 112650   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:17:12,868-Speed 3917.69 samples/sec  Loss 2.7415  LearningRate 0.0086  ProxyLR: 0.4304  Epoch: 19  Global Step: 112660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:15,497-Speed 3897.06 samples/sec  Loss 2.5926  LearningRate 0.0086  ProxyLR: 0.4301  Epoch: 19  Global Step: 112670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:18,124-Speed 3897.70 samples/sec  Loss 2.7243  LearningRate 0.0086  ProxyLR: 0.4298  Epoch: 19  Global Step: 112680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:20,752-Speed 3898.34 samples/sec  Loss 2.7142  LearningRate 0.0086  ProxyLR: 0.4295  Epoch: 19  Global Step: 112690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:23,381-Speed 3895.31 samples/sec  Loss 2.7599  LearningRate 0.0086  ProxyLR: 0.4292  Epoch: 19  Global Step: 112700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:26,009-Speed 3898.53 samples/sec  Loss 2.6572  LearningRate 0.0086  ProxyLR: 0.4289  Epoch: 19  Global Step: 112710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:28,636-Speed 3899.13 samples/sec  Loss 2.7208  LearningRate 0.0086  ProxyLR: 0.4286  Epoch: 19  Global Step: 112720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:31,262-Speed 3899.24 samples/sec  Loss 2.7220  LearningRate 0.0086  ProxyLR: 0.4283  Epoch: 19  Global Step: 112730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:33,891-Speed 3896.54 samples/sec  Loss 2.7258  LearningRate 0.0086  ProxyLR: 0.4281  Epoch: 19  Global Step: 112740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:36,519-Speed 3898.28 samples/sec  Loss 2.6463  LearningRate 0.0086  ProxyLR: 0.4278  Epoch: 19  Global Step: 112750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:39,133-Speed 3917.66 samples/sec  Loss 2.7367  LearningRate 0.0085  ProxyLR: 0.4275  Epoch: 19  Global Step: 112760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:41,761-Speed 3897.24 samples/sec  Loss 2.7197  LearningRate 0.0085  ProxyLR: 0.4272  Epoch: 19  Global Step: 112770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:44,389-Speed 3897.35 samples/sec  Loss 2.6916  LearningRate 0.0085  ProxyLR: 0.4269  Epoch: 19  Global Step: 112780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:47,019-Speed 3895.01 samples/sec  Loss 2.6846  LearningRate 0.0085  ProxyLR: 0.4266  Epoch: 19  Global Step: 112790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:49,646-Speed 3899.77 samples/sec  Loss 2.6976  LearningRate 0.0085  ProxyLR: 0.4263  Epoch: 19  Global Step: 112800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:52,273-Speed 3898.92 samples/sec  Loss 2.7204  LearningRate 0.0085  ProxyLR: 0.4260  Epoch: 19  Global Step: 112810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:54,901-Speed 3897.01 samples/sec  Loss 2.7221  LearningRate 0.0085  ProxyLR: 0.4257  Epoch: 19  Global Step: 112820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:17:57,528-Speed 3898.95 samples/sec  Loss 2.7872  LearningRate 0.0085  ProxyLR: 0.4254  Epoch: 19  Global Step: 112830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:00,155-Speed 3899.18 samples/sec  Loss 2.7202  LearningRate 0.0085  ProxyLR: 0.4251  Epoch: 19  Global Step: 112840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:02,781-Speed 3901.62 samples/sec  Loss 2.7170  LearningRate 0.0085  ProxyLR: 0.4249  Epoch: 19  Global Step: 112850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:05,407-Speed 3899.93 samples/sec  Loss 2.7732  LearningRate 0.0085  ProxyLR: 0.4246  Epoch: 19  Global Step: 112860   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:08,034-Speed 3899.72 samples/sec  Loss 2.7755  LearningRate 0.0085  ProxyLR: 0.4243  Epoch: 19  Global Step: 112870   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:10,660-Speed 3899.16 samples/sec  Loss 2.7252  LearningRate 0.0085  ProxyLR: 0.4240  Epoch: 19  Global Step: 112880   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:13,287-Speed 3899.96 samples/sec  Loss 2.7271  LearningRate 0.0085  ProxyLR: 0.4237  Epoch: 19  Global Step: 112890   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:15,914-Speed 3898.21 samples/sec  Loss 2.6811  LearningRate 0.0085  ProxyLR: 0.4234  Epoch: 19  Global Step: 112900   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:18,540-Speed 3901.54 samples/sec  Loss 2.6871  LearningRate 0.0085  ProxyLR: 0.4231  Epoch: 19  Global Step: 112910   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:21,167-Speed 3898.08 samples/sec  Loss 2.6022  LearningRate 0.0085  ProxyLR: 0.4228  Epoch: 19  Global Step: 112920   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:23,794-Speed 3899.11 samples/sec  Loss 2.7729  LearningRate 0.0085  ProxyLR: 0.4225  Epoch: 19  Global Step: 112930   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:26,421-Speed 3898.61 samples/sec  Loss 2.8023  LearningRate 0.0084  ProxyLR: 0.4222  Epoch: 19  Global Step: 112940   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:29,047-Speed 3900.52 samples/sec  Loss 2.7250  LearningRate 0.0084  ProxyLR: 0.4220  Epoch: 19  Global Step: 112950   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:31,675-Speed 3898.77 samples/sec  Loss 2.6942  LearningRate 0.0084  ProxyLR: 0.4217  Epoch: 19  Global Step: 112960   Fp16 Grad Scale: 1048576  Required: 2 hours
Training: 2023-05-04 23:18:34,288-Speed 3918.76 samples/sec  Loss 2.7100  LearningRate 0.0084  ProxyLR: 0.4214  Epoch: 19  Global Step: 112970   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:36,915-Speed 3899.34 samples/sec  Loss 2.7112  LearningRate 0.0084  ProxyLR: 0.4211  Epoch: 19  Global Step: 112980   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:18:39,530-Speed 3917.50 samples/sec  Loss 2.6601  LearningRate 0.0084  ProxyLR: 0.4208  Epoch: 19  Global Step: 112990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:42,158-Speed 3897.20 samples/sec  Loss 2.7212  LearningRate 0.0084  ProxyLR: 0.4205  Epoch: 19  Global Step: 113000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:44,785-Speed 3899.72 samples/sec  Loss 2.7367  LearningRate 0.0084  ProxyLR: 0.4202  Epoch: 19  Global Step: 113010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:47,413-Speed 3897.60 samples/sec  Loss 2.7488  LearningRate 0.0084  ProxyLR: 0.4199  Epoch: 19  Global Step: 113020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:50,040-Speed 3899.00 samples/sec  Loss 2.7870  LearningRate 0.0084  ProxyLR: 0.4197  Epoch: 19  Global Step: 113030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:52,669-Speed 3895.99 samples/sec  Loss 2.7034  LearningRate 0.0084  ProxyLR: 0.4194  Epoch: 19  Global Step: 113040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:55,298-Speed 3896.24 samples/sec  Loss 2.7092  LearningRate 0.0084  ProxyLR: 0.4191  Epoch: 19  Global Step: 113050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:18:57,925-Speed 3899.33 samples/sec  Loss 2.7014  LearningRate 0.0084  ProxyLR: 0.4188  Epoch: 19  Global Step: 113060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:00,553-Speed 3897.95 samples/sec  Loss 2.7942  LearningRate 0.0084  ProxyLR: 0.4185  Epoch: 19  Global Step: 113070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:03,179-Speed 3899.61 samples/sec  Loss 2.7471  LearningRate 0.0084  ProxyLR: 0.4182  Epoch: 19  Global Step: 113080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:05,806-Speed 3899.64 samples/sec  Loss 2.7662  LearningRate 0.0084  ProxyLR: 0.4179  Epoch: 19  Global Step: 113090   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:08,432-Speed 3899.41 samples/sec  Loss 2.6457  LearningRate 0.0084  ProxyLR: 0.4176  Epoch: 19  Global Step: 113100   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:11,059-Speed 3899.26 samples/sec  Loss 2.7243  LearningRate 0.0083  ProxyLR: 0.4173  Epoch: 19  Global Step: 113110   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:13,686-Speed 3900.34 samples/sec  Loss 2.6561  LearningRate 0.0083  ProxyLR: 0.4171  Epoch: 19  Global Step: 113120   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:16,314-Speed 3897.40 samples/sec  Loss 2.7947  LearningRate 0.0083  ProxyLR: 0.4168  Epoch: 19  Global Step: 113130   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:18,939-Speed 3901.64 samples/sec  Loss 2.6932  LearningRate 0.0083  ProxyLR: 0.4165  Epoch: 19  Global Step: 113140   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:21,551-Speed 3920.89 samples/sec  Loss 2.7108  LearningRate 0.0083  ProxyLR: 0.4162  Epoch: 19  Global Step: 113150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:24,177-Speed 3901.32 samples/sec  Loss 2.6528  LearningRate 0.0083  ProxyLR: 0.4159  Epoch: 19  Global Step: 113160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:26,803-Speed 3900.12 samples/sec  Loss 2.7204  LearningRate 0.0083  ProxyLR: 0.4156  Epoch: 19  Global Step: 113170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:29,429-Speed 3899.95 samples/sec  Loss 2.6699  LearningRate 0.0083  ProxyLR: 0.4153  Epoch: 19  Global Step: 113180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:32,056-Speed 3898.80 samples/sec  Loss 2.6964  LearningRate 0.0083  ProxyLR: 0.4151  Epoch: 19  Global Step: 113190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:34,685-Speed 3897.66 samples/sec  Loss 2.6677  LearningRate 0.0083  ProxyLR: 0.4148  Epoch: 19  Global Step: 113200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:37,316-Speed 3893.01 samples/sec  Loss 2.6831  LearningRate 0.0083  ProxyLR: 0.4145  Epoch: 19  Global Step: 113210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:39,947-Speed 3892.52 samples/sec  Loss 2.7063  LearningRate 0.0083  ProxyLR: 0.4142  Epoch: 19  Global Step: 113220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:42,580-Speed 3890.55 samples/sec  Loss 2.7616  LearningRate 0.0083  ProxyLR: 0.4139  Epoch: 19  Global Step: 113230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:45,217-Speed 3883.77 samples/sec  Loss 2.7540  LearningRate 0.0083  ProxyLR: 0.4136  Epoch: 19  Global Step: 113240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:47,851-Speed 3889.36 samples/sec  Loss 2.7965  LearningRate 0.0083  ProxyLR: 0.4133  Epoch: 19  Global Step: 113250   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:19:50,474-Speed 3903.96 samples/sec  Loss 2.7357  LearningRate 0.0083  ProxyLR: 0.4130  Epoch: 19  Global Step: 113260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:53,110-Speed 3886.43 samples/sec  Loss 2.6838  LearningRate 0.0083  ProxyLR: 0.4128  Epoch: 19  Global Step: 113270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:55,743-Speed 3889.08 samples/sec  Loss 2.7045  LearningRate 0.0082  ProxyLR: 0.4125  Epoch: 19  Global Step: 113280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:19:58,376-Speed 3890.44 samples/sec  Loss 2.7568  LearningRate 0.0082  ProxyLR: 0.4122  Epoch: 19  Global Step: 113290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:01,013-Speed 3885.02 samples/sec  Loss 2.7373  LearningRate 0.0082  ProxyLR: 0.4119  Epoch: 19  Global Step: 113300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:03,650-Speed 3883.86 samples/sec  Loss 2.7250  LearningRate 0.0082  ProxyLR: 0.4116  Epoch: 19  Global Step: 113310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:06,282-Speed 3890.79 samples/sec  Loss 2.6401  LearningRate 0.0082  ProxyLR: 0.4113  Epoch: 19  Global Step: 113320   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:08,915-Speed 3891.18 samples/sec  Loss 2.7601  LearningRate 0.0082  ProxyLR: 0.4110  Epoch: 19  Global Step: 113330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:11,548-Speed 3890.07 samples/sec  Loss 2.6994  LearningRate 0.0082  ProxyLR: 0.4108  Epoch: 19  Global Step: 113340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:14,184-Speed 3885.63 samples/sec  Loss 2.7521  LearningRate 0.0082  ProxyLR: 0.4105  Epoch: 19  Global Step: 113350   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:16,809-Speed 3902.05 samples/sec  Loss 2.7414  LearningRate 0.0082  ProxyLR: 0.4102  Epoch: 19  Global Step: 113360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:19,446-Speed 3883.90 samples/sec  Loss 2.7317  LearningRate 0.0082  ProxyLR: 0.4099  Epoch: 19  Global Step: 113370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:22,085-Speed 3881.23 samples/sec  Loss 2.6567  LearningRate 0.0082  ProxyLR: 0.4096  Epoch: 19  Global Step: 113380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:24,721-Speed 3885.84 samples/sec  Loss 2.7272  LearningRate 0.0082  ProxyLR: 0.4093  Epoch: 19  Global Step: 113390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:27,358-Speed 3883.72 samples/sec  Loss 2.7471  LearningRate 0.0082  ProxyLR: 0.4091  Epoch: 19  Global Step: 113400   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:29,994-Speed 3885.71 samples/sec  Loss 2.6794  LearningRate 0.0082  ProxyLR: 0.4088  Epoch: 19  Global Step: 113410   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:32,630-Speed 3885.64 samples/sec  Loss 2.7243  LearningRate 0.0082  ProxyLR: 0.4085  Epoch: 19  Global Step: 113420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:35,265-Speed 3887.31 samples/sec  Loss 2.7038  LearningRate 0.0082  ProxyLR: 0.4082  Epoch: 19  Global Step: 113430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:37,900-Speed 3887.31 samples/sec  Loss 2.6925  LearningRate 0.0082  ProxyLR: 0.4079  Epoch: 19  Global Step: 113440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:40,537-Speed 3885.14 samples/sec  Loss 2.7331  LearningRate 0.0082  ProxyLR: 0.4076  Epoch: 19  Global Step: 113450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:43,170-Speed 3889.42 samples/sec  Loss 2.7135  LearningRate 0.0081  ProxyLR: 0.4073  Epoch: 19  Global Step: 113460   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:20:45,802-Speed 3891.98 samples/sec  Loss 2.7766  LearningRate 0.0081  ProxyLR: 0.4071  Epoch: 19  Global Step: 113470   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:20:48,433-Speed 3893.35 samples/sec  Loss 2.6987  LearningRate 0.0081  ProxyLR: 0.4068  Epoch: 19  Global Step: 113480   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:20:51,050-Speed 3913.93 samples/sec  Loss 2.6863  LearningRate 0.0081  ProxyLR: 0.4065  Epoch: 19  Global Step: 113490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:53,682-Speed 3890.61 samples/sec  Loss 2.7087  LearningRate 0.0081  ProxyLR: 0.4062  Epoch: 19  Global Step: 113500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:56,314-Speed 3892.28 samples/sec  Loss 2.7386  LearningRate 0.0081  ProxyLR: 0.4059  Epoch: 19  Global Step: 113510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:20:58,945-Speed 3893.45 samples/sec  Loss 2.6782  LearningRate 0.0081  ProxyLR: 0.4056  Epoch: 19  Global Step: 113520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:01,576-Speed 3893.16 samples/sec  Loss 2.6624  LearningRate 0.0081  ProxyLR: 0.4054  Epoch: 19  Global Step: 113530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:04,207-Speed 3892.08 samples/sec  Loss 2.7933  LearningRate 0.0081  ProxyLR: 0.4051  Epoch: 19  Global Step: 113540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:06,840-Speed 3890.08 samples/sec  Loss 2.6731  LearningRate 0.0081  ProxyLR: 0.4048  Epoch: 19  Global Step: 113550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:09,473-Speed 3890.16 samples/sec  Loss 2.7018  LearningRate 0.0081  ProxyLR: 0.4045  Epoch: 19  Global Step: 113560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:12,103-Speed 3895.07 samples/sec  Loss 2.7039  LearningRate 0.0081  ProxyLR: 0.4042  Epoch: 19  Global Step: 113570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:14,735-Speed 3891.46 samples/sec  Loss 2.7437  LearningRate 0.0081  ProxyLR: 0.4039  Epoch: 19  Global Step: 113580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:17,366-Speed 3892.88 samples/sec  Loss 2.6643  LearningRate 0.0081  ProxyLR: 0.4037  Epoch: 19  Global Step: 113590   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:21:19,985-Speed 3912.04 samples/sec  Loss 2.6597  LearningRate 0.0081  ProxyLR: 0.4034  Epoch: 19  Global Step: 113600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:22,615-Speed 3894.44 samples/sec  Loss 2.7502  LearningRate 0.0081  ProxyLR: 0.4031  Epoch: 19  Global Step: 113610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:25,245-Speed 3894.81 samples/sec  Loss 2.6876  LearningRate 0.0081  ProxyLR: 0.4028  Epoch: 19  Global Step: 113620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:27,877-Speed 3890.90 samples/sec  Loss 2.6532  LearningRate 0.0081  ProxyLR: 0.4025  Epoch: 19  Global Step: 113630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:30,508-Speed 3892.81 samples/sec  Loss 2.6335  LearningRate 0.0080  ProxyLR: 0.4023  Epoch: 19  Global Step: 113640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:33,139-Speed 3893.83 samples/sec  Loss 2.7565  LearningRate 0.0080  ProxyLR: 0.4020  Epoch: 19  Global Step: 113650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:35,771-Speed 3890.34 samples/sec  Loss 2.8030  LearningRate 0.0080  ProxyLR: 0.4017  Epoch: 19  Global Step: 113660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:38,400-Speed 3896.87 samples/sec  Loss 2.7362  LearningRate 0.0080  ProxyLR: 0.4014  Epoch: 19  Global Step: 113670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:41,029-Speed 3895.09 samples/sec  Loss 2.6372  LearningRate 0.0080  ProxyLR: 0.4011  Epoch: 19  Global Step: 113680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:43,659-Speed 3894.85 samples/sec  Loss 2.6850  LearningRate 0.0080  ProxyLR: 0.4008  Epoch: 19  Global Step: 113690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:46,281-Speed 3907.11 samples/sec  Loss 2.7603  LearningRate 0.0080  ProxyLR: 0.4006  Epoch: 19  Global Step: 113700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:48,969-Speed 3810.06 samples/sec  Loss 2.7059  LearningRate 0.0080  ProxyLR: 0.4003  Epoch: 19  Global Step: 113710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:21:51,594-Speed 3901.85 samples/sec  Loss 2.7019  LearningRate 0.0080  ProxyLR: 0.4000  Epoch: 19  Global Step: 113720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:00,577-Speed 1140.02 samples/sec  Loss 9.9033  LearningRate 0.0080  ProxyLR: 0.3997  Epoch: 20  Global Step: 113730   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:03,213-Speed 3886.68 samples/sec  Loss 9.2577  LearningRate 0.0080  ProxyLR: 0.3994  Epoch: 20  Global Step: 113740   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:05,879-Speed 3841.88 samples/sec  Loss 8.7134  LearningRate 0.0080  ProxyLR: 0.3992  Epoch: 20  Global Step: 113750   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:08,513-Speed 3889.04 samples/sec  Loss 8.0582  LearningRate 0.0080  ProxyLR: 0.3989  Epoch: 20  Global Step: 113760   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:11,146-Speed 3889.18 samples/sec  Loss 7.7208  LearningRate 0.0080  ProxyLR: 0.3986  Epoch: 20  Global Step: 113770   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:13,780-Speed 3889.39 samples/sec  Loss 7.3620  LearningRate 0.0080  ProxyLR: 0.3983  Epoch: 20  Global Step: 113780   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:16,415-Speed 3886.57 samples/sec  Loss 7.0554  LearningRate 0.0080  ProxyLR: 0.3980  Epoch: 20  Global Step: 113790   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:19,049-Speed 3889.12 samples/sec  Loss 6.7639  LearningRate 0.0080  ProxyLR: 0.3978  Epoch: 20  Global Step: 113800   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:21,686-Speed 3883.20 samples/sec  Loss 6.7230  LearningRate 0.0079  ProxyLR: 0.3975  Epoch: 20  Global Step: 113810   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:24,322-Speed 3885.71 samples/sec  Loss 6.5536  LearningRate 0.0079  ProxyLR: 0.3972  Epoch: 20  Global Step: 113820   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:26,958-Speed 3886.15 samples/sec  Loss 6.4647  LearningRate 0.0079  ProxyLR: 0.3969  Epoch: 20  Global Step: 113830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:29,594-Speed 3885.36 samples/sec  Loss 6.3545  LearningRate 0.0079  ProxyLR: 0.3966  Epoch: 20  Global Step: 113840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:32,230-Speed 3886.46 samples/sec  Loss 6.3920  LearningRate 0.0079  ProxyLR: 0.3964  Epoch: 20  Global Step: 113850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:34,866-Speed 3884.58 samples/sec  Loss 6.2068  LearningRate 0.0079  ProxyLR: 0.3961  Epoch: 20  Global Step: 113860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:37,500-Speed 3888.86 samples/sec  Loss 6.2034  LearningRate 0.0079  ProxyLR: 0.3958  Epoch: 20  Global Step: 113870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:40,136-Speed 3885.69 samples/sec  Loss 6.2225  LearningRate 0.0079  ProxyLR: 0.3955  Epoch: 20  Global Step: 113880   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:22:42,807-Speed 3834.98 samples/sec  Loss 6.1218  LearningRate 0.0079  ProxyLR: 0.3952  Epoch: 20  Global Step: 113890   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:45,442-Speed 3887.77 samples/sec  Loss 6.0580  LearningRate 0.0079  ProxyLR: 0.3950  Epoch: 20  Global Step: 113900   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:48,077-Speed 3886.95 samples/sec  Loss 6.1334  LearningRate 0.0079  ProxyLR: 0.3947  Epoch: 20  Global Step: 113910   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:50,712-Speed 3887.93 samples/sec  Loss 6.0949  LearningRate 0.0079  ProxyLR: 0.3944  Epoch: 20  Global Step: 113920   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:53,347-Speed 3886.23 samples/sec  Loss 6.0566  LearningRate 0.0079  ProxyLR: 0.3941  Epoch: 20  Global Step: 113930   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:55,979-Speed 3891.59 samples/sec  Loss 5.9978  LearningRate 0.0079  ProxyLR: 0.3938  Epoch: 20  Global Step: 113940   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:22:58,613-Speed 3888.19 samples/sec  Loss 5.8952  LearningRate 0.0079  ProxyLR: 0.3936  Epoch: 20  Global Step: 113950   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:23:01,249-Speed 3885.82 samples/sec  Loss 6.0883  LearningRate 0.0079  ProxyLR: 0.3933  Epoch: 20  Global Step: 113960   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:23:03,883-Speed 3888.96 samples/sec  Loss 5.9119  LearningRate 0.0079  ProxyLR: 0.3930  Epoch: 20  Global Step: 113970   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:23:06,516-Speed 3890.43 samples/sec  Loss 5.9071  LearningRate 0.0079  ProxyLR: 0.3927  Epoch: 20  Global Step: 113980   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:23:09,150-Speed 3888.23 samples/sec  Loss 6.0468  LearningRate 0.0078  ProxyLR: 0.3924  Epoch: 20  Global Step: 113990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:23:11,785-Speed 3886.81 samples/sec  Loss 5.9644  LearningRate 0.0078  ProxyLR: 0.3922  Epoch: 20  Global Step: 114000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:24:01,437-[lfw][114000]XNorm: 22.059253
Training: 2023-05-04 23:24:01,437-[lfw][114000]Accuracy-Flip: 0.99733+-0.00249
Training: 2023-05-04 23:24:01,437-[lfw][114000]Accuracy-Highest: 0.99750
Training: 2023-05-04 23:24:01,437-[lfw][114000]TPR@1stNon-Zero-FPR of 0.00033: 0.99667
Training: 2023-05-04 23:24:01,437-[lfw][114000]Highest TPR@FPR: 0.99667
Training: 2023-05-04 23:24:58,435-[cfp_fp][114000]XNorm: 21.690285
Training: 2023-05-04 23:24:58,435-[cfp_fp][114000]Accuracy-Flip: 0.98314+-0.00728
Training: 2023-05-04 23:24:58,435-[cfp_fp][114000]Accuracy-Highest: 0.98314
Training: 2023-05-04 23:24:58,435-[cfp_fp][114000]TPR@1stNon-Zero-FPR of 0.00029: 0.86257
Training: 2023-05-04 23:24:58,435-[cfp_fp][114000]Highest TPR@FPR: 0.87086
Training: 2023-05-04 23:25:47,989-[agedb_30][114000]XNorm: 22.279862
Training: 2023-05-04 23:25:47,990-[agedb_30][114000]Accuracy-Flip: 0.97133+-0.00939
Training: 2023-05-04 23:25:47,990-[agedb_30][114000]Accuracy-Highest: 0.97300
Training: 2023-05-04 23:25:47,990-[agedb_30][114000]TPR@1stNon-Zero-FPR of 0.00033: 0.87200
Training: 2023-05-04 23:25:47,990-[agedb_30][114000]Highest TPR@FPR: 0.87200
Training: 2023-05-04 23:26:38,997-[calfw][114000]XNorm: 22.199277
Training: 2023-05-04 23:26:38,997-[calfw][114000]Accuracy-Flip: 0.95733+-0.01143
Training: 2023-05-04 23:26:38,997-[calfw][114000]Accuracy-Highest: 0.95833
Training: 2023-05-04 23:26:38,998-[calfw][114000]TPR@1stNon-Zero-FPR of 0.00033: 0.81133
Training: 2023-05-04 23:26:38,998-[calfw][114000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 23:27:29,972-[cplfw][114000]XNorm: 21.036157
Training: 2023-05-04 23:27:29,973-[cplfw][114000]Accuracy-Flip: 0.92900+-0.01415
Training: 2023-05-04 23:27:29,973-[cplfw][114000]Accuracy-Highest: 0.92900
Training: 2023-05-04 23:27:29,973-[cplfw][114000]TPR@1stNon-Zero-FPR of 0.00033: 0.05067
Training: 2023-05-04 23:27:29,973-[cplfw][114000]Highest TPR@FPR: 0.07633
Training: 2023-05-04 23:27:33,788-Speed 39.08 samples/sec  Loss 6.0545  LearningRate 0.0078  ProxyLR: 0.3919  Epoch: 20  Global Step: 114010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:27:36,413-Speed 3902.04 samples/sec  Loss 5.8651  LearningRate 0.0078  ProxyLR: 0.3916  Epoch: 20  Global Step: 114020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:27:39,039-Speed 3900.51 samples/sec  Loss 6.0140  LearningRate 0.0078  ProxyLR: 0.3913  Epoch: 20  Global Step: 114030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:27:41,667-Speed 3898.04 samples/sec  Loss 5.8850  LearningRate 0.0078  ProxyLR: 0.3910  Epoch: 20  Global Step: 114040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:27:44,279-Speed 3921.04 samples/sec  Loss 5.9330  LearningRate 0.0078  ProxyLR: 0.3908  Epoch: 20  Global Step: 114050   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:27:46,909-Speed 3896.47 samples/sec  Loss 5.9011  LearningRate 0.0078  ProxyLR: 0.3905  Epoch: 20  Global Step: 114060   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:27:49,538-Speed 3895.25 samples/sec  Loss 5.8726  LearningRate 0.0078  ProxyLR: 0.3902  Epoch: 20  Global Step: 114070   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:27:52,168-Speed 3895.27 samples/sec  Loss 5.9242  LearningRate 0.0078  ProxyLR: 0.3899  Epoch: 20  Global Step: 114080   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:27:54,796-Speed 3896.36 samples/sec  Loss 5.8715  LearningRate 0.0078  ProxyLR: 0.3897  Epoch: 20  Global Step: 114090   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:27:57,425-Speed 3896.21 samples/sec  Loss 5.8569  LearningRate 0.0078  ProxyLR: 0.3894  Epoch: 20  Global Step: 114100   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:28:00,056-Speed 3893.07 samples/sec  Loss 5.8334  LearningRate 0.0078  ProxyLR: 0.3891  Epoch: 20  Global Step: 114110   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:28:02,688-Speed 3890.83 samples/sec  Loss 5.9695  LearningRate 0.0078  ProxyLR: 0.3888  Epoch: 20  Global Step: 114120   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:28:05,322-Speed 3888.84 samples/sec  Loss 5.9387  LearningRate 0.0078  ProxyLR: 0.3885  Epoch: 20  Global Step: 114130   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:28:07,955-Speed 3890.57 samples/sec  Loss 5.7822  LearningRate 0.0078  ProxyLR: 0.3883  Epoch: 20  Global Step: 114140   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:28:10,589-Speed 3887.77 samples/sec  Loss 5.7798  LearningRate 0.0078  ProxyLR: 0.3880  Epoch: 20  Global Step: 114150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:13,224-Speed 3888.00 samples/sec  Loss 5.8399  LearningRate 0.0078  ProxyLR: 0.3877  Epoch: 20  Global Step: 114160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:15,858-Speed 3887.77 samples/sec  Loss 5.8485  LearningRate 0.0077  ProxyLR: 0.3874  Epoch: 20  Global Step: 114170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:18,491-Speed 3889.99 samples/sec  Loss 5.8315  LearningRate 0.0077  ProxyLR: 0.3872  Epoch: 20  Global Step: 114180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:21,126-Speed 3888.17 samples/sec  Loss 5.8611  LearningRate 0.0077  ProxyLR: 0.3869  Epoch: 20  Global Step: 114190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:23,757-Speed 3892.25 samples/sec  Loss 5.7637  LearningRate 0.0077  ProxyLR: 0.3866  Epoch: 20  Global Step: 114200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:26,386-Speed 3896.04 samples/sec  Loss 5.8335  LearningRate 0.0077  ProxyLR: 0.3863  Epoch: 20  Global Step: 114210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:29,020-Speed 3888.71 samples/sec  Loss 5.8305  LearningRate 0.0077  ProxyLR: 0.3861  Epoch: 20  Global Step: 114220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:31,656-Speed 3885.59 samples/sec  Loss 5.8533  LearningRate 0.0077  ProxyLR: 0.3858  Epoch: 20  Global Step: 114230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:34,290-Speed 3888.93 samples/sec  Loss 5.9357  LearningRate 0.0077  ProxyLR: 0.3855  Epoch: 20  Global Step: 114240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:36,910-Speed 3909.31 samples/sec  Loss 5.8045  LearningRate 0.0077  ProxyLR: 0.3852  Epoch: 20  Global Step: 114250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:39,543-Speed 3890.10 samples/sec  Loss 5.8737  LearningRate 0.0077  ProxyLR: 0.3849  Epoch: 20  Global Step: 114260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:42,173-Speed 3894.09 samples/sec  Loss 5.8127  LearningRate 0.0077  ProxyLR: 0.3847  Epoch: 20  Global Step: 114270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:44,803-Speed 3894.01 samples/sec  Loss 5.8090  LearningRate 0.0077  ProxyLR: 0.3844  Epoch: 20  Global Step: 114280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:47,434-Speed 3893.56 samples/sec  Loss 5.8307  LearningRate 0.0077  ProxyLR: 0.3841  Epoch: 20  Global Step: 114290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:50,064-Speed 3894.57 samples/sec  Loss 5.7727  LearningRate 0.0077  ProxyLR: 0.3838  Epoch: 20  Global Step: 114300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:52,694-Speed 3894.77 samples/sec  Loss 5.8471  LearningRate 0.0077  ProxyLR: 0.3836  Epoch: 20  Global Step: 114310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:55,325-Speed 3893.19 samples/sec  Loss 5.7761  LearningRate 0.0077  ProxyLR: 0.3833  Epoch: 20  Global Step: 114320   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:28:57,956-Speed 3892.68 samples/sec  Loss 5.8128  LearningRate 0.0077  ProxyLR: 0.3830  Epoch: 20  Global Step: 114330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:00,587-Speed 3892.26 samples/sec  Loss 5.9317  LearningRate 0.0077  ProxyLR: 0.3827  Epoch: 20  Global Step: 114340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:03,219-Speed 3892.31 samples/sec  Loss 5.7432  LearningRate 0.0076  ProxyLR: 0.3825  Epoch: 20  Global Step: 114350   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:29:05,837-Speed 3911.87 samples/sec  Loss 5.8174  LearningRate 0.0076  ProxyLR: 0.3822  Epoch: 20  Global Step: 114360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:08,468-Speed 3892.97 samples/sec  Loss 5.8172  LearningRate 0.0076  ProxyLR: 0.3819  Epoch: 20  Global Step: 114370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:11,099-Speed 3892.70 samples/sec  Loss 5.8178  LearningRate 0.0076  ProxyLR: 0.3816  Epoch: 20  Global Step: 114380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:13,729-Speed 3895.16 samples/sec  Loss 5.7965  LearningRate 0.0076  ProxyLR: 0.3814  Epoch: 20  Global Step: 114390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:16,360-Speed 3893.20 samples/sec  Loss 5.8276  LearningRate 0.0076  ProxyLR: 0.3811  Epoch: 20  Global Step: 114400   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:18,992-Speed 3890.89 samples/sec  Loss 5.8094  LearningRate 0.0076  ProxyLR: 0.3808  Epoch: 20  Global Step: 114410   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:21,624-Speed 3891.24 samples/sec  Loss 5.7730  LearningRate 0.0076  ProxyLR: 0.3805  Epoch: 20  Global Step: 114420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:24,256-Speed 3891.70 samples/sec  Loss 5.8006  LearningRate 0.0076  ProxyLR: 0.3803  Epoch: 20  Global Step: 114430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:26,887-Speed 3892.84 samples/sec  Loss 5.7781  LearningRate 0.0076  ProxyLR: 0.3800  Epoch: 20  Global Step: 114440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:29,506-Speed 3910.44 samples/sec  Loss 5.7980  LearningRate 0.0076  ProxyLR: 0.3797  Epoch: 20  Global Step: 114450   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:32,139-Speed 3891.18 samples/sec  Loss 5.8253  LearningRate 0.0076  ProxyLR: 0.3794  Epoch: 20  Global Step: 114460   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:34,772-Speed 3888.97 samples/sec  Loss 5.7303  LearningRate 0.0076  ProxyLR: 0.3792  Epoch: 20  Global Step: 114470   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:37,409-Speed 3885.29 samples/sec  Loss 5.8050  LearningRate 0.0076  ProxyLR: 0.3789  Epoch: 20  Global Step: 114480   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:40,044-Speed 3886.13 samples/sec  Loss 5.7993  LearningRate 0.0076  ProxyLR: 0.3786  Epoch: 20  Global Step: 114490   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:42,678-Speed 3888.75 samples/sec  Loss 5.7079  LearningRate 0.0076  ProxyLR: 0.3784  Epoch: 20  Global Step: 114500   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:45,313-Speed 3887.72 samples/sec  Loss 5.7517  LearningRate 0.0076  ProxyLR: 0.3781  Epoch: 20  Global Step: 114510   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:47,946-Speed 3888.75 samples/sec  Loss 5.7780  LearningRate 0.0076  ProxyLR: 0.3778  Epoch: 20  Global Step: 114520   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:50,579-Speed 3890.28 samples/sec  Loss 5.8713  LearningRate 0.0076  ProxyLR: 0.3775  Epoch: 20  Global Step: 114530   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:53,215-Speed 3886.35 samples/sec  Loss 5.6950  LearningRate 0.0075  ProxyLR: 0.3773  Epoch: 20  Global Step: 114540   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:29:55,849-Speed 3887.38 samples/sec  Loss 5.7598  LearningRate 0.0075  ProxyLR: 0.3770  Epoch: 20  Global Step: 114550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:29:58,484-Speed 3888.56 samples/sec  Loss 5.7145  LearningRate 0.0075  ProxyLR: 0.3767  Epoch: 20  Global Step: 114560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:01,115-Speed 3892.65 samples/sec  Loss 5.7865  LearningRate 0.0075  ProxyLR: 0.3764  Epoch: 20  Global Step: 114570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:03,749-Speed 3889.00 samples/sec  Loss 5.7310  LearningRate 0.0075  ProxyLR: 0.3762  Epoch: 20  Global Step: 114580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:06,384-Speed 3886.34 samples/sec  Loss 5.7810  LearningRate 0.0075  ProxyLR: 0.3759  Epoch: 20  Global Step: 114590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:09,019-Speed 3886.72 samples/sec  Loss 5.7490  LearningRate 0.0075  ProxyLR: 0.3756  Epoch: 20  Global Step: 114600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:11,652-Speed 3889.97 samples/sec  Loss 5.6940  LearningRate 0.0075  ProxyLR: 0.3753  Epoch: 20  Global Step: 114610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:14,285-Speed 3890.46 samples/sec  Loss 5.7263  LearningRate 0.0075  ProxyLR: 0.3751  Epoch: 20  Global Step: 114620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:16,918-Speed 3889.92 samples/sec  Loss 5.6786  LearningRate 0.0075  ProxyLR: 0.3748  Epoch: 20  Global Step: 114630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:19,551-Speed 3890.37 samples/sec  Loss 5.6671  LearningRate 0.0075  ProxyLR: 0.3745  Epoch: 20  Global Step: 114640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:22,169-Speed 3911.69 samples/sec  Loss 5.7094  LearningRate 0.0075  ProxyLR: 0.3743  Epoch: 20  Global Step: 114650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:24,801-Speed 3890.98 samples/sec  Loss 5.7102  LearningRate 0.0075  ProxyLR: 0.3740  Epoch: 20  Global Step: 114660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:27,420-Speed 3910.89 samples/sec  Loss 5.6871  LearningRate 0.0075  ProxyLR: 0.3737  Epoch: 20  Global Step: 114670   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:30,053-Speed 3890.05 samples/sec  Loss 5.7250  LearningRate 0.0075  ProxyLR: 0.3734  Epoch: 20  Global Step: 114680   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:32,684-Speed 3892.91 samples/sec  Loss 5.7425  LearningRate 0.0075  ProxyLR: 0.3732  Epoch: 20  Global Step: 114690   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:35,317-Speed 3890.78 samples/sec  Loss 5.6763  LearningRate 0.0075  ProxyLR: 0.3729  Epoch: 20  Global Step: 114700   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:37,948-Speed 3893.09 samples/sec  Loss 5.7685  LearningRate 0.0075  ProxyLR: 0.3726  Epoch: 20  Global Step: 114710   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:40,579-Speed 3892.33 samples/sec  Loss 5.6551  LearningRate 0.0074  ProxyLR: 0.3724  Epoch: 20  Global Step: 114720   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:43,209-Speed 3894.54 samples/sec  Loss 5.6562  LearningRate 0.0074  ProxyLR: 0.3721  Epoch: 20  Global Step: 114730   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:45,841-Speed 3891.49 samples/sec  Loss 5.7570  LearningRate 0.0074  ProxyLR: 0.3718  Epoch: 20  Global Step: 114740   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:48,474-Speed 3889.57 samples/sec  Loss 5.6580  LearningRate 0.0074  ProxyLR: 0.3715  Epoch: 20  Global Step: 114750   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:51,110-Speed 3887.15 samples/sec  Loss 5.6087  LearningRate 0.0074  ProxyLR: 0.3713  Epoch: 20  Global Step: 114760   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:30:53,748-Speed 3882.57 samples/sec  Loss 5.7149  LearningRate 0.0074  ProxyLR: 0.3710  Epoch: 20  Global Step: 114770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:56,381-Speed 3888.77 samples/sec  Loss 5.6835  LearningRate 0.0074  ProxyLR: 0.3707  Epoch: 20  Global Step: 114780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:30:59,017-Speed 3886.73 samples/sec  Loss 5.6511  LearningRate 0.0074  ProxyLR: 0.3705  Epoch: 20  Global Step: 114790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:01,650-Speed 3889.07 samples/sec  Loss 5.7722  LearningRate 0.0074  ProxyLR: 0.3702  Epoch: 20  Global Step: 114800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:04,287-Speed 3885.28 samples/sec  Loss 5.7913  LearningRate 0.0074  ProxyLR: 0.3699  Epoch: 20  Global Step: 114810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:06,921-Speed 3887.67 samples/sec  Loss 5.6130  LearningRate 0.0074  ProxyLR: 0.3696  Epoch: 20  Global Step: 114820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:09,556-Speed 3887.01 samples/sec  Loss 5.6977  LearningRate 0.0074  ProxyLR: 0.3694  Epoch: 20  Global Step: 114830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:12,187-Speed 3892.62 samples/sec  Loss 5.6781  LearningRate 0.0074  ProxyLR: 0.3691  Epoch: 20  Global Step: 114840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:14,824-Speed 3884.57 samples/sec  Loss 5.6627  LearningRate 0.0074  ProxyLR: 0.3688  Epoch: 20  Global Step: 114850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:17,459-Speed 3887.04 samples/sec  Loss 5.7004  LearningRate 0.0074  ProxyLR: 0.3686  Epoch: 20  Global Step: 114860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:20,077-Speed 3912.21 samples/sec  Loss 5.7235  LearningRate 0.0074  ProxyLR: 0.3683  Epoch: 20  Global Step: 114870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:22,695-Speed 3912.26 samples/sec  Loss 5.6827  LearningRate 0.0074  ProxyLR: 0.3680  Epoch: 20  Global Step: 114880   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:25,328-Speed 3890.56 samples/sec  Loss 5.7226  LearningRate 0.0074  ProxyLR: 0.3678  Epoch: 20  Global Step: 114890   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:27,960-Speed 3890.68 samples/sec  Loss 5.6726  LearningRate 0.0073  ProxyLR: 0.3675  Epoch: 20  Global Step: 114900   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:30,595-Speed 3887.87 samples/sec  Loss 5.7506  LearningRate 0.0073  ProxyLR: 0.3672  Epoch: 20  Global Step: 114910   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:33,227-Speed 3890.81 samples/sec  Loss 5.6508  LearningRate 0.0073  ProxyLR: 0.3669  Epoch: 20  Global Step: 114920   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:35,860-Speed 3890.84 samples/sec  Loss 5.7414  LearningRate 0.0073  ProxyLR: 0.3667  Epoch: 20  Global Step: 114930   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:38,491-Speed 3892.30 samples/sec  Loss 5.6919  LearningRate 0.0073  ProxyLR: 0.3664  Epoch: 20  Global Step: 114940   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:41,123-Speed 3891.90 samples/sec  Loss 5.6330  LearningRate 0.0073  ProxyLR: 0.3661  Epoch: 20  Global Step: 114950   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:43,756-Speed 3890.87 samples/sec  Loss 5.6345  LearningRate 0.0073  ProxyLR: 0.3659  Epoch: 20  Global Step: 114960   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:46,387-Speed 3892.22 samples/sec  Loss 5.6484  LearningRate 0.0073  ProxyLR: 0.3656  Epoch: 20  Global Step: 114970   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:31:49,018-Speed 3892.97 samples/sec  Loss 5.5188  LearningRate 0.0073  ProxyLR: 0.3653  Epoch: 20  Global Step: 114980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:51,652-Speed 3888.81 samples/sec  Loss 5.5551  LearningRate 0.0073  ProxyLR: 0.3651  Epoch: 20  Global Step: 114990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:54,283-Speed 3893.36 samples/sec  Loss 5.6856  LearningRate 0.0073  ProxyLR: 0.3648  Epoch: 20  Global Step: 115000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:56,914-Speed 3891.96 samples/sec  Loss 5.7031  LearningRate 0.0073  ProxyLR: 0.3645  Epoch: 20  Global Step: 115010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:31:59,546-Speed 3891.68 samples/sec  Loss 5.7221  LearningRate 0.0073  ProxyLR: 0.3643  Epoch: 20  Global Step: 115020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:02,180-Speed 3888.35 samples/sec  Loss 5.5543  LearningRate 0.0073  ProxyLR: 0.3640  Epoch: 20  Global Step: 115030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:04,812-Speed 3892.49 samples/sec  Loss 5.6418  LearningRate 0.0073  ProxyLR: 0.3637  Epoch: 20  Global Step: 115040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:07,443-Speed 3891.83 samples/sec  Loss 5.5404  LearningRate 0.0073  ProxyLR: 0.3635  Epoch: 20  Global Step: 115050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:10,074-Speed 3894.04 samples/sec  Loss 5.5543  LearningRate 0.0073  ProxyLR: 0.3632  Epoch: 20  Global Step: 115060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:12,705-Speed 3893.02 samples/sec  Loss 5.5458  LearningRate 0.0073  ProxyLR: 0.3629  Epoch: 20  Global Step: 115070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:15,325-Speed 3909.26 samples/sec  Loss 5.6141  LearningRate 0.0073  ProxyLR: 0.3626  Epoch: 20  Global Step: 115080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:17,955-Speed 3894.36 samples/sec  Loss 5.7039  LearningRate 0.0072  ProxyLR: 0.3624  Epoch: 20  Global Step: 115090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:20,587-Speed 3891.38 samples/sec  Loss 5.6494  LearningRate 0.0072  ProxyLR: 0.3621  Epoch: 20  Global Step: 115100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:23,204-Speed 3913.27 samples/sec  Loss 5.6298  LearningRate 0.0072  ProxyLR: 0.3618  Epoch: 20  Global Step: 115110   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:25,836-Speed 3892.01 samples/sec  Loss 5.6019  LearningRate 0.0072  ProxyLR: 0.3616  Epoch: 20  Global Step: 115120   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:28,467-Speed 3892.35 samples/sec  Loss 5.5325  LearningRate 0.0072  ProxyLR: 0.3613  Epoch: 20  Global Step: 115130   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:31,098-Speed 3892.67 samples/sec  Loss 5.6200  LearningRate 0.0072  ProxyLR: 0.3610  Epoch: 20  Global Step: 115140   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:33,732-Speed 3890.11 samples/sec  Loss 5.5980  LearningRate 0.0072  ProxyLR: 0.3608  Epoch: 20  Global Step: 115150   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:36,364-Speed 3891.63 samples/sec  Loss 5.5731  LearningRate 0.0072  ProxyLR: 0.3605  Epoch: 20  Global Step: 115160   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:38,994-Speed 3893.13 samples/sec  Loss 5.5706  LearningRate 0.0072  ProxyLR: 0.3602  Epoch: 20  Global Step: 115170   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:41,624-Speed 3895.20 samples/sec  Loss 5.6250  LearningRate 0.0072  ProxyLR: 0.3600  Epoch: 20  Global Step: 115180   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:44,255-Speed 3892.81 samples/sec  Loss 5.6296  LearningRate 0.0072  ProxyLR: 0.3597  Epoch: 20  Global Step: 115190   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:46,887-Speed 3891.66 samples/sec  Loss 5.6247  LearningRate 0.0072  ProxyLR: 0.3594  Epoch: 20  Global Step: 115200   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:32:49,521-Speed 3887.89 samples/sec  Loss 5.6089  LearningRate 0.0072  ProxyLR: 0.3592  Epoch: 20  Global Step: 115210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:52,153-Speed 3892.41 samples/sec  Loss 5.6143  LearningRate 0.0072  ProxyLR: 0.3589  Epoch: 20  Global Step: 115220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:54,786-Speed 3890.41 samples/sec  Loss 5.5168  LearningRate 0.0072  ProxyLR: 0.3586  Epoch: 20  Global Step: 115230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:32:57,403-Speed 3913.21 samples/sec  Loss 5.5832  LearningRate 0.0072  ProxyLR: 0.3584  Epoch: 20  Global Step: 115240   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:00,033-Speed 3894.54 samples/sec  Loss 5.5846  LearningRate 0.0072  ProxyLR: 0.3581  Epoch: 20  Global Step: 115250   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:02,664-Speed 3893.39 samples/sec  Loss 5.5351  LearningRate 0.0072  ProxyLR: 0.3578  Epoch: 20  Global Step: 115260   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:05,296-Speed 3890.38 samples/sec  Loss 5.6160  LearningRate 0.0072  ProxyLR: 0.3576  Epoch: 20  Global Step: 115270   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:07,930-Speed 3888.40 samples/sec  Loss 5.6660  LearningRate 0.0071  ProxyLR: 0.3573  Epoch: 20  Global Step: 115280   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:10,564-Speed 3888.38 samples/sec  Loss 5.5603  LearningRate 0.0071  ProxyLR: 0.3570  Epoch: 20  Global Step: 115290   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:13,435-Speed 3568.23 samples/sec  Loss 5.5766  LearningRate 0.0071  ProxyLR: 0.3568  Epoch: 20  Global Step: 115300   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:16,070-Speed 3886.41 samples/sec  Loss 5.5318  LearningRate 0.0071  ProxyLR: 0.3565  Epoch: 20  Global Step: 115310   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:18,705-Speed 3887.80 samples/sec  Loss 5.6151  LearningRate 0.0071  ProxyLR: 0.3562  Epoch: 20  Global Step: 115320   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:21,339-Speed 3888.19 samples/sec  Loss 5.5954  LearningRate 0.0071  ProxyLR: 0.3560  Epoch: 20  Global Step: 115330   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:23,977-Speed 3882.14 samples/sec  Loss 5.5070  LearningRate 0.0071  ProxyLR: 0.3557  Epoch: 20  Global Step: 115340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:26,615-Speed 3882.87 samples/sec  Loss 5.4316  LearningRate 0.0071  ProxyLR: 0.3554  Epoch: 20  Global Step: 115350   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:29,252-Speed 3884.88 samples/sec  Loss 5.5838  LearningRate 0.0071  ProxyLR: 0.3552  Epoch: 20  Global Step: 115360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:31,889-Speed 3884.28 samples/sec  Loss 5.6062  LearningRate 0.0071  ProxyLR: 0.3549  Epoch: 20  Global Step: 115370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:34,527-Speed 3882.57 samples/sec  Loss 5.4575  LearningRate 0.0071  ProxyLR: 0.3547  Epoch: 20  Global Step: 115380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:37,165-Speed 3882.41 samples/sec  Loss 5.5530  LearningRate 0.0071  ProxyLR: 0.3544  Epoch: 20  Global Step: 115390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:39,803-Speed 3882.96 samples/sec  Loss 5.5302  LearningRate 0.0071  ProxyLR: 0.3541  Epoch: 20  Global Step: 115400   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:33:42,427-Speed 3902.45 samples/sec  Loss 5.5565  LearningRate 0.0071  ProxyLR: 0.3539  Epoch: 20  Global Step: 115410   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:45,064-Speed 3883.68 samples/sec  Loss 5.5092  LearningRate 0.0071  ProxyLR: 0.3536  Epoch: 20  Global Step: 115420   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:47,704-Speed 3880.02 samples/sec  Loss 5.5638  LearningRate 0.0071  ProxyLR: 0.3533  Epoch: 20  Global Step: 115430   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:50,344-Speed 3880.51 samples/sec  Loss 5.4718  LearningRate 0.0071  ProxyLR: 0.3531  Epoch: 20  Global Step: 115440   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:52,982-Speed 3881.86 samples/sec  Loss 5.4881  LearningRate 0.0071  ProxyLR: 0.3528  Epoch: 20  Global Step: 115450   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:55,620-Speed 3882.81 samples/sec  Loss 5.6108  LearningRate 0.0071  ProxyLR: 0.3525  Epoch: 20  Global Step: 115460   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:33:58,259-Speed 3880.58 samples/sec  Loss 5.5946  LearningRate 0.0070  ProxyLR: 0.3523  Epoch: 20  Global Step: 115470   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:34:00,899-Speed 3881.01 samples/sec  Loss 5.4438  LearningRate 0.0070  ProxyLR: 0.3520  Epoch: 20  Global Step: 115480   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:34:03,535-Speed 3884.45 samples/sec  Loss 5.4893  LearningRate 0.0070  ProxyLR: 0.3517  Epoch: 20  Global Step: 115490   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:34:06,171-Speed 3885.82 samples/sec  Loss 5.5342  LearningRate 0.0070  ProxyLR: 0.3515  Epoch: 20  Global Step: 115500   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:34:08,808-Speed 3884.71 samples/sec  Loss 5.5795  LearningRate 0.0070  ProxyLR: 0.3512  Epoch: 20  Global Step: 115510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:11,445-Speed 3883.99 samples/sec  Loss 5.5593  LearningRate 0.0070  ProxyLR: 0.3510  Epoch: 20  Global Step: 115520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:14,078-Speed 3889.82 samples/sec  Loss 5.4629  LearningRate 0.0070  ProxyLR: 0.3507  Epoch: 20  Global Step: 115530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:16,712-Speed 3889.35 samples/sec  Loss 5.4823  LearningRate 0.0070  ProxyLR: 0.3504  Epoch: 20  Global Step: 115540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:19,342-Speed 3893.35 samples/sec  Loss 5.5053  LearningRate 0.0070  ProxyLR: 0.3502  Epoch: 20  Global Step: 115550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:21,976-Speed 3888.91 samples/sec  Loss 5.5285  LearningRate 0.0070  ProxyLR: 0.3499  Epoch: 20  Global Step: 115560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:24,606-Speed 3894.99 samples/sec  Loss 5.5166  LearningRate 0.0070  ProxyLR: 0.3496  Epoch: 20  Global Step: 115570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:27,237-Speed 3891.87 samples/sec  Loss 5.4643  LearningRate 0.0070  ProxyLR: 0.3494  Epoch: 20  Global Step: 115580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:29,873-Speed 3885.73 samples/sec  Loss 5.5070  LearningRate 0.0070  ProxyLR: 0.3491  Epoch: 20  Global Step: 115590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:32,507-Speed 3888.27 samples/sec  Loss 5.5134  LearningRate 0.0070  ProxyLR: 0.3488  Epoch: 20  Global Step: 115600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:35,127-Speed 3910.49 samples/sec  Loss 5.4426  LearningRate 0.0070  ProxyLR: 0.3486  Epoch: 20  Global Step: 115610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:37,759-Speed 3890.25 samples/sec  Loss 5.4630  LearningRate 0.0070  ProxyLR: 0.3483  Epoch: 20  Global Step: 115620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:40,394-Speed 3887.52 samples/sec  Loss 5.4899  LearningRate 0.0070  ProxyLR: 0.3481  Epoch: 20  Global Step: 115630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:43,025-Speed 3893.17 samples/sec  Loss 5.3882  LearningRate 0.0070  ProxyLR: 0.3478  Epoch: 20  Global Step: 115640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:45,656-Speed 3892.95 samples/sec  Loss 5.5133  LearningRate 0.0070  ProxyLR: 0.3475  Epoch: 20  Global Step: 115650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:48,290-Speed 3888.97 samples/sec  Loss 5.4436  LearningRate 0.0069  ProxyLR: 0.3473  Epoch: 20  Global Step: 115660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:50,921-Speed 3892.25 samples/sec  Loss 5.4930  LearningRate 0.0069  ProxyLR: 0.3470  Epoch: 20  Global Step: 115670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:53,553-Speed 3891.58 samples/sec  Loss 5.5100  LearningRate 0.0069  ProxyLR: 0.3467  Epoch: 20  Global Step: 115680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:56,184-Speed 3892.46 samples/sec  Loss 5.4113  LearningRate 0.0069  ProxyLR: 0.3465  Epoch: 20  Global Step: 115690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:34:58,818-Speed 3888.80 samples/sec  Loss 5.5346  LearningRate 0.0069  ProxyLR: 0.3462  Epoch: 20  Global Step: 115700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:01,438-Speed 3910.40 samples/sec  Loss 5.4615  LearningRate 0.0069  ProxyLR: 0.3460  Epoch: 20  Global Step: 115710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:04,070-Speed 3891.26 samples/sec  Loss 5.4514  LearningRate 0.0069  ProxyLR: 0.3457  Epoch: 20  Global Step: 115720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:06,702-Speed 3891.47 samples/sec  Loss 5.4879  LearningRate 0.0069  ProxyLR: 0.3454  Epoch: 20  Global Step: 115730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:09,337-Speed 3887.61 samples/sec  Loss 5.4071  LearningRate 0.0069  ProxyLR: 0.3452  Epoch: 20  Global Step: 115740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:11,967-Speed 3893.59 samples/sec  Loss 5.5000  LearningRate 0.0069  ProxyLR: 0.3449  Epoch: 20  Global Step: 115750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:14,601-Speed 3889.69 samples/sec  Loss 5.4576  LearningRate 0.0069  ProxyLR: 0.3447  Epoch: 20  Global Step: 115760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:17,232-Speed 3891.70 samples/sec  Loss 5.4742  LearningRate 0.0069  ProxyLR: 0.3444  Epoch: 20  Global Step: 115770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:19,864-Speed 3892.43 samples/sec  Loss 5.4688  LearningRate 0.0069  ProxyLR: 0.3441  Epoch: 20  Global Step: 115780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:22,496-Speed 3890.62 samples/sec  Loss 5.3759  LearningRate 0.0069  ProxyLR: 0.3439  Epoch: 20  Global Step: 115790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:25,129-Speed 3891.30 samples/sec  Loss 5.3887  LearningRate 0.0069  ProxyLR: 0.3436  Epoch: 20  Global Step: 115800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:27,748-Speed 3910.64 samples/sec  Loss 5.4612  LearningRate 0.0069  ProxyLR: 0.3434  Epoch: 20  Global Step: 115810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:30,380-Speed 3890.58 samples/sec  Loss 5.4895  LearningRate 0.0069  ProxyLR: 0.3431  Epoch: 20  Global Step: 115820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:33,012-Speed 3892.71 samples/sec  Loss 5.5280  LearningRate 0.0069  ProxyLR: 0.3428  Epoch: 20  Global Step: 115830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:35:35,631-Speed 3909.48 samples/sec  Loss 5.4007  LearningRate 0.0069  ProxyLR: 0.3426  Epoch: 20  Global Step: 115840   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:38,264-Speed 3890.43 samples/sec  Loss 5.4903  LearningRate 0.0068  ProxyLR: 0.3423  Epoch: 20  Global Step: 115850   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:40,899-Speed 3887.50 samples/sec  Loss 5.3938  LearningRate 0.0068  ProxyLR: 0.3420  Epoch: 20  Global Step: 115860   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:43,534-Speed 3886.80 samples/sec  Loss 5.5175  LearningRate 0.0068  ProxyLR: 0.3418  Epoch: 20  Global Step: 115870   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:46,169-Speed 3887.67 samples/sec  Loss 5.4691  LearningRate 0.0068  ProxyLR: 0.3415  Epoch: 20  Global Step: 115880   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:48,804-Speed 3886.49 samples/sec  Loss 5.4822  LearningRate 0.0068  ProxyLR: 0.3413  Epoch: 20  Global Step: 115890   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:51,438-Speed 3888.58 samples/sec  Loss 5.3864  LearningRate 0.0068  ProxyLR: 0.3410  Epoch: 20  Global Step: 115900   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:54,074-Speed 3885.77 samples/sec  Loss 5.4838  LearningRate 0.0068  ProxyLR: 0.3407  Epoch: 20  Global Step: 115910   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:56,949-Speed 3561.62 samples/sec  Loss 5.4231  LearningRate 0.0068  ProxyLR: 0.3405  Epoch: 20  Global Step: 115920   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:35:59,587-Speed 3883.76 samples/sec  Loss 5.3830  LearningRate 0.0068  ProxyLR: 0.3402  Epoch: 20  Global Step: 115930   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:36:02,223-Speed 3886.07 samples/sec  Loss 5.3691  LearningRate 0.0068  ProxyLR: 0.3400  Epoch: 20  Global Step: 115940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:36:04,859-Speed 3884.80 samples/sec  Loss 5.4263  LearningRate 0.0068  ProxyLR: 0.3397  Epoch: 20  Global Step: 115950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:36:07,497-Speed 3885.05 samples/sec  Loss 5.3405  LearningRate 0.0068  ProxyLR: 0.3395  Epoch: 20  Global Step: 115960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:36:10,137-Speed 3878.89 samples/sec  Loss 5.2717  LearningRate 0.0068  ProxyLR: 0.3392  Epoch: 20  Global Step: 115970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:36:12,772-Speed 3887.64 samples/sec  Loss 5.4730  LearningRate 0.0068  ProxyLR: 0.3389  Epoch: 20  Global Step: 115980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:36:15,409-Speed 3883.58 samples/sec  Loss 5.3158  LearningRate 0.0068  ProxyLR: 0.3387  Epoch: 20  Global Step: 115990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:36:18,048-Speed 3881.71 samples/sec  Loss 5.4527  LearningRate 0.0068  ProxyLR: 0.3384  Epoch: 20  Global Step: 116000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:37:07,633-[lfw][116000]XNorm: 22.030650
Training: 2023-05-04 23:37:07,633-[lfw][116000]Accuracy-Flip: 0.99783+-0.00236
Training: 2023-05-04 23:37:07,634-[lfw][116000]Accuracy-Highest: 0.99783
Training: 2023-05-04 23:37:07,634-[lfw][116000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-04 23:37:07,634-[lfw][116000]Highest TPR@FPR: 0.99667
Training: 2023-05-04 23:38:04,647-[cfp_fp][116000]XNorm: 21.757005
Training: 2023-05-04 23:38:04,648-[cfp_fp][116000]Accuracy-Flip: 0.98429+-0.00737
Training: 2023-05-04 23:38:04,648-[cfp_fp][116000]Accuracy-Highest: 0.98429
Training: 2023-05-04 23:38:04,648-[cfp_fp][116000]TPR@1stNon-Zero-FPR of 0.00029: 0.86029
Training: 2023-05-04 23:38:04,648-[cfp_fp][116000]Highest TPR@FPR: 0.87086
Training: 2023-05-04 23:38:54,224-[agedb_30][116000]XNorm: 22.354704
Training: 2023-05-04 23:38:54,224-[agedb_30][116000]Accuracy-Flip: 0.97367+-0.00714
Training: 2023-05-04 23:38:54,225-[agedb_30][116000]Accuracy-Highest: 0.97367
Training: 2023-05-04 23:38:54,225-[agedb_30][116000]TPR@1stNon-Zero-FPR of 0.00033: 0.88033
Training: 2023-05-04 23:38:54,225-[agedb_30][116000]Highest TPR@FPR: 0.88033
Training: 2023-05-04 23:39:45,167-[calfw][116000]XNorm: 22.225794
Training: 2023-05-04 23:39:45,167-[calfw][116000]Accuracy-Flip: 0.95567+-0.01254
Training: 2023-05-04 23:39:45,167-[calfw][116000]Accuracy-Highest: 0.95833
Training: 2023-05-04 23:39:45,168-[calfw][116000]TPR@1stNon-Zero-FPR of 0.00033: 0.82433
Training: 2023-05-04 23:39:45,168-[calfw][116000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 23:40:36,127-[cplfw][116000]XNorm: 21.098048
Training: 2023-05-04 23:40:36,127-[cplfw][116000]Accuracy-Flip: 0.92767+-0.01218
Training: 2023-05-04 23:40:36,127-[cplfw][116000]Accuracy-Highest: 0.92900
Training: 2023-05-04 23:40:36,128-[cplfw][116000]TPR@1stNon-Zero-FPR of 0.00033: 0.02333
Training: 2023-05-04 23:40:36,128-[cplfw][116000]Highest TPR@FPR: 0.07633
Training: 2023-05-04 23:40:39,458-Speed 39.17 samples/sec  Loss 5.3935  LearningRate 0.0068  ProxyLR: 0.3382  Epoch: 20  Global Step: 116010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:40:42,080-Speed 3905.89 samples/sec  Loss 5.3889  LearningRate 0.0068  ProxyLR: 0.3379  Epoch: 20  Global Step: 116020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:40:44,704-Speed 3903.08 samples/sec  Loss 5.4949  LearningRate 0.0068  ProxyLR: 0.3376  Epoch: 20  Global Step: 116030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:40:47,315-Speed 3924.47 samples/sec  Loss 5.3945  LearningRate 0.0067  ProxyLR: 0.3374  Epoch: 20  Global Step: 116040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:40:49,940-Speed 3901.20 samples/sec  Loss 5.3396  LearningRate 0.0067  ProxyLR: 0.3371  Epoch: 20  Global Step: 116050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:40:52,565-Speed 3901.67 samples/sec  Loss 5.3120  LearningRate 0.0067  ProxyLR: 0.3369  Epoch: 20  Global Step: 116060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:40:55,179-Speed 3919.56 samples/sec  Loss 5.3933  LearningRate 0.0067  ProxyLR: 0.3366  Epoch: 20  Global Step: 116070   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:40:57,803-Speed 3903.24 samples/sec  Loss 5.4354  LearningRate 0.0067  ProxyLR: 0.3363  Epoch: 20  Global Step: 116080   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:00,434-Speed 3893.37 samples/sec  Loss 5.4376  LearningRate 0.0067  ProxyLR: 0.3361  Epoch: 20  Global Step: 116090   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:03,065-Speed 3893.04 samples/sec  Loss 5.4226  LearningRate 0.0067  ProxyLR: 0.3358  Epoch: 20  Global Step: 116100   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:05,696-Speed 3892.35 samples/sec  Loss 5.3928  LearningRate 0.0067  ProxyLR: 0.3356  Epoch: 20  Global Step: 116110   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:08,330-Speed 3889.43 samples/sec  Loss 5.4366  LearningRate 0.0067  ProxyLR: 0.3353  Epoch: 20  Global Step: 116120   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:10,962-Speed 3891.42 samples/sec  Loss 5.4734  LearningRate 0.0067  ProxyLR: 0.3351  Epoch: 20  Global Step: 116130   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:13,592-Speed 3894.09 samples/sec  Loss 5.3023  LearningRate 0.0067  ProxyLR: 0.3348  Epoch: 20  Global Step: 116140   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:16,227-Speed 3887.68 samples/sec  Loss 5.4042  LearningRate 0.0067  ProxyLR: 0.3345  Epoch: 20  Global Step: 116150   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:18,864-Speed 3884.05 samples/sec  Loss 5.4575  LearningRate 0.0067  ProxyLR: 0.3343  Epoch: 20  Global Step: 116160   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:21,500-Speed 3884.89 samples/sec  Loss 5.4138  LearningRate 0.0067  ProxyLR: 0.3340  Epoch: 20  Global Step: 116170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:24,139-Speed 3882.00 samples/sec  Loss 5.3850  LearningRate 0.0067  ProxyLR: 0.3338  Epoch: 20  Global Step: 116180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:26,778-Speed 3881.09 samples/sec  Loss 5.3786  LearningRate 0.0067  ProxyLR: 0.3335  Epoch: 20  Global Step: 116190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:29,416-Speed 3881.88 samples/sec  Loss 5.4095  LearningRate 0.0067  ProxyLR: 0.3333  Epoch: 20  Global Step: 116200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:32,058-Speed 3877.83 samples/sec  Loss 5.3228  LearningRate 0.0067  ProxyLR: 0.3330  Epoch: 20  Global Step: 116210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:34,695-Speed 3883.15 samples/sec  Loss 5.3354  LearningRate 0.0067  ProxyLR: 0.3327  Epoch: 20  Global Step: 116220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:37,332-Speed 3884.35 samples/sec  Loss 5.3611  LearningRate 0.0066  ProxyLR: 0.3325  Epoch: 20  Global Step: 116230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:39,971-Speed 3881.79 samples/sec  Loss 5.3645  LearningRate 0.0066  ProxyLR: 0.3322  Epoch: 20  Global Step: 116240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:42,607-Speed 3886.01 samples/sec  Loss 5.3200  LearningRate 0.0066  ProxyLR: 0.3320  Epoch: 20  Global Step: 116250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:41:45,227-Speed 3908.06 samples/sec  Loss 5.3400  LearningRate 0.0066  ProxyLR: 0.3317  Epoch: 20  Global Step: 116260   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:47,867-Speed 3879.87 samples/sec  Loss 5.3877  LearningRate 0.0066  ProxyLR: 0.3315  Epoch: 20  Global Step: 116270   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:50,502-Speed 3887.22 samples/sec  Loss 5.4031  LearningRate 0.0066  ProxyLR: 0.3312  Epoch: 20  Global Step: 116280   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:53,138-Speed 3886.49 samples/sec  Loss 5.4164  LearningRate 0.0066  ProxyLR: 0.3310  Epoch: 20  Global Step: 116290   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:55,769-Speed 3891.88 samples/sec  Loss 5.4087  LearningRate 0.0066  ProxyLR: 0.3307  Epoch: 20  Global Step: 116300   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:41:58,403-Speed 3888.85 samples/sec  Loss 5.3225  LearningRate 0.0066  ProxyLR: 0.3304  Epoch: 20  Global Step: 116310   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:01,038-Speed 3887.26 samples/sec  Loss 5.2339  LearningRate 0.0066  ProxyLR: 0.3302  Epoch: 20  Global Step: 116320   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:03,673-Speed 3886.46 samples/sec  Loss 5.2894  LearningRate 0.0066  ProxyLR: 0.3299  Epoch: 20  Global Step: 116330   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:06,307-Speed 3888.35 samples/sec  Loss 5.2645  LearningRate 0.0066  ProxyLR: 0.3297  Epoch: 20  Global Step: 116340   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:08,941-Speed 3888.97 samples/sec  Loss 5.2889  LearningRate 0.0066  ProxyLR: 0.3294  Epoch: 20  Global Step: 116350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:11,572-Speed 3892.67 samples/sec  Loss 5.3143  LearningRate 0.0066  ProxyLR: 0.3292  Epoch: 20  Global Step: 116360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:42:14,193-Speed 3908.50 samples/sec  Loss 5.3197  LearningRate 0.0066  ProxyLR: 0.3289  Epoch: 20  Global Step: 116370   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:16,827-Speed 3888.80 samples/sec  Loss 5.3906  LearningRate 0.0066  ProxyLR: 0.3287  Epoch: 20  Global Step: 116380   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:19,460-Speed 3890.29 samples/sec  Loss 5.3615  LearningRate 0.0066  ProxyLR: 0.3284  Epoch: 20  Global Step: 116390   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:22,095-Speed 3886.42 samples/sec  Loss 5.3222  LearningRate 0.0066  ProxyLR: 0.3281  Epoch: 20  Global Step: 116400   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:24,731-Speed 3885.93 samples/sec  Loss 5.3148  LearningRate 0.0066  ProxyLR: 0.3279  Epoch: 20  Global Step: 116410   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:27,368-Speed 3883.50 samples/sec  Loss 5.3097  LearningRate 0.0066  ProxyLR: 0.3276  Epoch: 20  Global Step: 116420   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:30,003-Speed 3887.70 samples/sec  Loss 5.3513  LearningRate 0.0065  ProxyLR: 0.3274  Epoch: 20  Global Step: 116430   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:32,636-Speed 3889.61 samples/sec  Loss 5.2476  LearningRate 0.0065  ProxyLR: 0.3271  Epoch: 20  Global Step: 116440   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:35,272-Speed 3885.01 samples/sec  Loss 5.3038  LearningRate 0.0065  ProxyLR: 0.3269  Epoch: 20  Global Step: 116450   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:37,908-Speed 3886.31 samples/sec  Loss 5.3500  LearningRate 0.0065  ProxyLR: 0.3266  Epoch: 20  Global Step: 116460   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:40,544-Speed 3884.86 samples/sec  Loss 5.3326  LearningRate 0.0065  ProxyLR: 0.3264  Epoch: 20  Global Step: 116470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:42:43,180-Speed 3887.12 samples/sec  Loss 5.4078  LearningRate 0.0065  ProxyLR: 0.3261  Epoch: 20  Global Step: 116480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:42:45,814-Speed 3887.70 samples/sec  Loss 5.3980  LearningRate 0.0065  ProxyLR: 0.3259  Epoch: 20  Global Step: 116490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:42:48,450-Speed 3885.18 samples/sec  Loss 5.2614  LearningRate 0.0065  ProxyLR: 0.3256  Epoch: 20  Global Step: 116500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:42:51,087-Speed 3884.58 samples/sec  Loss 5.3007  LearningRate 0.0065  ProxyLR: 0.3253  Epoch: 20  Global Step: 116510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:42:53,709-Speed 3906.57 samples/sec  Loss 5.2117  LearningRate 0.0065  ProxyLR: 0.3251  Epoch: 20  Global Step: 116520   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:56,342-Speed 3889.61 samples/sec  Loss 5.1761  LearningRate 0.0065  ProxyLR: 0.3248  Epoch: 20  Global Step: 116530   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:42:58,976-Speed 3888.54 samples/sec  Loss 5.2611  LearningRate 0.0065  ProxyLR: 0.3246  Epoch: 20  Global Step: 116540   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:01,613-Speed 3884.83 samples/sec  Loss 5.3134  LearningRate 0.0065  ProxyLR: 0.3243  Epoch: 20  Global Step: 116550   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:04,249-Speed 3885.13 samples/sec  Loss 5.2935  LearningRate 0.0065  ProxyLR: 0.3241  Epoch: 20  Global Step: 116560   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:06,883-Speed 3888.75 samples/sec  Loss 5.2465  LearningRate 0.0065  ProxyLR: 0.3238  Epoch: 20  Global Step: 116570   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:09,516-Speed 3890.27 samples/sec  Loss 5.3007  LearningRate 0.0065  ProxyLR: 0.3236  Epoch: 20  Global Step: 116580   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:12,150-Speed 3888.87 samples/sec  Loss 5.2164  LearningRate 0.0065  ProxyLR: 0.3233  Epoch: 20  Global Step: 116590   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:14,784-Speed 3887.99 samples/sec  Loss 5.2592  LearningRate 0.0065  ProxyLR: 0.3231  Epoch: 20  Global Step: 116600   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:17,417-Speed 3889.59 samples/sec  Loss 5.3450  LearningRate 0.0065  ProxyLR: 0.3228  Epoch: 20  Global Step: 116610   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:20,050-Speed 3890.66 samples/sec  Loss 5.3825  LearningRate 0.0065  ProxyLR: 0.3226  Epoch: 20  Global Step: 116620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:22,690-Speed 3879.40 samples/sec  Loss 5.2429  LearningRate 0.0064  ProxyLR: 0.3223  Epoch: 20  Global Step: 116630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:25,329-Speed 3880.87 samples/sec  Loss 5.2052  LearningRate 0.0064  ProxyLR: 0.3221  Epoch: 20  Global Step: 116640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:27,968-Speed 3881.32 samples/sec  Loss 5.2912  LearningRate 0.0064  ProxyLR: 0.3218  Epoch: 20  Global Step: 116650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:30,608-Speed 3879.99 samples/sec  Loss 5.3568  LearningRate 0.0064  ProxyLR: 0.3215  Epoch: 20  Global Step: 116660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:33,246-Speed 3882.97 samples/sec  Loss 5.2794  LearningRate 0.0064  ProxyLR: 0.3213  Epoch: 20  Global Step: 116670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:35,884-Speed 3882.28 samples/sec  Loss 5.2787  LearningRate 0.0064  ProxyLR: 0.3210  Epoch: 20  Global Step: 116680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:38,523-Speed 3881.70 samples/sec  Loss 5.2328  LearningRate 0.0064  ProxyLR: 0.3208  Epoch: 20  Global Step: 116690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:43:41,148-Speed 3901.13 samples/sec  Loss 5.3036  LearningRate 0.0064  ProxyLR: 0.3205  Epoch: 20  Global Step: 116700   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:43,786-Speed 3882.81 samples/sec  Loss 5.3161  LearningRate 0.0064  ProxyLR: 0.3203  Epoch: 20  Global Step: 116710   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:46,423-Speed 3883.89 samples/sec  Loss 5.2127  LearningRate 0.0064  ProxyLR: 0.3200  Epoch: 20  Global Step: 116720   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:49,057-Speed 3888.90 samples/sec  Loss 5.2973  LearningRate 0.0064  ProxyLR: 0.3198  Epoch: 20  Global Step: 116730   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:51,689-Speed 3891.70 samples/sec  Loss 5.2527  LearningRate 0.0064  ProxyLR: 0.3195  Epoch: 20  Global Step: 116740   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:54,324-Speed 3886.52 samples/sec  Loss 5.2121  LearningRate 0.0064  ProxyLR: 0.3193  Epoch: 20  Global Step: 116750   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:56,961-Speed 3884.15 samples/sec  Loss 5.1755  LearningRate 0.0064  ProxyLR: 0.3190  Epoch: 20  Global Step: 116760   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:43:59,597-Speed 3885.17 samples/sec  Loss 5.2246  LearningRate 0.0064  ProxyLR: 0.3188  Epoch: 20  Global Step: 116770   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:02,232-Speed 3887.36 samples/sec  Loss 5.2411  LearningRate 0.0064  ProxyLR: 0.3185  Epoch: 20  Global Step: 116780   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:04,865-Speed 3889.56 samples/sec  Loss 5.3090  LearningRate 0.0064  ProxyLR: 0.3183  Epoch: 20  Global Step: 116790   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:07,485-Speed 3910.30 samples/sec  Loss 5.3076  LearningRate 0.0064  ProxyLR: 0.3180  Epoch: 20  Global Step: 116800   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:10,119-Speed 3888.67 samples/sec  Loss 5.2096  LearningRate 0.0064  ProxyLR: 0.3178  Epoch: 20  Global Step: 116810   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:12,752-Speed 3889.62 samples/sec  Loss 5.2558  LearningRate 0.0064  ProxyLR: 0.3175  Epoch: 20  Global Step: 116820   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:15,387-Speed 3886.92 samples/sec  Loss 5.2651  LearningRate 0.0063  ProxyLR: 0.3173  Epoch: 20  Global Step: 116830   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:18,020-Speed 3890.88 samples/sec  Loss 5.2259  LearningRate 0.0063  ProxyLR: 0.3170  Epoch: 20  Global Step: 116840   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:20,651-Speed 3892.51 samples/sec  Loss 5.2102  LearningRate 0.0063  ProxyLR: 0.3168  Epoch: 20  Global Step: 116850   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:23,281-Speed 3894.93 samples/sec  Loss 5.1990  LearningRate 0.0063  ProxyLR: 0.3165  Epoch: 20  Global Step: 116860   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:25,912-Speed 3892.24 samples/sec  Loss 5.2177  LearningRate 0.0063  ProxyLR: 0.3163  Epoch: 20  Global Step: 116870   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:28,545-Speed 3889.95 samples/sec  Loss 5.2697  LearningRate 0.0063  ProxyLR: 0.3160  Epoch: 20  Global Step: 116880   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:31,182-Speed 3884.23 samples/sec  Loss 5.2213  LearningRate 0.0063  ProxyLR: 0.3158  Epoch: 20  Global Step: 116890   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:33,817-Speed 3886.53 samples/sec  Loss 5.2765  LearningRate 0.0063  ProxyLR: 0.3155  Epoch: 20  Global Step: 116900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:44:36,443-Speed 3901.24 samples/sec  Loss 5.2703  LearningRate 0.0063  ProxyLR: 0.3153  Epoch: 20  Global Step: 116910   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:39,081-Speed 3881.54 samples/sec  Loss 5.2168  LearningRate 0.0063  ProxyLR: 0.3150  Epoch: 20  Global Step: 116920   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:41,718-Speed 3885.21 samples/sec  Loss 5.3029  LearningRate 0.0063  ProxyLR: 0.3148  Epoch: 20  Global Step: 116930   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:44,355-Speed 3883.81 samples/sec  Loss 5.1790  LearningRate 0.0063  ProxyLR: 0.3145  Epoch: 20  Global Step: 116940   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:46,994-Speed 3881.52 samples/sec  Loss 5.2156  LearningRate 0.0063  ProxyLR: 0.3143  Epoch: 20  Global Step: 116950   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:49,633-Speed 3880.95 samples/sec  Loss 5.2078  LearningRate 0.0063  ProxyLR: 0.3140  Epoch: 20  Global Step: 116960   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:52,271-Speed 3883.34 samples/sec  Loss 5.2250  LearningRate 0.0063  ProxyLR: 0.3138  Epoch: 20  Global Step: 116970   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:54,906-Speed 3885.68 samples/sec  Loss 5.1851  LearningRate 0.0063  ProxyLR: 0.3135  Epoch: 20  Global Step: 116980   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:44:57,546-Speed 3880.28 samples/sec  Loss 5.1984  LearningRate 0.0063  ProxyLR: 0.3133  Epoch: 20  Global Step: 116990   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:00,186-Speed 3880.55 samples/sec  Loss 5.2055  LearningRate 0.0063  ProxyLR: 0.3130  Epoch: 20  Global Step: 117000   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:02,823-Speed 3883.65 samples/sec  Loss 5.2543  LearningRate 0.0063  ProxyLR: 0.3128  Epoch: 20  Global Step: 117010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:05,461-Speed 3882.34 samples/sec  Loss 5.1920  LearningRate 0.0063  ProxyLR: 0.3125  Epoch: 20  Global Step: 117020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:08,100-Speed 3881.66 samples/sec  Loss 5.2486  LearningRate 0.0062  ProxyLR: 0.3123  Epoch: 20  Global Step: 117030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:10,738-Speed 3882.76 samples/sec  Loss 5.2401  LearningRate 0.0062  ProxyLR: 0.3120  Epoch: 20  Global Step: 117040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:13,377-Speed 3881.16 samples/sec  Loss 5.2011  LearningRate 0.0062  ProxyLR: 0.3118  Epoch: 20  Global Step: 117050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:16,017-Speed 3878.89 samples/sec  Loss 5.2385  LearningRate 0.0062  ProxyLR: 0.3115  Epoch: 20  Global Step: 117060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:18,656-Speed 3881.84 samples/sec  Loss 5.1557  LearningRate 0.0062  ProxyLR: 0.3113  Epoch: 20  Global Step: 117070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:21,295-Speed 3880.50 samples/sec  Loss 5.1654  LearningRate 0.0062  ProxyLR: 0.3110  Epoch: 20  Global Step: 117080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:23,933-Speed 3883.02 samples/sec  Loss 5.2855  LearningRate 0.0062  ProxyLR: 0.3108  Epoch: 20  Global Step: 117090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:26,567-Speed 3888.86 samples/sec  Loss 5.2150  LearningRate 0.0062  ProxyLR: 0.3105  Epoch: 20  Global Step: 117100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:29,191-Speed 3903.28 samples/sec  Loss 5.1630  LearningRate 0.0062  ProxyLR: 0.3103  Epoch: 20  Global Step: 117110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:31,827-Speed 3885.55 samples/sec  Loss 5.2628  LearningRate 0.0062  ProxyLR: 0.3100  Epoch: 20  Global Step: 117120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:34,462-Speed 3886.78 samples/sec  Loss 5.1303  LearningRate 0.0062  ProxyLR: 0.3098  Epoch: 20  Global Step: 117130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:37,097-Speed 3888.19 samples/sec  Loss 5.0926  LearningRate 0.0062  ProxyLR: 0.3096  Epoch: 20  Global Step: 117140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:39,735-Speed 3882.35 samples/sec  Loss 5.2244  LearningRate 0.0062  ProxyLR: 0.3093  Epoch: 20  Global Step: 117150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:45:42,355-Speed 3908.41 samples/sec  Loss 5.1710  LearningRate 0.0062  ProxyLR: 0.3091  Epoch: 20  Global Step: 117160   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:44,987-Speed 3892.66 samples/sec  Loss 5.1381  LearningRate 0.0062  ProxyLR: 0.3088  Epoch: 20  Global Step: 117170   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:47,614-Speed 3898.39 samples/sec  Loss 5.2383  LearningRate 0.0062  ProxyLR: 0.3086  Epoch: 20  Global Step: 117180   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:50,241-Speed 3899.06 samples/sec  Loss 5.1300  LearningRate 0.0062  ProxyLR: 0.3083  Epoch: 20  Global Step: 117190   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:52,867-Speed 3899.39 samples/sec  Loss 5.1550  LearningRate 0.0062  ProxyLR: 0.3081  Epoch: 20  Global Step: 117200   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:55,494-Speed 3899.99 samples/sec  Loss 5.1815  LearningRate 0.0062  ProxyLR: 0.3078  Epoch: 20  Global Step: 117210   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:45:58,120-Speed 3899.61 samples/sec  Loss 5.1762  LearningRate 0.0062  ProxyLR: 0.3076  Epoch: 20  Global Step: 117220   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:00,748-Speed 3897.81 samples/sec  Loss 5.1646  LearningRate 0.0061  ProxyLR: 0.3073  Epoch: 20  Global Step: 117230   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:03,376-Speed 3896.44 samples/sec  Loss 5.1805  LearningRate 0.0061  ProxyLR: 0.3071  Epoch: 20  Global Step: 117240   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:06,003-Speed 3899.04 samples/sec  Loss 5.0789  LearningRate 0.0061  ProxyLR: 0.3068  Epoch: 20  Global Step: 117250   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:08,635-Speed 3891.52 samples/sec  Loss 5.2156  LearningRate 0.0061  ProxyLR: 0.3066  Epoch: 20  Global Step: 117260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:11,511-Speed 3561.39 samples/sec  Loss 5.2012  LearningRate 0.0061  ProxyLR: 0.3063  Epoch: 20  Global Step: 117270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:14,143-Speed 3891.44 samples/sec  Loss 5.1737  LearningRate 0.0061  ProxyLR: 0.3061  Epoch: 20  Global Step: 117280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:16,775-Speed 3892.14 samples/sec  Loss 5.0591  LearningRate 0.0061  ProxyLR: 0.3059  Epoch: 20  Global Step: 117290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:19,408-Speed 3889.14 samples/sec  Loss 5.1786  LearningRate 0.0061  ProxyLR: 0.3056  Epoch: 20  Global Step: 117300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:22,044-Speed 3886.33 samples/sec  Loss 5.0870  LearningRate 0.0061  ProxyLR: 0.3054  Epoch: 20  Global Step: 117310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:24,678-Speed 3888.00 samples/sec  Loss 5.2005  LearningRate 0.0061  ProxyLR: 0.3051  Epoch: 20  Global Step: 117320   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:27,309-Speed 3893.97 samples/sec  Loss 5.1434  LearningRate 0.0061  ProxyLR: 0.3049  Epoch: 20  Global Step: 117330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:29,924-Speed 3916.25 samples/sec  Loss 5.1829  LearningRate 0.0061  ProxyLR: 0.3046  Epoch: 20  Global Step: 117340   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:32,553-Speed 3895.80 samples/sec  Loss 5.1349  LearningRate 0.0061  ProxyLR: 0.3044  Epoch: 20  Global Step: 117350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:35,182-Speed 3895.58 samples/sec  Loss 5.0921  LearningRate 0.0061  ProxyLR: 0.3041  Epoch: 20  Global Step: 117360   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:37,810-Speed 3897.66 samples/sec  Loss 5.1951  LearningRate 0.0061  ProxyLR: 0.3039  Epoch: 20  Global Step: 117370   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:40,440-Speed 3895.38 samples/sec  Loss 5.1453  LearningRate 0.0061  ProxyLR: 0.3036  Epoch: 20  Global Step: 117380   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:43,071-Speed 3892.59 samples/sec  Loss 5.1183  LearningRate 0.0061  ProxyLR: 0.3034  Epoch: 20  Global Step: 117390   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:45,699-Speed 3896.88 samples/sec  Loss 5.1545  LearningRate 0.0061  ProxyLR: 0.3031  Epoch: 20  Global Step: 117400   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:48,328-Speed 3895.97 samples/sec  Loss 5.1821  LearningRate 0.0061  ProxyLR: 0.3029  Epoch: 20  Global Step: 117410   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:50,958-Speed 3894.47 samples/sec  Loss 5.1901  LearningRate 0.0061  ProxyLR: 0.3027  Epoch: 20  Global Step: 117420   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:53,586-Speed 3896.84 samples/sec  Loss 5.0875  LearningRate 0.0060  ProxyLR: 0.3024  Epoch: 20  Global Step: 117430   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:46:56,215-Speed 3896.39 samples/sec  Loss 5.0888  LearningRate 0.0060  ProxyLR: 0.3022  Epoch: 20  Global Step: 117440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:46:58,846-Speed 3893.43 samples/sec  Loss 5.1384  LearningRate 0.0060  ProxyLR: 0.3019  Epoch: 20  Global Step: 117450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:01,476-Speed 3894.77 samples/sec  Loss 5.2568  LearningRate 0.0060  ProxyLR: 0.3017  Epoch: 20  Global Step: 117460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:04,105-Speed 3894.86 samples/sec  Loss 5.1638  LearningRate 0.0060  ProxyLR: 0.3014  Epoch: 20  Global Step: 117470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:06,735-Speed 3894.21 samples/sec  Loss 5.1115  LearningRate 0.0060  ProxyLR: 0.3012  Epoch: 20  Global Step: 117480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:09,364-Speed 3895.86 samples/sec  Loss 5.1803  LearningRate 0.0060  ProxyLR: 0.3009  Epoch: 20  Global Step: 117490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:11,993-Speed 3897.00 samples/sec  Loss 5.1064  LearningRate 0.0060  ProxyLR: 0.3007  Epoch: 20  Global Step: 117500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:14,621-Speed 3896.86 samples/sec  Loss 5.1087  LearningRate 0.0060  ProxyLR: 0.3005  Epoch: 20  Global Step: 117510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:17,251-Speed 3895.63 samples/sec  Loss 5.1261  LearningRate 0.0060  ProxyLR: 0.3002  Epoch: 20  Global Step: 117520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:19,881-Speed 3893.66 samples/sec  Loss 5.1336  LearningRate 0.0060  ProxyLR: 0.3000  Epoch: 20  Global Step: 117530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:22,495-Speed 3917.51 samples/sec  Loss 5.1346  LearningRate 0.0060  ProxyLR: 0.2997  Epoch: 20  Global Step: 117540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:25,125-Speed 3895.00 samples/sec  Loss 5.0945  LearningRate 0.0060  ProxyLR: 0.2995  Epoch: 20  Global Step: 117550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:27,755-Speed 3894.32 samples/sec  Loss 5.0836  LearningRate 0.0060  ProxyLR: 0.2992  Epoch: 20  Global Step: 117560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:30,385-Speed 3894.53 samples/sec  Loss 5.0862  LearningRate 0.0060  ProxyLR: 0.2990  Epoch: 20  Global Step: 117570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:33,012-Speed 3899.03 samples/sec  Loss 5.1248  LearningRate 0.0060  ProxyLR: 0.2988  Epoch: 20  Global Step: 117580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:35,643-Speed 3892.44 samples/sec  Loss 5.0720  LearningRate 0.0060  ProxyLR: 0.2985  Epoch: 20  Global Step: 117590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:38,273-Speed 3894.41 samples/sec  Loss 5.1917  LearningRate 0.0060  ProxyLR: 0.2983  Epoch: 20  Global Step: 117600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:40,903-Speed 3895.15 samples/sec  Loss 5.1588  LearningRate 0.0060  ProxyLR: 0.2980  Epoch: 20  Global Step: 117610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:43,531-Speed 3896.70 samples/sec  Loss 5.1558  LearningRate 0.0060  ProxyLR: 0.2978  Epoch: 20  Global Step: 117620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:46,162-Speed 3893.61 samples/sec  Loss 5.0164  LearningRate 0.0060  ProxyLR: 0.2975  Epoch: 20  Global Step: 117630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:48,793-Speed 3892.66 samples/sec  Loss 5.0913  LearningRate 0.0059  ProxyLR: 0.2973  Epoch: 20  Global Step: 117640   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:47:51,410-Speed 3913.21 samples/sec  Loss 5.0791  LearningRate 0.0059  ProxyLR: 0.2971  Epoch: 20  Global Step: 117650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:54,041-Speed 3894.21 samples/sec  Loss 5.0920  LearningRate 0.0059  ProxyLR: 0.2968  Epoch: 20  Global Step: 117660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:56,671-Speed 3894.09 samples/sec  Loss 5.1136  LearningRate 0.0059  ProxyLR: 0.2966  Epoch: 20  Global Step: 117670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:47:59,300-Speed 3895.05 samples/sec  Loss 5.1120  LearningRate 0.0059  ProxyLR: 0.2963  Epoch: 20  Global Step: 117680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:01,932-Speed 3891.55 samples/sec  Loss 5.1668  LearningRate 0.0059  ProxyLR: 0.2961  Epoch: 20  Global Step: 117690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:04,562-Speed 3894.41 samples/sec  Loss 5.1111  LearningRate 0.0059  ProxyLR: 0.2958  Epoch: 20  Global Step: 117700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:07,193-Speed 3894.64 samples/sec  Loss 5.1065  LearningRate 0.0059  ProxyLR: 0.2956  Epoch: 20  Global Step: 117710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:09,823-Speed 3894.59 samples/sec  Loss 5.1219  LearningRate 0.0059  ProxyLR: 0.2954  Epoch: 20  Global Step: 117720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:12,452-Speed 3895.52 samples/sec  Loss 5.0850  LearningRate 0.0059  ProxyLR: 0.2951  Epoch: 20  Global Step: 117730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:15,082-Speed 3893.65 samples/sec  Loss 5.0571  LearningRate 0.0059  ProxyLR: 0.2949  Epoch: 20  Global Step: 117740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:17,698-Speed 3915.44 samples/sec  Loss 5.1677  LearningRate 0.0059  ProxyLR: 0.2946  Epoch: 20  Global Step: 117750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:20,327-Speed 3896.62 samples/sec  Loss 5.2007  LearningRate 0.0059  ProxyLR: 0.2944  Epoch: 20  Global Step: 117760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:22,956-Speed 3894.88 samples/sec  Loss 5.1740  LearningRate 0.0059  ProxyLR: 0.2942  Epoch: 20  Global Step: 117770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:25,585-Speed 3896.11 samples/sec  Loss 4.9650  LearningRate 0.0059  ProxyLR: 0.2939  Epoch: 20  Global Step: 117780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:28,216-Speed 3893.89 samples/sec  Loss 5.1898  LearningRate 0.0059  ProxyLR: 0.2937  Epoch: 20  Global Step: 117790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:30,847-Speed 3892.61 samples/sec  Loss 5.1370  LearningRate 0.0059  ProxyLR: 0.2934  Epoch: 20  Global Step: 117800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:33,474-Speed 3898.23 samples/sec  Loss 5.0876  LearningRate 0.0059  ProxyLR: 0.2932  Epoch: 20  Global Step: 117810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:36,104-Speed 3894.62 samples/sec  Loss 5.0268  LearningRate 0.0059  ProxyLR: 0.2929  Epoch: 20  Global Step: 117820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:38,736-Speed 3892.11 samples/sec  Loss 5.1492  LearningRate 0.0059  ProxyLR: 0.2927  Epoch: 20  Global Step: 117830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:41,368-Speed 3890.70 samples/sec  Loss 5.1039  LearningRate 0.0058  ProxyLR: 0.2925  Epoch: 20  Global Step: 117840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:48:43,975-Speed 3929.56 samples/sec  Loss 5.2000  LearningRate 0.0058  ProxyLR: 0.2922  Epoch: 20  Global Step: 117850   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:48:46,608-Speed 3889.83 samples/sec  Loss 5.0847  LearningRate 0.0058  ProxyLR: 0.2920  Epoch: 20  Global Step: 117860   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:48:49,239-Speed 3892.89 samples/sec  Loss 5.1813  LearningRate 0.0058  ProxyLR: 0.2917  Epoch: 20  Global Step: 117870   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:48:51,873-Speed 3887.88 samples/sec  Loss 5.0515  LearningRate 0.0058  ProxyLR: 0.2915  Epoch: 20  Global Step: 117880   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:48:54,509-Speed 3886.72 samples/sec  Loss 5.1110  LearningRate 0.0058  ProxyLR: 0.2913  Epoch: 20  Global Step: 117890   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:48:57,145-Speed 3885.53 samples/sec  Loss 5.0931  LearningRate 0.0058  ProxyLR: 0.2910  Epoch: 20  Global Step: 117900   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:48:59,781-Speed 3885.57 samples/sec  Loss 5.0585  LearningRate 0.0058  ProxyLR: 0.2908  Epoch: 20  Global Step: 117910   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:49:02,416-Speed 3887.32 samples/sec  Loss 5.0293  LearningRate 0.0058  ProxyLR: 0.2905  Epoch: 20  Global Step: 117920   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:49:05,052-Speed 3885.50 samples/sec  Loss 5.1103  LearningRate 0.0058  ProxyLR: 0.2903  Epoch: 20  Global Step: 117930   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:49:07,687-Speed 3886.45 samples/sec  Loss 5.0355  LearningRate 0.0058  ProxyLR: 0.2901  Epoch: 20  Global Step: 117940   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:49:10,318-Speed 3892.94 samples/sec  Loss 5.1097  LearningRate 0.0058  ProxyLR: 0.2898  Epoch: 20  Global Step: 117950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:49:12,950-Speed 3891.50 samples/sec  Loss 5.1014  LearningRate 0.0058  ProxyLR: 0.2896  Epoch: 20  Global Step: 117960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:49:15,582-Speed 3891.45 samples/sec  Loss 5.0272  LearningRate 0.0058  ProxyLR: 0.2893  Epoch: 20  Global Step: 117970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:49:18,213-Speed 3893.48 samples/sec  Loss 5.0338  LearningRate 0.0058  ProxyLR: 0.2891  Epoch: 20  Global Step: 117980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:49:20,843-Speed 3893.88 samples/sec  Loss 4.9707  LearningRate 0.0058  ProxyLR: 0.2889  Epoch: 20  Global Step: 117990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:49:23,475-Speed 3891.30 samples/sec  Loss 5.0599  LearningRate 0.0058  ProxyLR: 0.2886  Epoch: 20  Global Step: 118000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:50:13,434-[lfw][118000]XNorm: 22.070107
Training: 2023-05-04 23:50:13,434-[lfw][118000]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-04 23:50:13,435-[lfw][118000]Accuracy-Highest: 0.99783
Training: 2023-05-04 23:50:13,435-[lfw][118000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-04 23:50:13,435-[lfw][118000]Highest TPR@FPR: 0.99667
Training: 2023-05-04 23:51:10,701-[cfp_fp][118000]XNorm: 21.804741
Training: 2023-05-04 23:51:10,701-[cfp_fp][118000]Accuracy-Flip: 0.98400+-0.00502
Training: 2023-05-04 23:51:10,702-[cfp_fp][118000]Accuracy-Highest: 0.98429
Training: 2023-05-04 23:51:10,702-[cfp_fp][118000]TPR@1stNon-Zero-FPR of 0.00029: 0.86286
Training: 2023-05-04 23:51:10,702-[cfp_fp][118000]Highest TPR@FPR: 0.87086
Training: 2023-05-04 23:52:01,150-[agedb_30][118000]XNorm: 22.281748
Training: 2023-05-04 23:52:01,150-[agedb_30][118000]Accuracy-Flip: 0.97450+-0.00882
Training: 2023-05-04 23:52:01,150-[agedb_30][118000]Accuracy-Highest: 0.97450
Training: 2023-05-04 23:52:01,150-[agedb_30][118000]TPR@1stNon-Zero-FPR of 0.00033: 0.87933
Training: 2023-05-04 23:52:01,151-[agedb_30][118000]Highest TPR@FPR: 0.88033
Training: 2023-05-04 23:52:53,242-[calfw][118000]XNorm: 22.232530
Training: 2023-05-04 23:52:53,243-[calfw][118000]Accuracy-Flip: 0.95567+-0.01239
Training: 2023-05-04 23:52:53,243-[calfw][118000]Accuracy-Highest: 0.95833
Training: 2023-05-04 23:52:53,243-[calfw][118000]TPR@1stNon-Zero-FPR of 0.00033: 0.81267
Training: 2023-05-04 23:52:53,243-[calfw][118000]Highest TPR@FPR: 0.85533
Training: 2023-05-04 23:53:45,155-[cplfw][118000]XNorm: 21.103922
Training: 2023-05-04 23:53:45,155-[cplfw][118000]Accuracy-Flip: 0.92750+-0.01184
Training: 2023-05-04 23:53:45,155-[cplfw][118000]Accuracy-Highest: 0.92900
Training: 2023-05-04 23:53:45,155-[cplfw][118000]TPR@1stNon-Zero-FPR of 0.00033: 0.02633
Training: 2023-05-04 23:53:45,156-[cplfw][118000]Highest TPR@FPR: 0.07633
Training: 2023-05-04 23:53:49,556-Speed 38.48 samples/sec  Loss 5.0848  LearningRate 0.0058  ProxyLR: 0.2884  Epoch: 20  Global Step: 118010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:53:52,163-Speed 3928.95 samples/sec  Loss 5.0577  LearningRate 0.0058  ProxyLR: 0.2882  Epoch: 20  Global Step: 118020   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:53:54,784-Speed 3908.35 samples/sec  Loss 5.0340  LearningRate 0.0058  ProxyLR: 0.2879  Epoch: 20  Global Step: 118030   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:53:57,405-Speed 3907.28 samples/sec  Loss 5.0283  LearningRate 0.0058  ProxyLR: 0.2877  Epoch: 20  Global Step: 118040   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:00,029-Speed 3903.94 samples/sec  Loss 5.0086  LearningRate 0.0057  ProxyLR: 0.2874  Epoch: 20  Global Step: 118050   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:02,652-Speed 3905.81 samples/sec  Loss 5.0778  LearningRate 0.0057  ProxyLR: 0.2872  Epoch: 20  Global Step: 118060   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:05,277-Speed 3902.82 samples/sec  Loss 5.1403  LearningRate 0.0057  ProxyLR: 0.2870  Epoch: 20  Global Step: 118070   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:07,901-Speed 3903.70 samples/sec  Loss 5.0839  LearningRate 0.0057  ProxyLR: 0.2867  Epoch: 20  Global Step: 118080   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:10,525-Speed 3903.44 samples/sec  Loss 4.9862  LearningRate 0.0057  ProxyLR: 0.2865  Epoch: 20  Global Step: 118090   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:13,150-Speed 3901.14 samples/sec  Loss 5.0328  LearningRate 0.0057  ProxyLR: 0.2862  Epoch: 20  Global Step: 118100   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:15,776-Speed 3900.45 samples/sec  Loss 5.0163  LearningRate 0.0057  ProxyLR: 0.2860  Epoch: 20  Global Step: 118110   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:54:18,402-Speed 3900.75 samples/sec  Loss 5.0725  LearningRate 0.0057  ProxyLR: 0.2858  Epoch: 20  Global Step: 118120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:21,030-Speed 3897.15 samples/sec  Loss 5.0544  LearningRate 0.0057  ProxyLR: 0.2855  Epoch: 20  Global Step: 118130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:23,659-Speed 3895.46 samples/sec  Loss 4.9579  LearningRate 0.0057  ProxyLR: 0.2853  Epoch: 20  Global Step: 118140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:26,286-Speed 3900.07 samples/sec  Loss 5.0736  LearningRate 0.0057  ProxyLR: 0.2851  Epoch: 20  Global Step: 118150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:28,915-Speed 3895.33 samples/sec  Loss 5.0915  LearningRate 0.0057  ProxyLR: 0.2848  Epoch: 20  Global Step: 118160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:31,544-Speed 3895.74 samples/sec  Loss 5.0915  LearningRate 0.0057  ProxyLR: 0.2846  Epoch: 20  Global Step: 118170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:34,173-Speed 3896.46 samples/sec  Loss 5.1213  LearningRate 0.0057  ProxyLR: 0.2843  Epoch: 20  Global Step: 118180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:36,803-Speed 3895.03 samples/sec  Loss 5.0747  LearningRate 0.0057  ProxyLR: 0.2841  Epoch: 20  Global Step: 118190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:39,432-Speed 3896.02 samples/sec  Loss 4.9405  LearningRate 0.0057  ProxyLR: 0.2839  Epoch: 20  Global Step: 118200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:42,064-Speed 3891.46 samples/sec  Loss 4.9878  LearningRate 0.0057  ProxyLR: 0.2836  Epoch: 20  Global Step: 118210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:44,694-Speed 3893.96 samples/sec  Loss 5.0360  LearningRate 0.0057  ProxyLR: 0.2834  Epoch: 20  Global Step: 118220   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-04 23:54:47,311-Speed 3914.24 samples/sec  Loss 4.9472  LearningRate 0.0057  ProxyLR: 0.2832  Epoch: 20  Global Step: 118230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:49,941-Speed 3895.08 samples/sec  Loss 4.9647  LearningRate 0.0057  ProxyLR: 0.2829  Epoch: 20  Global Step: 118240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:52,570-Speed 3895.26 samples/sec  Loss 5.0269  LearningRate 0.0057  ProxyLR: 0.2827  Epoch: 20  Global Step: 118250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:55,201-Speed 3892.98 samples/sec  Loss 4.9175  LearningRate 0.0056  ProxyLR: 0.2824  Epoch: 20  Global Step: 118260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:54:57,833-Speed 3891.76 samples/sec  Loss 5.0679  LearningRate 0.0056  ProxyLR: 0.2822  Epoch: 20  Global Step: 118270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:55:00,464-Speed 3893.03 samples/sec  Loss 4.9193  LearningRate 0.0056  ProxyLR: 0.2820  Epoch: 20  Global Step: 118280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:55:03,095-Speed 3892.18 samples/sec  Loss 4.9190  LearningRate 0.0056  ProxyLR: 0.2817  Epoch: 20  Global Step: 118290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:55:05,730-Speed 3888.32 samples/sec  Loss 5.0358  LearningRate 0.0056  ProxyLR: 0.2815  Epoch: 20  Global Step: 118300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:55:08,346-Speed 3914.78 samples/sec  Loss 4.9078  LearningRate 0.0056  ProxyLR: 0.2813  Epoch: 20  Global Step: 118310   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:10,976-Speed 3894.07 samples/sec  Loss 4.9954  LearningRate 0.0056  ProxyLR: 0.2810  Epoch: 20  Global Step: 118320   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:13,607-Speed 3894.30 samples/sec  Loss 5.0104  LearningRate 0.0056  ProxyLR: 0.2808  Epoch: 20  Global Step: 118330   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:16,237-Speed 3893.75 samples/sec  Loss 5.0298  LearningRate 0.0056  ProxyLR: 0.2806  Epoch: 20  Global Step: 118340   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:18,870-Speed 3890.52 samples/sec  Loss 4.9499  LearningRate 0.0056  ProxyLR: 0.2803  Epoch: 20  Global Step: 118350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:21,500-Speed 3893.78 samples/sec  Loss 4.9966  LearningRate 0.0056  ProxyLR: 0.2801  Epoch: 20  Global Step: 118360   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:24,130-Speed 3895.04 samples/sec  Loss 4.9918  LearningRate 0.0056  ProxyLR: 0.2799  Epoch: 20  Global Step: 118370   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:26,760-Speed 3894.03 samples/sec  Loss 5.0531  LearningRate 0.0056  ProxyLR: 0.2796  Epoch: 20  Global Step: 118380   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:29,390-Speed 3894.94 samples/sec  Loss 4.9169  LearningRate 0.0056  ProxyLR: 0.2794  Epoch: 20  Global Step: 118390   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:55:32,008-Speed 3911.85 samples/sec  Loss 5.0092  LearningRate 0.0056  ProxyLR: 0.2791  Epoch: 20  Global Step: 118400   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:34,642-Speed 3888.40 samples/sec  Loss 4.9723  LearningRate 0.0056  ProxyLR: 0.2789  Epoch: 20  Global Step: 118410   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:37,275-Speed 3890.07 samples/sec  Loss 4.9703  LearningRate 0.0056  ProxyLR: 0.2787  Epoch: 20  Global Step: 118420   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:39,912-Speed 3884.81 samples/sec  Loss 5.0591  LearningRate 0.0056  ProxyLR: 0.2784  Epoch: 20  Global Step: 118430   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:42,549-Speed 3883.92 samples/sec  Loss 4.9992  LearningRate 0.0056  ProxyLR: 0.2782  Epoch: 20  Global Step: 118440   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:45,188-Speed 3881.05 samples/sec  Loss 4.9700  LearningRate 0.0056  ProxyLR: 0.2780  Epoch: 20  Global Step: 118450   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:47,829-Speed 3878.81 samples/sec  Loss 4.9851  LearningRate 0.0056  ProxyLR: 0.2777  Epoch: 20  Global Step: 118460   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:50,467-Speed 3882.39 samples/sec  Loss 5.0044  LearningRate 0.0056  ProxyLR: 0.2775  Epoch: 20  Global Step: 118470   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:53,100-Speed 3889.77 samples/sec  Loss 4.9322  LearningRate 0.0055  ProxyLR: 0.2773  Epoch: 20  Global Step: 118480   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:55,735-Speed 3887.26 samples/sec  Loss 4.9554  LearningRate 0.0055  ProxyLR: 0.2770  Epoch: 20  Global Step: 118490   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-04 23:55:58,369-Speed 3888.46 samples/sec  Loss 4.9720  LearningRate 0.0055  ProxyLR: 0.2768  Epoch: 20  Global Step: 118500   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:01,004-Speed 3887.86 samples/sec  Loss 4.9094  LearningRate 0.0055  ProxyLR: 0.2766  Epoch: 20  Global Step: 118510   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:03,637-Speed 3888.60 samples/sec  Loss 5.0414  LearningRate 0.0055  ProxyLR: 0.2763  Epoch: 20  Global Step: 118520   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:06,269-Speed 3892.16 samples/sec  Loss 4.9444  LearningRate 0.0055  ProxyLR: 0.2761  Epoch: 20  Global Step: 118530   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:08,903-Speed 3888.95 samples/sec  Loss 4.9125  LearningRate 0.0055  ProxyLR: 0.2759  Epoch: 20  Global Step: 118540   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:11,534-Speed 3893.17 samples/sec  Loss 4.9965  LearningRate 0.0055  ProxyLR: 0.2756  Epoch: 20  Global Step: 118550   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:14,165-Speed 3892.34 samples/sec  Loss 5.0338  LearningRate 0.0055  ProxyLR: 0.2754  Epoch: 20  Global Step: 118560   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:16,797-Speed 3891.32 samples/sec  Loss 5.0081  LearningRate 0.0055  ProxyLR: 0.2752  Epoch: 20  Global Step: 118570   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:19,431-Speed 3889.05 samples/sec  Loss 4.9658  LearningRate 0.0055  ProxyLR: 0.2749  Epoch: 20  Global Step: 118580   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:22,062-Speed 3893.06 samples/sec  Loss 4.9958  LearningRate 0.0055  ProxyLR: 0.2747  Epoch: 20  Global Step: 118590   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:24,695-Speed 3890.64 samples/sec  Loss 4.8854  LearningRate 0.0055  ProxyLR: 0.2745  Epoch: 20  Global Step: 118600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:56:27,328-Speed 3890.31 samples/sec  Loss 4.9204  LearningRate 0.0055  ProxyLR: 0.2742  Epoch: 20  Global Step: 118610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:56:29,961-Speed 3889.30 samples/sec  Loss 5.0181  LearningRate 0.0055  ProxyLR: 0.2740  Epoch: 20  Global Step: 118620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:56:32,594-Speed 3890.38 samples/sec  Loss 4.9891  LearningRate 0.0055  ProxyLR: 0.2738  Epoch: 20  Global Step: 118630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:56:35,212-Speed 3912.38 samples/sec  Loss 4.9240  LearningRate 0.0055  ProxyLR: 0.2735  Epoch: 20  Global Step: 118640   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:37,843-Speed 3892.95 samples/sec  Loss 4.9056  LearningRate 0.0055  ProxyLR: 0.2733  Epoch: 20  Global Step: 118650   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:40,476-Speed 3890.93 samples/sec  Loss 4.9596  LearningRate 0.0055  ProxyLR: 0.2731  Epoch: 20  Global Step: 118660   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:43,107-Speed 3892.41 samples/sec  Loss 4.9864  LearningRate 0.0055  ProxyLR: 0.2728  Epoch: 20  Global Step: 118670   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:45,738-Speed 3893.25 samples/sec  Loss 5.0068  LearningRate 0.0055  ProxyLR: 0.2726  Epoch: 20  Global Step: 118680   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:48,369-Speed 3892.39 samples/sec  Loss 5.0185  LearningRate 0.0054  ProxyLR: 0.2724  Epoch: 20  Global Step: 118690   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:51,002-Speed 3890.81 samples/sec  Loss 4.9300  LearningRate 0.0054  ProxyLR: 0.2721  Epoch: 20  Global Step: 118700   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:53,635-Speed 3890.01 samples/sec  Loss 4.9338  LearningRate 0.0054  ProxyLR: 0.2719  Epoch: 20  Global Step: 118710   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:56,266-Speed 3893.00 samples/sec  Loss 4.9954  LearningRate 0.0054  ProxyLR: 0.2717  Epoch: 20  Global Step: 118720   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:56:58,898-Speed 3891.64 samples/sec  Loss 4.9299  LearningRate 0.0054  ProxyLR: 0.2714  Epoch: 20  Global Step: 118730   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:57:01,528-Speed 3893.80 samples/sec  Loss 4.9239  LearningRate 0.0054  ProxyLR: 0.2712  Epoch: 20  Global Step: 118740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:04,161-Speed 3890.06 samples/sec  Loss 4.9265  LearningRate 0.0054  ProxyLR: 0.2710  Epoch: 20  Global Step: 118750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:06,793-Speed 3891.39 samples/sec  Loss 4.9050  LearningRate 0.0054  ProxyLR: 0.2707  Epoch: 20  Global Step: 118760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:09,426-Speed 3891.29 samples/sec  Loss 4.9877  LearningRate 0.0054  ProxyLR: 0.2705  Epoch: 20  Global Step: 118770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:12,059-Speed 3889.44 samples/sec  Loss 4.9671  LearningRate 0.0054  ProxyLR: 0.2703  Epoch: 20  Global Step: 118780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:14,691-Speed 3892.61 samples/sec  Loss 4.9459  LearningRate 0.0054  ProxyLR: 0.2701  Epoch: 20  Global Step: 118790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:17,322-Speed 3892.59 samples/sec  Loss 4.9170  LearningRate 0.0054  ProxyLR: 0.2698  Epoch: 20  Global Step: 118800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:19,954-Speed 3891.67 samples/sec  Loss 4.9613  LearningRate 0.0054  ProxyLR: 0.2696  Epoch: 20  Global Step: 118810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:22,585-Speed 3893.11 samples/sec  Loss 4.9503  LearningRate 0.0054  ProxyLR: 0.2694  Epoch: 20  Global Step: 118820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:25,215-Speed 3894.39 samples/sec  Loss 4.9522  LearningRate 0.0054  ProxyLR: 0.2691  Epoch: 20  Global Step: 118830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:27,831-Speed 3915.28 samples/sec  Loss 4.9251  LearningRate 0.0054  ProxyLR: 0.2689  Epoch: 20  Global Step: 118840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:30,462-Speed 3892.89 samples/sec  Loss 4.9568  LearningRate 0.0054  ProxyLR: 0.2687  Epoch: 20  Global Step: 118850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:33,091-Speed 3895.11 samples/sec  Loss 4.8492  LearningRate 0.0054  ProxyLR: 0.2684  Epoch: 20  Global Step: 118860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:35,721-Speed 3894.44 samples/sec  Loss 4.9527  LearningRate 0.0054  ProxyLR: 0.2682  Epoch: 20  Global Step: 118870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:38,352-Speed 3892.88 samples/sec  Loss 4.8704  LearningRate 0.0054  ProxyLR: 0.2680  Epoch: 20  Global Step: 118880   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:40,984-Speed 3892.27 samples/sec  Loss 5.0213  LearningRate 0.0054  ProxyLR: 0.2677  Epoch: 20  Global Step: 118890   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:43,614-Speed 3894.25 samples/sec  Loss 4.9202  LearningRate 0.0054  ProxyLR: 0.2675  Epoch: 20  Global Step: 118900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:46,245-Speed 3893.41 samples/sec  Loss 4.9339  LearningRate 0.0053  ProxyLR: 0.2673  Epoch: 20  Global Step: 118910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:48,878-Speed 3890.82 samples/sec  Loss 4.8961  LearningRate 0.0053  ProxyLR: 0.2671  Epoch: 20  Global Step: 118920   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:51,509-Speed 3892.80 samples/sec  Loss 4.9710  LearningRate 0.0053  ProxyLR: 0.2668  Epoch: 20  Global Step: 118930   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:54,131-Speed 3906.58 samples/sec  Loss 4.9022  LearningRate 0.0053  ProxyLR: 0.2666  Epoch: 20  Global Step: 118940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:56,765-Speed 3887.82 samples/sec  Loss 4.9419  LearningRate 0.0053  ProxyLR: 0.2664  Epoch: 20  Global Step: 118950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:57:59,402-Speed 3884.92 samples/sec  Loss 4.9954  LearningRate 0.0053  ProxyLR: 0.2661  Epoch: 20  Global Step: 118960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:02,041-Speed 3880.22 samples/sec  Loss 4.9025  LearningRate 0.0053  ProxyLR: 0.2659  Epoch: 20  Global Step: 118970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:04,679-Speed 3883.16 samples/sec  Loss 4.9032  LearningRate 0.0053  ProxyLR: 0.2657  Epoch: 20  Global Step: 118980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:07,315-Speed 3884.95 samples/sec  Loss 4.9338  LearningRate 0.0053  ProxyLR: 0.2655  Epoch: 20  Global Step: 118990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:09,952-Speed 3884.61 samples/sec  Loss 4.9689  LearningRate 0.0053  ProxyLR: 0.2652  Epoch: 20  Global Step: 119000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:12,589-Speed 3884.44 samples/sec  Loss 4.7821  LearningRate 0.0053  ProxyLR: 0.2650  Epoch: 20  Global Step: 119010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:15,228-Speed 3881.85 samples/sec  Loss 4.8439  LearningRate 0.0053  ProxyLR: 0.2648  Epoch: 20  Global Step: 119020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:17,852-Speed 3903.39 samples/sec  Loss 4.9487  LearningRate 0.0053  ProxyLR: 0.2645  Epoch: 20  Global Step: 119030   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:20,486-Speed 3888.74 samples/sec  Loss 4.8950  LearningRate 0.0053  ProxyLR: 0.2643  Epoch: 20  Global Step: 119040   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:23,120-Speed 3887.70 samples/sec  Loss 4.8675  LearningRate 0.0053  ProxyLR: 0.2641  Epoch: 20  Global Step: 119050   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:25,757-Speed 3884.21 samples/sec  Loss 4.9141  LearningRate 0.0053  ProxyLR: 0.2638  Epoch: 20  Global Step: 119060   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:28,394-Speed 3885.11 samples/sec  Loss 4.9341  LearningRate 0.0053  ProxyLR: 0.2636  Epoch: 20  Global Step: 119070   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:31,032-Speed 3882.15 samples/sec  Loss 4.8511  LearningRate 0.0053  ProxyLR: 0.2634  Epoch: 20  Global Step: 119080   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:33,668-Speed 3886.12 samples/sec  Loss 4.8823  LearningRate 0.0053  ProxyLR: 0.2632  Epoch: 20  Global Step: 119090   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:36,305-Speed 3884.26 samples/sec  Loss 4.9979  LearningRate 0.0053  ProxyLR: 0.2629  Epoch: 20  Global Step: 119100   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:38,944-Speed 3880.50 samples/sec  Loss 4.8746  LearningRate 0.0053  ProxyLR: 0.2627  Epoch: 20  Global Step: 119110   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:41,582-Speed 3882.19 samples/sec  Loss 4.9045  LearningRate 0.0052  ProxyLR: 0.2625  Epoch: 20  Global Step: 119120   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-04 23:58:44,220-Speed 3883.57 samples/sec  Loss 4.8279  LearningRate 0.0052  ProxyLR: 0.2623  Epoch: 20  Global Step: 119130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:46,861-Speed 3878.28 samples/sec  Loss 4.8666  LearningRate 0.0052  ProxyLR: 0.2620  Epoch: 20  Global Step: 119140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:49,495-Speed 3888.93 samples/sec  Loss 4.8925  LearningRate 0.0052  ProxyLR: 0.2618  Epoch: 20  Global Step: 119150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:52,131-Speed 3885.57 samples/sec  Loss 4.8486  LearningRate 0.0052  ProxyLR: 0.2616  Epoch: 20  Global Step: 119160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:54,767-Speed 3886.01 samples/sec  Loss 4.9119  LearningRate 0.0052  ProxyLR: 0.2613  Epoch: 20  Global Step: 119170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:58:57,406-Speed 3880.63 samples/sec  Loss 4.8150  LearningRate 0.0052  ProxyLR: 0.2611  Epoch: 20  Global Step: 119180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:00,043-Speed 3884.06 samples/sec  Loss 4.8776  LearningRate 0.0052  ProxyLR: 0.2609  Epoch: 20  Global Step: 119190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:02,681-Speed 3882.11 samples/sec  Loss 4.8367  LearningRate 0.0052  ProxyLR: 0.2607  Epoch: 20  Global Step: 119200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:05,320-Speed 3881.89 samples/sec  Loss 4.8588  LearningRate 0.0052  ProxyLR: 0.2604  Epoch: 20  Global Step: 119210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:07,958-Speed 3882.95 samples/sec  Loss 4.8644  LearningRate 0.0052  ProxyLR: 0.2602  Epoch: 20  Global Step: 119220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:10,574-Speed 3915.07 samples/sec  Loss 4.8623  LearningRate 0.0052  ProxyLR: 0.2600  Epoch: 20  Global Step: 119230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:13,203-Speed 3896.00 samples/sec  Loss 4.8635  LearningRate 0.0052  ProxyLR: 0.2598  Epoch: 20  Global Step: 119240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:15,835-Speed 3892.34 samples/sec  Loss 4.8996  LearningRate 0.0052  ProxyLR: 0.2595  Epoch: 20  Global Step: 119250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:18,472-Speed 3884.09 samples/sec  Loss 4.9118  LearningRate 0.0052  ProxyLR: 0.2593  Epoch: 20  Global Step: 119260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:21,104-Speed 3891.07 samples/sec  Loss 4.7677  LearningRate 0.0052  ProxyLR: 0.2591  Epoch: 20  Global Step: 119270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:23,735-Speed 3892.68 samples/sec  Loss 4.8720  LearningRate 0.0052  ProxyLR: 0.2588  Epoch: 20  Global Step: 119280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:26,364-Speed 3895.78 samples/sec  Loss 4.8409  LearningRate 0.0052  ProxyLR: 0.2586  Epoch: 20  Global Step: 119290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:28,994-Speed 3895.25 samples/sec  Loss 4.8577  LearningRate 0.0052  ProxyLR: 0.2584  Epoch: 20  Global Step: 119300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:31,623-Speed 3895.53 samples/sec  Loss 4.9295  LearningRate 0.0052  ProxyLR: 0.2582  Epoch: 20  Global Step: 119310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:34,250-Speed 3900.06 samples/sec  Loss 4.9151  LearningRate 0.0052  ProxyLR: 0.2579  Epoch: 20  Global Step: 119320   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:36,866-Speed 3914.10 samples/sec  Loss 4.6867  LearningRate 0.0052  ProxyLR: 0.2577  Epoch: 20  Global Step: 119330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:39,496-Speed 3894.98 samples/sec  Loss 4.8162  LearningRate 0.0051  ProxyLR: 0.2575  Epoch: 20  Global Step: 119340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:42,127-Speed 3893.76 samples/sec  Loss 4.9325  LearningRate 0.0051  ProxyLR: 0.2573  Epoch: 20  Global Step: 119350   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:44,756-Speed 3895.73 samples/sec  Loss 4.8387  LearningRate 0.0051  ProxyLR: 0.2570  Epoch: 20  Global Step: 119360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:47,385-Speed 3895.00 samples/sec  Loss 4.9215  LearningRate 0.0051  ProxyLR: 0.2568  Epoch: 20  Global Step: 119370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:50,015-Speed 3894.72 samples/sec  Loss 4.8886  LearningRate 0.0051  ProxyLR: 0.2566  Epoch: 20  Global Step: 119380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:52,644-Speed 3897.06 samples/sec  Loss 4.8509  LearningRate 0.0051  ProxyLR: 0.2564  Epoch: 20  Global Step: 119390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-04 23:59:55,334-Speed 3807.06 samples/sec  Loss 4.8693  LearningRate 0.0051  ProxyLR: 0.2561  Epoch: 20  Global Step: 119400   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:07,959-Speed 811.19 samples/sec  Loss 4.7587  LearningRate 0.0051  ProxyLR: 0.2559  Epoch: 21  Global Step: 119410   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:10,600-Speed 3878.33 samples/sec  Loss 4.5388  LearningRate 0.0051  ProxyLR: 0.2557  Epoch: 21  Global Step: 119420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:13,218-Speed 3911.18 samples/sec  Loss 4.4638  LearningRate 0.0051  ProxyLR: 0.2555  Epoch: 21  Global Step: 119430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:15,875-Speed 3855.55 samples/sec  Loss 4.5401  LearningRate 0.0051  ProxyLR: 0.2552  Epoch: 21  Global Step: 119440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:18,500-Speed 3902.21 samples/sec  Loss 4.5274  LearningRate 0.0051  ProxyLR: 0.2550  Epoch: 21  Global Step: 119450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:21,125-Speed 3901.78 samples/sec  Loss 4.4737  LearningRate 0.0051  ProxyLR: 0.2548  Epoch: 21  Global Step: 119460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:23,758-Speed 3891.08 samples/sec  Loss 4.4748  LearningRate 0.0051  ProxyLR: 0.2546  Epoch: 21  Global Step: 119470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:26,391-Speed 3890.30 samples/sec  Loss 4.5411  LearningRate 0.0051  ProxyLR: 0.2543  Epoch: 21  Global Step: 119480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:29,017-Speed 3899.42 samples/sec  Loss 4.4972  LearningRate 0.0051  ProxyLR: 0.2541  Epoch: 21  Global Step: 119490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:31,645-Speed 3897.81 samples/sec  Loss 4.4925  LearningRate 0.0051  ProxyLR: 0.2539  Epoch: 21  Global Step: 119500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:34,270-Speed 3901.22 samples/sec  Loss 4.5081  LearningRate 0.0051  ProxyLR: 0.2537  Epoch: 21  Global Step: 119510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:36,898-Speed 3898.22 samples/sec  Loss 4.5286  LearningRate 0.0051  ProxyLR: 0.2534  Epoch: 21  Global Step: 119520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:39,518-Speed 3908.67 samples/sec  Loss 4.4228  LearningRate 0.0051  ProxyLR: 0.2532  Epoch: 21  Global Step: 119530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:42,176-Speed 3854.44 samples/sec  Loss 4.5629  LearningRate 0.0051  ProxyLR: 0.2530  Epoch: 21  Global Step: 119540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:44,816-Speed 3879.26 samples/sec  Loss 4.4740  LearningRate 0.0051  ProxyLR: 0.2528  Epoch: 21  Global Step: 119550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:47,449-Speed 3890.05 samples/sec  Loss 4.4833  LearningRate 0.0051  ProxyLR: 0.2525  Epoch: 21  Global Step: 119560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:50,082-Speed 3889.98 samples/sec  Loss 4.4352  LearningRate 0.0050  ProxyLR: 0.2523  Epoch: 21  Global Step: 119570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:52,716-Speed 3889.24 samples/sec  Loss 4.5486  LearningRate 0.0050  ProxyLR: 0.2521  Epoch: 21  Global Step: 119580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:55,399-Speed 3817.17 samples/sec  Loss 4.4909  LearningRate 0.0050  ProxyLR: 0.2519  Epoch: 21  Global Step: 119590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:00:58,031-Speed 3891.79 samples/sec  Loss 4.4755  LearningRate 0.0050  ProxyLR: 0.2517  Epoch: 21  Global Step: 119600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:00,665-Speed 3887.89 samples/sec  Loss 4.4326  LearningRate 0.0050  ProxyLR: 0.2514  Epoch: 21  Global Step: 119610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:03,306-Speed 3879.33 samples/sec  Loss 4.4836  LearningRate 0.0050  ProxyLR: 0.2512  Epoch: 21  Global Step: 119620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:05,964-Speed 3853.04 samples/sec  Loss 4.5188  LearningRate 0.0050  ProxyLR: 0.2510  Epoch: 21  Global Step: 119630   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-05 00:01:08,584-Speed 3909.96 samples/sec  Loss 4.5376  LearningRate 0.0050  ProxyLR: 0.2508  Epoch: 21  Global Step: 119640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:11,216-Speed 3890.42 samples/sec  Loss 4.5410  LearningRate 0.0050  ProxyLR: 0.2505  Epoch: 21  Global Step: 119650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:13,834-Speed 3912.99 samples/sec  Loss 4.5506  LearningRate 0.0050  ProxyLR: 0.2503  Epoch: 21  Global Step: 119660   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:16,468-Speed 3889.27 samples/sec  Loss 4.4536  LearningRate 0.0050  ProxyLR: 0.2501  Epoch: 21  Global Step: 119670   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:19,097-Speed 3894.74 samples/sec  Loss 4.4949  LearningRate 0.0050  ProxyLR: 0.2499  Epoch: 21  Global Step: 119680   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:21,729-Speed 3892.40 samples/sec  Loss 4.4403  LearningRate 0.0050  ProxyLR: 0.2496  Epoch: 21  Global Step: 119690   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:24,361-Speed 3891.29 samples/sec  Loss 4.5133  LearningRate 0.0050  ProxyLR: 0.2494  Epoch: 21  Global Step: 119700   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:26,994-Speed 3890.12 samples/sec  Loss 4.4849  LearningRate 0.0050  ProxyLR: 0.2492  Epoch: 21  Global Step: 119710   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:29,625-Speed 3893.62 samples/sec  Loss 4.4787  LearningRate 0.0050  ProxyLR: 0.2490  Epoch: 21  Global Step: 119720   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:32,257-Speed 3891.42 samples/sec  Loss 4.5387  LearningRate 0.0050  ProxyLR: 0.2488  Epoch: 21  Global Step: 119730   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:34,892-Speed 3888.11 samples/sec  Loss 4.5186  LearningRate 0.0050  ProxyLR: 0.2485  Epoch: 21  Global Step: 119740   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:37,526-Speed 3888.09 samples/sec  Loss 4.4624  LearningRate 0.0050  ProxyLR: 0.2483  Epoch: 21  Global Step: 119750   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:01:40,161-Speed 3886.87 samples/sec  Loss 4.4173  LearningRate 0.0050  ProxyLR: 0.2481  Epoch: 21  Global Step: 119760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:42,794-Speed 3890.69 samples/sec  Loss 4.5391  LearningRate 0.0050  ProxyLR: 0.2479  Epoch: 21  Global Step: 119770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:45,424-Speed 3893.56 samples/sec  Loss 4.4697  LearningRate 0.0050  ProxyLR: 0.2476  Epoch: 21  Global Step: 119780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:48,056-Speed 3891.85 samples/sec  Loss 4.4513  LearningRate 0.0049  ProxyLR: 0.2474  Epoch: 21  Global Step: 119790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:50,692-Speed 3885.84 samples/sec  Loss 4.4891  LearningRate 0.0049  ProxyLR: 0.2472  Epoch: 21  Global Step: 119800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:53,327-Speed 3886.99 samples/sec  Loss 4.4544  LearningRate 0.0049  ProxyLR: 0.2470  Epoch: 21  Global Step: 119810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:55,962-Speed 3887.57 samples/sec  Loss 4.5166  LearningRate 0.0049  ProxyLR: 0.2468  Epoch: 21  Global Step: 119820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:01:58,594-Speed 3891.49 samples/sec  Loss 4.4723  LearningRate 0.0049  ProxyLR: 0.2465  Epoch: 21  Global Step: 119830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:01,227-Speed 3890.20 samples/sec  Loss 4.4470  LearningRate 0.0049  ProxyLR: 0.2463  Epoch: 21  Global Step: 119840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:03,860-Speed 3890.46 samples/sec  Loss 4.5125  LearningRate 0.0049  ProxyLR: 0.2461  Epoch: 21  Global Step: 119850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:06,479-Speed 3910.22 samples/sec  Loss 4.5043  LearningRate 0.0049  ProxyLR: 0.2459  Epoch: 21  Global Step: 119860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:09,110-Speed 3894.08 samples/sec  Loss 4.4620  LearningRate 0.0049  ProxyLR: 0.2457  Epoch: 21  Global Step: 119870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:11,740-Speed 3894.03 samples/sec  Loss 4.4413  LearningRate 0.0049  ProxyLR: 0.2454  Epoch: 21  Global Step: 119880   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:14,373-Speed 3889.98 samples/sec  Loss 4.4528  LearningRate 0.0049  ProxyLR: 0.2452  Epoch: 21  Global Step: 119890   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:17,005-Speed 3892.16 samples/sec  Loss 4.4168  LearningRate 0.0049  ProxyLR: 0.2450  Epoch: 21  Global Step: 119900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:19,638-Speed 3889.74 samples/sec  Loss 4.5852  LearningRate 0.0049  ProxyLR: 0.2448  Epoch: 21  Global Step: 119910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:22,273-Speed 3887.44 samples/sec  Loss 4.4086  LearningRate 0.0049  ProxyLR: 0.2446  Epoch: 21  Global Step: 119920   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:24,904-Speed 3892.89 samples/sec  Loss 4.4038  LearningRate 0.0049  ProxyLR: 0.2443  Epoch: 21  Global Step: 119930   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:27,534-Speed 3894.59 samples/sec  Loss 4.5663  LearningRate 0.0049  ProxyLR: 0.2441  Epoch: 21  Global Step: 119940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:30,171-Speed 3884.11 samples/sec  Loss 4.3929  LearningRate 0.0049  ProxyLR: 0.2439  Epoch: 21  Global Step: 119950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:32,795-Speed 3904.03 samples/sec  Loss 4.4940  LearningRate 0.0049  ProxyLR: 0.2437  Epoch: 21  Global Step: 119960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:35,430-Speed 3886.32 samples/sec  Loss 4.5189  LearningRate 0.0049  ProxyLR: 0.2435  Epoch: 21  Global Step: 119970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:38,068-Speed 3883.36 samples/sec  Loss 4.4998  LearningRate 0.0049  ProxyLR: 0.2432  Epoch: 21  Global Step: 119980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:40,703-Speed 3886.65 samples/sec  Loss 4.4937  LearningRate 0.0049  ProxyLR: 0.2430  Epoch: 21  Global Step: 119990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:02:43,336-Speed 3889.36 samples/sec  Loss 4.4271  LearningRate 0.0049  ProxyLR: 0.2428  Epoch: 21  Global Step: 120000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:03:32,979-[lfw][120000]XNorm: 22.078096
Training: 2023-05-05 00:03:32,980-[lfw][120000]Accuracy-Flip: 0.99767+-0.00213
Training: 2023-05-05 00:03:32,980-[lfw][120000]Accuracy-Highest: 0.99783
Training: 2023-05-05 00:03:32,980-[lfw][120000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 00:03:32,980-[lfw][120000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 00:04:30,115-[cfp_fp][120000]XNorm: 21.800364
Training: 2023-05-05 00:04:30,115-[cfp_fp][120000]Accuracy-Flip: 0.98514+-0.00400
Training: 2023-05-05 00:04:30,115-[cfp_fp][120000]Accuracy-Highest: 0.98514
Training: 2023-05-05 00:04:30,115-[cfp_fp][120000]TPR@1stNon-Zero-FPR of 0.00029: 0.89371
Training: 2023-05-05 00:04:30,115-[cfp_fp][120000]Highest TPR@FPR: 0.89371
Training: 2023-05-05 00:05:19,804-[agedb_30][120000]XNorm: 22.263652
Training: 2023-05-05 00:05:19,804-[agedb_30][120000]Accuracy-Flip: 0.97433+-0.00850
Training: 2023-05-05 00:05:19,804-[agedb_30][120000]Accuracy-Highest: 0.97450
Training: 2023-05-05 00:05:19,805-[agedb_30][120000]TPR@1stNon-Zero-FPR of 0.00033: 0.87133
Training: 2023-05-05 00:05:19,805-[agedb_30][120000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 00:06:10,793-[calfw][120000]XNorm: 22.243352
Training: 2023-05-05 00:06:10,794-[calfw][120000]Accuracy-Flip: 0.95867+-0.01242
Training: 2023-05-05 00:06:10,794-[calfw][120000]Accuracy-Highest: 0.95867
Training: 2023-05-05 00:06:10,794-[calfw][120000]TPR@1stNon-Zero-FPR of 0.00033: 0.81033
Training: 2023-05-05 00:06:10,794-[calfw][120000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 00:07:01,780-[cplfw][120000]XNorm: 21.158804
Training: 2023-05-05 00:07:01,780-[cplfw][120000]Accuracy-Flip: 0.92917+-0.01223
Training: 2023-05-05 00:07:01,781-[cplfw][120000]Accuracy-Highest: 0.92917
Training: 2023-05-05 00:07:01,781-[cplfw][120000]TPR@1stNon-Zero-FPR of 0.00033: 0.05400
Training: 2023-05-05 00:07:01,781-[cplfw][120000]Highest TPR@FPR: 0.07633
Training: 2023-05-05 00:07:05,548-Speed 39.05 samples/sec  Loss 4.4795  LearningRate 0.0049  ProxyLR: 0.2426  Epoch: 21  Global Step: 120010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:08,174-Speed 3901.24 samples/sec  Loss 4.5300  LearningRate 0.0048  ProxyLR: 0.2424  Epoch: 21  Global Step: 120020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:10,798-Speed 3903.09 samples/sec  Loss 4.5004  LearningRate 0.0048  ProxyLR: 0.2421  Epoch: 21  Global Step: 120030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:13,423-Speed 3901.67 samples/sec  Loss 4.4148  LearningRate 0.0048  ProxyLR: 0.2419  Epoch: 21  Global Step: 120040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:16,050-Speed 3899.56 samples/sec  Loss 4.4785  LearningRate 0.0048  ProxyLR: 0.2417  Epoch: 21  Global Step: 120050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:18,664-Speed 3918.69 samples/sec  Loss 4.5210  LearningRate 0.0048  ProxyLR: 0.2415  Epoch: 21  Global Step: 120060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:21,297-Speed 3890.62 samples/sec  Loss 4.5191  LearningRate 0.0048  ProxyLR: 0.2413  Epoch: 21  Global Step: 120070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:23,930-Speed 3889.97 samples/sec  Loss 4.4642  LearningRate 0.0048  ProxyLR: 0.2411  Epoch: 21  Global Step: 120080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:26,561-Speed 3892.65 samples/sec  Loss 4.5166  LearningRate 0.0048  ProxyLR: 0.2408  Epoch: 21  Global Step: 120090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:29,193-Speed 3891.17 samples/sec  Loss 4.4861  LearningRate 0.0048  ProxyLR: 0.2406  Epoch: 21  Global Step: 120100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:31,825-Speed 3891.34 samples/sec  Loss 4.3778  LearningRate 0.0048  ProxyLR: 0.2404  Epoch: 21  Global Step: 120110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:34,458-Speed 3889.86 samples/sec  Loss 4.4176  LearningRate 0.0048  ProxyLR: 0.2402  Epoch: 21  Global Step: 120120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:37,092-Speed 3889.01 samples/sec  Loss 4.4900  LearningRate 0.0048  ProxyLR: 0.2400  Epoch: 21  Global Step: 120130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:39,726-Speed 3888.77 samples/sec  Loss 4.4373  LearningRate 0.0048  ProxyLR: 0.2397  Epoch: 21  Global Step: 120140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:42,360-Speed 3888.19 samples/sec  Loss 4.5169  LearningRate 0.0048  ProxyLR: 0.2395  Epoch: 21  Global Step: 120150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:44,996-Speed 3886.18 samples/sec  Loss 4.4879  LearningRate 0.0048  ProxyLR: 0.2393  Epoch: 21  Global Step: 120160   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-05 00:07:47,607-Speed 3922.06 samples/sec  Loss 4.4466  LearningRate 0.0048  ProxyLR: 0.2391  Epoch: 21  Global Step: 120170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:50,232-Speed 3902.58 samples/sec  Loss 4.4476  LearningRate 0.0048  ProxyLR: 0.2389  Epoch: 21  Global Step: 120180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:07:52,844-Speed 3922.06 samples/sec  Loss 4.4149  LearningRate 0.0048  ProxyLR: 0.2387  Epoch: 21  Global Step: 120190   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:07:55,468-Speed 3903.12 samples/sec  Loss 4.5263  LearningRate 0.0048  ProxyLR: 0.2384  Epoch: 21  Global Step: 120200   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:07:58,090-Speed 3905.48 samples/sec  Loss 4.4923  LearningRate 0.0048  ProxyLR: 0.2382  Epoch: 21  Global Step: 120210   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:00,714-Speed 3903.39 samples/sec  Loss 4.4076  LearningRate 0.0048  ProxyLR: 0.2380  Epoch: 21  Global Step: 120220   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:03,339-Speed 3901.55 samples/sec  Loss 4.4872  LearningRate 0.0048  ProxyLR: 0.2378  Epoch: 21  Global Step: 120230   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:05,963-Speed 3904.65 samples/sec  Loss 4.4384  LearningRate 0.0048  ProxyLR: 0.2376  Epoch: 21  Global Step: 120240   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:08,588-Speed 3901.44 samples/sec  Loss 4.4563  LearningRate 0.0047  ProxyLR: 0.2374  Epoch: 21  Global Step: 120250   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:11,212-Speed 3903.95 samples/sec  Loss 4.5485  LearningRate 0.0047  ProxyLR: 0.2371  Epoch: 21  Global Step: 120260   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:13,836-Speed 3903.03 samples/sec  Loss 4.4725  LearningRate 0.0047  ProxyLR: 0.2369  Epoch: 21  Global Step: 120270   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:16,458-Speed 3905.93 samples/sec  Loss 4.5184  LearningRate 0.0047  ProxyLR: 0.2367  Epoch: 21  Global Step: 120280   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:19,084-Speed 3900.53 samples/sec  Loss 4.3754  LearningRate 0.0047  ProxyLR: 0.2365  Epoch: 21  Global Step: 120290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:08:21,708-Speed 3903.27 samples/sec  Loss 4.5084  LearningRate 0.0047  ProxyLR: 0.2363  Epoch: 21  Global Step: 120300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:08:24,332-Speed 3903.66 samples/sec  Loss 4.5211  LearningRate 0.0047  ProxyLR: 0.2361  Epoch: 21  Global Step: 120310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:08:26,944-Speed 3922.46 samples/sec  Loss 4.4317  LearningRate 0.0047  ProxyLR: 0.2358  Epoch: 21  Global Step: 120320   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:29,567-Speed 3903.58 samples/sec  Loss 4.5211  LearningRate 0.0047  ProxyLR: 0.2356  Epoch: 21  Global Step: 120330   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:32,192-Speed 3902.37 samples/sec  Loss 4.4698  LearningRate 0.0047  ProxyLR: 0.2354  Epoch: 21  Global Step: 120340   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:34,817-Speed 3902.06 samples/sec  Loss 4.4639  LearningRate 0.0047  ProxyLR: 0.2352  Epoch: 21  Global Step: 120350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:37,440-Speed 3905.13 samples/sec  Loss 4.5050  LearningRate 0.0047  ProxyLR: 0.2350  Epoch: 21  Global Step: 120360   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:40,064-Speed 3902.59 samples/sec  Loss 4.4463  LearningRate 0.0047  ProxyLR: 0.2348  Epoch: 21  Global Step: 120370   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:42,688-Speed 3903.45 samples/sec  Loss 4.4670  LearningRate 0.0047  ProxyLR: 0.2345  Epoch: 21  Global Step: 120380   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:45,313-Speed 3903.04 samples/sec  Loss 4.4524  LearningRate 0.0047  ProxyLR: 0.2343  Epoch: 21  Global Step: 120390   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:47,938-Speed 3902.00 samples/sec  Loss 4.4333  LearningRate 0.0047  ProxyLR: 0.2341  Epoch: 21  Global Step: 120400   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:50,561-Speed 3904.61 samples/sec  Loss 4.4288  LearningRate 0.0047  ProxyLR: 0.2339  Epoch: 21  Global Step: 120410   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:08:53,186-Speed 3901.68 samples/sec  Loss 4.4601  LearningRate 0.0047  ProxyLR: 0.2337  Epoch: 21  Global Step: 120420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:08:55,812-Speed 3901.11 samples/sec  Loss 4.4890  LearningRate 0.0047  ProxyLR: 0.2335  Epoch: 21  Global Step: 120430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:08:58,438-Speed 3900.03 samples/sec  Loss 4.4356  LearningRate 0.0047  ProxyLR: 0.2333  Epoch: 21  Global Step: 120440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:01,065-Speed 3899.84 samples/sec  Loss 4.4664  LearningRate 0.0047  ProxyLR: 0.2330  Epoch: 21  Global Step: 120450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:03,691-Speed 3899.76 samples/sec  Loss 4.4586  LearningRate 0.0047  ProxyLR: 0.2328  Epoch: 21  Global Step: 120460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:06,320-Speed 3896.09 samples/sec  Loss 4.4563  LearningRate 0.0047  ProxyLR: 0.2326  Epoch: 21  Global Step: 120470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:08,950-Speed 3895.23 samples/sec  Loss 4.4709  LearningRate 0.0046  ProxyLR: 0.2324  Epoch: 21  Global Step: 120480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:11,578-Speed 3896.21 samples/sec  Loss 4.4433  LearningRate 0.0046  ProxyLR: 0.2322  Epoch: 21  Global Step: 120490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:14,210-Speed 3891.80 samples/sec  Loss 4.3818  LearningRate 0.0046  ProxyLR: 0.2320  Epoch: 21  Global Step: 120500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:16,840-Speed 3894.97 samples/sec  Loss 4.4641  LearningRate 0.0046  ProxyLR: 0.2318  Epoch: 21  Global Step: 120510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:19,457-Speed 3914.11 samples/sec  Loss 4.4885  LearningRate 0.0046  ProxyLR: 0.2315  Epoch: 21  Global Step: 120520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:22,087-Speed 3894.01 samples/sec  Loss 4.5342  LearningRate 0.0046  ProxyLR: 0.2313  Epoch: 21  Global Step: 120530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:24,716-Speed 3896.37 samples/sec  Loss 4.4866  LearningRate 0.0046  ProxyLR: 0.2311  Epoch: 21  Global Step: 120540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:27,345-Speed 3895.81 samples/sec  Loss 4.4149  LearningRate 0.0046  ProxyLR: 0.2309  Epoch: 21  Global Step: 120550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:29,976-Speed 3893.63 samples/sec  Loss 4.4652  LearningRate 0.0046  ProxyLR: 0.2307  Epoch: 21  Global Step: 120560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:32,603-Speed 3899.19 samples/sec  Loss 4.4421  LearningRate 0.0046  ProxyLR: 0.2305  Epoch: 21  Global Step: 120570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:35,232-Speed 3895.93 samples/sec  Loss 4.4180  LearningRate 0.0046  ProxyLR: 0.2303  Epoch: 21  Global Step: 120580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:37,859-Speed 3897.77 samples/sec  Loss 4.4751  LearningRate 0.0046  ProxyLR: 0.2300  Epoch: 21  Global Step: 120590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:40,488-Speed 3896.85 samples/sec  Loss 4.4098  LearningRate 0.0046  ProxyLR: 0.2298  Epoch: 21  Global Step: 120600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:43,117-Speed 3895.96 samples/sec  Loss 4.4074  LearningRate 0.0046  ProxyLR: 0.2296  Epoch: 21  Global Step: 120610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:45,735-Speed 3911.63 samples/sec  Loss 4.5356  LearningRate 0.0046  ProxyLR: 0.2294  Epoch: 21  Global Step: 120620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:48,366-Speed 3893.82 samples/sec  Loss 4.3775  LearningRate 0.0046  ProxyLR: 0.2292  Epoch: 21  Global Step: 120630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:50,994-Speed 3896.63 samples/sec  Loss 4.5465  LearningRate 0.0046  ProxyLR: 0.2290  Epoch: 21  Global Step: 120640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:53,629-Speed 3888.21 samples/sec  Loss 4.4261  LearningRate 0.0046  ProxyLR: 0.2288  Epoch: 21  Global Step: 120650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:56,259-Speed 3894.87 samples/sec  Loss 4.4172  LearningRate 0.0046  ProxyLR: 0.2285  Epoch: 21  Global Step: 120660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:09:58,888-Speed 3895.67 samples/sec  Loss 4.4920  LearningRate 0.0046  ProxyLR: 0.2283  Epoch: 21  Global Step: 120670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:01,519-Speed 3893.41 samples/sec  Loss 4.4308  LearningRate 0.0046  ProxyLR: 0.2281  Epoch: 21  Global Step: 120680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:04,147-Speed 3896.64 samples/sec  Loss 4.3930  LearningRate 0.0046  ProxyLR: 0.2279  Epoch: 21  Global Step: 120690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:06,773-Speed 3900.34 samples/sec  Loss 4.3831  LearningRate 0.0046  ProxyLR: 0.2277  Epoch: 21  Global Step: 120700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:09,399-Speed 3900.14 samples/sec  Loss 4.4490  LearningRate 0.0045  ProxyLR: 0.2275  Epoch: 21  Global Step: 120710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:12,014-Speed 3916.97 samples/sec  Loss 4.4649  LearningRate 0.0045  ProxyLR: 0.2273  Epoch: 21  Global Step: 120720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:14,640-Speed 3900.07 samples/sec  Loss 4.4114  LearningRate 0.0045  ProxyLR: 0.2271  Epoch: 21  Global Step: 120730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:17,267-Speed 3899.45 samples/sec  Loss 4.4141  LearningRate 0.0045  ProxyLR: 0.2269  Epoch: 21  Global Step: 120740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:19,894-Speed 3898.44 samples/sec  Loss 4.4191  LearningRate 0.0045  ProxyLR: 0.2266  Epoch: 21  Global Step: 120750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:22,521-Speed 3899.61 samples/sec  Loss 4.3860  LearningRate 0.0045  ProxyLR: 0.2264  Epoch: 21  Global Step: 120760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:25,147-Speed 3899.80 samples/sec  Loss 4.2923  LearningRate 0.0045  ProxyLR: 0.2262  Epoch: 21  Global Step: 120770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:27,775-Speed 3898.07 samples/sec  Loss 4.4967  LearningRate 0.0045  ProxyLR: 0.2260  Epoch: 21  Global Step: 120780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:30,402-Speed 3899.67 samples/sec  Loss 4.4592  LearningRate 0.0045  ProxyLR: 0.2258  Epoch: 21  Global Step: 120790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:33,031-Speed 3895.13 samples/sec  Loss 4.4885  LearningRate 0.0045  ProxyLR: 0.2256  Epoch: 21  Global Step: 120800   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:35,657-Speed 3900.42 samples/sec  Loss 4.4734  LearningRate 0.0045  ProxyLR: 0.2254  Epoch: 21  Global Step: 120810   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:38,271-Speed 3919.63 samples/sec  Loss 4.4547  LearningRate 0.0045  ProxyLR: 0.2252  Epoch: 21  Global Step: 120820   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:40,897-Speed 3899.89 samples/sec  Loss 4.3826  LearningRate 0.0045  ProxyLR: 0.2249  Epoch: 21  Global Step: 120830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:43,523-Speed 3900.48 samples/sec  Loss 4.3170  LearningRate 0.0045  ProxyLR: 0.2247  Epoch: 21  Global Step: 120840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:46,149-Speed 3900.45 samples/sec  Loss 4.3760  LearningRate 0.0045  ProxyLR: 0.2245  Epoch: 21  Global Step: 120850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:48,777-Speed 3897.94 samples/sec  Loss 4.3884  LearningRate 0.0045  ProxyLR: 0.2243  Epoch: 21  Global Step: 120860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:51,403-Speed 3900.36 samples/sec  Loss 4.4026  LearningRate 0.0045  ProxyLR: 0.2241  Epoch: 21  Global Step: 120870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:54,030-Speed 3899.34 samples/sec  Loss 4.4506  LearningRate 0.0045  ProxyLR: 0.2239  Epoch: 21  Global Step: 120880   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:56,656-Speed 3899.15 samples/sec  Loss 4.3946  LearningRate 0.0045  ProxyLR: 0.2237  Epoch: 21  Global Step: 120890   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:10:59,282-Speed 3901.58 samples/sec  Loss 4.4808  LearningRate 0.0045  ProxyLR: 0.2235  Epoch: 21  Global Step: 120900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:01,907-Speed 3900.84 samples/sec  Loss 4.5096  LearningRate 0.0045  ProxyLR: 0.2233  Epoch: 21  Global Step: 120910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:04,535-Speed 3898.63 samples/sec  Loss 4.4936  LearningRate 0.0045  ProxyLR: 0.2231  Epoch: 21  Global Step: 120920   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-05 00:11:07,148-Speed 3919.43 samples/sec  Loss 4.3789  LearningRate 0.0045  ProxyLR: 0.2228  Epoch: 21  Global Step: 120930   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:09,773-Speed 3901.79 samples/sec  Loss 4.3467  LearningRate 0.0045  ProxyLR: 0.2226  Epoch: 21  Global Step: 120940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:12,400-Speed 3898.73 samples/sec  Loss 4.4087  LearningRate 0.0044  ProxyLR: 0.2224  Epoch: 21  Global Step: 120950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:15,026-Speed 3900.80 samples/sec  Loss 4.3775  LearningRate 0.0044  ProxyLR: 0.2222  Epoch: 21  Global Step: 120960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:17,652-Speed 3900.46 samples/sec  Loss 4.3830  LearningRate 0.0044  ProxyLR: 0.2220  Epoch: 21  Global Step: 120970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:20,264-Speed 3921.60 samples/sec  Loss 4.4809  LearningRate 0.0044  ProxyLR: 0.2218  Epoch: 21  Global Step: 120980   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:22,890-Speed 3900.10 samples/sec  Loss 4.4180  LearningRate 0.0044  ProxyLR: 0.2216  Epoch: 21  Global Step: 120990   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:25,516-Speed 3900.58 samples/sec  Loss 4.4555  LearningRate 0.0044  ProxyLR: 0.2214  Epoch: 21  Global Step: 121000   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:28,143-Speed 3898.65 samples/sec  Loss 4.4326  LearningRate 0.0044  ProxyLR: 0.2212  Epoch: 21  Global Step: 121010   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:30,770-Speed 3898.78 samples/sec  Loss 4.4270  LearningRate 0.0044  ProxyLR: 0.2210  Epoch: 21  Global Step: 121020   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:33,398-Speed 3897.37 samples/sec  Loss 4.3820  LearningRate 0.0044  ProxyLR: 0.2207  Epoch: 21  Global Step: 121030   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:36,023-Speed 3902.55 samples/sec  Loss 4.3979  LearningRate 0.0044  ProxyLR: 0.2205  Epoch: 21  Global Step: 121040   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:38,649-Speed 3900.05 samples/sec  Loss 4.4057  LearningRate 0.0044  ProxyLR: 0.2203  Epoch: 21  Global Step: 121050   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:41,275-Speed 3900.78 samples/sec  Loss 4.4129  LearningRate 0.0044  ProxyLR: 0.2201  Epoch: 21  Global Step: 121060   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:43,901-Speed 3899.94 samples/sec  Loss 4.4044  LearningRate 0.0044  ProxyLR: 0.2199  Epoch: 21  Global Step: 121070   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:11:46,528-Speed 3899.33 samples/sec  Loss 4.4153  LearningRate 0.0044  ProxyLR: 0.2197  Epoch: 21  Global Step: 121080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:49,155-Speed 3899.78 samples/sec  Loss 4.4741  LearningRate 0.0044  ProxyLR: 0.2195  Epoch: 21  Global Step: 121090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:51,783-Speed 3897.03 samples/sec  Loss 4.4878  LearningRate 0.0044  ProxyLR: 0.2193  Epoch: 21  Global Step: 121100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:54,408-Speed 3901.32 samples/sec  Loss 4.4126  LearningRate 0.0044  ProxyLR: 0.2191  Epoch: 21  Global Step: 121110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:57,036-Speed 3897.34 samples/sec  Loss 4.4132  LearningRate 0.0044  ProxyLR: 0.2189  Epoch: 21  Global Step: 121120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:11:59,667-Speed 3893.60 samples/sec  Loss 4.3996  LearningRate 0.0044  ProxyLR: 0.2187  Epoch: 21  Global Step: 121130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:02,296-Speed 3896.73 samples/sec  Loss 4.4172  LearningRate 0.0044  ProxyLR: 0.2185  Epoch: 21  Global Step: 121140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:04,925-Speed 3895.12 samples/sec  Loss 4.4496  LearningRate 0.0044  ProxyLR: 0.2182  Epoch: 21  Global Step: 121150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:07,555-Speed 3894.64 samples/sec  Loss 4.3768  LearningRate 0.0044  ProxyLR: 0.2180  Epoch: 21  Global Step: 121160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:10,184-Speed 3895.54 samples/sec  Loss 4.4421  LearningRate 0.0044  ProxyLR: 0.2178  Epoch: 21  Global Step: 121170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:12,802-Speed 3912.40 samples/sec  Loss 4.4272  LearningRate 0.0044  ProxyLR: 0.2176  Epoch: 21  Global Step: 121180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:15,438-Speed 3885.35 samples/sec  Loss 4.3839  LearningRate 0.0043  ProxyLR: 0.2174  Epoch: 21  Global Step: 121190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:18,075-Speed 3884.80 samples/sec  Loss 4.4334  LearningRate 0.0043  ProxyLR: 0.2172  Epoch: 21  Global Step: 121200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:20,710-Speed 3887.65 samples/sec  Loss 4.4380  LearningRate 0.0043  ProxyLR: 0.2170  Epoch: 21  Global Step: 121210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:23,348-Speed 3883.56 samples/sec  Loss 4.3732  LearningRate 0.0043  ProxyLR: 0.2168  Epoch: 21  Global Step: 121220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:25,985-Speed 3884.16 samples/sec  Loss 4.3750  LearningRate 0.0043  ProxyLR: 0.2166  Epoch: 21  Global Step: 121230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:28,622-Speed 3882.84 samples/sec  Loss 4.3989  LearningRate 0.0043  ProxyLR: 0.2164  Epoch: 21  Global Step: 121240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:31,258-Speed 3885.94 samples/sec  Loss 4.4317  LearningRate 0.0043  ProxyLR: 0.2162  Epoch: 21  Global Step: 121250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:33,894-Speed 3885.73 samples/sec  Loss 4.3823  LearningRate 0.0043  ProxyLR: 0.2160  Epoch: 21  Global Step: 121260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:36,530-Speed 3885.57 samples/sec  Loss 4.3434  LearningRate 0.0043  ProxyLR: 0.2158  Epoch: 21  Global Step: 121270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:39,153-Speed 3905.36 samples/sec  Loss 4.4768  LearningRate 0.0043  ProxyLR: 0.2156  Epoch: 21  Global Step: 121280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:41,786-Speed 3890.38 samples/sec  Loss 4.3854  LearningRate 0.0043  ProxyLR: 0.2153  Epoch: 21  Global Step: 121290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:44,421-Speed 3885.87 samples/sec  Loss 4.4071  LearningRate 0.0043  ProxyLR: 0.2151  Epoch: 21  Global Step: 121300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:47,056-Speed 3887.20 samples/sec  Loss 4.4029  LearningRate 0.0043  ProxyLR: 0.2149  Epoch: 21  Global Step: 121310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:12:49,674-Speed 3913.68 samples/sec  Loss 4.4255  LearningRate 0.0043  ProxyLR: 0.2147  Epoch: 21  Global Step: 121320   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:12:52,305-Speed 3892.59 samples/sec  Loss 4.4176  LearningRate 0.0043  ProxyLR: 0.2145  Epoch: 21  Global Step: 121330   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:12:54,935-Speed 3894.36 samples/sec  Loss 4.4361  LearningRate 0.0043  ProxyLR: 0.2143  Epoch: 21  Global Step: 121340   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:12:57,564-Speed 3895.45 samples/sec  Loss 4.3477  LearningRate 0.0043  ProxyLR: 0.2141  Epoch: 21  Global Step: 121350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:00,194-Speed 3895.75 samples/sec  Loss 4.4444  LearningRate 0.0043  ProxyLR: 0.2139  Epoch: 21  Global Step: 121360   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:02,825-Speed 3892.31 samples/sec  Loss 4.4646  LearningRate 0.0043  ProxyLR: 0.2137  Epoch: 21  Global Step: 121370   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:05,455-Speed 3894.84 samples/sec  Loss 4.4528  LearningRate 0.0043  ProxyLR: 0.2135  Epoch: 21  Global Step: 121380   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:08,086-Speed 3892.55 samples/sec  Loss 4.4073  LearningRate 0.0043  ProxyLR: 0.2133  Epoch: 21  Global Step: 121390   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:10,717-Speed 3893.97 samples/sec  Loss 4.3962  LearningRate 0.0043  ProxyLR: 0.2131  Epoch: 21  Global Step: 121400   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:13,349-Speed 3891.26 samples/sec  Loss 4.4806  LearningRate 0.0043  ProxyLR: 0.2129  Epoch: 21  Global Step: 121410   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:13:15,977-Speed 3897.52 samples/sec  Loss 4.3979  LearningRate 0.0043  ProxyLR: 0.2127  Epoch: 21  Global Step: 121420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:18,606-Speed 3896.16 samples/sec  Loss 4.3211  LearningRate 0.0042  ProxyLR: 0.2125  Epoch: 21  Global Step: 121430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:21,235-Speed 3895.02 samples/sec  Loss 4.3391  LearningRate 0.0042  ProxyLR: 0.2123  Epoch: 21  Global Step: 121440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:23,866-Speed 3893.58 samples/sec  Loss 4.3443  LearningRate 0.0042  ProxyLR: 0.2121  Epoch: 21  Global Step: 121450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:26,495-Speed 3896.86 samples/sec  Loss 4.3657  LearningRate 0.0042  ProxyLR: 0.2118  Epoch: 21  Global Step: 121460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:29,126-Speed 3892.67 samples/sec  Loss 4.3818  LearningRate 0.0042  ProxyLR: 0.2116  Epoch: 21  Global Step: 121470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:31,756-Speed 3893.90 samples/sec  Loss 4.3827  LearningRate 0.0042  ProxyLR: 0.2114  Epoch: 21  Global Step: 121480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:34,388-Speed 3892.34 samples/sec  Loss 4.3323  LearningRate 0.0042  ProxyLR: 0.2112  Epoch: 21  Global Step: 121490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:37,016-Speed 3897.29 samples/sec  Loss 4.4089  LearningRate 0.0042  ProxyLR: 0.2110  Epoch: 21  Global Step: 121500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:39,646-Speed 3893.70 samples/sec  Loss 4.4038  LearningRate 0.0042  ProxyLR: 0.2108  Epoch: 21  Global Step: 121510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:42,265-Speed 3911.87 samples/sec  Loss 4.3724  LearningRate 0.0042  ProxyLR: 0.2106  Epoch: 21  Global Step: 121520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:44,894-Speed 3895.87 samples/sec  Loss 4.4381  LearningRate 0.0042  ProxyLR: 0.2104  Epoch: 21  Global Step: 121530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:47,524-Speed 3893.83 samples/sec  Loss 4.4586  LearningRate 0.0042  ProxyLR: 0.2102  Epoch: 21  Global Step: 121540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:50,156-Speed 3892.03 samples/sec  Loss 4.4803  LearningRate 0.0042  ProxyLR: 0.2100  Epoch: 21  Global Step: 121550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:52,788-Speed 3891.23 samples/sec  Loss 4.3613  LearningRate 0.0042  ProxyLR: 0.2098  Epoch: 21  Global Step: 121560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:55,418-Speed 3894.14 samples/sec  Loss 4.3170  LearningRate 0.0042  ProxyLR: 0.2096  Epoch: 21  Global Step: 121570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:13:58,048-Speed 3894.21 samples/sec  Loss 4.3384  LearningRate 0.0042  ProxyLR: 0.2094  Epoch: 21  Global Step: 121580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:00,680-Speed 3891.92 samples/sec  Loss 4.4534  LearningRate 0.0042  ProxyLR: 0.2092  Epoch: 21  Global Step: 121590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:03,310-Speed 3894.69 samples/sec  Loss 4.4001  LearningRate 0.0042  ProxyLR: 0.2090  Epoch: 21  Global Step: 121600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:05,941-Speed 3893.08 samples/sec  Loss 4.3646  LearningRate 0.0042  ProxyLR: 0.2088  Epoch: 21  Global Step: 121610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:08,557-Speed 3915.42 samples/sec  Loss 4.3497  LearningRate 0.0042  ProxyLR: 0.2086  Epoch: 21  Global Step: 121620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:11,174-Speed 3913.79 samples/sec  Loss 4.4657  LearningRate 0.0042  ProxyLR: 0.2084  Epoch: 21  Global Step: 121630   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:13,803-Speed 3896.01 samples/sec  Loss 4.3562  LearningRate 0.0042  ProxyLR: 0.2082  Epoch: 21  Global Step: 121640   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:16,432-Speed 3896.58 samples/sec  Loss 4.3602  LearningRate 0.0042  ProxyLR: 0.2080  Epoch: 21  Global Step: 121650   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:19,061-Speed 3895.57 samples/sec  Loss 4.3778  LearningRate 0.0042  ProxyLR: 0.2078  Epoch: 21  Global Step: 121660   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:21,691-Speed 3894.45 samples/sec  Loss 4.3889  LearningRate 0.0042  ProxyLR: 0.2076  Epoch: 21  Global Step: 121670   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:24,322-Speed 3892.96 samples/sec  Loss 4.3716  LearningRate 0.0041  ProxyLR: 0.2074  Epoch: 21  Global Step: 121680   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:26,953-Speed 3893.76 samples/sec  Loss 4.4245  LearningRate 0.0041  ProxyLR: 0.2072  Epoch: 21  Global Step: 121690   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:29,581-Speed 3896.88 samples/sec  Loss 4.3628  LearningRate 0.0041  ProxyLR: 0.2070  Epoch: 21  Global Step: 121700   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:32,209-Speed 3897.26 samples/sec  Loss 4.3072  LearningRate 0.0041  ProxyLR: 0.2068  Epoch: 21  Global Step: 121710   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:34,836-Speed 3898.88 samples/sec  Loss 4.3594  LearningRate 0.0041  ProxyLR: 0.2066  Epoch: 21  Global Step: 121720   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:37,464-Speed 3898.63 samples/sec  Loss 4.3963  LearningRate 0.0041  ProxyLR: 0.2064  Epoch: 21  Global Step: 121730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:40,091-Speed 3897.88 samples/sec  Loss 4.3336  LearningRate 0.0041  ProxyLR: 0.2062  Epoch: 21  Global Step: 121740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:42,718-Speed 3898.88 samples/sec  Loss 4.4730  LearningRate 0.0041  ProxyLR: 0.2060  Epoch: 21  Global Step: 121750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:45,347-Speed 3897.23 samples/sec  Loss 4.3025  LearningRate 0.0041  ProxyLR: 0.2058  Epoch: 21  Global Step: 121760   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:47,975-Speed 3896.38 samples/sec  Loss 4.3807  LearningRate 0.0041  ProxyLR: 0.2055  Epoch: 21  Global Step: 121770   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:50,603-Speed 3897.67 samples/sec  Loss 4.3365  LearningRate 0.0041  ProxyLR: 0.2053  Epoch: 21  Global Step: 121780   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:53,231-Speed 3898.14 samples/sec  Loss 4.3824  LearningRate 0.0041  ProxyLR: 0.2051  Epoch: 21  Global Step: 121790   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:14:55,847-Speed 3915.85 samples/sec  Loss 4.4429  LearningRate 0.0041  ProxyLR: 0.2049  Epoch: 21  Global Step: 121800   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:14:58,475-Speed 3897.26 samples/sec  Loss 4.2978  LearningRate 0.0041  ProxyLR: 0.2047  Epoch: 21  Global Step: 121810   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:01,104-Speed 3896.27 samples/sec  Loss 4.4093  LearningRate 0.0041  ProxyLR: 0.2045  Epoch: 21  Global Step: 121820   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:03,732-Speed 3896.64 samples/sec  Loss 4.4001  LearningRate 0.0041  ProxyLR: 0.2043  Epoch: 21  Global Step: 121830   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:06,360-Speed 3897.54 samples/sec  Loss 4.3427  LearningRate 0.0041  ProxyLR: 0.2041  Epoch: 21  Global Step: 121840   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:08,988-Speed 3897.08 samples/sec  Loss 4.4069  LearningRate 0.0041  ProxyLR: 0.2039  Epoch: 21  Global Step: 121850   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:11,617-Speed 3896.37 samples/sec  Loss 4.3554  LearningRate 0.0041  ProxyLR: 0.2037  Epoch: 21  Global Step: 121860   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:14,246-Speed 3896.04 samples/sec  Loss 4.3803  LearningRate 0.0041  ProxyLR: 0.2035  Epoch: 21  Global Step: 121870   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:16,873-Speed 3898.33 samples/sec  Loss 4.3330  LearningRate 0.0041  ProxyLR: 0.2033  Epoch: 21  Global Step: 121880   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:19,501-Speed 3898.44 samples/sec  Loss 4.3781  LearningRate 0.0041  ProxyLR: 0.2031  Epoch: 21  Global Step: 121890   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:22,130-Speed 3895.10 samples/sec  Loss 4.3294  LearningRate 0.0041  ProxyLR: 0.2029  Epoch: 21  Global Step: 121900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:24,763-Speed 3890.52 samples/sec  Loss 4.3973  LearningRate 0.0041  ProxyLR: 0.2027  Epoch: 21  Global Step: 121910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:27,392-Speed 3896.41 samples/sec  Loss 4.4068  LearningRate 0.0041  ProxyLR: 0.2025  Epoch: 21  Global Step: 121920   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:30,020-Speed 3897.60 samples/sec  Loss 4.4026  LearningRate 0.0040  ProxyLR: 0.2023  Epoch: 21  Global Step: 121930   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:32,647-Speed 3898.80 samples/sec  Loss 4.2958  LearningRate 0.0040  ProxyLR: 0.2021  Epoch: 21  Global Step: 121940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:35,276-Speed 3895.75 samples/sec  Loss 4.2590  LearningRate 0.0040  ProxyLR: 0.2019  Epoch: 21  Global Step: 121950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:37,902-Speed 3899.47 samples/sec  Loss 4.2830  LearningRate 0.0040  ProxyLR: 0.2017  Epoch: 21  Global Step: 121960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:15:40,518-Speed 3916.52 samples/sec  Loss 4.3227  LearningRate 0.0040  ProxyLR: 0.2015  Epoch: 21  Global Step: 121970   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:43,145-Speed 3898.09 samples/sec  Loss 4.4086  LearningRate 0.0040  ProxyLR: 0.2013  Epoch: 21  Global Step: 121980   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:45,774-Speed 3897.10 samples/sec  Loss 4.2786  LearningRate 0.0040  ProxyLR: 0.2011  Epoch: 21  Global Step: 121990   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:15:48,402-Speed 3897.52 samples/sec  Loss 4.4040  LearningRate 0.0040  ProxyLR: 0.2009  Epoch: 21  Global Step: 122000   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:16:37,997-[lfw][122000]XNorm: 21.917847
Training: 2023-05-05 00:16:37,997-[lfw][122000]Accuracy-Flip: 0.99767+-0.00238
Training: 2023-05-05 00:16:37,998-[lfw][122000]Accuracy-Highest: 0.99783
Training: 2023-05-05 00:16:37,998-[lfw][122000]TPR@1stNon-Zero-FPR of 0.00033: 0.99567
Training: 2023-05-05 00:16:37,998-[lfw][122000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 00:17:34,724-[cfp_fp][122000]XNorm: 21.666030
Training: 2023-05-05 00:17:34,724-[cfp_fp][122000]Accuracy-Flip: 0.98429+-0.00655
Training: 2023-05-05 00:17:34,724-[cfp_fp][122000]Accuracy-Highest: 0.98514
Training: 2023-05-05 00:17:34,725-[cfp_fp][122000]TPR@1stNon-Zero-FPR of 0.00029: 0.90657
Training: 2023-05-05 00:17:34,725-[cfp_fp][122000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 00:18:24,059-[agedb_30][122000]XNorm: 22.142913
Training: 2023-05-05 00:18:24,059-[agedb_30][122000]Accuracy-Flip: 0.97417+-0.00917
Training: 2023-05-05 00:18:24,059-[agedb_30][122000]Accuracy-Highest: 0.97450
Training: 2023-05-05 00:18:24,060-[agedb_30][122000]TPR@1stNon-Zero-FPR of 0.00033: 0.87167
Training: 2023-05-05 00:18:24,060-[agedb_30][122000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 00:19:14,807-[calfw][122000]XNorm: 22.102197
Training: 2023-05-05 00:19:14,807-[calfw][122000]Accuracy-Flip: 0.95583+-0.01193
Training: 2023-05-05 00:19:14,808-[calfw][122000]Accuracy-Highest: 0.95867
Training: 2023-05-05 00:19:14,808-[calfw][122000]TPR@1stNon-Zero-FPR of 0.00033: 0.82533
Training: 2023-05-05 00:19:14,808-[calfw][122000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 00:20:05,486-[cplfw][122000]XNorm: 21.028101
Training: 2023-05-05 00:20:05,486-[cplfw][122000]Accuracy-Flip: 0.92750+-0.01409
Training: 2023-05-05 00:20:05,487-[cplfw][122000]Accuracy-Highest: 0.92917
Training: 2023-05-05 00:20:05,487-[cplfw][122000]TPR@1stNon-Zero-FPR of 0.00033: 0.06833
Training: 2023-05-05 00:20:05,487-[cplfw][122000]Highest TPR@FPR: 0.07633
Training: 2023-05-05 00:20:08,941-Speed 39.30 samples/sec  Loss 4.3389  LearningRate 0.0040  ProxyLR: 0.2007  Epoch: 21  Global Step: 122010   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:11,560-Speed 3910.97 samples/sec  Loss 4.3977  LearningRate 0.0040  ProxyLR: 0.2005  Epoch: 21  Global Step: 122020   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:14,179-Speed 3911.39 samples/sec  Loss 4.3978  LearningRate 0.0040  ProxyLR: 0.2003  Epoch: 21  Global Step: 122030   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:16,798-Speed 3909.84 samples/sec  Loss 4.3264  LearningRate 0.0040  ProxyLR: 0.2001  Epoch: 21  Global Step: 122040   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:19,418-Speed 3909.66 samples/sec  Loss 4.3307  LearningRate 0.0040  ProxyLR: 0.1999  Epoch: 21  Global Step: 122050   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:22,037-Speed 3910.11 samples/sec  Loss 4.3070  LearningRate 0.0040  ProxyLR: 0.1997  Epoch: 21  Global Step: 122060   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:24,659-Speed 3909.07 samples/sec  Loss 4.4134  LearningRate 0.0040  ProxyLR: 0.1995  Epoch: 21  Global Step: 122070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:20:27,280-Speed 3908.01 samples/sec  Loss 4.3244  LearningRate 0.0040  ProxyLR: 0.1993  Epoch: 21  Global Step: 122080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:20:29,903-Speed 3904.96 samples/sec  Loss 4.3225  LearningRate 0.0040  ProxyLR: 0.1991  Epoch: 21  Global Step: 122090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:20:32,526-Speed 3904.81 samples/sec  Loss 4.3646  LearningRate 0.0040  ProxyLR: 0.1989  Epoch: 21  Global Step: 122100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:20:35,150-Speed 3903.04 samples/sec  Loss 4.3213  LearningRate 0.0040  ProxyLR: 0.1987  Epoch: 21  Global Step: 122110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:20:37,776-Speed 3901.23 samples/sec  Loss 4.3269  LearningRate 0.0040  ProxyLR: 0.1985  Epoch: 21  Global Step: 122120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:20:40,386-Speed 3923.57 samples/sec  Loss 4.3984  LearningRate 0.0040  ProxyLR: 0.1984  Epoch: 21  Global Step: 122130   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:43,011-Speed 3902.65 samples/sec  Loss 4.3185  LearningRate 0.0040  ProxyLR: 0.1982  Epoch: 21  Global Step: 122140   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:45,636-Speed 3901.94 samples/sec  Loss 4.3028  LearningRate 0.0040  ProxyLR: 0.1980  Epoch: 21  Global Step: 122150   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:48,261-Speed 3901.22 samples/sec  Loss 4.3137  LearningRate 0.0040  ProxyLR: 0.1978  Epoch: 21  Global Step: 122160   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:50,888-Speed 3898.76 samples/sec  Loss 4.3822  LearningRate 0.0040  ProxyLR: 0.1976  Epoch: 21  Global Step: 122170   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:53,517-Speed 3896.16 samples/sec  Loss 4.2931  LearningRate 0.0039  ProxyLR: 0.1974  Epoch: 21  Global Step: 122180   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:56,146-Speed 3896.75 samples/sec  Loss 4.4222  LearningRate 0.0039  ProxyLR: 0.1972  Epoch: 21  Global Step: 122190   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:20:58,775-Speed 3896.65 samples/sec  Loss 4.2542  LearningRate 0.0039  ProxyLR: 0.1970  Epoch: 21  Global Step: 122200   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:01,402-Speed 3898.83 samples/sec  Loss 4.3679  LearningRate 0.0039  ProxyLR: 0.1968  Epoch: 21  Global Step: 122210   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:04,029-Speed 3898.58 samples/sec  Loss 4.2738  LearningRate 0.0039  ProxyLR: 0.1966  Epoch: 21  Global Step: 122220   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:06,660-Speed 3893.39 samples/sec  Loss 4.2529  LearningRate 0.0039  ProxyLR: 0.1964  Epoch: 21  Global Step: 122230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:09,289-Speed 3895.80 samples/sec  Loss 4.3233  LearningRate 0.0039  ProxyLR: 0.1962  Epoch: 21  Global Step: 122240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:11,918-Speed 3895.49 samples/sec  Loss 4.3322  LearningRate 0.0039  ProxyLR: 0.1960  Epoch: 21  Global Step: 122250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:14,548-Speed 3895.16 samples/sec  Loss 4.2872  LearningRate 0.0039  ProxyLR: 0.1958  Epoch: 21  Global Step: 122260   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:17,176-Speed 3896.34 samples/sec  Loss 4.3547  LearningRate 0.0039  ProxyLR: 0.1956  Epoch: 21  Global Step: 122270   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:19,806-Speed 3894.23 samples/sec  Loss 4.3434  LearningRate 0.0039  ProxyLR: 0.1954  Epoch: 21  Global Step: 122280   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:22,436-Speed 3895.32 samples/sec  Loss 4.3235  LearningRate 0.0039  ProxyLR: 0.1952  Epoch: 21  Global Step: 122290   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:25,066-Speed 3894.29 samples/sec  Loss 4.3286  LearningRate 0.0039  ProxyLR: 0.1950  Epoch: 21  Global Step: 122300   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:27,695-Speed 3896.86 samples/sec  Loss 4.3365  LearningRate 0.0039  ProxyLR: 0.1948  Epoch: 21  Global Step: 122310   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:30,322-Speed 3898.91 samples/sec  Loss 4.3872  LearningRate 0.0039  ProxyLR: 0.1946  Epoch: 21  Global Step: 122320   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:32,936-Speed 3918.06 samples/sec  Loss 4.2703  LearningRate 0.0039  ProxyLR: 0.1944  Epoch: 21  Global Step: 122330   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:35,563-Speed 3898.29 samples/sec  Loss 4.2983  LearningRate 0.0039  ProxyLR: 0.1942  Epoch: 21  Global Step: 122340   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:21:38,176-Speed 3919.90 samples/sec  Loss 4.3606  LearningRate 0.0039  ProxyLR: 0.1940  Epoch: 21  Global Step: 122350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:40,805-Speed 3896.68 samples/sec  Loss 4.3789  LearningRate 0.0039  ProxyLR: 0.1938  Epoch: 21  Global Step: 122360   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:43,436-Speed 3892.62 samples/sec  Loss 4.3736  LearningRate 0.0039  ProxyLR: 0.1936  Epoch: 21  Global Step: 122370   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:46,066-Speed 3895.39 samples/sec  Loss 4.2766  LearningRate 0.0039  ProxyLR: 0.1934  Epoch: 21  Global Step: 122380   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:48,695-Speed 3894.78 samples/sec  Loss 4.3723  LearningRate 0.0039  ProxyLR: 0.1932  Epoch: 21  Global Step: 122390   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:51,325-Speed 3895.27 samples/sec  Loss 4.3383  LearningRate 0.0039  ProxyLR: 0.1930  Epoch: 21  Global Step: 122400   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:53,955-Speed 3894.06 samples/sec  Loss 4.3614  LearningRate 0.0039  ProxyLR: 0.1928  Epoch: 21  Global Step: 122410   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:56,584-Speed 3896.46 samples/sec  Loss 4.3804  LearningRate 0.0039  ProxyLR: 0.1926  Epoch: 21  Global Step: 122420   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:21:59,215-Speed 3892.85 samples/sec  Loss 4.2676  LearningRate 0.0038  ProxyLR: 0.1925  Epoch: 21  Global Step: 122430   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:22:01,847-Speed 3891.86 samples/sec  Loss 4.2902  LearningRate 0.0038  ProxyLR: 0.1923  Epoch: 21  Global Step: 122440   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:22:04,477-Speed 3894.99 samples/sec  Loss 4.3464  LearningRate 0.0038  ProxyLR: 0.1921  Epoch: 21  Global Step: 122450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:07,110-Speed 3889.98 samples/sec  Loss 4.2930  LearningRate 0.0038  ProxyLR: 0.1919  Epoch: 21  Global Step: 122460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:09,741-Speed 3893.16 samples/sec  Loss 4.2738  LearningRate 0.0038  ProxyLR: 0.1917  Epoch: 21  Global Step: 122470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:12,373-Speed 3891.57 samples/sec  Loss 4.3730  LearningRate 0.0038  ProxyLR: 0.1915  Epoch: 21  Global Step: 122480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:15,006-Speed 3891.15 samples/sec  Loss 4.3340  LearningRate 0.0038  ProxyLR: 0.1913  Epoch: 21  Global Step: 122490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:17,638-Speed 3890.92 samples/sec  Loss 4.3237  LearningRate 0.0038  ProxyLR: 0.1911  Epoch: 21  Global Step: 122500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:20,273-Speed 3886.88 samples/sec  Loss 4.3084  LearningRate 0.0038  ProxyLR: 0.1909  Epoch: 21  Global Step: 122510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:22,906-Speed 3889.78 samples/sec  Loss 4.3934  LearningRate 0.0038  ProxyLR: 0.1907  Epoch: 21  Global Step: 122520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:25,538-Speed 3891.61 samples/sec  Loss 4.3479  LearningRate 0.0038  ProxyLR: 0.1905  Epoch: 21  Global Step: 122530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:28,171-Speed 3890.58 samples/sec  Loss 4.2721  LearningRate 0.0038  ProxyLR: 0.1903  Epoch: 21  Global Step: 122540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:30,801-Speed 3894.13 samples/sec  Loss 4.4000  LearningRate 0.0038  ProxyLR: 0.1901  Epoch: 21  Global Step: 122550   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-05 00:22:33,419-Speed 3913.42 samples/sec  Loss 4.2888  LearningRate 0.0038  ProxyLR: 0.1899  Epoch: 21  Global Step: 122560   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:36,051-Speed 3891.72 samples/sec  Loss 4.3758  LearningRate 0.0038  ProxyLR: 0.1897  Epoch: 21  Global Step: 122570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:38,683-Speed 3891.87 samples/sec  Loss 4.2647  LearningRate 0.0038  ProxyLR: 0.1895  Epoch: 21  Global Step: 122580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:41,315-Speed 3891.88 samples/sec  Loss 4.2916  LearningRate 0.0038  ProxyLR: 0.1893  Epoch: 21  Global Step: 122590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:43,946-Speed 3892.88 samples/sec  Loss 4.2808  LearningRate 0.0038  ProxyLR: 0.1891  Epoch: 21  Global Step: 122600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:46,578-Speed 3892.10 samples/sec  Loss 4.2692  LearningRate 0.0038  ProxyLR: 0.1890  Epoch: 21  Global Step: 122610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:49,205-Speed 3897.67 samples/sec  Loss 4.2953  LearningRate 0.0038  ProxyLR: 0.1888  Epoch: 21  Global Step: 122620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:51,838-Speed 3890.77 samples/sec  Loss 4.3457  LearningRate 0.0038  ProxyLR: 0.1886  Epoch: 21  Global Step: 122630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:54,471-Speed 3889.55 samples/sec  Loss 4.2990  LearningRate 0.0038  ProxyLR: 0.1884  Epoch: 21  Global Step: 122640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:57,106-Speed 3887.14 samples/sec  Loss 4.3513  LearningRate 0.0038  ProxyLR: 0.1882  Epoch: 21  Global Step: 122650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:22:59,728-Speed 3907.35 samples/sec  Loss 4.3292  LearningRate 0.0038  ProxyLR: 0.1880  Epoch: 21  Global Step: 122660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:02,369-Speed 3879.04 samples/sec  Loss 4.3070  LearningRate 0.0038  ProxyLR: 0.1878  Epoch: 21  Global Step: 122670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:05,007-Speed 3882.13 samples/sec  Loss 4.3634  LearningRate 0.0038  ProxyLR: 0.1876  Epoch: 21  Global Step: 122680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:07,642-Speed 3887.52 samples/sec  Loss 4.2819  LearningRate 0.0037  ProxyLR: 0.1874  Epoch: 21  Global Step: 122690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:10,277-Speed 3886.55 samples/sec  Loss 4.3221  LearningRate 0.0037  ProxyLR: 0.1872  Epoch: 21  Global Step: 122700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:12,913-Speed 3886.29 samples/sec  Loss 4.2988  LearningRate 0.0037  ProxyLR: 0.1870  Epoch: 21  Global Step: 122710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:15,547-Speed 3888.74 samples/sec  Loss 4.3295  LearningRate 0.0037  ProxyLR: 0.1868  Epoch: 21  Global Step: 122720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:18,166-Speed 3909.95 samples/sec  Loss 4.3584  LearningRate 0.0037  ProxyLR: 0.1866  Epoch: 21  Global Step: 122730   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:20,797-Speed 3894.11 samples/sec  Loss 4.3314  LearningRate 0.0037  ProxyLR: 0.1864  Epoch: 21  Global Step: 122740   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:23,428-Speed 3892.60 samples/sec  Loss 4.2560  LearningRate 0.0037  ProxyLR: 0.1863  Epoch: 21  Global Step: 122750   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:26,058-Speed 3895.60 samples/sec  Loss 4.2269  LearningRate 0.0037  ProxyLR: 0.1861  Epoch: 21  Global Step: 122760   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:28,686-Speed 3896.26 samples/sec  Loss 4.3338  LearningRate 0.0037  ProxyLR: 0.1859  Epoch: 21  Global Step: 122770   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:31,318-Speed 3892.28 samples/sec  Loss 4.2783  LearningRate 0.0037  ProxyLR: 0.1857  Epoch: 21  Global Step: 122780   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:33,951-Speed 3889.83 samples/sec  Loss 4.2665  LearningRate 0.0037  ProxyLR: 0.1855  Epoch: 21  Global Step: 122790   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:36,587-Speed 3885.51 samples/sec  Loss 4.3014  LearningRate 0.0037  ProxyLR: 0.1853  Epoch: 21  Global Step: 122800   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:39,222-Speed 3887.81 samples/sec  Loss 4.4073  LearningRate 0.0037  ProxyLR: 0.1851  Epoch: 21  Global Step: 122810   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:41,861-Speed 3881.56 samples/sec  Loss 4.2063  LearningRate 0.0037  ProxyLR: 0.1849  Epoch: 21  Global Step: 122820   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:23:44,495-Speed 3889.06 samples/sec  Loss 4.3017  LearningRate 0.0037  ProxyLR: 0.1847  Epoch: 21  Global Step: 122830   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:47,128-Speed 3889.34 samples/sec  Loss 4.3104  LearningRate 0.0037  ProxyLR: 0.1845  Epoch: 21  Global Step: 122840   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:49,759-Speed 3893.02 samples/sec  Loss 4.2042  LearningRate 0.0037  ProxyLR: 0.1843  Epoch: 21  Global Step: 122850   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:52,390-Speed 3893.94 samples/sec  Loss 4.3324  LearningRate 0.0037  ProxyLR: 0.1841  Epoch: 21  Global Step: 122860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:55,020-Speed 3894.60 samples/sec  Loss 4.2800  LearningRate 0.0037  ProxyLR: 0.1840  Epoch: 21  Global Step: 122870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:23:57,651-Speed 3892.06 samples/sec  Loss 4.2818  LearningRate 0.0037  ProxyLR: 0.1838  Epoch: 21  Global Step: 122880   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:00,279-Speed 3897.97 samples/sec  Loss 4.3029  LearningRate 0.0037  ProxyLR: 0.1836  Epoch: 21  Global Step: 122890   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:02,908-Speed 3896.09 samples/sec  Loss 4.2264  LearningRate 0.0037  ProxyLR: 0.1834  Epoch: 21  Global Step: 122900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:05,537-Speed 3895.54 samples/sec  Loss 4.3815  LearningRate 0.0037  ProxyLR: 0.1832  Epoch: 21  Global Step: 122910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:08,168-Speed 3893.00 samples/sec  Loss 4.2995  LearningRate 0.0037  ProxyLR: 0.1830  Epoch: 21  Global Step: 122920   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:10,785-Speed 3914.35 samples/sec  Loss 4.2937  LearningRate 0.0037  ProxyLR: 0.1828  Epoch: 21  Global Step: 122930   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:13,415-Speed 3894.41 samples/sec  Loss 4.2590  LearningRate 0.0037  ProxyLR: 0.1826  Epoch: 21  Global Step: 122940   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:16,047-Speed 3892.26 samples/sec  Loss 4.2599  LearningRate 0.0036  ProxyLR: 0.1824  Epoch: 21  Global Step: 122950   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:18,676-Speed 3895.27 samples/sec  Loss 4.2771  LearningRate 0.0036  ProxyLR: 0.1822  Epoch: 21  Global Step: 122960   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:21,304-Speed 3898.30 samples/sec  Loss 4.3041  LearningRate 0.0036  ProxyLR: 0.1821  Epoch: 21  Global Step: 122970   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:23,934-Speed 3894.55 samples/sec  Loss 4.3755  LearningRate 0.0036  ProxyLR: 0.1819  Epoch: 21  Global Step: 122980   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:26,566-Speed 3891.22 samples/sec  Loss 4.3112  LearningRate 0.0036  ProxyLR: 0.1817  Epoch: 21  Global Step: 122990   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:29,197-Speed 3893.37 samples/sec  Loss 4.3491  LearningRate 0.0036  ProxyLR: 0.1815  Epoch: 21  Global Step: 123000   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:31,827-Speed 3895.80 samples/sec  Loss 4.3065  LearningRate 0.0036  ProxyLR: 0.1813  Epoch: 21  Global Step: 123010   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:34,455-Speed 3897.13 samples/sec  Loss 4.3220  LearningRate 0.0036  ProxyLR: 0.1811  Epoch: 21  Global Step: 123020   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:37,072-Speed 3914.35 samples/sec  Loss 4.2728  LearningRate 0.0036  ProxyLR: 0.1809  Epoch: 21  Global Step: 123030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:39,701-Speed 3895.39 samples/sec  Loss 4.2882  LearningRate 0.0036  ProxyLR: 0.1807  Epoch: 21  Global Step: 123040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:42,328-Speed 3898.65 samples/sec  Loss 4.2282  LearningRate 0.0036  ProxyLR: 0.1805  Epoch: 21  Global Step: 123050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:44,956-Speed 3897.67 samples/sec  Loss 4.3459  LearningRate 0.0036  ProxyLR: 0.1804  Epoch: 21  Global Step: 123060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:47,583-Speed 3899.37 samples/sec  Loss 4.2710  LearningRate 0.0036  ProxyLR: 0.1802  Epoch: 21  Global Step: 123070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:50,210-Speed 3898.24 samples/sec  Loss 4.3196  LearningRate 0.0036  ProxyLR: 0.1800  Epoch: 21  Global Step: 123080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:52,839-Speed 3895.94 samples/sec  Loss 4.2622  LearningRate 0.0036  ProxyLR: 0.1798  Epoch: 21  Global Step: 123090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:55,467-Speed 3898.33 samples/sec  Loss 4.2711  LearningRate 0.0036  ProxyLR: 0.1796  Epoch: 21  Global Step: 123100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:24:58,094-Speed 3898.08 samples/sec  Loss 4.3032  LearningRate 0.0036  ProxyLR: 0.1794  Epoch: 21  Global Step: 123110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:00,722-Speed 3897.79 samples/sec  Loss 4.3007  LearningRate 0.0036  ProxyLR: 0.1792  Epoch: 21  Global Step: 123120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:03,338-Speed 3915.67 samples/sec  Loss 4.2799  LearningRate 0.0036  ProxyLR: 0.1790  Epoch: 21  Global Step: 123130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:05,966-Speed 3897.76 samples/sec  Loss 4.2152  LearningRate 0.0036  ProxyLR: 0.1788  Epoch: 21  Global Step: 123140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:08,593-Speed 3898.22 samples/sec  Loss 4.3730  LearningRate 0.0036  ProxyLR: 0.1787  Epoch: 21  Global Step: 123150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:11,219-Speed 3900.23 samples/sec  Loss 4.2651  LearningRate 0.0036  ProxyLR: 0.1785  Epoch: 21  Global Step: 123160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:13,844-Speed 3902.36 samples/sec  Loss 4.2299  LearningRate 0.0036  ProxyLR: 0.1783  Epoch: 21  Global Step: 123170   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:16,470-Speed 3900.46 samples/sec  Loss 4.2570  LearningRate 0.0036  ProxyLR: 0.1781  Epoch: 21  Global Step: 123180   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:19,095-Speed 3901.43 samples/sec  Loss 4.3440  LearningRate 0.0036  ProxyLR: 0.1779  Epoch: 21  Global Step: 123190   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:21,720-Speed 3902.43 samples/sec  Loss 4.1853  LearningRate 0.0036  ProxyLR: 0.1777  Epoch: 21  Global Step: 123200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:24,345-Speed 3902.45 samples/sec  Loss 4.2572  LearningRate 0.0036  ProxyLR: 0.1775  Epoch: 21  Global Step: 123210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:26,970-Speed 3901.45 samples/sec  Loss 4.2281  LearningRate 0.0035  ProxyLR: 0.1773  Epoch: 21  Global Step: 123220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:29,582-Speed 3921.51 samples/sec  Loss 4.2514  LearningRate 0.0035  ProxyLR: 0.1772  Epoch: 21  Global Step: 123230   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:32,209-Speed 3899.63 samples/sec  Loss 4.2507  LearningRate 0.0035  ProxyLR: 0.1770  Epoch: 21  Global Step: 123240   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:34,834-Speed 3901.83 samples/sec  Loss 4.2024  LearningRate 0.0035  ProxyLR: 0.1768  Epoch: 21  Global Step: 123250   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:25:37,446-Speed 3921.56 samples/sec  Loss 4.2940  LearningRate 0.0035  ProxyLR: 0.1766  Epoch: 21  Global Step: 123260   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:40,072-Speed 3900.26 samples/sec  Loss 4.2766  LearningRate 0.0035  ProxyLR: 0.1764  Epoch: 21  Global Step: 123270   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:42,698-Speed 3900.24 samples/sec  Loss 4.3616  LearningRate 0.0035  ProxyLR: 0.1762  Epoch: 21  Global Step: 123280   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:45,324-Speed 3899.78 samples/sec  Loss 4.3550  LearningRate 0.0035  ProxyLR: 0.1760  Epoch: 21  Global Step: 123290   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:47,951-Speed 3900.15 samples/sec  Loss 4.3527  LearningRate 0.0035  ProxyLR: 0.1758  Epoch: 21  Global Step: 123300   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:50,577-Speed 3899.51 samples/sec  Loss 4.3071  LearningRate 0.0035  ProxyLR: 0.1757  Epoch: 21  Global Step: 123310   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:53,206-Speed 3896.44 samples/sec  Loss 4.2728  LearningRate 0.0035  ProxyLR: 0.1755  Epoch: 21  Global Step: 123320   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:55,837-Speed 3892.60 samples/sec  Loss 4.3160  LearningRate 0.0035  ProxyLR: 0.1753  Epoch: 21  Global Step: 123330   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:25:58,468-Speed 3893.38 samples/sec  Loss 4.2281  LearningRate 0.0035  ProxyLR: 0.1751  Epoch: 21  Global Step: 123340   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:26:01,099-Speed 3893.91 samples/sec  Loss 4.3319  LearningRate 0.0035  ProxyLR: 0.1749  Epoch: 21  Global Step: 123350   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:26:03,729-Speed 3894.03 samples/sec  Loss 4.2866  LearningRate 0.0035  ProxyLR: 0.1747  Epoch: 21  Global Step: 123360   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:06,358-Speed 3895.57 samples/sec  Loss 4.3053  LearningRate 0.0035  ProxyLR: 0.1745  Epoch: 21  Global Step: 123370   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:08,987-Speed 3896.14 samples/sec  Loss 4.2512  LearningRate 0.0035  ProxyLR: 0.1744  Epoch: 21  Global Step: 123380   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:11,618-Speed 3893.41 samples/sec  Loss 4.3568  LearningRate 0.0035  ProxyLR: 0.1742  Epoch: 21  Global Step: 123390   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:14,247-Speed 3895.82 samples/sec  Loss 4.2846  LearningRate 0.0035  ProxyLR: 0.1740  Epoch: 21  Global Step: 123400   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:16,875-Speed 3898.02 samples/sec  Loss 4.2267  LearningRate 0.0035  ProxyLR: 0.1738  Epoch: 21  Global Step: 123410   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:19,504-Speed 3896.11 samples/sec  Loss 4.3366  LearningRate 0.0035  ProxyLR: 0.1736  Epoch: 21  Global Step: 123420   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:22,133-Speed 3895.34 samples/sec  Loss 4.3877  LearningRate 0.0035  ProxyLR: 0.1734  Epoch: 21  Global Step: 123430   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:24,763-Speed 3895.27 samples/sec  Loss 4.2106  LearningRate 0.0035  ProxyLR: 0.1732  Epoch: 21  Global Step: 123440   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:27,392-Speed 3895.62 samples/sec  Loss 4.3163  LearningRate 0.0035  ProxyLR: 0.1731  Epoch: 21  Global Step: 123450   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:30,009-Speed 3914.29 samples/sec  Loss 4.2834  LearningRate 0.0035  ProxyLR: 0.1729  Epoch: 21  Global Step: 123460   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:32,639-Speed 3894.79 samples/sec  Loss 4.1884  LearningRate 0.0035  ProxyLR: 0.1727  Epoch: 21  Global Step: 123470   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:35,266-Speed 3898.48 samples/sec  Loss 4.2506  LearningRate 0.0035  ProxyLR: 0.1725  Epoch: 21  Global Step: 123480   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:37,894-Speed 3897.40 samples/sec  Loss 4.3502  LearningRate 0.0034  ProxyLR: 0.1723  Epoch: 21  Global Step: 123490   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:40,526-Speed 3891.85 samples/sec  Loss 4.3124  LearningRate 0.0034  ProxyLR: 0.1721  Epoch: 21  Global Step: 123500   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:43,155-Speed 3896.20 samples/sec  Loss 4.2793  LearningRate 0.0034  ProxyLR: 0.1719  Epoch: 21  Global Step: 123510   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:45,784-Speed 3896.11 samples/sec  Loss 4.2376  LearningRate 0.0034  ProxyLR: 0.1718  Epoch: 21  Global Step: 123520   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:48,417-Speed 3889.16 samples/sec  Loss 4.3143  LearningRate 0.0034  ProxyLR: 0.1716  Epoch: 21  Global Step: 123530   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:51,050-Speed 3890.63 samples/sec  Loss 4.2933  LearningRate 0.0034  ProxyLR: 0.1714  Epoch: 21  Global Step: 123540   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:53,682-Speed 3892.51 samples/sec  Loss 4.3277  LearningRate 0.0034  ProxyLR: 0.1712  Epoch: 21  Global Step: 123550   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:26:56,311-Speed 3895.99 samples/sec  Loss 4.3175  LearningRate 0.0034  ProxyLR: 0.1710  Epoch: 21  Global Step: 123560   Fp16 Grad Scale: 524288  Required: 2 hours
Training: 2023-05-05 00:26:58,926-Speed 3915.68 samples/sec  Loss 4.1934  LearningRate 0.0034  ProxyLR: 0.1708  Epoch: 21  Global Step: 123570   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:01,555-Speed 3896.49 samples/sec  Loss 4.2453  LearningRate 0.0034  ProxyLR: 0.1707  Epoch: 21  Global Step: 123580   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:04,185-Speed 3894.52 samples/sec  Loss 4.2402  LearningRate 0.0034  ProxyLR: 0.1705  Epoch: 21  Global Step: 123590   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:06,814-Speed 3896.92 samples/sec  Loss 4.2593  LearningRate 0.0034  ProxyLR: 0.1703  Epoch: 21  Global Step: 123600   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:09,444-Speed 3894.27 samples/sec  Loss 4.1708  LearningRate 0.0034  ProxyLR: 0.1701  Epoch: 21  Global Step: 123610   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:12,073-Speed 3895.68 samples/sec  Loss 4.2218  LearningRate 0.0034  ProxyLR: 0.1699  Epoch: 21  Global Step: 123620   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:14,702-Speed 3896.68 samples/sec  Loss 4.2556  LearningRate 0.0034  ProxyLR: 0.1697  Epoch: 21  Global Step: 123630   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:17,335-Speed 3890.38 samples/sec  Loss 4.1939  LearningRate 0.0034  ProxyLR: 0.1696  Epoch: 21  Global Step: 123640   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:19,967-Speed 3891.39 samples/sec  Loss 4.2202  LearningRate 0.0034  ProxyLR: 0.1694  Epoch: 21  Global Step: 123650   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:22,601-Speed 3887.59 samples/sec  Loss 4.2566  LearningRate 0.0034  ProxyLR: 0.1692  Epoch: 21  Global Step: 123660   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:25,225-Speed 3904.77 samples/sec  Loss 4.2168  LearningRate 0.0034  ProxyLR: 0.1690  Epoch: 21  Global Step: 123670   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:27,860-Speed 3886.95 samples/sec  Loss 4.2467  LearningRate 0.0034  ProxyLR: 0.1688  Epoch: 21  Global Step: 123680   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:30,498-Speed 3882.39 samples/sec  Loss 4.2261  LearningRate 0.0034  ProxyLR: 0.1686  Epoch: 21  Global Step: 123690   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:33,134-Speed 3885.58 samples/sec  Loss 4.2397  LearningRate 0.0034  ProxyLR: 0.1685  Epoch: 21  Global Step: 123700   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:35,771-Speed 3883.63 samples/sec  Loss 4.2070  LearningRate 0.0034  ProxyLR: 0.1683  Epoch: 21  Global Step: 123710   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:38,408-Speed 3884.80 samples/sec  Loss 4.1973  LearningRate 0.0034  ProxyLR: 0.1681  Epoch: 21  Global Step: 123720   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:41,043-Speed 3886.35 samples/sec  Loss 4.1996  LearningRate 0.0034  ProxyLR: 0.1679  Epoch: 21  Global Step: 123730   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:43,679-Speed 3886.73 samples/sec  Loss 4.1900  LearningRate 0.0034  ProxyLR: 0.1677  Epoch: 21  Global Step: 123740   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:46,314-Speed 3887.11 samples/sec  Loss 4.2270  LearningRate 0.0034  ProxyLR: 0.1675  Epoch: 21  Global Step: 123750   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:27:48,935-Speed 3908.08 samples/sec  Loss 4.2317  LearningRate 0.0033  ProxyLR: 0.1674  Epoch: 21  Global Step: 123760   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:27:51,571-Speed 3885.83 samples/sec  Loss 4.2520  LearningRate 0.0033  ProxyLR: 0.1672  Epoch: 21  Global Step: 123770   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:27:54,207-Speed 3885.51 samples/sec  Loss 4.2836  LearningRate 0.0033  ProxyLR: 0.1670  Epoch: 21  Global Step: 123780   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:27:56,842-Speed 3887.63 samples/sec  Loss 4.2046  LearningRate 0.0033  ProxyLR: 0.1668  Epoch: 21  Global Step: 123790   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:27:59,477-Speed 3886.49 samples/sec  Loss 4.1843  LearningRate 0.0033  ProxyLR: 0.1666  Epoch: 21  Global Step: 123800   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:02,113-Speed 3886.27 samples/sec  Loss 4.1961  LearningRate 0.0033  ProxyLR: 0.1665  Epoch: 21  Global Step: 123810   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:04,750-Speed 3884.24 samples/sec  Loss 4.2589  LearningRate 0.0033  ProxyLR: 0.1663  Epoch: 21  Global Step: 123820   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:07,383-Speed 3889.30 samples/sec  Loss 4.2885  LearningRate 0.0033  ProxyLR: 0.1661  Epoch: 21  Global Step: 123830   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:10,016-Speed 3890.15 samples/sec  Loss 4.2760  LearningRate 0.0033  ProxyLR: 0.1659  Epoch: 21  Global Step: 123840   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:12,651-Speed 3887.84 samples/sec  Loss 4.1700  LearningRate 0.0033  ProxyLR: 0.1657  Epoch: 21  Global Step: 123850   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:15,286-Speed 3887.47 samples/sec  Loss 4.3268  LearningRate 0.0033  ProxyLR: 0.1656  Epoch: 21  Global Step: 123860   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:17,921-Speed 3887.59 samples/sec  Loss 4.2694  LearningRate 0.0033  ProxyLR: 0.1654  Epoch: 21  Global Step: 123870   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:20,557-Speed 3885.62 samples/sec  Loss 4.2031  LearningRate 0.0033  ProxyLR: 0.1652  Epoch: 21  Global Step: 123880   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:23,191-Speed 3888.30 samples/sec  Loss 4.2152  LearningRate 0.0033  ProxyLR: 0.1650  Epoch: 21  Global Step: 123890   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:25,825-Speed 3888.25 samples/sec  Loss 4.3331  LearningRate 0.0033  ProxyLR: 0.1648  Epoch: 21  Global Step: 123900   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:28,459-Speed 3888.44 samples/sec  Loss 4.2665  LearningRate 0.0033  ProxyLR: 0.1646  Epoch: 21  Global Step: 123910   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:31,090-Speed 3893.10 samples/sec  Loss 4.2220  LearningRate 0.0033  ProxyLR: 0.1645  Epoch: 21  Global Step: 123920   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:28:33,710-Speed 3910.17 samples/sec  Loss 4.2367  LearningRate 0.0033  ProxyLR: 0.1643  Epoch: 21  Global Step: 123930   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:36,342-Speed 3891.54 samples/sec  Loss 4.2749  LearningRate 0.0033  ProxyLR: 0.1641  Epoch: 21  Global Step: 123940   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:38,974-Speed 3890.49 samples/sec  Loss 4.2731  LearningRate 0.0033  ProxyLR: 0.1639  Epoch: 21  Global Step: 123950   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:41,608-Speed 3889.12 samples/sec  Loss 4.2208  LearningRate 0.0033  ProxyLR: 0.1637  Epoch: 21  Global Step: 123960   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:44,241-Speed 3890.12 samples/sec  Loss 4.2984  LearningRate 0.0033  ProxyLR: 0.1636  Epoch: 21  Global Step: 123970   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:46,873-Speed 3891.23 samples/sec  Loss 4.2258  LearningRate 0.0033  ProxyLR: 0.1634  Epoch: 21  Global Step: 123980   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:49,506-Speed 3891.39 samples/sec  Loss 4.3047  LearningRate 0.0033  ProxyLR: 0.1632  Epoch: 21  Global Step: 123990   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:28:52,138-Speed 3891.22 samples/sec  Loss 4.2106  LearningRate 0.0033  ProxyLR: 0.1630  Epoch: 21  Global Step: 124000   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:29:41,337-[lfw][124000]XNorm: 22.050525
Training: 2023-05-05 00:29:41,337-[lfw][124000]Accuracy-Flip: 0.99733+-0.00238
Training: 2023-05-05 00:29:41,337-[lfw][124000]Accuracy-Highest: 0.99783
Training: 2023-05-05 00:29:41,337-[lfw][124000]TPR@1stNon-Zero-FPR of 0.00033: 0.99567
Training: 2023-05-05 00:29:41,337-[lfw][124000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 00:30:37,984-[cfp_fp][124000]XNorm: 21.751093
Training: 2023-05-05 00:30:37,984-[cfp_fp][124000]Accuracy-Flip: 0.98529+-0.00511
Training: 2023-05-05 00:30:37,984-[cfp_fp][124000]Accuracy-Highest: 0.98529
Training: 2023-05-05 00:30:37,984-[cfp_fp][124000]TPR@1stNon-Zero-FPR of 0.00029: 0.89200
Training: 2023-05-05 00:30:37,984-[cfp_fp][124000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 00:31:27,281-[agedb_30][124000]XNorm: 22.212883
Training: 2023-05-05 00:31:27,281-[agedb_30][124000]Accuracy-Flip: 0.97483+-0.00736
Training: 2023-05-05 00:31:27,281-[agedb_30][124000]Accuracy-Highest: 0.97483
Training: 2023-05-05 00:31:27,282-[agedb_30][124000]TPR@1stNon-Zero-FPR of 0.00033: 0.86333
Training: 2023-05-05 00:31:27,282-[agedb_30][124000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 00:32:17,953-[calfw][124000]XNorm: 22.209118
Training: 2023-05-05 00:32:17,953-[calfw][124000]Accuracy-Flip: 0.95717+-0.01229
Training: 2023-05-05 00:32:17,953-[calfw][124000]Accuracy-Highest: 0.95867
Training: 2023-05-05 00:32:17,954-[calfw][124000]TPR@1stNon-Zero-FPR of 0.00033: 0.83233
Training: 2023-05-05 00:32:17,954-[calfw][124000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 00:33:08,746-[cplfw][124000]XNorm: 21.147447
Training: 2023-05-05 00:33:08,746-[cplfw][124000]Accuracy-Flip: 0.92933+-0.01205
Training: 2023-05-05 00:33:08,746-[cplfw][124000]Accuracy-Highest: 0.92933
Training: 2023-05-05 00:33:08,746-[cplfw][124000]TPR@1stNon-Zero-FPR of 0.00033: 0.13267
Training: 2023-05-05 00:33:08,746-[cplfw][124000]Highest TPR@FPR: 0.13267
Training: 2023-05-05 00:33:16,507-Speed 38.73 samples/sec  Loss 4.2089  LearningRate 0.0033  ProxyLR: 0.1628  Epoch: 21  Global Step: 124010   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:33:19,130-Speed 3905.29 samples/sec  Loss 4.2370  LearningRate 0.0033  ProxyLR: 0.1627  Epoch: 21  Global Step: 124020   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 00:33:21,753-Speed 3904.50 samples/sec  Loss 4.1738  LearningRate 0.0032  ProxyLR: 0.1625  Epoch: 21  Global Step: 124030   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:24,377-Speed 3903.41 samples/sec  Loss 4.1573  LearningRate 0.0032  ProxyLR: 0.1623  Epoch: 21  Global Step: 124040   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:27,002-Speed 3903.32 samples/sec  Loss 4.1096  LearningRate 0.0032  ProxyLR: 0.1621  Epoch: 21  Global Step: 124050   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:29,630-Speed 3898.27 samples/sec  Loss 4.2055  LearningRate 0.0032  ProxyLR: 0.1620  Epoch: 21  Global Step: 124060   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:32,257-Speed 3898.99 samples/sec  Loss 4.2046  LearningRate 0.0032  ProxyLR: 0.1618  Epoch: 21  Global Step: 124070   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:34,885-Speed 3897.31 samples/sec  Loss 4.1639  LearningRate 0.0032  ProxyLR: 0.1616  Epoch: 21  Global Step: 124080   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:37,514-Speed 3896.76 samples/sec  Loss 4.2109  LearningRate 0.0032  ProxyLR: 0.1614  Epoch: 21  Global Step: 124090   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:40,141-Speed 3898.31 samples/sec  Loss 4.2391  LearningRate 0.0032  ProxyLR: 0.1612  Epoch: 21  Global Step: 124100   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:42,771-Speed 3894.51 samples/sec  Loss 4.2366  LearningRate 0.0032  ProxyLR: 0.1611  Epoch: 21  Global Step: 124110   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:45,402-Speed 3893.02 samples/sec  Loss 4.1630  LearningRate 0.0032  ProxyLR: 0.1609  Epoch: 21  Global Step: 124120   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:48,022-Speed 3908.89 samples/sec  Loss 4.2718  LearningRate 0.0032  ProxyLR: 0.1607  Epoch: 21  Global Step: 124130   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:50,657-Speed 3887.42 samples/sec  Loss 4.2019  LearningRate 0.0032  ProxyLR: 0.1605  Epoch: 21  Global Step: 124140   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:53,291-Speed 3889.37 samples/sec  Loss 4.2355  LearningRate 0.0032  ProxyLR: 0.1603  Epoch: 21  Global Step: 124150   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:55,921-Speed 3893.66 samples/sec  Loss 4.2067  LearningRate 0.0032  ProxyLR: 0.1602  Epoch: 21  Global Step: 124160   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 00:33:58,558-Speed 3884.96 samples/sec  Loss 4.2517  LearningRate 0.0032  ProxyLR: 0.1600  Epoch: 21  Global Step: 124170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:01,192-Speed 3888.67 samples/sec  Loss 4.1629  LearningRate 0.0032  ProxyLR: 0.1598  Epoch: 21  Global Step: 124180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:03,824-Speed 3892.13 samples/sec  Loss 4.2263  LearningRate 0.0032  ProxyLR: 0.1596  Epoch: 21  Global Step: 124190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:06,456-Speed 3891.07 samples/sec  Loss 4.2990  LearningRate 0.0032  ProxyLR: 0.1595  Epoch: 21  Global Step: 124200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:09,090-Speed 3889.44 samples/sec  Loss 4.1746  LearningRate 0.0032  ProxyLR: 0.1593  Epoch: 21  Global Step: 124210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:11,718-Speed 3896.21 samples/sec  Loss 4.2095  LearningRate 0.0032  ProxyLR: 0.1591  Epoch: 21  Global Step: 124220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:14,336-Speed 3912.78 samples/sec  Loss 4.2959  LearningRate 0.0032  ProxyLR: 0.1589  Epoch: 21  Global Step: 124230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:16,966-Speed 3894.60 samples/sec  Loss 4.2174  LearningRate 0.0032  ProxyLR: 0.1587  Epoch: 21  Global Step: 124240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:19,597-Speed 3893.50 samples/sec  Loss 4.1890  LearningRate 0.0032  ProxyLR: 0.1586  Epoch: 21  Global Step: 124250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:22,228-Speed 3893.73 samples/sec  Loss 4.1643  LearningRate 0.0032  ProxyLR: 0.1584  Epoch: 21  Global Step: 124260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:24,859-Speed 3892.68 samples/sec  Loss 4.2154  LearningRate 0.0032  ProxyLR: 0.1582  Epoch: 21  Global Step: 124270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:27,490-Speed 3892.37 samples/sec  Loss 4.2500  LearningRate 0.0032  ProxyLR: 0.1580  Epoch: 21  Global Step: 124280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:30,126-Speed 3886.14 samples/sec  Loss 4.2763  LearningRate 0.0032  ProxyLR: 0.1579  Epoch: 21  Global Step: 124290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:32,757-Speed 3892.91 samples/sec  Loss 4.1924  LearningRate 0.0032  ProxyLR: 0.1577  Epoch: 21  Global Step: 124300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:35,390-Speed 3890.08 samples/sec  Loss 4.1978  LearningRate 0.0032  ProxyLR: 0.1575  Epoch: 21  Global Step: 124310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:38,023-Speed 3890.63 samples/sec  Loss 4.2327  LearningRate 0.0031  ProxyLR: 0.1573  Epoch: 21  Global Step: 124320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:40,640-Speed 3914.63 samples/sec  Loss 4.1658  LearningRate 0.0031  ProxyLR: 0.1572  Epoch: 21  Global Step: 124330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:43,270-Speed 3894.49 samples/sec  Loss 4.2403  LearningRate 0.0031  ProxyLR: 0.1570  Epoch: 21  Global Step: 124340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:45,901-Speed 3892.71 samples/sec  Loss 4.1672  LearningRate 0.0031  ProxyLR: 0.1568  Epoch: 21  Global Step: 124350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:48,531-Speed 3894.50 samples/sec  Loss 4.1799  LearningRate 0.0031  ProxyLR: 0.1566  Epoch: 21  Global Step: 124360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:51,163-Speed 3892.34 samples/sec  Loss 4.1513  LearningRate 0.0031  ProxyLR: 0.1564  Epoch: 21  Global Step: 124370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:53,795-Speed 3891.08 samples/sec  Loss 4.2262  LearningRate 0.0031  ProxyLR: 0.1563  Epoch: 21  Global Step: 124380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:56,427-Speed 3890.86 samples/sec  Loss 4.1466  LearningRate 0.0031  ProxyLR: 0.1561  Epoch: 21  Global Step: 124390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:34:59,060-Speed 3890.91 samples/sec  Loss 4.2717  LearningRate 0.0031  ProxyLR: 0.1559  Epoch: 21  Global Step: 124400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:01,690-Speed 3893.74 samples/sec  Loss 4.1861  LearningRate 0.0031  ProxyLR: 0.1557  Epoch: 21  Global Step: 124410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:04,321-Speed 3894.37 samples/sec  Loss 4.2453  LearningRate 0.0031  ProxyLR: 0.1556  Epoch: 21  Global Step: 124420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:06,951-Speed 3894.42 samples/sec  Loss 4.2108  LearningRate 0.0031  ProxyLR: 0.1554  Epoch: 21  Global Step: 124430   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 00:35:09,572-Speed 3907.35 samples/sec  Loss 4.2632  LearningRate 0.0031  ProxyLR: 0.1552  Epoch: 21  Global Step: 124440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:12,209-Speed 3885.05 samples/sec  Loss 4.1660  LearningRate 0.0031  ProxyLR: 0.1550  Epoch: 21  Global Step: 124450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:14,846-Speed 3883.93 samples/sec  Loss 4.1482  LearningRate 0.0031  ProxyLR: 0.1549  Epoch: 21  Global Step: 124460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:17,481-Speed 3887.62 samples/sec  Loss 4.1742  LearningRate 0.0031  ProxyLR: 0.1547  Epoch: 21  Global Step: 124470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:20,121-Speed 3879.78 samples/sec  Loss 4.1716  LearningRate 0.0031  ProxyLR: 0.1545  Epoch: 21  Global Step: 124480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:22,759-Speed 3881.97 samples/sec  Loss 4.2447  LearningRate 0.0031  ProxyLR: 0.1543  Epoch: 21  Global Step: 124490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:25,397-Speed 3882.82 samples/sec  Loss 4.1716  LearningRate 0.0031  ProxyLR: 0.1542  Epoch: 21  Global Step: 124500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:28,036-Speed 3881.88 samples/sec  Loss 4.2410  LearningRate 0.0031  ProxyLR: 0.1540  Epoch: 21  Global Step: 124510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:30,673-Speed 3882.95 samples/sec  Loss 4.2047  LearningRate 0.0031  ProxyLR: 0.1538  Epoch: 21  Global Step: 124520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:33,310-Speed 3885.17 samples/sec  Loss 4.2348  LearningRate 0.0031  ProxyLR: 0.1536  Epoch: 21  Global Step: 124530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:35,932-Speed 3906.93 samples/sec  Loss 4.2192  LearningRate 0.0031  ProxyLR: 0.1535  Epoch: 21  Global Step: 124540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:38,565-Speed 3890.38 samples/sec  Loss 4.1296  LearningRate 0.0031  ProxyLR: 0.1533  Epoch: 21  Global Step: 124550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:41,198-Speed 3890.37 samples/sec  Loss 4.2228  LearningRate 0.0031  ProxyLR: 0.1531  Epoch: 21  Global Step: 124560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:43,831-Speed 3888.97 samples/sec  Loss 4.2269  LearningRate 0.0031  ProxyLR: 0.1529  Epoch: 21  Global Step: 124570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:46,463-Speed 3891.84 samples/sec  Loss 4.1972  LearningRate 0.0031  ProxyLR: 0.1528  Epoch: 21  Global Step: 124580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:49,094-Speed 3892.68 samples/sec  Loss 4.2222  LearningRate 0.0031  ProxyLR: 0.1526  Epoch: 21  Global Step: 124590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:51,729-Speed 3887.38 samples/sec  Loss 4.2956  LearningRate 0.0030  ProxyLR: 0.1524  Epoch: 21  Global Step: 124600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:54,364-Speed 3888.26 samples/sec  Loss 4.2012  LearningRate 0.0030  ProxyLR: 0.1523  Epoch: 21  Global Step: 124610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:56,996-Speed 3891.45 samples/sec  Loss 4.2364  LearningRate 0.0030  ProxyLR: 0.1521  Epoch: 21  Global Step: 124620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:35:59,630-Speed 3887.66 samples/sec  Loss 4.2392  LearningRate 0.0030  ProxyLR: 0.1519  Epoch: 21  Global Step: 124630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:02,249-Speed 3911.44 samples/sec  Loss 4.2480  LearningRate 0.0030  ProxyLR: 0.1517  Epoch: 21  Global Step: 124640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:04,882-Speed 3889.83 samples/sec  Loss 4.2794  LearningRate 0.0030  ProxyLR: 0.1516  Epoch: 21  Global Step: 124650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:07,515-Speed 3890.05 samples/sec  Loss 4.2160  LearningRate 0.0030  ProxyLR: 0.1514  Epoch: 21  Global Step: 124660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:10,148-Speed 3891.18 samples/sec  Loss 4.2212  LearningRate 0.0030  ProxyLR: 0.1512  Epoch: 21  Global Step: 124670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:12,781-Speed 3889.62 samples/sec  Loss 4.2115  LearningRate 0.0030  ProxyLR: 0.1510  Epoch: 21  Global Step: 124680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:15,413-Speed 3891.47 samples/sec  Loss 4.2426  LearningRate 0.0030  ProxyLR: 0.1509  Epoch: 21  Global Step: 124690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:18,047-Speed 3888.37 samples/sec  Loss 4.2242  LearningRate 0.0030  ProxyLR: 0.1507  Epoch: 21  Global Step: 124700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:20,683-Speed 3886.92 samples/sec  Loss 4.2349  LearningRate 0.0030  ProxyLR: 0.1505  Epoch: 21  Global Step: 124710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:23,316-Speed 3889.75 samples/sec  Loss 4.2211  LearningRate 0.0030  ProxyLR: 0.1503  Epoch: 21  Global Step: 124720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:25,950-Speed 3888.90 samples/sec  Loss 4.2083  LearningRate 0.0030  ProxyLR: 0.1502  Epoch: 21  Global Step: 124730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:28,570-Speed 3908.55 samples/sec  Loss 4.2215  LearningRate 0.0030  ProxyLR: 0.1500  Epoch: 21  Global Step: 124740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:31,204-Speed 3888.68 samples/sec  Loss 4.2208  LearningRate 0.0030  ProxyLR: 0.1498  Epoch: 21  Global Step: 124750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:33,838-Speed 3888.82 samples/sec  Loss 4.2102  LearningRate 0.0030  ProxyLR: 0.1497  Epoch: 21  Global Step: 124760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:36,473-Speed 3886.91 samples/sec  Loss 4.2382  LearningRate 0.0030  ProxyLR: 0.1495  Epoch: 21  Global Step: 124770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:39,107-Speed 3889.30 samples/sec  Loss 4.1939  LearningRate 0.0030  ProxyLR: 0.1493  Epoch: 21  Global Step: 124780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:41,738-Speed 3892.89 samples/sec  Loss 4.2068  LearningRate 0.0030  ProxyLR: 0.1491  Epoch: 21  Global Step: 124790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:44,370-Speed 3892.74 samples/sec  Loss 4.1687  LearningRate 0.0030  ProxyLR: 0.1490  Epoch: 21  Global Step: 124800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:47,001-Speed 3892.20 samples/sec  Loss 4.1778  LearningRate 0.0030  ProxyLR: 0.1488  Epoch: 21  Global Step: 124810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:49,631-Speed 3895.05 samples/sec  Loss 4.2477  LearningRate 0.0030  ProxyLR: 0.1486  Epoch: 21  Global Step: 124820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:52,261-Speed 3893.98 samples/sec  Loss 4.1782  LearningRate 0.0030  ProxyLR: 0.1485  Epoch: 21  Global Step: 124830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:54,879-Speed 3912.90 samples/sec  Loss 4.2485  LearningRate 0.0030  ProxyLR: 0.1483  Epoch: 21  Global Step: 124840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:36:57,508-Speed 3895.71 samples/sec  Loss 4.2467  LearningRate 0.0030  ProxyLR: 0.1481  Epoch: 21  Global Step: 124850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:00,141-Speed 3891.00 samples/sec  Loss 4.1529  LearningRate 0.0030  ProxyLR: 0.1479  Epoch: 21  Global Step: 124860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:02,771-Speed 3893.60 samples/sec  Loss 4.1764  LearningRate 0.0030  ProxyLR: 0.1478  Epoch: 21  Global Step: 124870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:05,402-Speed 3893.22 samples/sec  Loss 4.1373  LearningRate 0.0030  ProxyLR: 0.1476  Epoch: 21  Global Step: 124880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:08,033-Speed 3893.83 samples/sec  Loss 4.3096  LearningRate 0.0029  ProxyLR: 0.1474  Epoch: 21  Global Step: 124890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:10,664-Speed 3892.66 samples/sec  Loss 4.2223  LearningRate 0.0029  ProxyLR: 0.1473  Epoch: 21  Global Step: 124900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:13,295-Speed 3892.87 samples/sec  Loss 4.2039  LearningRate 0.0029  ProxyLR: 0.1471  Epoch: 21  Global Step: 124910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:15,913-Speed 3912.60 samples/sec  Loss 4.1365  LearningRate 0.0029  ProxyLR: 0.1469  Epoch: 21  Global Step: 124920   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:18,543-Speed 3894.77 samples/sec  Loss 4.2198  LearningRate 0.0029  ProxyLR: 0.1467  Epoch: 21  Global Step: 124930   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:21,172-Speed 3896.39 samples/sec  Loss 4.1422  LearningRate 0.0029  ProxyLR: 0.1466  Epoch: 21  Global Step: 124940   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:23,802-Speed 3895.04 samples/sec  Loss 4.2469  LearningRate 0.0029  ProxyLR: 0.1464  Epoch: 21  Global Step: 124950   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:26,433-Speed 3891.95 samples/sec  Loss 4.1441  LearningRate 0.0029  ProxyLR: 0.1462  Epoch: 21  Global Step: 124960   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:29,064-Speed 3893.05 samples/sec  Loss 4.1518  LearningRate 0.0029  ProxyLR: 0.1461  Epoch: 21  Global Step: 124970   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:31,694-Speed 3895.01 samples/sec  Loss 4.2185  LearningRate 0.0029  ProxyLR: 0.1459  Epoch: 21  Global Step: 124980   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:34,324-Speed 3894.04 samples/sec  Loss 4.2294  LearningRate 0.0029  ProxyLR: 0.1457  Epoch: 21  Global Step: 124990   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:36,955-Speed 3893.34 samples/sec  Loss 4.1891  LearningRate 0.0029  ProxyLR: 0.1456  Epoch: 21  Global Step: 125000   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:39,585-Speed 3895.23 samples/sec  Loss 4.1797  LearningRate 0.0029  ProxyLR: 0.1454  Epoch: 21  Global Step: 125010   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:37:42,217-Speed 3891.41 samples/sec  Loss 4.2439  LearningRate 0.0029  ProxyLR: 0.1452  Epoch: 21  Global Step: 125020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:44,846-Speed 3895.72 samples/sec  Loss 4.1869  LearningRate 0.0029  ProxyLR: 0.1450  Epoch: 21  Global Step: 125030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:47,478-Speed 3891.84 samples/sec  Loss 4.2171  LearningRate 0.0029  ProxyLR: 0.1449  Epoch: 21  Global Step: 125040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:50,110-Speed 3892.66 samples/sec  Loss 4.2355  LearningRate 0.0029  ProxyLR: 0.1447  Epoch: 21  Global Step: 125050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:52,741-Speed 3891.91 samples/sec  Loss 4.1551  LearningRate 0.0029  ProxyLR: 0.1445  Epoch: 21  Global Step: 125060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:55,374-Speed 3891.07 samples/sec  Loss 4.2263  LearningRate 0.0029  ProxyLR: 0.1444  Epoch: 21  Global Step: 125070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:37:58,006-Speed 3890.89 samples/sec  Loss 4.1709  LearningRate 0.0029  ProxyLR: 0.1442  Epoch: 21  Global Step: 125080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:00,695-Speed 3809.26 samples/sec  Loss 4.1701  LearningRate 0.0029  ProxyLR: 0.1440  Epoch: 21  Global Step: 125090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:37,623-Speed 277.31 samples/sec  Loss 4.0669  LearningRate 0.0029  ProxyLR: 0.1439  Epoch: 22  Global Step: 125100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:40,280-Speed 3854.68 samples/sec  Loss 3.9422  LearningRate 0.0029  ProxyLR: 0.1437  Epoch: 22  Global Step: 125110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:42,890-Speed 3924.61 samples/sec  Loss 4.0421  LearningRate 0.0029  ProxyLR: 0.1435  Epoch: 22  Global Step: 125120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:45,516-Speed 3900.08 samples/sec  Loss 3.9805  LearningRate 0.0029  ProxyLR: 0.1434  Epoch: 22  Global Step: 125130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:48,173-Speed 3854.97 samples/sec  Loss 3.9191  LearningRate 0.0029  ProxyLR: 0.1432  Epoch: 22  Global Step: 125140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:50,802-Speed 3896.11 samples/sec  Loss 3.9694  LearningRate 0.0029  ProxyLR: 0.1430  Epoch: 22  Global Step: 125150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:53,438-Speed 3884.91 samples/sec  Loss 3.8942  LearningRate 0.0029  ProxyLR: 0.1429  Epoch: 22  Global Step: 125160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:56,074-Speed 3886.33 samples/sec  Loss 3.9631  LearningRate 0.0029  ProxyLR: 0.1427  Epoch: 22  Global Step: 125170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:38:58,703-Speed 3895.37 samples/sec  Loss 3.8711  LearningRate 0.0029  ProxyLR: 0.1425  Epoch: 22  Global Step: 125180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:01,332-Speed 3897.08 samples/sec  Loss 3.9442  LearningRate 0.0028  ProxyLR: 0.1424  Epoch: 22  Global Step: 125190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:03,990-Speed 3852.51 samples/sec  Loss 3.9656  LearningRate 0.0028  ProxyLR: 0.1422  Epoch: 22  Global Step: 125200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:06,610-Speed 3909.36 samples/sec  Loss 3.9708  LearningRate 0.0028  ProxyLR: 0.1420  Epoch: 22  Global Step: 125210   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:09,245-Speed 3887.39 samples/sec  Loss 3.8978  LearningRate 0.0028  ProxyLR: 0.1418  Epoch: 22  Global Step: 125220   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:11,882-Speed 3883.96 samples/sec  Loss 3.8932  LearningRate 0.0028  ProxyLR: 0.1417  Epoch: 22  Global Step: 125230   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:14,570-Speed 3810.66 samples/sec  Loss 3.9902  LearningRate 0.0028  ProxyLR: 0.1415  Epoch: 22  Global Step: 125240   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:17,205-Speed 3887.30 samples/sec  Loss 3.8924  LearningRate 0.0028  ProxyLR: 0.1413  Epoch: 22  Global Step: 125250   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:19,892-Speed 3810.62 samples/sec  Loss 3.9643  LearningRate 0.0028  ProxyLR: 0.1412  Epoch: 22  Global Step: 125260   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:22,529-Speed 3884.48 samples/sec  Loss 3.8387  LearningRate 0.0028  ProxyLR: 0.1410  Epoch: 22  Global Step: 125270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:25,162-Speed 3890.35 samples/sec  Loss 3.9185  LearningRate 0.0028  ProxyLR: 0.1408  Epoch: 22  Global Step: 125280   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:27,794-Speed 3892.06 samples/sec  Loss 3.9285  LearningRate 0.0028  ProxyLR: 0.1407  Epoch: 22  Global Step: 125290   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:30,426-Speed 3890.52 samples/sec  Loss 3.8987  LearningRate 0.0028  ProxyLR: 0.1405  Epoch: 22  Global Step: 125300   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:39:33,060-Speed 3889.27 samples/sec  Loss 3.9817  LearningRate 0.0028  ProxyLR: 0.1403  Epoch: 22  Global Step: 125310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:35,693-Speed 3890.27 samples/sec  Loss 3.9664  LearningRate 0.0028  ProxyLR: 0.1402  Epoch: 22  Global Step: 125320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:38,327-Speed 3887.71 samples/sec  Loss 3.9884  LearningRate 0.0028  ProxyLR: 0.1400  Epoch: 22  Global Step: 125330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:40,964-Speed 3884.65 samples/sec  Loss 3.9722  LearningRate 0.0028  ProxyLR: 0.1398  Epoch: 22  Global Step: 125340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:43,599-Speed 3886.42 samples/sec  Loss 3.9399  LearningRate 0.0028  ProxyLR: 0.1397  Epoch: 22  Global Step: 125350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:46,235-Speed 3885.36 samples/sec  Loss 3.9009  LearningRate 0.0028  ProxyLR: 0.1395  Epoch: 22  Global Step: 125360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:48,873-Speed 3883.56 samples/sec  Loss 3.8716  LearningRate 0.0028  ProxyLR: 0.1393  Epoch: 22  Global Step: 125370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:51,517-Speed 3874.22 samples/sec  Loss 3.9497  LearningRate 0.0028  ProxyLR: 0.1392  Epoch: 22  Global Step: 125380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:54,155-Speed 3882.00 samples/sec  Loss 3.9233  LearningRate 0.0028  ProxyLR: 0.1390  Epoch: 22  Global Step: 125390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:56,790-Speed 3886.87 samples/sec  Loss 3.9569  LearningRate 0.0028  ProxyLR: 0.1388  Epoch: 22  Global Step: 125400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:39:59,416-Speed 3901.42 samples/sec  Loss 3.9123  LearningRate 0.0028  ProxyLR: 0.1387  Epoch: 22  Global Step: 125410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:02,050-Speed 3888.59 samples/sec  Loss 3.8865  LearningRate 0.0028  ProxyLR: 0.1385  Epoch: 22  Global Step: 125420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:04,687-Speed 3883.70 samples/sec  Loss 3.8904  LearningRate 0.0028  ProxyLR: 0.1383  Epoch: 22  Global Step: 125430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:07,323-Speed 3885.42 samples/sec  Loss 3.9301  LearningRate 0.0028  ProxyLR: 0.1382  Epoch: 22  Global Step: 125440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:09,959-Speed 3885.28 samples/sec  Loss 3.9996  LearningRate 0.0028  ProxyLR: 0.1380  Epoch: 22  Global Step: 125450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:12,595-Speed 3886.23 samples/sec  Loss 3.9012  LearningRate 0.0028  ProxyLR: 0.1379  Epoch: 22  Global Step: 125460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:15,233-Speed 3882.46 samples/sec  Loss 3.8661  LearningRate 0.0028  ProxyLR: 0.1377  Epoch: 22  Global Step: 125470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:17,870-Speed 3884.57 samples/sec  Loss 3.9578  LearningRate 0.0028  ProxyLR: 0.1375  Epoch: 22  Global Step: 125480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:20,506-Speed 3884.83 samples/sec  Loss 3.9293  LearningRate 0.0027  ProxyLR: 0.1374  Epoch: 22  Global Step: 125490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:23,140-Speed 3888.08 samples/sec  Loss 4.0216  LearningRate 0.0027  ProxyLR: 0.1372  Epoch: 22  Global Step: 125500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:25,771-Speed 3893.01 samples/sec  Loss 3.8969  LearningRate 0.0027  ProxyLR: 0.1370  Epoch: 22  Global Step: 125510   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 00:40:28,389-Speed 3913.55 samples/sec  Loss 3.8570  LearningRate 0.0027  ProxyLR: 0.1369  Epoch: 22  Global Step: 125520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:31,021-Speed 3891.06 samples/sec  Loss 3.9617  LearningRate 0.0027  ProxyLR: 0.1367  Epoch: 22  Global Step: 125530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:33,654-Speed 3890.14 samples/sec  Loss 3.8779  LearningRate 0.0027  ProxyLR: 0.1365  Epoch: 22  Global Step: 125540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:40:36,273-Speed 3911.49 samples/sec  Loss 3.9044  LearningRate 0.0027  ProxyLR: 0.1364  Epoch: 22  Global Step: 125550   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:38,906-Speed 3889.56 samples/sec  Loss 3.8764  LearningRate 0.0027  ProxyLR: 0.1362  Epoch: 22  Global Step: 125560   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:41,539-Speed 3890.68 samples/sec  Loss 3.9836  LearningRate 0.0027  ProxyLR: 0.1360  Epoch: 22  Global Step: 125570   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:44,170-Speed 3891.72 samples/sec  Loss 3.8431  LearningRate 0.0027  ProxyLR: 0.1359  Epoch: 22  Global Step: 125580   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:46,803-Speed 3890.12 samples/sec  Loss 4.0682  LearningRate 0.0027  ProxyLR: 0.1357  Epoch: 22  Global Step: 125590   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:49,438-Speed 3887.50 samples/sec  Loss 3.9227  LearningRate 0.0027  ProxyLR: 0.1356  Epoch: 22  Global Step: 125600   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:52,073-Speed 3886.83 samples/sec  Loss 3.8891  LearningRate 0.0027  ProxyLR: 0.1354  Epoch: 22  Global Step: 125610   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:54,704-Speed 3892.66 samples/sec  Loss 3.9268  LearningRate 0.0027  ProxyLR: 0.1352  Epoch: 22  Global Step: 125620   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:57,336-Speed 3891.64 samples/sec  Loss 3.9489  LearningRate 0.0027  ProxyLR: 0.1351  Epoch: 22  Global Step: 125630   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:40:59,971-Speed 3888.01 samples/sec  Loss 3.9080  LearningRate 0.0027  ProxyLR: 0.1349  Epoch: 22  Global Step: 125640   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:41:02,603-Speed 3890.83 samples/sec  Loss 3.9769  LearningRate 0.0027  ProxyLR: 0.1347  Epoch: 22  Global Step: 125650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:05,237-Speed 3888.23 samples/sec  Loss 3.9323  LearningRate 0.0027  ProxyLR: 0.1346  Epoch: 22  Global Step: 125660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:07,875-Speed 3882.93 samples/sec  Loss 4.0055  LearningRate 0.0027  ProxyLR: 0.1344  Epoch: 22  Global Step: 125670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:10,510-Speed 3887.60 samples/sec  Loss 3.8694  LearningRate 0.0027  ProxyLR: 0.1342  Epoch: 22  Global Step: 125680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:13,145-Speed 3886.15 samples/sec  Loss 3.9040  LearningRate 0.0027  ProxyLR: 0.1341  Epoch: 22  Global Step: 125690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:15,779-Speed 3888.78 samples/sec  Loss 3.8736  LearningRate 0.0027  ProxyLR: 0.1339  Epoch: 22  Global Step: 125700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:18,414-Speed 3887.04 samples/sec  Loss 3.9465  LearningRate 0.0027  ProxyLR: 0.1338  Epoch: 22  Global Step: 125710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:21,049-Speed 3887.80 samples/sec  Loss 3.9970  LearningRate 0.0027  ProxyLR: 0.1336  Epoch: 22  Global Step: 125720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:23,680-Speed 3891.88 samples/sec  Loss 3.9173  LearningRate 0.0027  ProxyLR: 0.1334  Epoch: 22  Global Step: 125730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:26,313-Speed 3890.53 samples/sec  Loss 3.8964  LearningRate 0.0027  ProxyLR: 0.1333  Epoch: 22  Global Step: 125740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:28,933-Speed 3909.77 samples/sec  Loss 3.8776  LearningRate 0.0027  ProxyLR: 0.1331  Epoch: 22  Global Step: 125750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:31,565-Speed 3890.76 samples/sec  Loss 3.8847  LearningRate 0.0027  ProxyLR: 0.1329  Epoch: 22  Global Step: 125760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:34,196-Speed 3893.46 samples/sec  Loss 3.8690  LearningRate 0.0027  ProxyLR: 0.1328  Epoch: 22  Global Step: 125770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:36,828-Speed 3891.84 samples/sec  Loss 3.9192  LearningRate 0.0027  ProxyLR: 0.1326  Epoch: 22  Global Step: 125780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:39,458-Speed 3894.11 samples/sec  Loss 3.9819  LearningRate 0.0026  ProxyLR: 0.1325  Epoch: 22  Global Step: 125790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:42,089-Speed 3892.81 samples/sec  Loss 3.9945  LearningRate 0.0026  ProxyLR: 0.1323  Epoch: 22  Global Step: 125800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:44,722-Speed 3890.57 samples/sec  Loss 3.9979  LearningRate 0.0026  ProxyLR: 0.1321  Epoch: 22  Global Step: 125810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:47,354-Speed 3891.18 samples/sec  Loss 3.9341  LearningRate 0.0026  ProxyLR: 0.1320  Epoch: 22  Global Step: 125820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:49,987-Speed 3890.76 samples/sec  Loss 3.9343  LearningRate 0.0026  ProxyLR: 0.1318  Epoch: 22  Global Step: 125830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:41:52,604-Speed 3913.04 samples/sec  Loss 3.8497  LearningRate 0.0026  ProxyLR: 0.1316  Epoch: 22  Global Step: 125840   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:41:55,234-Speed 3894.14 samples/sec  Loss 3.9031  LearningRate 0.0026  ProxyLR: 0.1315  Epoch: 22  Global Step: 125850   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:41:57,866-Speed 3892.08 samples/sec  Loss 3.9472  LearningRate 0.0026  ProxyLR: 0.1313  Epoch: 22  Global Step: 125860   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:00,498-Speed 3892.06 samples/sec  Loss 4.0260  LearningRate 0.0026  ProxyLR: 0.1312  Epoch: 22  Global Step: 125870   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:03,131-Speed 3889.50 samples/sec  Loss 4.0095  LearningRate 0.0026  ProxyLR: 0.1310  Epoch: 22  Global Step: 125880   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:05,764-Speed 3890.05 samples/sec  Loss 3.9000  LearningRate 0.0026  ProxyLR: 0.1308  Epoch: 22  Global Step: 125890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:08,396-Speed 3891.50 samples/sec  Loss 3.9796  LearningRate 0.0026  ProxyLR: 0.1307  Epoch: 22  Global Step: 125900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:11,030-Speed 3887.91 samples/sec  Loss 3.8962  LearningRate 0.0026  ProxyLR: 0.1305  Epoch: 22  Global Step: 125910   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:13,666-Speed 3885.43 samples/sec  Loss 3.9165  LearningRate 0.0026  ProxyLR: 0.1304  Epoch: 22  Global Step: 125920   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:16,302-Speed 3886.88 samples/sec  Loss 3.8728  LearningRate 0.0026  ProxyLR: 0.1302  Epoch: 22  Global Step: 125930   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:42:18,936-Speed 3888.06 samples/sec  Loss 3.9459  LearningRate 0.0026  ProxyLR: 0.1300  Epoch: 22  Global Step: 125940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:42:21,570-Speed 3887.94 samples/sec  Loss 3.9650  LearningRate 0.0026  ProxyLR: 0.1299  Epoch: 22  Global Step: 125950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:42:24,207-Speed 3884.77 samples/sec  Loss 4.0141  LearningRate 0.0026  ProxyLR: 0.1297  Epoch: 22  Global Step: 125960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:42:26,844-Speed 3883.48 samples/sec  Loss 3.9538  LearningRate 0.0026  ProxyLR: 0.1296  Epoch: 22  Global Step: 125970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:42:29,480-Speed 3886.56 samples/sec  Loss 3.9354  LearningRate 0.0026  ProxyLR: 0.1294  Epoch: 22  Global Step: 125980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:42:32,115-Speed 3886.59 samples/sec  Loss 3.9019  LearningRate 0.0026  ProxyLR: 0.1292  Epoch: 22  Global Step: 125990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:42:34,750-Speed 3886.99 samples/sec  Loss 3.9627  LearningRate 0.0026  ProxyLR: 0.1291  Epoch: 22  Global Step: 126000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:43:24,050-[lfw][126000]XNorm: 21.976096
Training: 2023-05-05 00:43:24,050-[lfw][126000]Accuracy-Flip: 0.99800+-0.00163
Training: 2023-05-05 00:43:24,050-[lfw][126000]Accuracy-Highest: 0.99800
Training: 2023-05-05 00:43:24,051-[lfw][126000]TPR@1stNon-Zero-FPR of 0.00033: 0.99567
Training: 2023-05-05 00:43:24,051-[lfw][126000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 00:44:20,762-[cfp_fp][126000]XNorm: 21.730411
Training: 2023-05-05 00:44:20,762-[cfp_fp][126000]Accuracy-Flip: 0.98471+-0.00593
Training: 2023-05-05 00:44:20,762-[cfp_fp][126000]Accuracy-Highest: 0.98529
Training: 2023-05-05 00:44:20,763-[cfp_fp][126000]TPR@1stNon-Zero-FPR of 0.00029: 0.85829
Training: 2023-05-05 00:44:20,763-[cfp_fp][126000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 00:45:10,064-[agedb_30][126000]XNorm: 22.200984
Training: 2023-05-05 00:45:10,064-[agedb_30][126000]Accuracy-Flip: 0.97500+-0.00891
Training: 2023-05-05 00:45:10,064-[agedb_30][126000]Accuracy-Highest: 0.97500
Training: 2023-05-05 00:45:10,064-[agedb_30][126000]TPR@1stNon-Zero-FPR of 0.00033: 0.87833
Training: 2023-05-05 00:45:10,064-[agedb_30][126000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 00:46:00,766-[calfw][126000]XNorm: 22.140459
Training: 2023-05-05 00:46:00,766-[calfw][126000]Accuracy-Flip: 0.95583+-0.01254
Training: 2023-05-05 00:46:00,766-[calfw][126000]Accuracy-Highest: 0.95867
Training: 2023-05-05 00:46:00,767-[calfw][126000]TPR@1stNon-Zero-FPR of 0.00033: 0.84000
Training: 2023-05-05 00:46:00,767-[calfw][126000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 00:46:51,476-[cplfw][126000]XNorm: 21.150364
Training: 2023-05-05 00:46:51,476-[cplfw][126000]Accuracy-Flip: 0.92683+-0.01136
Training: 2023-05-05 00:46:51,477-[cplfw][126000]Accuracy-Highest: 0.92933
Training: 2023-05-05 00:46:51,477-[cplfw][126000]TPR@1stNon-Zero-FPR of 0.00033: 0.10833
Training: 2023-05-05 00:46:51,477-[cplfw][126000]Highest TPR@FPR: 0.13267
Training: 2023-05-05 00:46:56,141-Speed 39.18 samples/sec  Loss 3.9897  LearningRate 0.0026  ProxyLR: 0.1289  Epoch: 22  Global Step: 126010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:46:58,755-Speed 3918.29 samples/sec  Loss 3.9779  LearningRate 0.0026  ProxyLR: 0.1288  Epoch: 22  Global Step: 126020   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:01,384-Speed 3896.11 samples/sec  Loss 4.0037  LearningRate 0.0026  ProxyLR: 0.1286  Epoch: 22  Global Step: 126030   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:04,012-Speed 3896.91 samples/sec  Loss 3.9059  LearningRate 0.0026  ProxyLR: 0.1284  Epoch: 22  Global Step: 126040   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:06,642-Speed 3895.68 samples/sec  Loss 3.9955  LearningRate 0.0026  ProxyLR: 0.1283  Epoch: 22  Global Step: 126050   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:09,272-Speed 3895.52 samples/sec  Loss 3.9003  LearningRate 0.0026  ProxyLR: 0.1281  Epoch: 22  Global Step: 126060   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:11,903-Speed 3894.06 samples/sec  Loss 3.8748  LearningRate 0.0026  ProxyLR: 0.1280  Epoch: 22  Global Step: 126070   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:14,533-Speed 3894.33 samples/sec  Loss 3.9468  LearningRate 0.0026  ProxyLR: 0.1278  Epoch: 22  Global Step: 126080   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:17,163-Speed 3893.54 samples/sec  Loss 3.9533  LearningRate 0.0026  ProxyLR: 0.1276  Epoch: 22  Global Step: 126090   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:19,795-Speed 3892.41 samples/sec  Loss 3.8872  LearningRate 0.0025  ProxyLR: 0.1275  Epoch: 22  Global Step: 126100   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:22,428-Speed 3889.99 samples/sec  Loss 3.9413  LearningRate 0.0025  ProxyLR: 0.1273  Epoch: 22  Global Step: 126110   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:47:25,061-Speed 3890.22 samples/sec  Loss 3.9441  LearningRate 0.0025  ProxyLR: 0.1272  Epoch: 22  Global Step: 126120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:27,693-Speed 3890.64 samples/sec  Loss 3.8864  LearningRate 0.0025  ProxyLR: 0.1270  Epoch: 22  Global Step: 126130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:30,326-Speed 3890.20 samples/sec  Loss 4.0083  LearningRate 0.0025  ProxyLR: 0.1268  Epoch: 22  Global Step: 126140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:32,957-Speed 3893.65 samples/sec  Loss 3.9268  LearningRate 0.0025  ProxyLR: 0.1267  Epoch: 22  Global Step: 126150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:35,586-Speed 3895.27 samples/sec  Loss 3.9075  LearningRate 0.0025  ProxyLR: 0.1265  Epoch: 22  Global Step: 126160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:38,217-Speed 3893.34 samples/sec  Loss 3.9276  LearningRate 0.0025  ProxyLR: 0.1264  Epoch: 22  Global Step: 126170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:40,846-Speed 3895.39 samples/sec  Loss 3.9407  LearningRate 0.0025  ProxyLR: 0.1262  Epoch: 22  Global Step: 126180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:43,477-Speed 3893.51 samples/sec  Loss 3.9907  LearningRate 0.0025  ProxyLR: 0.1261  Epoch: 22  Global Step: 126190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:46,107-Speed 3895.19 samples/sec  Loss 3.8798  LearningRate 0.0025  ProxyLR: 0.1259  Epoch: 22  Global Step: 126200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:48,737-Speed 3894.31 samples/sec  Loss 3.9448  LearningRate 0.0025  ProxyLR: 0.1257  Epoch: 22  Global Step: 126210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:51,352-Speed 3916.08 samples/sec  Loss 3.9140  LearningRate 0.0025  ProxyLR: 0.1256  Epoch: 22  Global Step: 126220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:53,984-Speed 3892.26 samples/sec  Loss 3.9361  LearningRate 0.0025  ProxyLR: 0.1254  Epoch: 22  Global Step: 126230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:56,615-Speed 3892.42 samples/sec  Loss 3.9461  LearningRate 0.0025  ProxyLR: 0.1253  Epoch: 22  Global Step: 126240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:47:59,246-Speed 3892.99 samples/sec  Loss 3.9378  LearningRate 0.0025  ProxyLR: 0.1251  Epoch: 22  Global Step: 126250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:01,875-Speed 3895.73 samples/sec  Loss 3.8326  LearningRate 0.0025  ProxyLR: 0.1250  Epoch: 22  Global Step: 126260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:04,491-Speed 3915.75 samples/sec  Loss 3.8816  LearningRate 0.0025  ProxyLR: 0.1248  Epoch: 22  Global Step: 126270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:07,121-Speed 3894.43 samples/sec  Loss 3.9337  LearningRate 0.0025  ProxyLR: 0.1246  Epoch: 22  Global Step: 126280   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:09,750-Speed 3895.34 samples/sec  Loss 3.8864  LearningRate 0.0025  ProxyLR: 0.1245  Epoch: 22  Global Step: 126290   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:12,381-Speed 3893.70 samples/sec  Loss 3.9069  LearningRate 0.0025  ProxyLR: 0.1243  Epoch: 22  Global Step: 126300   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:15,012-Speed 3892.85 samples/sec  Loss 4.0301  LearningRate 0.0025  ProxyLR: 0.1242  Epoch: 22  Global Step: 126310   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:17,642-Speed 3894.30 samples/sec  Loss 3.9878  LearningRate 0.0025  ProxyLR: 0.1240  Epoch: 22  Global Step: 126320   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:20,273-Speed 3892.72 samples/sec  Loss 3.9719  LearningRate 0.0025  ProxyLR: 0.1239  Epoch: 22  Global Step: 126330   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:22,902-Speed 3896.57 samples/sec  Loss 3.8969  LearningRate 0.0025  ProxyLR: 0.1237  Epoch: 22  Global Step: 126340   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:25,532-Speed 3894.27 samples/sec  Loss 3.9366  LearningRate 0.0025  ProxyLR: 0.1235  Epoch: 22  Global Step: 126350   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:28,162-Speed 3893.87 samples/sec  Loss 3.9037  LearningRate 0.0025  ProxyLR: 0.1234  Epoch: 22  Global Step: 126360   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:48:30,793-Speed 3892.64 samples/sec  Loss 3.8487  LearningRate 0.0025  ProxyLR: 0.1232  Epoch: 22  Global Step: 126370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:33,422-Speed 3896.93 samples/sec  Loss 4.0036  LearningRate 0.0025  ProxyLR: 0.1231  Epoch: 22  Global Step: 126380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:36,052-Speed 3894.64 samples/sec  Loss 3.9607  LearningRate 0.0025  ProxyLR: 0.1229  Epoch: 22  Global Step: 126390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:38,684-Speed 3890.94 samples/sec  Loss 3.9554  LearningRate 0.0025  ProxyLR: 0.1228  Epoch: 22  Global Step: 126400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:41,315-Speed 3893.15 samples/sec  Loss 3.9571  LearningRate 0.0025  ProxyLR: 0.1226  Epoch: 22  Global Step: 126410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:43,945-Speed 3894.20 samples/sec  Loss 3.9638  LearningRate 0.0024  ProxyLR: 0.1225  Epoch: 22  Global Step: 126420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:46,573-Speed 3897.56 samples/sec  Loss 3.8615  LearningRate 0.0024  ProxyLR: 0.1223  Epoch: 22  Global Step: 126430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:49,205-Speed 3891.95 samples/sec  Loss 3.9320  LearningRate 0.0024  ProxyLR: 0.1221  Epoch: 22  Global Step: 126440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:51,835-Speed 3894.02 samples/sec  Loss 3.8556  LearningRate 0.0024  ProxyLR: 0.1220  Epoch: 22  Global Step: 126450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:54,464-Speed 3896.04 samples/sec  Loss 3.8223  LearningRate 0.0024  ProxyLR: 0.1218  Epoch: 22  Global Step: 126460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:57,080-Speed 3914.71 samples/sec  Loss 3.9053  LearningRate 0.0024  ProxyLR: 0.1217  Epoch: 22  Global Step: 126470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:48:59,712-Speed 3891.64 samples/sec  Loss 3.9641  LearningRate 0.0024  ProxyLR: 0.1215  Epoch: 22  Global Step: 126480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:02,342-Speed 3895.02 samples/sec  Loss 3.8386  LearningRate 0.0024  ProxyLR: 0.1214  Epoch: 22  Global Step: 126490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:04,973-Speed 3893.42 samples/sec  Loss 3.9624  LearningRate 0.0024  ProxyLR: 0.1212  Epoch: 22  Global Step: 126500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:07,605-Speed 3890.23 samples/sec  Loss 3.8696  LearningRate 0.0024  ProxyLR: 0.1211  Epoch: 22  Global Step: 126510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:10,240-Speed 3887.56 samples/sec  Loss 3.9153  LearningRate 0.0024  ProxyLR: 0.1209  Epoch: 22  Global Step: 126520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:12,873-Speed 3890.16 samples/sec  Loss 3.9497  LearningRate 0.0024  ProxyLR: 0.1207  Epoch: 22  Global Step: 126530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:15,507-Speed 3888.85 samples/sec  Loss 3.8649  LearningRate 0.0024  ProxyLR: 0.1206  Epoch: 22  Global Step: 126540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:18,140-Speed 3889.88 samples/sec  Loss 3.9114  LearningRate 0.0024  ProxyLR: 0.1204  Epoch: 22  Global Step: 126550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:20,774-Speed 3888.75 samples/sec  Loss 3.8108  LearningRate 0.0024  ProxyLR: 0.1203  Epoch: 22  Global Step: 126560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:23,397-Speed 3904.79 samples/sec  Loss 3.9357  LearningRate 0.0024  ProxyLR: 0.1201  Epoch: 22  Global Step: 126570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:26,030-Speed 3890.16 samples/sec  Loss 3.9231  LearningRate 0.0024  ProxyLR: 0.1200  Epoch: 22  Global Step: 126580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:28,663-Speed 3889.81 samples/sec  Loss 3.8208  LearningRate 0.0024  ProxyLR: 0.1198  Epoch: 22  Global Step: 126590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:49:31,284-Speed 3908.18 samples/sec  Loss 3.9515  LearningRate 0.0024  ProxyLR: 0.1197  Epoch: 22  Global Step: 126600   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:33,917-Speed 3888.83 samples/sec  Loss 3.9426  LearningRate 0.0024  ProxyLR: 0.1195  Epoch: 22  Global Step: 126610   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:36,550-Speed 3890.94 samples/sec  Loss 3.9136  LearningRate 0.0024  ProxyLR: 0.1194  Epoch: 22  Global Step: 126620   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:39,185-Speed 3886.76 samples/sec  Loss 3.9234  LearningRate 0.0024  ProxyLR: 0.1192  Epoch: 22  Global Step: 126630   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:41,818-Speed 3890.63 samples/sec  Loss 3.8527  LearningRate 0.0024  ProxyLR: 0.1191  Epoch: 22  Global Step: 126640   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:44,450-Speed 3890.81 samples/sec  Loss 3.8627  LearningRate 0.0024  ProxyLR: 0.1189  Epoch: 22  Global Step: 126650   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:47,083-Speed 3890.28 samples/sec  Loss 3.8855  LearningRate 0.0024  ProxyLR: 0.1187  Epoch: 22  Global Step: 126660   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:49,714-Speed 3893.27 samples/sec  Loss 3.9481  LearningRate 0.0024  ProxyLR: 0.1186  Epoch: 22  Global Step: 126670   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:52,345-Speed 3892.48 samples/sec  Loss 4.0292  LearningRate 0.0024  ProxyLR: 0.1184  Epoch: 22  Global Step: 126680   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:54,976-Speed 3892.49 samples/sec  Loss 3.8189  LearningRate 0.0024  ProxyLR: 0.1183  Epoch: 22  Global Step: 126690   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:49:57,606-Speed 3894.36 samples/sec  Loss 3.8933  LearningRate 0.0024  ProxyLR: 0.1181  Epoch: 22  Global Step: 126700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:00,237-Speed 3894.08 samples/sec  Loss 3.9546  LearningRate 0.0024  ProxyLR: 0.1180  Epoch: 22  Global Step: 126710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:02,854-Speed 3913.48 samples/sec  Loss 3.8975  LearningRate 0.0024  ProxyLR: 0.1178  Epoch: 22  Global Step: 126720   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:05,485-Speed 3892.09 samples/sec  Loss 3.9250  LearningRate 0.0024  ProxyLR: 0.1177  Epoch: 22  Global Step: 126730   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:08,117-Speed 3892.41 samples/sec  Loss 3.8829  LearningRate 0.0024  ProxyLR: 0.1175  Epoch: 22  Global Step: 126740   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:10,748-Speed 3893.16 samples/sec  Loss 3.9205  LearningRate 0.0023  ProxyLR: 0.1174  Epoch: 22  Global Step: 126750   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:13,378-Speed 3893.83 samples/sec  Loss 3.9073  LearningRate 0.0023  ProxyLR: 0.1172  Epoch: 22  Global Step: 126760   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:16,009-Speed 3892.58 samples/sec  Loss 3.8570  LearningRate 0.0023  ProxyLR: 0.1171  Epoch: 22  Global Step: 126770   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:18,642-Speed 3890.41 samples/sec  Loss 3.9112  LearningRate 0.0023  ProxyLR: 0.1169  Epoch: 22  Global Step: 126780   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:21,273-Speed 3893.43 samples/sec  Loss 3.8408  LearningRate 0.0023  ProxyLR: 0.1168  Epoch: 22  Global Step: 126790   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:23,904-Speed 3892.77 samples/sec  Loss 3.8403  LearningRate 0.0023  ProxyLR: 0.1166  Epoch: 22  Global Step: 126800   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:26,536-Speed 3891.04 samples/sec  Loss 3.8811  LearningRate 0.0023  ProxyLR: 0.1165  Epoch: 22  Global Step: 126810   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:50:29,169-Speed 3890.75 samples/sec  Loss 3.9328  LearningRate 0.0023  ProxyLR: 0.1163  Epoch: 22  Global Step: 126820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:31,800-Speed 3893.18 samples/sec  Loss 3.8879  LearningRate 0.0023  ProxyLR: 0.1162  Epoch: 22  Global Step: 126830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:34,431-Speed 3892.14 samples/sec  Loss 3.9016  LearningRate 0.0023  ProxyLR: 0.1160  Epoch: 22  Global Step: 126840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:37,064-Speed 3890.46 samples/sec  Loss 3.9377  LearningRate 0.0023  ProxyLR: 0.1158  Epoch: 22  Global Step: 126850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:39,695-Speed 3892.25 samples/sec  Loss 4.0134  LearningRate 0.0023  ProxyLR: 0.1157  Epoch: 22  Global Step: 126860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:42,327-Speed 3892.23 samples/sec  Loss 3.8832  LearningRate 0.0023  ProxyLR: 0.1155  Epoch: 22  Global Step: 126870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:44,959-Speed 3891.25 samples/sec  Loss 3.9177  LearningRate 0.0023  ProxyLR: 0.1154  Epoch: 22  Global Step: 126880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:47,591-Speed 3891.53 samples/sec  Loss 3.8592  LearningRate 0.0023  ProxyLR: 0.1152  Epoch: 22  Global Step: 126890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:50,223-Speed 3891.26 samples/sec  Loss 3.8992  LearningRate 0.0023  ProxyLR: 0.1151  Epoch: 22  Global Step: 126900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:52,855-Speed 3891.75 samples/sec  Loss 3.9713  LearningRate 0.0023  ProxyLR: 0.1149  Epoch: 22  Global Step: 126910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:55,476-Speed 3908.45 samples/sec  Loss 3.8449  LearningRate 0.0023  ProxyLR: 0.1148  Epoch: 22  Global Step: 126920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:50:58,107-Speed 3892.46 samples/sec  Loss 3.8686  LearningRate 0.0023  ProxyLR: 0.1146  Epoch: 22  Global Step: 126930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:00,726-Speed 3909.92 samples/sec  Loss 3.8045  LearningRate 0.0023  ProxyLR: 0.1145  Epoch: 22  Global Step: 126940   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:03,360-Speed 3889.67 samples/sec  Loss 3.8706  LearningRate 0.0023  ProxyLR: 0.1143  Epoch: 22  Global Step: 126950   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:05,992-Speed 3892.05 samples/sec  Loss 3.8259  LearningRate 0.0023  ProxyLR: 0.1142  Epoch: 22  Global Step: 126960   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:08,625-Speed 3889.65 samples/sec  Loss 3.9366  LearningRate 0.0023  ProxyLR: 0.1140  Epoch: 22  Global Step: 126970   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:11,256-Speed 3892.43 samples/sec  Loss 3.9330  LearningRate 0.0023  ProxyLR: 0.1139  Epoch: 22  Global Step: 126980   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:13,887-Speed 3892.67 samples/sec  Loss 3.8895  LearningRate 0.0023  ProxyLR: 0.1137  Epoch: 22  Global Step: 126990   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:16,520-Speed 3889.84 samples/sec  Loss 3.9110  LearningRate 0.0023  ProxyLR: 0.1136  Epoch: 22  Global Step: 127000   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:19,151-Speed 3892.80 samples/sec  Loss 3.9965  LearningRate 0.0023  ProxyLR: 0.1134  Epoch: 22  Global Step: 127010   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:21,783-Speed 3892.14 samples/sec  Loss 3.9345  LearningRate 0.0023  ProxyLR: 0.1133  Epoch: 22  Global Step: 127020   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:24,415-Speed 3891.91 samples/sec  Loss 3.8970  LearningRate 0.0023  ProxyLR: 0.1131  Epoch: 22  Global Step: 127030   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:51:27,048-Speed 3888.96 samples/sec  Loss 3.8855  LearningRate 0.0023  ProxyLR: 0.1130  Epoch: 22  Global Step: 127040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:29,680-Speed 3892.51 samples/sec  Loss 3.9003  LearningRate 0.0023  ProxyLR: 0.1128  Epoch: 22  Global Step: 127050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:32,312-Speed 3891.76 samples/sec  Loss 3.9201  LearningRate 0.0023  ProxyLR: 0.1127  Epoch: 22  Global Step: 127060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:34,945-Speed 3889.44 samples/sec  Loss 3.8396  LearningRate 0.0023  ProxyLR: 0.1125  Epoch: 22  Global Step: 127070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:37,576-Speed 3892.88 samples/sec  Loss 3.8518  LearningRate 0.0022  ProxyLR: 0.1124  Epoch: 22  Global Step: 127080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:40,208-Speed 3891.74 samples/sec  Loss 3.9080  LearningRate 0.0022  ProxyLR: 0.1122  Epoch: 22  Global Step: 127090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:42,842-Speed 3889.27 samples/sec  Loss 3.9036  LearningRate 0.0022  ProxyLR: 0.1121  Epoch: 22  Global Step: 127100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:45,472-Speed 3893.35 samples/sec  Loss 3.8760  LearningRate 0.0022  ProxyLR: 0.1119  Epoch: 22  Global Step: 127110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:48,104-Speed 3892.84 samples/sec  Loss 3.9175  LearningRate 0.0022  ProxyLR: 0.1118  Epoch: 22  Global Step: 127120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:50,737-Speed 3888.70 samples/sec  Loss 3.8944  LearningRate 0.0022  ProxyLR: 0.1116  Epoch: 22  Global Step: 127130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:53,355-Speed 3913.63 samples/sec  Loss 3.8960  LearningRate 0.0022  ProxyLR: 0.1115  Epoch: 22  Global Step: 127140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:55,987-Speed 3890.55 samples/sec  Loss 3.8566  LearningRate 0.0022  ProxyLR: 0.1113  Epoch: 22  Global Step: 127150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:51:58,618-Speed 3892.78 samples/sec  Loss 3.9431  LearningRate 0.0022  ProxyLR: 0.1112  Epoch: 22  Global Step: 127160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:01,249-Speed 3892.91 samples/sec  Loss 3.9336  LearningRate 0.0022  ProxyLR: 0.1111  Epoch: 22  Global Step: 127170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:03,882-Speed 3890.50 samples/sec  Loss 3.8854  LearningRate 0.0022  ProxyLR: 0.1109  Epoch: 22  Global Step: 127180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:06,516-Speed 3889.15 samples/sec  Loss 3.8743  LearningRate 0.0022  ProxyLR: 0.1108  Epoch: 22  Global Step: 127190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:09,148-Speed 3891.95 samples/sec  Loss 3.9106  LearningRate 0.0022  ProxyLR: 0.1106  Epoch: 22  Global Step: 127200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:11,778-Speed 3893.17 samples/sec  Loss 3.9328  LearningRate 0.0022  ProxyLR: 0.1105  Epoch: 22  Global Step: 127210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:14,410-Speed 3892.21 samples/sec  Loss 3.8573  LearningRate 0.0022  ProxyLR: 0.1103  Epoch: 22  Global Step: 127220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:17,043-Speed 3890.50 samples/sec  Loss 3.8771  LearningRate 0.0022  ProxyLR: 0.1102  Epoch: 22  Global Step: 127230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:19,660-Speed 3913.96 samples/sec  Loss 3.8780  LearningRate 0.0022  ProxyLR: 0.1100  Epoch: 22  Global Step: 127240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:22,291-Speed 3891.90 samples/sec  Loss 3.8569  LearningRate 0.0022  ProxyLR: 0.1099  Epoch: 22  Global Step: 127250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:24,922-Speed 3893.41 samples/sec  Loss 3.8673  LearningRate 0.0022  ProxyLR: 0.1097  Epoch: 22  Global Step: 127260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:27,554-Speed 3891.66 samples/sec  Loss 3.8948  LearningRate 0.0022  ProxyLR: 0.1096  Epoch: 22  Global Step: 127270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:30,186-Speed 3891.55 samples/sec  Loss 3.8223  LearningRate 0.0022  ProxyLR: 0.1094  Epoch: 22  Global Step: 127280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:32,820-Speed 3889.85 samples/sec  Loss 3.9374  LearningRate 0.0022  ProxyLR: 0.1093  Epoch: 22  Global Step: 127290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:35,449-Speed 3895.04 samples/sec  Loss 3.9038  LearningRate 0.0022  ProxyLR: 0.1091  Epoch: 22  Global Step: 127300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:38,081-Speed 3892.64 samples/sec  Loss 3.8536  LearningRate 0.0022  ProxyLR: 0.1090  Epoch: 22  Global Step: 127310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:40,712-Speed 3892.94 samples/sec  Loss 3.9062  LearningRate 0.0022  ProxyLR: 0.1088  Epoch: 22  Global Step: 127320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:43,342-Speed 3893.33 samples/sec  Loss 3.8489  LearningRate 0.0022  ProxyLR: 0.1087  Epoch: 22  Global Step: 127330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:45,973-Speed 3892.76 samples/sec  Loss 3.9439  LearningRate 0.0022  ProxyLR: 0.1085  Epoch: 22  Global Step: 127340   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 00:52:48,591-Speed 3913.48 samples/sec  Loss 3.9336  LearningRate 0.0022  ProxyLR: 0.1084  Epoch: 22  Global Step: 127350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:51,222-Speed 3892.95 samples/sec  Loss 3.9821  LearningRate 0.0022  ProxyLR: 0.1083  Epoch: 22  Global Step: 127360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:53,852-Speed 3893.13 samples/sec  Loss 3.8171  LearningRate 0.0022  ProxyLR: 0.1081  Epoch: 22  Global Step: 127370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:56,483-Speed 3893.26 samples/sec  Loss 3.8669  LearningRate 0.0022  ProxyLR: 0.1080  Epoch: 22  Global Step: 127380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:52:59,114-Speed 3893.54 samples/sec  Loss 3.8437  LearningRate 0.0022  ProxyLR: 0.1078  Epoch: 22  Global Step: 127390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:01,746-Speed 3891.84 samples/sec  Loss 3.8453  LearningRate 0.0022  ProxyLR: 0.1077  Epoch: 22  Global Step: 127400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:04,377-Speed 3892.50 samples/sec  Loss 3.7937  LearningRate 0.0022  ProxyLR: 0.1075  Epoch: 22  Global Step: 127410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:07,008-Speed 3892.27 samples/sec  Loss 3.8877  LearningRate 0.0021  ProxyLR: 0.1074  Epoch: 22  Global Step: 127420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:09,641-Speed 3890.79 samples/sec  Loss 3.8771  LearningRate 0.0021  ProxyLR: 0.1072  Epoch: 22  Global Step: 127430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:12,272-Speed 3892.31 samples/sec  Loss 3.8553  LearningRate 0.0021  ProxyLR: 0.1071  Epoch: 22  Global Step: 127440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:14,889-Speed 3913.90 samples/sec  Loss 3.9229  LearningRate 0.0021  ProxyLR: 0.1069  Epoch: 22  Global Step: 127450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:17,519-Speed 3894.22 samples/sec  Loss 3.8793  LearningRate 0.0021  ProxyLR: 0.1068  Epoch: 22  Global Step: 127460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:20,149-Speed 3895.07 samples/sec  Loss 3.9118  LearningRate 0.0021  ProxyLR: 0.1066  Epoch: 22  Global Step: 127470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:22,780-Speed 3892.90 samples/sec  Loss 3.9600  LearningRate 0.0021  ProxyLR: 0.1065  Epoch: 22  Global Step: 127480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:25,410-Speed 3894.47 samples/sec  Loss 3.8559  LearningRate 0.0021  ProxyLR: 0.1064  Epoch: 22  Global Step: 127490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:28,041-Speed 3893.34 samples/sec  Loss 3.9274  LearningRate 0.0021  ProxyLR: 0.1062  Epoch: 22  Global Step: 127500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:30,672-Speed 3892.77 samples/sec  Loss 3.8695  LearningRate 0.0021  ProxyLR: 0.1061  Epoch: 22  Global Step: 127510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:33,303-Speed 3893.20 samples/sec  Loss 3.9676  LearningRate 0.0021  ProxyLR: 0.1059  Epoch: 22  Global Step: 127520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:35,934-Speed 3892.54 samples/sec  Loss 3.8983  LearningRate 0.0021  ProxyLR: 0.1058  Epoch: 22  Global Step: 127530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:38,564-Speed 3894.22 samples/sec  Loss 3.9100  LearningRate 0.0021  ProxyLR: 0.1056  Epoch: 22  Global Step: 127540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:41,182-Speed 3913.23 samples/sec  Loss 3.8883  LearningRate 0.0021  ProxyLR: 0.1055  Epoch: 22  Global Step: 127550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:43,812-Speed 3893.28 samples/sec  Loss 3.9156  LearningRate 0.0021  ProxyLR: 0.1053  Epoch: 22  Global Step: 127560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:46,443-Speed 3893.69 samples/sec  Loss 3.9006  LearningRate 0.0021  ProxyLR: 0.1052  Epoch: 22  Global Step: 127570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:49,074-Speed 3892.37 samples/sec  Loss 3.9088  LearningRate 0.0021  ProxyLR: 0.1051  Epoch: 22  Global Step: 127580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:51,706-Speed 3892.60 samples/sec  Loss 3.9423  LearningRate 0.0021  ProxyLR: 0.1049  Epoch: 22  Global Step: 127590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:54,338-Speed 3890.26 samples/sec  Loss 3.8510  LearningRate 0.0021  ProxyLR: 0.1048  Epoch: 22  Global Step: 127600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:56,969-Speed 3893.67 samples/sec  Loss 3.8822  LearningRate 0.0021  ProxyLR: 0.1046  Epoch: 22  Global Step: 127610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:53:59,599-Speed 3893.82 samples/sec  Loss 3.8593  LearningRate 0.0021  ProxyLR: 0.1045  Epoch: 22  Global Step: 127620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:02,231-Speed 3892.16 samples/sec  Loss 3.9189  LearningRate 0.0021  ProxyLR: 0.1043  Epoch: 22  Global Step: 127630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:04,862-Speed 3892.65 samples/sec  Loss 3.8714  LearningRate 0.0021  ProxyLR: 0.1042  Epoch: 22  Global Step: 127640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:07,480-Speed 3912.56 samples/sec  Loss 3.8591  LearningRate 0.0021  ProxyLR: 0.1041  Epoch: 22  Global Step: 127650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:10,112-Speed 3892.30 samples/sec  Loss 3.9838  LearningRate 0.0021  ProxyLR: 0.1039  Epoch: 22  Global Step: 127660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:12,742-Speed 3893.61 samples/sec  Loss 3.9032  LearningRate 0.0021  ProxyLR: 0.1038  Epoch: 22  Global Step: 127670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:15,377-Speed 3887.12 samples/sec  Loss 3.8619  LearningRate 0.0021  ProxyLR: 0.1036  Epoch: 22  Global Step: 127680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:18,011-Speed 3889.38 samples/sec  Loss 3.9423  LearningRate 0.0021  ProxyLR: 0.1035  Epoch: 22  Global Step: 127690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:20,646-Speed 3886.48 samples/sec  Loss 3.8965  LearningRate 0.0021  ProxyLR: 0.1033  Epoch: 22  Global Step: 127700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:23,281-Speed 3887.85 samples/sec  Loss 3.8690  LearningRate 0.0021  ProxyLR: 0.1032  Epoch: 22  Global Step: 127710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:25,918-Speed 3883.17 samples/sec  Loss 3.8280  LearningRate 0.0021  ProxyLR: 0.1030  Epoch: 22  Global Step: 127720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:28,555-Speed 3885.02 samples/sec  Loss 3.9380  LearningRate 0.0021  ProxyLR: 0.1029  Epoch: 22  Global Step: 127730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:31,192-Speed 3883.79 samples/sec  Loss 3.9702  LearningRate 0.0021  ProxyLR: 0.1028  Epoch: 22  Global Step: 127740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:33,815-Speed 3904.84 samples/sec  Loss 3.8319  LearningRate 0.0021  ProxyLR: 0.1026  Epoch: 22  Global Step: 127750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:36,451-Speed 3885.13 samples/sec  Loss 3.8760  LearningRate 0.0020  ProxyLR: 0.1025  Epoch: 22  Global Step: 127760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:39,087-Speed 3885.26 samples/sec  Loss 3.8139  LearningRate 0.0020  ProxyLR: 0.1023  Epoch: 22  Global Step: 127770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:41,723-Speed 3885.75 samples/sec  Loss 3.8537  LearningRate 0.0020  ProxyLR: 0.1022  Epoch: 22  Global Step: 127780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:44,360-Speed 3884.40 samples/sec  Loss 3.8975  LearningRate 0.0020  ProxyLR: 0.1021  Epoch: 22  Global Step: 127790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:46,995-Speed 3886.86 samples/sec  Loss 3.8553  LearningRate 0.0020  ProxyLR: 0.1019  Epoch: 22  Global Step: 127800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:49,631-Speed 3885.68 samples/sec  Loss 4.0404  LearningRate 0.0020  ProxyLR: 0.1018  Epoch: 22  Global Step: 127810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:54:52,253-Speed 3905.59 samples/sec  Loss 3.8884  LearningRate 0.0020  ProxyLR: 0.1016  Epoch: 22  Global Step: 127820   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:54:54,888-Speed 3887.98 samples/sec  Loss 4.0025  LearningRate 0.0020  ProxyLR: 0.1015  Epoch: 22  Global Step: 127830   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:54:57,521-Speed 3890.14 samples/sec  Loss 3.8766  LearningRate 0.0020  ProxyLR: 0.1013  Epoch: 22  Global Step: 127840   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:00,152-Speed 3892.02 samples/sec  Loss 3.9457  LearningRate 0.0020  ProxyLR: 0.1012  Epoch: 22  Global Step: 127850   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:02,785-Speed 3890.27 samples/sec  Loss 3.8525  LearningRate 0.0020  ProxyLR: 0.1011  Epoch: 22  Global Step: 127860   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:05,422-Speed 3884.89 samples/sec  Loss 3.8915  LearningRate 0.0020  ProxyLR: 0.1009  Epoch: 22  Global Step: 127870   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:08,057-Speed 3887.08 samples/sec  Loss 3.9113  LearningRate 0.0020  ProxyLR: 0.1008  Epoch: 22  Global Step: 127880   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:10,690-Speed 3889.90 samples/sec  Loss 3.8885  LearningRate 0.0020  ProxyLR: 0.1006  Epoch: 22  Global Step: 127890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:13,324-Speed 3888.18 samples/sec  Loss 3.8569  LearningRate 0.0020  ProxyLR: 0.1005  Epoch: 22  Global Step: 127900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:15,959-Speed 3887.54 samples/sec  Loss 3.9092  LearningRate 0.0020  ProxyLR: 0.1004  Epoch: 22  Global Step: 127910   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 00:55:18,594-Speed 3887.09 samples/sec  Loss 3.8851  LearningRate 0.0020  ProxyLR: 0.1002  Epoch: 22  Global Step: 127920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:21,231-Speed 3883.76 samples/sec  Loss 3.8983  LearningRate 0.0020  ProxyLR: 0.1001  Epoch: 22  Global Step: 127930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:23,866-Speed 3886.38 samples/sec  Loss 3.8454  LearningRate 0.0020  ProxyLR: 0.0999  Epoch: 22  Global Step: 127940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:26,503-Speed 3884.64 samples/sec  Loss 3.8387  LearningRate 0.0020  ProxyLR: 0.0998  Epoch: 22  Global Step: 127950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:29,138-Speed 3887.75 samples/sec  Loss 3.9325  LearningRate 0.0020  ProxyLR: 0.0996  Epoch: 22  Global Step: 127960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:31,772-Speed 3888.02 samples/sec  Loss 3.8395  LearningRate 0.0020  ProxyLR: 0.0995  Epoch: 22  Global Step: 127970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:34,406-Speed 3887.98 samples/sec  Loss 3.8767  LearningRate 0.0020  ProxyLR: 0.0994  Epoch: 22  Global Step: 127980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:37,042-Speed 3885.36 samples/sec  Loss 3.8265  LearningRate 0.0020  ProxyLR: 0.0992  Epoch: 22  Global Step: 127990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:55:39,677-Speed 3888.26 samples/sec  Loss 3.8290  LearningRate 0.0020  ProxyLR: 0.0991  Epoch: 22  Global Step: 128000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 00:56:29,246-[lfw][128000]XNorm: 21.988847
Training: 2023-05-05 00:56:29,246-[lfw][128000]Accuracy-Flip: 0.99767+-0.00238
Training: 2023-05-05 00:56:29,246-[lfw][128000]Accuracy-Highest: 0.99800
Training: 2023-05-05 00:56:29,246-[lfw][128000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 00:56:29,246-[lfw][128000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 00:57:26,249-[cfp_fp][128000]XNorm: 21.750426
Training: 2023-05-05 00:57:26,249-[cfp_fp][128000]Accuracy-Flip: 0.98500+-0.00488
Training: 2023-05-05 00:57:26,250-[cfp_fp][128000]Accuracy-Highest: 0.98529
Training: 2023-05-05 00:57:26,250-[cfp_fp][128000]TPR@1stNon-Zero-FPR of 0.00029: 0.88514
Training: 2023-05-05 00:57:26,250-[cfp_fp][128000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 00:58:15,906-[agedb_30][128000]XNorm: 22.155979
Training: 2023-05-05 00:58:15,906-[agedb_30][128000]Accuracy-Flip: 0.97617+-0.00771
Training: 2023-05-05 00:58:15,907-[agedb_30][128000]Accuracy-Highest: 0.97617
Training: 2023-05-05 00:58:15,907-[agedb_30][128000]TPR@1stNon-Zero-FPR of 0.00033: 0.87133
Training: 2023-05-05 00:58:15,907-[agedb_30][128000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 00:59:06,796-[calfw][128000]XNorm: 22.143230
Training: 2023-05-05 00:59:06,796-[calfw][128000]Accuracy-Flip: 0.95600+-0.01230
Training: 2023-05-05 00:59:06,796-[calfw][128000]Accuracy-Highest: 0.95867
Training: 2023-05-05 00:59:06,796-[calfw][128000]TPR@1stNon-Zero-FPR of 0.00033: 0.83600
Training: 2023-05-05 00:59:06,796-[calfw][128000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 00:59:57,707-[cplfw][128000]XNorm: 21.124065
Training: 2023-05-05 00:59:57,707-[cplfw][128000]Accuracy-Flip: 0.92733+-0.01300
Training: 2023-05-05 00:59:57,707-[cplfw][128000]Accuracy-Highest: 0.92933
Training: 2023-05-05 00:59:57,708-[cplfw][128000]TPR@1stNon-Zero-FPR of 0.00033: 0.05500
Training: 2023-05-05 00:59:57,708-[cplfw][128000]Highest TPR@FPR: 0.13267
Training: 2023-05-05 01:00:01,005-Speed 39.18 samples/sec  Loss 3.8859  LearningRate 0.0020  ProxyLR: 0.0989  Epoch: 22  Global Step: 128010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:03,614-Speed 3925.38 samples/sec  Loss 3.8782  LearningRate 0.0020  ProxyLR: 0.0988  Epoch: 22  Global Step: 128020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:06,238-Speed 3903.54 samples/sec  Loss 3.8792  LearningRate 0.0020  ProxyLR: 0.0987  Epoch: 22  Global Step: 128030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:08,863-Speed 3901.49 samples/sec  Loss 3.8641  LearningRate 0.0020  ProxyLR: 0.0985  Epoch: 22  Global Step: 128040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:11,488-Speed 3904.19 samples/sec  Loss 3.8842  LearningRate 0.0020  ProxyLR: 0.0984  Epoch: 22  Global Step: 128050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:14,113-Speed 3900.84 samples/sec  Loss 3.8986  LearningRate 0.0020  ProxyLR: 0.0982  Epoch: 22  Global Step: 128060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:16,744-Speed 3892.98 samples/sec  Loss 3.8460  LearningRate 0.0020  ProxyLR: 0.0981  Epoch: 22  Global Step: 128070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:19,375-Speed 3893.32 samples/sec  Loss 3.8010  LearningRate 0.0020  ProxyLR: 0.0980  Epoch: 22  Global Step: 128080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:22,006-Speed 3892.55 samples/sec  Loss 3.8990  LearningRate 0.0020  ProxyLR: 0.0978  Epoch: 22  Global Step: 128090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:24,639-Speed 3891.29 samples/sec  Loss 3.8237  LearningRate 0.0020  ProxyLR: 0.0977  Epoch: 22  Global Step: 128100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:27,272-Speed 3889.21 samples/sec  Loss 3.9863  LearningRate 0.0020  ProxyLR: 0.0976  Epoch: 22  Global Step: 128110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:29,891-Speed 3911.20 samples/sec  Loss 3.9863  LearningRate 0.0019  ProxyLR: 0.0974  Epoch: 22  Global Step: 128120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:32,524-Speed 3889.06 samples/sec  Loss 3.8953  LearningRate 0.0019  ProxyLR: 0.0973  Epoch: 22  Global Step: 128130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:35,158-Speed 3889.68 samples/sec  Loss 3.8133  LearningRate 0.0019  ProxyLR: 0.0971  Epoch: 22  Global Step: 128140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:37,789-Speed 3892.48 samples/sec  Loss 3.8941  LearningRate 0.0019  ProxyLR: 0.0970  Epoch: 22  Global Step: 128150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:40,422-Speed 3890.09 samples/sec  Loss 3.8425  LearningRate 0.0019  ProxyLR: 0.0969  Epoch: 22  Global Step: 128160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:43,061-Speed 3881.06 samples/sec  Loss 3.8612  LearningRate 0.0019  ProxyLR: 0.0967  Epoch: 22  Global Step: 128170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:45,698-Speed 3884.62 samples/sec  Loss 3.8455  LearningRate 0.0019  ProxyLR: 0.0966  Epoch: 22  Global Step: 128180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:48,333-Speed 3886.32 samples/sec  Loss 3.8912  LearningRate 0.0019  ProxyLR: 0.0964  Epoch: 22  Global Step: 128190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:50,970-Speed 3884.27 samples/sec  Loss 3.9512  LearningRate 0.0019  ProxyLR: 0.0963  Epoch: 22  Global Step: 128200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:53,608-Speed 3883.23 samples/sec  Loss 3.8705  LearningRate 0.0019  ProxyLR: 0.0962  Epoch: 22  Global Step: 128210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:56,231-Speed 3903.93 samples/sec  Loss 3.9339  LearningRate 0.0019  ProxyLR: 0.0960  Epoch: 22  Global Step: 128220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:00:58,871-Speed 3880.31 samples/sec  Loss 3.8947  LearningRate 0.0019  ProxyLR: 0.0959  Epoch: 22  Global Step: 128230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:01,507-Speed 3885.70 samples/sec  Loss 3.9103  LearningRate 0.0019  ProxyLR: 0.0958  Epoch: 22  Global Step: 128240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:04,144-Speed 3884.47 samples/sec  Loss 3.9363  LearningRate 0.0019  ProxyLR: 0.0956  Epoch: 22  Global Step: 128250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:06,782-Speed 3882.42 samples/sec  Loss 3.8731  LearningRate 0.0019  ProxyLR: 0.0955  Epoch: 22  Global Step: 128260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:09,420-Speed 3882.83 samples/sec  Loss 3.8127  LearningRate 0.0019  ProxyLR: 0.0953  Epoch: 22  Global Step: 128270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:12,057-Speed 3883.06 samples/sec  Loss 3.9083  LearningRate 0.0019  ProxyLR: 0.0952  Epoch: 22  Global Step: 128280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:14,695-Speed 3883.81 samples/sec  Loss 3.8130  LearningRate 0.0019  ProxyLR: 0.0951  Epoch: 22  Global Step: 128290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:17,332-Speed 3883.93 samples/sec  Loss 3.7948  LearningRate 0.0019  ProxyLR: 0.0949  Epoch: 22  Global Step: 128300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:19,966-Speed 3888.50 samples/sec  Loss 3.8687  LearningRate 0.0019  ProxyLR: 0.0948  Epoch: 22  Global Step: 128310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:22,587-Speed 3908.01 samples/sec  Loss 3.8153  LearningRate 0.0019  ProxyLR: 0.0947  Epoch: 22  Global Step: 128320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:25,219-Speed 3890.37 samples/sec  Loss 3.8381  LearningRate 0.0019  ProxyLR: 0.0945  Epoch: 22  Global Step: 128330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:27,854-Speed 3888.19 samples/sec  Loss 3.8814  LearningRate 0.0019  ProxyLR: 0.0944  Epoch: 22  Global Step: 128340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:30,489-Speed 3886.47 samples/sec  Loss 3.8911  LearningRate 0.0019  ProxyLR: 0.0942  Epoch: 22  Global Step: 128350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:33,124-Speed 3886.66 samples/sec  Loss 3.8524  LearningRate 0.0019  ProxyLR: 0.0941  Epoch: 22  Global Step: 128360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:35,758-Speed 3888.26 samples/sec  Loss 3.8972  LearningRate 0.0019  ProxyLR: 0.0940  Epoch: 22  Global Step: 128370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:38,394-Speed 3886.22 samples/sec  Loss 3.8558  LearningRate 0.0019  ProxyLR: 0.0938  Epoch: 22  Global Step: 128380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:41,026-Speed 3891.86 samples/sec  Loss 3.8801  LearningRate 0.0019  ProxyLR: 0.0937  Epoch: 22  Global Step: 128390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:43,658-Speed 3891.23 samples/sec  Loss 3.8542  LearningRate 0.0019  ProxyLR: 0.0936  Epoch: 22  Global Step: 128400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:46,289-Speed 3892.55 samples/sec  Loss 3.8838  LearningRate 0.0019  ProxyLR: 0.0934  Epoch: 22  Global Step: 128410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:48,907-Speed 3912.71 samples/sec  Loss 3.9296  LearningRate 0.0019  ProxyLR: 0.0933  Epoch: 22  Global Step: 128420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:51,537-Speed 3894.37 samples/sec  Loss 3.8221  LearningRate 0.0019  ProxyLR: 0.0932  Epoch: 22  Global Step: 128430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:54,169-Speed 3891.78 samples/sec  Loss 3.8858  LearningRate 0.0019  ProxyLR: 0.0930  Epoch: 22  Global Step: 128440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:56,799-Speed 3895.04 samples/sec  Loss 3.8716  LearningRate 0.0019  ProxyLR: 0.0929  Epoch: 22  Global Step: 128450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:01:59,431-Speed 3891.15 samples/sec  Loss 3.8889  LearningRate 0.0019  ProxyLR: 0.0927  Epoch: 22  Global Step: 128460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:02,062-Speed 3892.06 samples/sec  Loss 3.8726  LearningRate 0.0019  ProxyLR: 0.0926  Epoch: 22  Global Step: 128470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:04,692-Speed 3894.25 samples/sec  Loss 3.7439  LearningRate 0.0018  ProxyLR: 0.0925  Epoch: 22  Global Step: 128480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:07,322-Speed 3894.98 samples/sec  Loss 3.8329  LearningRate 0.0018  ProxyLR: 0.0923  Epoch: 22  Global Step: 128490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:09,952-Speed 3894.04 samples/sec  Loss 3.8929  LearningRate 0.0018  ProxyLR: 0.0922  Epoch: 22  Global Step: 128500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:12,583-Speed 3893.01 samples/sec  Loss 3.8689  LearningRate 0.0018  ProxyLR: 0.0921  Epoch: 22  Global Step: 128510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:15,213-Speed 3895.23 samples/sec  Loss 3.8829  LearningRate 0.0018  ProxyLR: 0.0919  Epoch: 22  Global Step: 128520   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:02:17,831-Speed 3912.38 samples/sec  Loss 3.8554  LearningRate 0.0018  ProxyLR: 0.0918  Epoch: 22  Global Step: 128530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:20,462-Speed 3892.91 samples/sec  Loss 3.8846  LearningRate 0.0018  ProxyLR: 0.0917  Epoch: 22  Global Step: 128540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:23,079-Speed 3913.70 samples/sec  Loss 3.8485  LearningRate 0.0018  ProxyLR: 0.0915  Epoch: 22  Global Step: 128550   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:25,711-Speed 3891.51 samples/sec  Loss 3.8860  LearningRate 0.0018  ProxyLR: 0.0914  Epoch: 22  Global Step: 128560   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:28,348-Speed 3883.28 samples/sec  Loss 3.7994  LearningRate 0.0018  ProxyLR: 0.0913  Epoch: 22  Global Step: 128570   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:30,979-Speed 3892.97 samples/sec  Loss 3.8818  LearningRate 0.0018  ProxyLR: 0.0911  Epoch: 22  Global Step: 128580   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:33,610-Speed 3893.93 samples/sec  Loss 3.9448  LearningRate 0.0018  ProxyLR: 0.0910  Epoch: 22  Global Step: 128590   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:36,239-Speed 3894.88 samples/sec  Loss 3.8217  LearningRate 0.0018  ProxyLR: 0.0909  Epoch: 22  Global Step: 128600   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:38,868-Speed 3895.89 samples/sec  Loss 3.8007  LearningRate 0.0018  ProxyLR: 0.0907  Epoch: 22  Global Step: 128610   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:41,497-Speed 3896.43 samples/sec  Loss 3.8473  LearningRate 0.0018  ProxyLR: 0.0906  Epoch: 22  Global Step: 128620   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:44,127-Speed 3894.92 samples/sec  Loss 3.8866  LearningRate 0.0018  ProxyLR: 0.0905  Epoch: 22  Global Step: 128630   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:46,756-Speed 3895.20 samples/sec  Loss 3.8769  LearningRate 0.0018  ProxyLR: 0.0903  Epoch: 22  Global Step: 128640   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:02:49,385-Speed 3896.51 samples/sec  Loss 3.8034  LearningRate 0.0018  ProxyLR: 0.0902  Epoch: 22  Global Step: 128650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:52,015-Speed 3893.53 samples/sec  Loss 3.9350  LearningRate 0.0018  ProxyLR: 0.0901  Epoch: 22  Global Step: 128660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:54,646-Speed 3894.02 samples/sec  Loss 3.8494  LearningRate 0.0018  ProxyLR: 0.0899  Epoch: 22  Global Step: 128670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:57,275-Speed 3894.81 samples/sec  Loss 3.8977  LearningRate 0.0018  ProxyLR: 0.0898  Epoch: 22  Global Step: 128680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:02:59,907-Speed 3891.71 samples/sec  Loss 3.7914  LearningRate 0.0018  ProxyLR: 0.0897  Epoch: 22  Global Step: 128690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:02,538-Speed 3893.71 samples/sec  Loss 3.8945  LearningRate 0.0018  ProxyLR: 0.0895  Epoch: 22  Global Step: 128700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:05,171-Speed 3889.76 samples/sec  Loss 3.8894  LearningRate 0.0018  ProxyLR: 0.0894  Epoch: 22  Global Step: 128710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:07,791-Speed 3909.11 samples/sec  Loss 3.8661  LearningRate 0.0018  ProxyLR: 0.0893  Epoch: 22  Global Step: 128720   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:10,423-Speed 3892.09 samples/sec  Loss 3.7944  LearningRate 0.0018  ProxyLR: 0.0891  Epoch: 22  Global Step: 128730   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:13,057-Speed 3887.50 samples/sec  Loss 3.8122  LearningRate 0.0018  ProxyLR: 0.0890  Epoch: 22  Global Step: 128740   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:15,692-Speed 3887.82 samples/sec  Loss 3.8950  LearningRate 0.0018  ProxyLR: 0.0889  Epoch: 22  Global Step: 128750   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:18,327-Speed 3886.81 samples/sec  Loss 3.8707  LearningRate 0.0018  ProxyLR: 0.0887  Epoch: 22  Global Step: 128760   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:20,959-Speed 3891.52 samples/sec  Loss 3.8877  LearningRate 0.0018  ProxyLR: 0.0886  Epoch: 22  Global Step: 128770   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:23,592-Speed 3890.51 samples/sec  Loss 3.9467  LearningRate 0.0018  ProxyLR: 0.0885  Epoch: 22  Global Step: 128780   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:26,223-Speed 3893.21 samples/sec  Loss 3.8163  LearningRate 0.0018  ProxyLR: 0.0883  Epoch: 22  Global Step: 128790   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:28,855-Speed 3890.27 samples/sec  Loss 3.9191  LearningRate 0.0018  ProxyLR: 0.0882  Epoch: 22  Global Step: 128800   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:31,487-Speed 3891.39 samples/sec  Loss 3.8324  LearningRate 0.0018  ProxyLR: 0.0881  Epoch: 22  Global Step: 128810   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:34,124-Speed 3884.90 samples/sec  Loss 3.8492  LearningRate 0.0018  ProxyLR: 0.0879  Epoch: 22  Global Step: 128820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:36,760-Speed 3885.59 samples/sec  Loss 3.8378  LearningRate 0.0018  ProxyLR: 0.0878  Epoch: 22  Global Step: 128830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:39,395-Speed 3886.33 samples/sec  Loss 3.9205  LearningRate 0.0018  ProxyLR: 0.0877  Epoch: 22  Global Step: 128840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:42,031-Speed 3886.02 samples/sec  Loss 3.8512  LearningRate 0.0018  ProxyLR: 0.0875  Epoch: 22  Global Step: 128850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:44,664-Speed 3890.04 samples/sec  Loss 3.8911  LearningRate 0.0017  ProxyLR: 0.0874  Epoch: 22  Global Step: 128860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:03:47,286-Speed 3906.68 samples/sec  Loss 3.8268  LearningRate 0.0017  ProxyLR: 0.0873  Epoch: 22  Global Step: 128870   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:49,924-Speed 3881.91 samples/sec  Loss 3.9120  LearningRate 0.0017  ProxyLR: 0.0871  Epoch: 22  Global Step: 128880   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:52,560-Speed 3886.72 samples/sec  Loss 3.7904  LearningRate 0.0017  ProxyLR: 0.0870  Epoch: 22  Global Step: 128890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:55,195-Speed 3887.33 samples/sec  Loss 3.8479  LearningRate 0.0017  ProxyLR: 0.0869  Epoch: 22  Global Step: 128900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:03:57,832-Speed 3883.16 samples/sec  Loss 3.8612  LearningRate 0.0017  ProxyLR: 0.0868  Epoch: 22  Global Step: 128910   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:04:00,467-Speed 3887.32 samples/sec  Loss 3.8240  LearningRate 0.0017  ProxyLR: 0.0866  Epoch: 22  Global Step: 128920   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:04:03,101-Speed 3887.98 samples/sec  Loss 3.8468  LearningRate 0.0017  ProxyLR: 0.0865  Epoch: 22  Global Step: 128930   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:04:05,738-Speed 3883.96 samples/sec  Loss 3.8880  LearningRate 0.0017  ProxyLR: 0.0864  Epoch: 22  Global Step: 128940   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:04:08,373-Speed 3887.44 samples/sec  Loss 3.7717  LearningRate 0.0017  ProxyLR: 0.0862  Epoch: 22  Global Step: 128950   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:04:11,009-Speed 3886.43 samples/sec  Loss 3.8641  LearningRate 0.0017  ProxyLR: 0.0861  Epoch: 22  Global Step: 128960   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:04:13,644-Speed 3886.81 samples/sec  Loss 3.8446  LearningRate 0.0017  ProxyLR: 0.0860  Epoch: 22  Global Step: 128970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:16,281-Speed 3883.65 samples/sec  Loss 3.8766  LearningRate 0.0017  ProxyLR: 0.0858  Epoch: 22  Global Step: 128980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:18,918-Speed 3884.42 samples/sec  Loss 3.8841  LearningRate 0.0017  ProxyLR: 0.0857  Epoch: 22  Global Step: 128990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:21,554-Speed 3885.36 samples/sec  Loss 3.7876  LearningRate 0.0017  ProxyLR: 0.0856  Epoch: 22  Global Step: 129000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:24,189-Speed 3886.94 samples/sec  Loss 3.9066  LearningRate 0.0017  ProxyLR: 0.0854  Epoch: 22  Global Step: 129010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:26,827-Speed 3883.04 samples/sec  Loss 3.8452  LearningRate 0.0017  ProxyLR: 0.0853  Epoch: 22  Global Step: 129020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:29,463-Speed 3885.20 samples/sec  Loss 3.7950  LearningRate 0.0017  ProxyLR: 0.0852  Epoch: 22  Global Step: 129030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:32,098-Speed 3887.03 samples/sec  Loss 3.7936  LearningRate 0.0017  ProxyLR: 0.0851  Epoch: 22  Global Step: 129040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:34,732-Speed 3888.25 samples/sec  Loss 3.8516  LearningRate 0.0017  ProxyLR: 0.0849  Epoch: 22  Global Step: 129050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:37,366-Speed 3889.01 samples/sec  Loss 3.8596  LearningRate 0.0017  ProxyLR: 0.0848  Epoch: 22  Global Step: 129060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:39,985-Speed 3910.31 samples/sec  Loss 3.8169  LearningRate 0.0017  ProxyLR: 0.0847  Epoch: 22  Global Step: 129070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:42,619-Speed 3888.66 samples/sec  Loss 3.8906  LearningRate 0.0017  ProxyLR: 0.0845  Epoch: 22  Global Step: 129080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:45,252-Speed 3889.47 samples/sec  Loss 3.8593  LearningRate 0.0017  ProxyLR: 0.0844  Epoch: 22  Global Step: 129090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:47,887-Speed 3888.02 samples/sec  Loss 3.8928  LearningRate 0.0017  ProxyLR: 0.0843  Epoch: 22  Global Step: 129100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:50,518-Speed 3892.17 samples/sec  Loss 3.8461  LearningRate 0.0017  ProxyLR: 0.0842  Epoch: 22  Global Step: 129110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:53,153-Speed 3887.78 samples/sec  Loss 3.8659  LearningRate 0.0017  ProxyLR: 0.0840  Epoch: 22  Global Step: 129120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:55,786-Speed 3889.16 samples/sec  Loss 3.8600  LearningRate 0.0017  ProxyLR: 0.0839  Epoch: 22  Global Step: 129130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:04:58,419-Speed 3889.62 samples/sec  Loss 3.8299  LearningRate 0.0017  ProxyLR: 0.0838  Epoch: 22  Global Step: 129140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:01,054-Speed 3887.96 samples/sec  Loss 3.8825  LearningRate 0.0017  ProxyLR: 0.0836  Epoch: 22  Global Step: 129150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:03,688-Speed 3888.09 samples/sec  Loss 3.8995  LearningRate 0.0017  ProxyLR: 0.0835  Epoch: 22  Global Step: 129160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:06,323-Speed 3887.18 samples/sec  Loss 3.8507  LearningRate 0.0017  ProxyLR: 0.0834  Epoch: 22  Global Step: 129170   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:05:08,943-Speed 3909.10 samples/sec  Loss 3.8763  LearningRate 0.0017  ProxyLR: 0.0833  Epoch: 22  Global Step: 129180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:11,578-Speed 3887.01 samples/sec  Loss 3.8053  LearningRate 0.0017  ProxyLR: 0.0831  Epoch: 22  Global Step: 129190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:14,215-Speed 3884.13 samples/sec  Loss 3.8960  LearningRate 0.0017  ProxyLR: 0.0830  Epoch: 22  Global Step: 129200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:16,862-Speed 3869.47 samples/sec  Loss 3.9272  LearningRate 0.0017  ProxyLR: 0.0829  Epoch: 22  Global Step: 129210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:19,502-Speed 3880.30 samples/sec  Loss 3.7936  LearningRate 0.0017  ProxyLR: 0.0827  Epoch: 22  Global Step: 129220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:22,141-Speed 3881.20 samples/sec  Loss 3.8542  LearningRate 0.0017  ProxyLR: 0.0826  Epoch: 22  Global Step: 129230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:24,778-Speed 3883.64 samples/sec  Loss 3.8024  LearningRate 0.0016  ProxyLR: 0.0825  Epoch: 22  Global Step: 129240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:27,415-Speed 3884.32 samples/sec  Loss 3.7784  LearningRate 0.0016  ProxyLR: 0.0824  Epoch: 22  Global Step: 129250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:30,055-Speed 3881.03 samples/sec  Loss 3.8098  LearningRate 0.0016  ProxyLR: 0.0822  Epoch: 22  Global Step: 129260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:32,691-Speed 3885.36 samples/sec  Loss 3.8443  LearningRate 0.0016  ProxyLR: 0.0821  Epoch: 22  Global Step: 129270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:35,314-Speed 3905.25 samples/sec  Loss 3.7765  LearningRate 0.0016  ProxyLR: 0.0820  Epoch: 22  Global Step: 129280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:37,955-Speed 3878.45 samples/sec  Loss 3.8301  LearningRate 0.0016  ProxyLR: 0.0818  Epoch: 22  Global Step: 129290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:40,592-Speed 3883.91 samples/sec  Loss 3.8342  LearningRate 0.0016  ProxyLR: 0.0817  Epoch: 22  Global Step: 129300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:43,230-Speed 3882.47 samples/sec  Loss 3.9026  LearningRate 0.0016  ProxyLR: 0.0816  Epoch: 22  Global Step: 129310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:45,867-Speed 3884.64 samples/sec  Loss 3.8214  LearningRate 0.0016  ProxyLR: 0.0815  Epoch: 22  Global Step: 129320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:48,504-Speed 3882.79 samples/sec  Loss 3.8721  LearningRate 0.0016  ProxyLR: 0.0813  Epoch: 22  Global Step: 129330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:51,141-Speed 3884.17 samples/sec  Loss 3.8326  LearningRate 0.0016  ProxyLR: 0.0812  Epoch: 22  Global Step: 129340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:53,781-Speed 3880.15 samples/sec  Loss 3.8431  LearningRate 0.0016  ProxyLR: 0.0811  Epoch: 22  Global Step: 129350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:56,422-Speed 3878.90 samples/sec  Loss 3.8928  LearningRate 0.0016  ProxyLR: 0.0810  Epoch: 22  Global Step: 129360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:05:59,060-Speed 3882.70 samples/sec  Loss 3.8731  LearningRate 0.0016  ProxyLR: 0.0808  Epoch: 22  Global Step: 129370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:01,682-Speed 3906.14 samples/sec  Loss 3.7926  LearningRate 0.0016  ProxyLR: 0.0807  Epoch: 22  Global Step: 129380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:04,324-Speed 3877.34 samples/sec  Loss 3.8590  LearningRate 0.0016  ProxyLR: 0.0806  Epoch: 22  Global Step: 129390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:06,962-Speed 3881.60 samples/sec  Loss 3.7876  LearningRate 0.0016  ProxyLR: 0.0805  Epoch: 22  Global Step: 129400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:09,599-Speed 3885.26 samples/sec  Loss 3.8272  LearningRate 0.0016  ProxyLR: 0.0803  Epoch: 22  Global Step: 129410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:12,222-Speed 3904.67 samples/sec  Loss 3.9213  LearningRate 0.0016  ProxyLR: 0.0802  Epoch: 22  Global Step: 129420   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:14,858-Speed 3885.19 samples/sec  Loss 3.8386  LearningRate 0.0016  ProxyLR: 0.0801  Epoch: 22  Global Step: 129430   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:17,497-Speed 3881.06 samples/sec  Loss 3.7510  LearningRate 0.0016  ProxyLR: 0.0799  Epoch: 22  Global Step: 129440   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:20,130-Speed 3889.90 samples/sec  Loss 3.9328  LearningRate 0.0016  ProxyLR: 0.0798  Epoch: 22  Global Step: 129450   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:22,765-Speed 3887.31 samples/sec  Loss 3.7928  LearningRate 0.0016  ProxyLR: 0.0797  Epoch: 22  Global Step: 129460   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:25,401-Speed 3886.46 samples/sec  Loss 3.7967  LearningRate 0.0016  ProxyLR: 0.0796  Epoch: 22  Global Step: 129470   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:28,035-Speed 3887.82 samples/sec  Loss 3.8407  LearningRate 0.0016  ProxyLR: 0.0794  Epoch: 22  Global Step: 129480   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:30,670-Speed 3887.64 samples/sec  Loss 3.8853  LearningRate 0.0016  ProxyLR: 0.0793  Epoch: 22  Global Step: 129490   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:33,304-Speed 3887.79 samples/sec  Loss 3.9043  LearningRate 0.0016  ProxyLR: 0.0792  Epoch: 22  Global Step: 129500   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:35,941-Speed 3884.12 samples/sec  Loss 3.8195  LearningRate 0.0016  ProxyLR: 0.0791  Epoch: 22  Global Step: 129510   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:06:38,578-Speed 3885.25 samples/sec  Loss 3.9452  LearningRate 0.0016  ProxyLR: 0.0789  Epoch: 22  Global Step: 129520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:41,214-Speed 3885.37 samples/sec  Loss 3.8575  LearningRate 0.0016  ProxyLR: 0.0788  Epoch: 22  Global Step: 129530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:43,850-Speed 3885.58 samples/sec  Loss 3.8558  LearningRate 0.0016  ProxyLR: 0.0787  Epoch: 22  Global Step: 129540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:46,486-Speed 3886.30 samples/sec  Loss 3.8835  LearningRate 0.0016  ProxyLR: 0.0786  Epoch: 22  Global Step: 129550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:49,121-Speed 3886.06 samples/sec  Loss 3.8643  LearningRate 0.0016  ProxyLR: 0.0784  Epoch: 22  Global Step: 129560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:51,759-Speed 3883.67 samples/sec  Loss 3.8713  LearningRate 0.0016  ProxyLR: 0.0783  Epoch: 22  Global Step: 129570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:54,396-Speed 3882.71 samples/sec  Loss 3.8454  LearningRate 0.0016  ProxyLR: 0.0782  Epoch: 22  Global Step: 129580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:57,033-Speed 3884.88 samples/sec  Loss 3.8927  LearningRate 0.0016  ProxyLR: 0.0781  Epoch: 22  Global Step: 129590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:06:59,669-Speed 3884.93 samples/sec  Loss 3.8727  LearningRate 0.0016  ProxyLR: 0.0779  Epoch: 22  Global Step: 129600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:02,306-Speed 3884.62 samples/sec  Loss 3.8924  LearningRate 0.0016  ProxyLR: 0.0778  Epoch: 22  Global Step: 129610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:04,930-Speed 3903.39 samples/sec  Loss 3.8751  LearningRate 0.0016  ProxyLR: 0.0777  Epoch: 22  Global Step: 129620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:07,564-Speed 3889.46 samples/sec  Loss 3.7984  LearningRate 0.0016  ProxyLR: 0.0776  Epoch: 22  Global Step: 129630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:10,195-Speed 3892.31 samples/sec  Loss 3.8133  LearningRate 0.0015  ProxyLR: 0.0774  Epoch: 22  Global Step: 129640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:12,829-Speed 3888.32 samples/sec  Loss 3.7646  LearningRate 0.0015  ProxyLR: 0.0773  Epoch: 22  Global Step: 129650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:15,465-Speed 3886.94 samples/sec  Loss 3.7967  LearningRate 0.0015  ProxyLR: 0.0772  Epoch: 22  Global Step: 129660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:18,102-Speed 3883.29 samples/sec  Loss 3.8359  LearningRate 0.0015  ProxyLR: 0.0771  Epoch: 22  Global Step: 129670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:20,738-Speed 3886.17 samples/sec  Loss 3.8848  LearningRate 0.0015  ProxyLR: 0.0770  Epoch: 22  Global Step: 129680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:23,372-Speed 3889.09 samples/sec  Loss 3.8040  LearningRate 0.0015  ProxyLR: 0.0768  Epoch: 22  Global Step: 129690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:26,006-Speed 3888.34 samples/sec  Loss 3.8233  LearningRate 0.0015  ProxyLR: 0.0767  Epoch: 22  Global Step: 129700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:28,626-Speed 3908.29 samples/sec  Loss 3.8196  LearningRate 0.0015  ProxyLR: 0.0766  Epoch: 22  Global Step: 129710   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:31,263-Speed 3884.81 samples/sec  Loss 3.8074  LearningRate 0.0015  ProxyLR: 0.0765  Epoch: 22  Global Step: 129720   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:33,898-Speed 3886.77 samples/sec  Loss 3.7474  LearningRate 0.0015  ProxyLR: 0.0763  Epoch: 22  Global Step: 129730   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:36,537-Speed 3882.13 samples/sec  Loss 3.8446  LearningRate 0.0015  ProxyLR: 0.0762  Epoch: 22  Global Step: 129740   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:39,172-Speed 3886.28 samples/sec  Loss 3.8547  LearningRate 0.0015  ProxyLR: 0.0761  Epoch: 22  Global Step: 129750   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:41,811-Speed 3881.85 samples/sec  Loss 3.8761  LearningRate 0.0015  ProxyLR: 0.0760  Epoch: 22  Global Step: 129760   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:44,448-Speed 3883.30 samples/sec  Loss 3.8141  LearningRate 0.0015  ProxyLR: 0.0758  Epoch: 22  Global Step: 129770   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:47,085-Speed 3885.30 samples/sec  Loss 3.8372  LearningRate 0.0015  ProxyLR: 0.0757  Epoch: 22  Global Step: 129780   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:49,720-Speed 3886.28 samples/sec  Loss 3.8511  LearningRate 0.0015  ProxyLR: 0.0756  Epoch: 22  Global Step: 129790   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:52,358-Speed 3883.49 samples/sec  Loss 3.8635  LearningRate 0.0015  ProxyLR: 0.0755  Epoch: 22  Global Step: 129800   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:07:54,992-Speed 3888.36 samples/sec  Loss 3.8352  LearningRate 0.0015  ProxyLR: 0.0754  Epoch: 22  Global Step: 129810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:07:57,627-Speed 3887.36 samples/sec  Loss 3.8567  LearningRate 0.0015  ProxyLR: 0.0752  Epoch: 22  Global Step: 129820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:00,261-Speed 3888.62 samples/sec  Loss 3.8033  LearningRate 0.0015  ProxyLR: 0.0751  Epoch: 22  Global Step: 129830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:02,902-Speed 3877.36 samples/sec  Loss 3.8166  LearningRate 0.0015  ProxyLR: 0.0750  Epoch: 22  Global Step: 129840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:05,540-Speed 3883.12 samples/sec  Loss 3.8291  LearningRate 0.0015  ProxyLR: 0.0749  Epoch: 22  Global Step: 129850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:08,175-Speed 3887.92 samples/sec  Loss 3.8130  LearningRate 0.0015  ProxyLR: 0.0747  Epoch: 22  Global Step: 129860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:10,808-Speed 3889.71 samples/sec  Loss 3.8935  LearningRate 0.0015  ProxyLR: 0.0746  Epoch: 22  Global Step: 129870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:13,443-Speed 3887.07 samples/sec  Loss 3.9007  LearningRate 0.0015  ProxyLR: 0.0745  Epoch: 22  Global Step: 129880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:16,063-Speed 3909.72 samples/sec  Loss 3.9381  LearningRate 0.0015  ProxyLR: 0.0744  Epoch: 22  Global Step: 129890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:18,696-Speed 3889.25 samples/sec  Loss 3.8721  LearningRate 0.0015  ProxyLR: 0.0743  Epoch: 22  Global Step: 129900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:21,331-Speed 3887.31 samples/sec  Loss 3.7817  LearningRate 0.0015  ProxyLR: 0.0741  Epoch: 22  Global Step: 129910   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:23,966-Speed 3887.60 samples/sec  Loss 3.8374  LearningRate 0.0015  ProxyLR: 0.0740  Epoch: 22  Global Step: 129920   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:26,598-Speed 3890.92 samples/sec  Loss 3.8112  LearningRate 0.0015  ProxyLR: 0.0739  Epoch: 22  Global Step: 129930   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:29,229-Speed 3893.46 samples/sec  Loss 3.7789  LearningRate 0.0015  ProxyLR: 0.0738  Epoch: 22  Global Step: 129940   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:31,861-Speed 3891.84 samples/sec  Loss 3.9742  LearningRate 0.0015  ProxyLR: 0.0737  Epoch: 22  Global Step: 129950   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:34,493-Speed 3890.66 samples/sec  Loss 3.9170  LearningRate 0.0015  ProxyLR: 0.0735  Epoch: 22  Global Step: 129960   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:37,128-Speed 3888.09 samples/sec  Loss 3.7873  LearningRate 0.0015  ProxyLR: 0.0734  Epoch: 22  Global Step: 129970   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:39,759-Speed 3892.76 samples/sec  Loss 3.8253  LearningRate 0.0015  ProxyLR: 0.0733  Epoch: 22  Global Step: 129980   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:08:42,391-Speed 3891.81 samples/sec  Loss 3.7413  LearningRate 0.0015  ProxyLR: 0.0732  Epoch: 22  Global Step: 129990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:08:45,021-Speed 3893.35 samples/sec  Loss 3.8144  LearningRate 0.0015  ProxyLR: 0.0731  Epoch: 22  Global Step: 130000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:09:34,573-[lfw][130000]XNorm: 22.044929
Training: 2023-05-05 01:09:34,574-[lfw][130000]Accuracy-Flip: 0.99783+-0.00211
Training: 2023-05-05 01:09:34,574-[lfw][130000]Accuracy-Highest: 0.99800
Training: 2023-05-05 01:09:34,574-[lfw][130000]TPR@1stNon-Zero-FPR of 0.00033: 0.99567
Training: 2023-05-05 01:09:34,574-[lfw][130000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 01:10:31,489-[cfp_fp][130000]XNorm: 21.818068
Training: 2023-05-05 01:10:31,489-[cfp_fp][130000]Accuracy-Flip: 0.98571+-0.00469
Training: 2023-05-05 01:10:31,489-[cfp_fp][130000]Accuracy-Highest: 0.98571
Training: 2023-05-05 01:10:31,490-[cfp_fp][130000]TPR@1stNon-Zero-FPR of 0.00029: 0.88286
Training: 2023-05-05 01:10:31,490-[cfp_fp][130000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 01:11:21,002-[agedb_30][130000]XNorm: 22.238364
Training: 2023-05-05 01:11:21,002-[agedb_30][130000]Accuracy-Flip: 0.97617+-0.00823
Training: 2023-05-05 01:11:21,003-[agedb_30][130000]Accuracy-Highest: 0.97617
Training: 2023-05-05 01:11:21,003-[agedb_30][130000]TPR@1stNon-Zero-FPR of 0.00033: 0.86100
Training: 2023-05-05 01:11:21,003-[agedb_30][130000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 01:12:11,940-[calfw][130000]XNorm: 22.207222
Training: 2023-05-05 01:12:11,940-[calfw][130000]Accuracy-Flip: 0.95667+-0.01287
Training: 2023-05-05 01:12:11,940-[calfw][130000]Accuracy-Highest: 0.95867
Training: 2023-05-05 01:12:11,940-[calfw][130000]TPR@1stNon-Zero-FPR of 0.00033: 0.84700
Training: 2023-05-05 01:12:11,940-[calfw][130000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 01:13:02,812-[cplfw][130000]XNorm: 21.199586
Training: 2023-05-05 01:13:02,813-[cplfw][130000]Accuracy-Flip: 0.92733+-0.01202
Training: 2023-05-05 01:13:02,813-[cplfw][130000]Accuracy-Highest: 0.92933
Training: 2023-05-05 01:13:02,813-[cplfw][130000]TPR@1stNon-Zero-FPR of 0.00033: 0.14133
Training: 2023-05-05 01:13:02,813-[cplfw][130000]Highest TPR@FPR: 0.14133
Training: 2023-05-05 01:13:06,117-Speed 39.22 samples/sec  Loss 3.9543  LearningRate 0.0015  ProxyLR: 0.0729  Epoch: 22  Global Step: 130010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:08,737-Speed 3909.60 samples/sec  Loss 3.8629  LearningRate 0.0015  ProxyLR: 0.0728  Epoch: 22  Global Step: 130020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:11,360-Speed 3905.64 samples/sec  Loss 3.7955  LearningRate 0.0015  ProxyLR: 0.0727  Epoch: 22  Global Step: 130030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:13,980-Speed 3908.31 samples/sec  Loss 3.8001  LearningRate 0.0015  ProxyLR: 0.0726  Epoch: 22  Global Step: 130040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:16,604-Speed 3903.70 samples/sec  Loss 3.8452  LearningRate 0.0014  ProxyLR: 0.0725  Epoch: 22  Global Step: 130050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:19,228-Speed 3904.07 samples/sec  Loss 3.8012  LearningRate 0.0014  ProxyLR: 0.0723  Epoch: 22  Global Step: 130060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:21,859-Speed 3894.15 samples/sec  Loss 3.7846  LearningRate 0.0014  ProxyLR: 0.0722  Epoch: 22  Global Step: 130070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:24,483-Speed 3903.57 samples/sec  Loss 3.8789  LearningRate 0.0014  ProxyLR: 0.0721  Epoch: 22  Global Step: 130080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:27,108-Speed 3901.81 samples/sec  Loss 3.8489  LearningRate 0.0014  ProxyLR: 0.0720  Epoch: 22  Global Step: 130090   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:13:29,720-Speed 3920.03 samples/sec  Loss 3.7478  LearningRate 0.0014  ProxyLR: 0.0719  Epoch: 22  Global Step: 130100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:32,347-Speed 3899.26 samples/sec  Loss 3.8546  LearningRate 0.0014  ProxyLR: 0.0717  Epoch: 22  Global Step: 130110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:34,976-Speed 3896.57 samples/sec  Loss 3.8388  LearningRate 0.0014  ProxyLR: 0.0716  Epoch: 22  Global Step: 130120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:37,610-Speed 3887.61 samples/sec  Loss 3.8183  LearningRate 0.0014  ProxyLR: 0.0715  Epoch: 22  Global Step: 130130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:40,240-Speed 3894.82 samples/sec  Loss 3.8044  LearningRate 0.0014  ProxyLR: 0.0714  Epoch: 22  Global Step: 130140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:42,872-Speed 3892.48 samples/sec  Loss 3.8198  LearningRate 0.0014  ProxyLR: 0.0713  Epoch: 22  Global Step: 130150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:45,506-Speed 3888.44 samples/sec  Loss 3.8177  LearningRate 0.0014  ProxyLR: 0.0711  Epoch: 22  Global Step: 130160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:48,137-Speed 3891.98 samples/sec  Loss 3.9300  LearningRate 0.0014  ProxyLR: 0.0710  Epoch: 22  Global Step: 130170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:50,768-Speed 3893.35 samples/sec  Loss 3.8307  LearningRate 0.0014  ProxyLR: 0.0709  Epoch: 22  Global Step: 130180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:53,399-Speed 3893.55 samples/sec  Loss 3.8722  LearningRate 0.0014  ProxyLR: 0.0708  Epoch: 22  Global Step: 130190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:56,018-Speed 3911.23 samples/sec  Loss 3.7645  LearningRate 0.0014  ProxyLR: 0.0707  Epoch: 22  Global Step: 130200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:13:58,651-Speed 3889.79 samples/sec  Loss 3.9226  LearningRate 0.0014  ProxyLR: 0.0706  Epoch: 22  Global Step: 130210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:01,283-Speed 3891.50 samples/sec  Loss 3.8968  LearningRate 0.0014  ProxyLR: 0.0704  Epoch: 22  Global Step: 130220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:03,917-Speed 3889.49 samples/sec  Loss 3.8218  LearningRate 0.0014  ProxyLR: 0.0703  Epoch: 22  Global Step: 130230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:06,550-Speed 3889.56 samples/sec  Loss 3.8549  LearningRate 0.0014  ProxyLR: 0.0702  Epoch: 22  Global Step: 130240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:09,169-Speed 3911.70 samples/sec  Loss 3.8222  LearningRate 0.0014  ProxyLR: 0.0701  Epoch: 22  Global Step: 130250   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:11,801-Speed 3890.84 samples/sec  Loss 3.8316  LearningRate 0.0014  ProxyLR: 0.0700  Epoch: 22  Global Step: 130260   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:14,433-Speed 3891.26 samples/sec  Loss 3.8848  LearningRate 0.0014  ProxyLR: 0.0698  Epoch: 22  Global Step: 130270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:17,065-Speed 3891.50 samples/sec  Loss 3.8837  LearningRate 0.0014  ProxyLR: 0.0697  Epoch: 22  Global Step: 130280   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:19,697-Speed 3892.20 samples/sec  Loss 3.7731  LearningRate 0.0014  ProxyLR: 0.0696  Epoch: 22  Global Step: 130290   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:22,329-Speed 3890.83 samples/sec  Loss 3.8061  LearningRate 0.0014  ProxyLR: 0.0695  Epoch: 22  Global Step: 130300   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:24,965-Speed 3886.41 samples/sec  Loss 3.8583  LearningRate 0.0014  ProxyLR: 0.0694  Epoch: 22  Global Step: 130310   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:27,596-Speed 3892.36 samples/sec  Loss 3.9062  LearningRate 0.0014  ProxyLR: 0.0693  Epoch: 22  Global Step: 130320   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:30,229-Speed 3891.16 samples/sec  Loss 3.8148  LearningRate 0.0014  ProxyLR: 0.0691  Epoch: 22  Global Step: 130330   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:32,861-Speed 3890.47 samples/sec  Loss 3.8621  LearningRate 0.0014  ProxyLR: 0.0690  Epoch: 22  Global Step: 130340   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:35,494-Speed 3890.04 samples/sec  Loss 3.8819  LearningRate 0.0014  ProxyLR: 0.0689  Epoch: 22  Global Step: 130350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:38,127-Speed 3890.61 samples/sec  Loss 3.8290  LearningRate 0.0014  ProxyLR: 0.0688  Epoch: 22  Global Step: 130360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:40,761-Speed 3889.15 samples/sec  Loss 3.8635  LearningRate 0.0014  ProxyLR: 0.0687  Epoch: 22  Global Step: 130370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:14:43,379-Speed 3911.34 samples/sec  Loss 3.7922  LearningRate 0.0014  ProxyLR: 0.0686  Epoch: 22  Global Step: 130380   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:46,012-Speed 3891.30 samples/sec  Loss 3.9439  LearningRate 0.0014  ProxyLR: 0.0684  Epoch: 22  Global Step: 130390   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:48,643-Speed 3891.88 samples/sec  Loss 3.8704  LearningRate 0.0014  ProxyLR: 0.0683  Epoch: 22  Global Step: 130400   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:51,276-Speed 3889.77 samples/sec  Loss 3.8300  LearningRate 0.0014  ProxyLR: 0.0682  Epoch: 22  Global Step: 130410   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:53,909-Speed 3890.67 samples/sec  Loss 3.8393  LearningRate 0.0014  ProxyLR: 0.0681  Epoch: 22  Global Step: 130420   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:56,543-Speed 3889.35 samples/sec  Loss 3.8524  LearningRate 0.0014  ProxyLR: 0.0680  Epoch: 22  Global Step: 130430   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:14:59,175-Speed 3890.52 samples/sec  Loss 3.8304  LearningRate 0.0014  ProxyLR: 0.0679  Epoch: 22  Global Step: 130440   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:15:01,812-Speed 3884.68 samples/sec  Loss 3.8458  LearningRate 0.0014  ProxyLR: 0.0677  Epoch: 22  Global Step: 130450   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:15:04,447-Speed 3887.48 samples/sec  Loss 3.8858  LearningRate 0.0014  ProxyLR: 0.0676  Epoch: 22  Global Step: 130460   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:15:07,083-Speed 3884.96 samples/sec  Loss 3.7750  LearningRate 0.0014  ProxyLR: 0.0675  Epoch: 22  Global Step: 130470   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:15:09,719-Speed 3885.59 samples/sec  Loss 3.8698  LearningRate 0.0013  ProxyLR: 0.0674  Epoch: 22  Global Step: 130480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:12,356-Speed 3883.92 samples/sec  Loss 3.8947  LearningRate 0.0013  ProxyLR: 0.0673  Epoch: 22  Global Step: 130490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:14,991-Speed 3888.58 samples/sec  Loss 3.8439  LearningRate 0.0013  ProxyLR: 0.0672  Epoch: 22  Global Step: 130500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:17,626-Speed 3886.58 samples/sec  Loss 3.9001  LearningRate 0.0013  ProxyLR: 0.0671  Epoch: 22  Global Step: 130510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:20,259-Speed 3889.80 samples/sec  Loss 3.8271  LearningRate 0.0013  ProxyLR: 0.0669  Epoch: 22  Global Step: 130520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:22,894-Speed 3887.76 samples/sec  Loss 3.8104  LearningRate 0.0013  ProxyLR: 0.0668  Epoch: 22  Global Step: 130530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:25,530-Speed 3885.48 samples/sec  Loss 3.7454  LearningRate 0.0013  ProxyLR: 0.0667  Epoch: 22  Global Step: 130540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:28,167-Speed 3884.39 samples/sec  Loss 3.8535  LearningRate 0.0013  ProxyLR: 0.0666  Epoch: 22  Global Step: 130550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:30,801-Speed 3887.82 samples/sec  Loss 3.8614  LearningRate 0.0013  ProxyLR: 0.0665  Epoch: 22  Global Step: 130560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:33,437-Speed 3886.42 samples/sec  Loss 3.8791  LearningRate 0.0013  ProxyLR: 0.0664  Epoch: 22  Global Step: 130570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:36,072-Speed 3887.50 samples/sec  Loss 3.7789  LearningRate 0.0013  ProxyLR: 0.0662  Epoch: 22  Global Step: 130580   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:15:38,693-Speed 3907.09 samples/sec  Loss 3.7960  LearningRate 0.0013  ProxyLR: 0.0661  Epoch: 22  Global Step: 130590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:41,329-Speed 3886.43 samples/sec  Loss 3.7579  LearningRate 0.0013  ProxyLR: 0.0660  Epoch: 22  Global Step: 130600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:43,966-Speed 3883.23 samples/sec  Loss 3.8275  LearningRate 0.0013  ProxyLR: 0.0659  Epoch: 22  Global Step: 130610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:46,604-Speed 3883.01 samples/sec  Loss 3.7693  LearningRate 0.0013  ProxyLR: 0.0658  Epoch: 22  Global Step: 130620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:49,242-Speed 3882.97 samples/sec  Loss 3.8431  LearningRate 0.0013  ProxyLR: 0.0657  Epoch: 22  Global Step: 130630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:51,877-Speed 3886.13 samples/sec  Loss 3.8101  LearningRate 0.0013  ProxyLR: 0.0656  Epoch: 22  Global Step: 130640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:54,511-Speed 3889.68 samples/sec  Loss 3.7995  LearningRate 0.0013  ProxyLR: 0.0654  Epoch: 22  Global Step: 130650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:57,144-Speed 3889.73 samples/sec  Loss 3.8262  LearningRate 0.0013  ProxyLR: 0.0653  Epoch: 22  Global Step: 130660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:15:59,778-Speed 3888.71 samples/sec  Loss 3.8293  LearningRate 0.0013  ProxyLR: 0.0652  Epoch: 22  Global Step: 130670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:02,410-Speed 3890.91 samples/sec  Loss 3.9044  LearningRate 0.0013  ProxyLR: 0.0651  Epoch: 22  Global Step: 130680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:05,031-Speed 3908.26 samples/sec  Loss 3.7824  LearningRate 0.0013  ProxyLR: 0.0650  Epoch: 22  Global Step: 130690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:07,663-Speed 3891.08 samples/sec  Loss 3.8715  LearningRate 0.0013  ProxyLR: 0.0649  Epoch: 22  Global Step: 130700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:10,293-Speed 3894.55 samples/sec  Loss 3.8432  LearningRate 0.0013  ProxyLR: 0.0648  Epoch: 22  Global Step: 130710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:12,924-Speed 3893.92 samples/sec  Loss 3.7469  LearningRate 0.0013  ProxyLR: 0.0647  Epoch: 22  Global Step: 130720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:15,556-Speed 3891.62 samples/sec  Loss 3.8249  LearningRate 0.0013  ProxyLR: 0.0645  Epoch: 22  Global Step: 130730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:18,187-Speed 3893.45 samples/sec  Loss 3.8352  LearningRate 0.0013  ProxyLR: 0.0644  Epoch: 22  Global Step: 130740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:20,818-Speed 3892.48 samples/sec  Loss 3.8300  LearningRate 0.0013  ProxyLR: 0.0643  Epoch: 22  Global Step: 130750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:23,448-Speed 3894.38 samples/sec  Loss 3.8185  LearningRate 0.0013  ProxyLR: 0.0642  Epoch: 22  Global Step: 130760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:26,135-Speed 3811.58 samples/sec  Loss 3.8015  LearningRate 0.0013  ProxyLR: 0.0641  Epoch: 22  Global Step: 130770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:35,162-Speed 1134.51 samples/sec  Loss 3.7795  LearningRate 0.0013  ProxyLR: 0.0640  Epoch: 23  Global Step: 130780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:37,854-Speed 3805.41 samples/sec  Loss 3.6766  LearningRate 0.0013  ProxyLR: 0.0639  Epoch: 23  Global Step: 130790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:40,513-Speed 3851.92 samples/sec  Loss 3.7594  LearningRate 0.0013  ProxyLR: 0.0638  Epoch: 23  Global Step: 130800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:43,149-Speed 3886.07 samples/sec  Loss 3.6117  LearningRate 0.0013  ProxyLR: 0.0636  Epoch: 23  Global Step: 130810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:45,779-Speed 3894.18 samples/sec  Loss 3.6558  LearningRate 0.0013  ProxyLR: 0.0635  Epoch: 23  Global Step: 130820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:48,435-Speed 3857.46 samples/sec  Loss 3.6519  LearningRate 0.0013  ProxyLR: 0.0634  Epoch: 23  Global Step: 130830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:51,068-Speed 3890.24 samples/sec  Loss 3.6733  LearningRate 0.0013  ProxyLR: 0.0633  Epoch: 23  Global Step: 130840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:53,748-Speed 3820.82 samples/sec  Loss 3.7147  LearningRate 0.0013  ProxyLR: 0.0632  Epoch: 23  Global Step: 130850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:56,379-Speed 3893.92 samples/sec  Loss 3.6864  LearningRate 0.0013  ProxyLR: 0.0631  Epoch: 23  Global Step: 130860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:16:59,011-Speed 3890.66 samples/sec  Loss 3.5613  LearningRate 0.0013  ProxyLR: 0.0630  Epoch: 23  Global Step: 130870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:01,645-Speed 3889.94 samples/sec  Loss 3.7495  LearningRate 0.0013  ProxyLR: 0.0629  Epoch: 23  Global Step: 130880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:04,263-Speed 3912.02 samples/sec  Loss 3.6644  LearningRate 0.0013  ProxyLR: 0.0627  Epoch: 23  Global Step: 130890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:06,893-Speed 3894.97 samples/sec  Loss 3.7437  LearningRate 0.0013  ProxyLR: 0.0626  Epoch: 23  Global Step: 130900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:09,522-Speed 3895.65 samples/sec  Loss 3.6730  LearningRate 0.0013  ProxyLR: 0.0625  Epoch: 23  Global Step: 130910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:12,206-Speed 3816.86 samples/sec  Loss 3.6644  LearningRate 0.0012  ProxyLR: 0.0624  Epoch: 23  Global Step: 130920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:14,837-Speed 3892.01 samples/sec  Loss 3.6713  LearningRate 0.0012  ProxyLR: 0.0623  Epoch: 23  Global Step: 130930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:17,468-Speed 3893.50 samples/sec  Loss 3.6961  LearningRate 0.0012  ProxyLR: 0.0622  Epoch: 23  Global Step: 130940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:20,097-Speed 3895.44 samples/sec  Loss 3.6642  LearningRate 0.0012  ProxyLR: 0.0621  Epoch: 23  Global Step: 130950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:22,753-Speed 3856.98 samples/sec  Loss 3.7093  LearningRate 0.0012  ProxyLR: 0.0620  Epoch: 23  Global Step: 130960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:25,382-Speed 3896.49 samples/sec  Loss 3.6358  LearningRate 0.0012  ProxyLR: 0.0619  Epoch: 23  Global Step: 130970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:28,010-Speed 3897.18 samples/sec  Loss 3.6727  LearningRate 0.0012  ProxyLR: 0.0617  Epoch: 23  Global Step: 130980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:30,633-Speed 3904.43 samples/sec  Loss 3.6761  LearningRate 0.0012  ProxyLR: 0.0616  Epoch: 23  Global Step: 130990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:33,266-Speed 3890.82 samples/sec  Loss 3.6609  LearningRate 0.0012  ProxyLR: 0.0615  Epoch: 23  Global Step: 131000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:35,895-Speed 3896.33 samples/sec  Loss 3.6369  LearningRate 0.0012  ProxyLR: 0.0614  Epoch: 23  Global Step: 131010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:38,523-Speed 3897.09 samples/sec  Loss 3.6855  LearningRate 0.0012  ProxyLR: 0.0613  Epoch: 23  Global Step: 131020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:41,155-Speed 3892.22 samples/sec  Loss 3.6695  LearningRate 0.0012  ProxyLR: 0.0612  Epoch: 23  Global Step: 131030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:43,784-Speed 3895.82 samples/sec  Loss 3.7250  LearningRate 0.0012  ProxyLR: 0.0611  Epoch: 23  Global Step: 131040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:46,415-Speed 3893.33 samples/sec  Loss 3.6281  LearningRate 0.0012  ProxyLR: 0.0610  Epoch: 23  Global Step: 131050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:49,044-Speed 3896.15 samples/sec  Loss 3.7249  LearningRate 0.0012  ProxyLR: 0.0609  Epoch: 23  Global Step: 131060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:51,674-Speed 3894.38 samples/sec  Loss 3.7065  LearningRate 0.0012  ProxyLR: 0.0608  Epoch: 23  Global Step: 131070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:54,303-Speed 3895.53 samples/sec  Loss 3.7208  LearningRate 0.0012  ProxyLR: 0.0606  Epoch: 23  Global Step: 131080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:17:56,934-Speed 3893.78 samples/sec  Loss 3.7048  LearningRate 0.0012  ProxyLR: 0.0605  Epoch: 23  Global Step: 131090   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:17:59,552-Speed 3912.29 samples/sec  Loss 3.6342  LearningRate 0.0012  ProxyLR: 0.0604  Epoch: 23  Global Step: 131100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:02,180-Speed 3896.38 samples/sec  Loss 3.6484  LearningRate 0.0012  ProxyLR: 0.0603  Epoch: 23  Global Step: 131110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:04,811-Speed 3893.87 samples/sec  Loss 3.7306  LearningRate 0.0012  ProxyLR: 0.0602  Epoch: 23  Global Step: 131120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:07,439-Speed 3897.63 samples/sec  Loss 3.6795  LearningRate 0.0012  ProxyLR: 0.0601  Epoch: 23  Global Step: 131130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:10,067-Speed 3898.13 samples/sec  Loss 3.7034  LearningRate 0.0012  ProxyLR: 0.0600  Epoch: 23  Global Step: 131140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:12,695-Speed 3897.16 samples/sec  Loss 3.7004  LearningRate 0.0012  ProxyLR: 0.0599  Epoch: 23  Global Step: 131150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:15,323-Speed 3897.71 samples/sec  Loss 3.6566  LearningRate 0.0012  ProxyLR: 0.0598  Epoch: 23  Global Step: 131160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:17,953-Speed 3894.06 samples/sec  Loss 3.7362  LearningRate 0.0012  ProxyLR: 0.0597  Epoch: 23  Global Step: 131170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:20,581-Speed 3897.81 samples/sec  Loss 3.6764  LearningRate 0.0012  ProxyLR: 0.0596  Epoch: 23  Global Step: 131180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:23,208-Speed 3899.33 samples/sec  Loss 3.6496  LearningRate 0.0012  ProxyLR: 0.0594  Epoch: 23  Global Step: 131190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:25,823-Speed 3916.30 samples/sec  Loss 3.6827  LearningRate 0.0012  ProxyLR: 0.0593  Epoch: 23  Global Step: 131200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:28,451-Speed 3897.46 samples/sec  Loss 3.6486  LearningRate 0.0012  ProxyLR: 0.0592  Epoch: 23  Global Step: 131210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:31,079-Speed 3898.26 samples/sec  Loss 3.6854  LearningRate 0.0012  ProxyLR: 0.0591  Epoch: 23  Global Step: 131220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:33,706-Speed 3898.73 samples/sec  Loss 3.6943  LearningRate 0.0012  ProxyLR: 0.0590  Epoch: 23  Global Step: 131230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:36,334-Speed 3897.00 samples/sec  Loss 3.6488  LearningRate 0.0012  ProxyLR: 0.0589  Epoch: 23  Global Step: 131240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:18:38,949-Speed 3917.24 samples/sec  Loss 3.6270  LearningRate 0.0012  ProxyLR: 0.0588  Epoch: 23  Global Step: 131250   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:41,578-Speed 3897.15 samples/sec  Loss 3.6534  LearningRate 0.0012  ProxyLR: 0.0587  Epoch: 23  Global Step: 131260   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:44,205-Speed 3898.53 samples/sec  Loss 3.6130  LearningRate 0.0012  ProxyLR: 0.0586  Epoch: 23  Global Step: 131270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:46,833-Speed 3897.70 samples/sec  Loss 3.6508  LearningRate 0.0012  ProxyLR: 0.0585  Epoch: 23  Global Step: 131280   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:49,463-Speed 3894.46 samples/sec  Loss 3.7331  LearningRate 0.0012  ProxyLR: 0.0584  Epoch: 23  Global Step: 131290   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:52,091-Speed 3897.50 samples/sec  Loss 3.6392  LearningRate 0.0012  ProxyLR: 0.0583  Epoch: 23  Global Step: 131300   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:54,721-Speed 3894.77 samples/sec  Loss 3.7112  LearningRate 0.0012  ProxyLR: 0.0582  Epoch: 23  Global Step: 131310   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:57,350-Speed 3895.44 samples/sec  Loss 3.6898  LearningRate 0.0012  ProxyLR: 0.0580  Epoch: 23  Global Step: 131320   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:18:59,981-Speed 3892.54 samples/sec  Loss 3.6378  LearningRate 0.0012  ProxyLR: 0.0579  Epoch: 23  Global Step: 131330   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:19:02,608-Speed 3899.51 samples/sec  Loss 3.6266  LearningRate 0.0012  ProxyLR: 0.0578  Epoch: 23  Global Step: 131340   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:19:05,236-Speed 3897.35 samples/sec  Loss 3.6108  LearningRate 0.0012  ProxyLR: 0.0577  Epoch: 23  Global Step: 131350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:07,866-Speed 3894.32 samples/sec  Loss 3.6840  LearningRate 0.0012  ProxyLR: 0.0576  Epoch: 23  Global Step: 131360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:10,493-Speed 3899.64 samples/sec  Loss 3.6866  LearningRate 0.0012  ProxyLR: 0.0575  Epoch: 23  Global Step: 131370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:13,124-Speed 3894.20 samples/sec  Loss 3.6212  LearningRate 0.0011  ProxyLR: 0.0574  Epoch: 23  Global Step: 131380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:15,749-Speed 3900.33 samples/sec  Loss 3.7475  LearningRate 0.0011  ProxyLR: 0.0573  Epoch: 23  Global Step: 131390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:18,377-Speed 3898.89 samples/sec  Loss 3.7073  LearningRate 0.0011  ProxyLR: 0.0572  Epoch: 23  Global Step: 131400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:21,004-Speed 3897.94 samples/sec  Loss 3.7312  LearningRate 0.0011  ProxyLR: 0.0571  Epoch: 23  Global Step: 131410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:23,632-Speed 3898.16 samples/sec  Loss 3.6233  LearningRate 0.0011  ProxyLR: 0.0570  Epoch: 23  Global Step: 131420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:26,261-Speed 3895.80 samples/sec  Loss 3.6519  LearningRate 0.0011  ProxyLR: 0.0569  Epoch: 23  Global Step: 131430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:28,888-Speed 3898.73 samples/sec  Loss 3.6396  LearningRate 0.0011  ProxyLR: 0.0568  Epoch: 23  Global Step: 131440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:31,501-Speed 3920.02 samples/sec  Loss 3.6651  LearningRate 0.0011  ProxyLR: 0.0567  Epoch: 23  Global Step: 131450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:34,128-Speed 3899.23 samples/sec  Loss 3.6636  LearningRate 0.0011  ProxyLR: 0.0566  Epoch: 23  Global Step: 131460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:36,755-Speed 3899.04 samples/sec  Loss 3.7068  LearningRate 0.0011  ProxyLR: 0.0564  Epoch: 23  Global Step: 131470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:39,382-Speed 3899.43 samples/sec  Loss 3.6558  LearningRate 0.0011  ProxyLR: 0.0563  Epoch: 23  Global Step: 131480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:42,008-Speed 3900.24 samples/sec  Loss 3.7204  LearningRate 0.0011  ProxyLR: 0.0562  Epoch: 23  Global Step: 131490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:44,636-Speed 3898.16 samples/sec  Loss 3.6684  LearningRate 0.0011  ProxyLR: 0.0561  Epoch: 23  Global Step: 131500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:47,263-Speed 3898.68 samples/sec  Loss 3.6948  LearningRate 0.0011  ProxyLR: 0.0560  Epoch: 23  Global Step: 131510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:49,890-Speed 3898.50 samples/sec  Loss 3.6477  LearningRate 0.0011  ProxyLR: 0.0559  Epoch: 23  Global Step: 131520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:52,518-Speed 3898.24 samples/sec  Loss 3.7058  LearningRate 0.0011  ProxyLR: 0.0558  Epoch: 23  Global Step: 131530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:55,145-Speed 3897.98 samples/sec  Loss 3.7160  LearningRate 0.0011  ProxyLR: 0.0557  Epoch: 23  Global Step: 131540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:19:57,759-Speed 3919.71 samples/sec  Loss 3.6057  LearningRate 0.0011  ProxyLR: 0.0556  Epoch: 23  Global Step: 131550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:00,698-Speed 3484.51 samples/sec  Loss 3.6595  LearningRate 0.0011  ProxyLR: 0.0555  Epoch: 23  Global Step: 131560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:03,325-Speed 3899.24 samples/sec  Loss 3.6906  LearningRate 0.0011  ProxyLR: 0.0554  Epoch: 23  Global Step: 131570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:05,953-Speed 3897.89 samples/sec  Loss 3.6953  LearningRate 0.0011  ProxyLR: 0.0553  Epoch: 23  Global Step: 131580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:08,580-Speed 3898.11 samples/sec  Loss 3.6499  LearningRate 0.0011  ProxyLR: 0.0552  Epoch: 23  Global Step: 131590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:11,206-Speed 3900.09 samples/sec  Loss 3.7056  LearningRate 0.0011  ProxyLR: 0.0551  Epoch: 23  Global Step: 131600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:13,837-Speed 3893.45 samples/sec  Loss 3.6855  LearningRate 0.0011  ProxyLR: 0.0550  Epoch: 23  Global Step: 131610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:16,469-Speed 3892.16 samples/sec  Loss 3.6980  LearningRate 0.0011  ProxyLR: 0.0549  Epoch: 23  Global Step: 131620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:19,104-Speed 3887.65 samples/sec  Loss 3.7163  LearningRate 0.0011  ProxyLR: 0.0548  Epoch: 23  Global Step: 131630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:21,738-Speed 3887.72 samples/sec  Loss 3.5812  LearningRate 0.0011  ProxyLR: 0.0547  Epoch: 23  Global Step: 131640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:24,360-Speed 3906.70 samples/sec  Loss 3.7413  LearningRate 0.0011  ProxyLR: 0.0546  Epoch: 23  Global Step: 131650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:26,991-Speed 3893.53 samples/sec  Loss 3.7423  LearningRate 0.0011  ProxyLR: 0.0545  Epoch: 23  Global Step: 131660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:29,625-Speed 3888.96 samples/sec  Loss 3.6130  LearningRate 0.0011  ProxyLR: 0.0544  Epoch: 23  Global Step: 131670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:32,260-Speed 3887.40 samples/sec  Loss 3.6315  LearningRate 0.0011  ProxyLR: 0.0542  Epoch: 23  Global Step: 131680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:34,894-Speed 3888.58 samples/sec  Loss 3.6506  LearningRate 0.0011  ProxyLR: 0.0541  Epoch: 23  Global Step: 131690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:37,529-Speed 3887.74 samples/sec  Loss 3.6566  LearningRate 0.0011  ProxyLR: 0.0540  Epoch: 23  Global Step: 131700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:40,159-Speed 3894.44 samples/sec  Loss 3.7283  LearningRate 0.0011  ProxyLR: 0.0539  Epoch: 23  Global Step: 131710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:42,789-Speed 3893.87 samples/sec  Loss 3.7222  LearningRate 0.0011  ProxyLR: 0.0538  Epoch: 23  Global Step: 131720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:45,420-Speed 3893.98 samples/sec  Loss 3.6863  LearningRate 0.0011  ProxyLR: 0.0537  Epoch: 23  Global Step: 131730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:48,046-Speed 3900.24 samples/sec  Loss 3.5947  LearningRate 0.0011  ProxyLR: 0.0536  Epoch: 23  Global Step: 131740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:50,662-Speed 3915.34 samples/sec  Loss 3.6296  LearningRate 0.0011  ProxyLR: 0.0535  Epoch: 23  Global Step: 131750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:53,292-Speed 3893.91 samples/sec  Loss 3.6243  LearningRate 0.0011  ProxyLR: 0.0534  Epoch: 23  Global Step: 131760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:55,920-Speed 3897.28 samples/sec  Loss 3.6642  LearningRate 0.0011  ProxyLR: 0.0533  Epoch: 23  Global Step: 131770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:20:58,550-Speed 3895.59 samples/sec  Loss 3.6849  LearningRate 0.0011  ProxyLR: 0.0532  Epoch: 23  Global Step: 131780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:01,178-Speed 3896.65 samples/sec  Loss 3.6235  LearningRate 0.0011  ProxyLR: 0.0531  Epoch: 23  Global Step: 131790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:03,807-Speed 3896.88 samples/sec  Loss 3.6186  LearningRate 0.0011  ProxyLR: 0.0530  Epoch: 23  Global Step: 131800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:06,436-Speed 3896.47 samples/sec  Loss 3.7229  LearningRate 0.0011  ProxyLR: 0.0529  Epoch: 23  Global Step: 131810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:09,063-Speed 3897.64 samples/sec  Loss 3.6204  LearningRate 0.0011  ProxyLR: 0.0528  Epoch: 23  Global Step: 131820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:11,692-Speed 3896.84 samples/sec  Loss 3.6715  LearningRate 0.0011  ProxyLR: 0.0527  Epoch: 23  Global Step: 131830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:14,319-Speed 3898.20 samples/sec  Loss 3.6903  LearningRate 0.0011  ProxyLR: 0.0526  Epoch: 23  Global Step: 131840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:16,933-Speed 3918.99 samples/sec  Loss 3.6531  LearningRate 0.0011  ProxyLR: 0.0525  Epoch: 23  Global Step: 131850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:19,561-Speed 3898.29 samples/sec  Loss 3.7132  LearningRate 0.0010  ProxyLR: 0.0524  Epoch: 23  Global Step: 131860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:22,188-Speed 3898.35 samples/sec  Loss 3.6448  LearningRate 0.0010  ProxyLR: 0.0523  Epoch: 23  Global Step: 131870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:24,802-Speed 3919.00 samples/sec  Loss 3.6409  LearningRate 0.0010  ProxyLR: 0.0522  Epoch: 23  Global Step: 131880   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:27,429-Speed 3899.42 samples/sec  Loss 3.7034  LearningRate 0.0010  ProxyLR: 0.0521  Epoch: 23  Global Step: 131890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:30,057-Speed 3897.83 samples/sec  Loss 3.6761  LearningRate 0.0010  ProxyLR: 0.0520  Epoch: 23  Global Step: 131900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:32,685-Speed 3896.43 samples/sec  Loss 3.6869  LearningRate 0.0010  ProxyLR: 0.0519  Epoch: 23  Global Step: 131910   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:35,314-Speed 3896.04 samples/sec  Loss 3.7216  LearningRate 0.0010  ProxyLR: 0.0518  Epoch: 23  Global Step: 131920   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:37,943-Speed 3897.29 samples/sec  Loss 3.6647  LearningRate 0.0010  ProxyLR: 0.0517  Epoch: 23  Global Step: 131930   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:40,570-Speed 3898.16 samples/sec  Loss 3.6744  LearningRate 0.0010  ProxyLR: 0.0516  Epoch: 23  Global Step: 131940   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:43,200-Speed 3894.57 samples/sec  Loss 3.6569  LearningRate 0.0010  ProxyLR: 0.0515  Epoch: 23  Global Step: 131950   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:45,826-Speed 3900.48 samples/sec  Loss 3.6511  LearningRate 0.0010  ProxyLR: 0.0514  Epoch: 23  Global Step: 131960   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:48,455-Speed 3896.55 samples/sec  Loss 3.6390  LearningRate 0.0010  ProxyLR: 0.0513  Epoch: 23  Global Step: 131970   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:21:51,083-Speed 3896.93 samples/sec  Loss 3.6510  LearningRate 0.0010  ProxyLR: 0.0512  Epoch: 23  Global Step: 131980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:53,711-Speed 3898.38 samples/sec  Loss 3.6706  LearningRate 0.0010  ProxyLR: 0.0511  Epoch: 23  Global Step: 131990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:21:56,338-Speed 3899.01 samples/sec  Loss 3.6926  LearningRate 0.0010  ProxyLR: 0.0510  Epoch: 23  Global Step: 132000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:22:45,771-[lfw][132000]XNorm: 22.036158
Training: 2023-05-05 01:22:45,772-[lfw][132000]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-05 01:22:45,772-[lfw][132000]Accuracy-Highest: 0.99800
Training: 2023-05-05 01:22:45,772-[lfw][132000]TPR@1stNon-Zero-FPR of 0.00033: 0.99567
Training: 2023-05-05 01:22:45,772-[lfw][132000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 01:23:42,602-[cfp_fp][132000]XNorm: 21.819376
Training: 2023-05-05 01:23:42,602-[cfp_fp][132000]Accuracy-Flip: 0.98514+-0.00448
Training: 2023-05-05 01:23:42,602-[cfp_fp][132000]Accuracy-Highest: 0.98571
Training: 2023-05-05 01:23:42,602-[cfp_fp][132000]TPR@1stNon-Zero-FPR of 0.00029: 0.86800
Training: 2023-05-05 01:23:42,602-[cfp_fp][132000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 01:24:32,069-[agedb_30][132000]XNorm: 22.215299
Training: 2023-05-05 01:24:32,069-[agedb_30][132000]Accuracy-Flip: 0.97517+-0.00841
Training: 2023-05-05 01:24:32,070-[agedb_30][132000]Accuracy-Highest: 0.97617
Training: 2023-05-05 01:24:32,070-[agedb_30][132000]TPR@1stNon-Zero-FPR of 0.00033: 0.86367
Training: 2023-05-05 01:24:32,070-[agedb_30][132000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 01:25:23,235-[calfw][132000]XNorm: 22.184208
Training: 2023-05-05 01:25:23,235-[calfw][132000]Accuracy-Flip: 0.95867+-0.01217
Training: 2023-05-05 01:25:23,236-[calfw][132000]Accuracy-Highest: 0.95867
Training: 2023-05-05 01:25:23,236-[calfw][132000]TPR@1stNon-Zero-FPR of 0.00033: 0.84333
Training: 2023-05-05 01:25:23,236-[calfw][132000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 01:26:14,436-[cplfw][132000]XNorm: 21.201951
Training: 2023-05-05 01:26:14,436-[cplfw][132000]Accuracy-Flip: 0.92883+-0.01167
Training: 2023-05-05 01:26:14,436-[cplfw][132000]Accuracy-Highest: 0.92933
Training: 2023-05-05 01:26:14,437-[cplfw][132000]TPR@1stNon-Zero-FPR of 0.00033: 0.14133
Training: 2023-05-05 01:26:14,437-[cplfw][132000]Highest TPR@FPR: 0.14133
Training: 2023-05-05 01:26:17,722-Speed 39.18 samples/sec  Loss 3.7019  LearningRate 0.0010  ProxyLR: 0.0509  Epoch: 23  Global Step: 132010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:20,342-Speed 3908.87 samples/sec  Loss 3.6254  LearningRate 0.0010  ProxyLR: 0.0508  Epoch: 23  Global Step: 132020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:22,961-Speed 3911.30 samples/sec  Loss 3.6258  LearningRate 0.0010  ProxyLR: 0.0507  Epoch: 23  Global Step: 132030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:25,580-Speed 3910.57 samples/sec  Loss 3.6239  LearningRate 0.0010  ProxyLR: 0.0506  Epoch: 23  Global Step: 132040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:28,201-Speed 3908.15 samples/sec  Loss 3.6478  LearningRate 0.0010  ProxyLR: 0.0505  Epoch: 23  Global Step: 132050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:30,824-Speed 3905.88 samples/sec  Loss 3.6488  LearningRate 0.0010  ProxyLR: 0.0504  Epoch: 23  Global Step: 132060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:33,445-Speed 3907.86 samples/sec  Loss 3.6383  LearningRate 0.0010  ProxyLR: 0.0503  Epoch: 23  Global Step: 132070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:36,053-Speed 3927.94 samples/sec  Loss 3.6367  LearningRate 0.0010  ProxyLR: 0.0502  Epoch: 23  Global Step: 132080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:38,917-Speed 3576.05 samples/sec  Loss 3.6746  LearningRate 0.0010  ProxyLR: 0.0501  Epoch: 23  Global Step: 132090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:41,539-Speed 3905.89 samples/sec  Loss 3.6421  LearningRate 0.0010  ProxyLR: 0.0500  Epoch: 23  Global Step: 132100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:44,161-Speed 3906.34 samples/sec  Loss 3.7145  LearningRate 0.0010  ProxyLR: 0.0499  Epoch: 23  Global Step: 132110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:46,786-Speed 3902.81 samples/sec  Loss 3.5906  LearningRate 0.0010  ProxyLR: 0.0498  Epoch: 23  Global Step: 132120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:49,410-Speed 3903.84 samples/sec  Loss 3.6394  LearningRate 0.0010  ProxyLR: 0.0497  Epoch: 23  Global Step: 132130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:26:52,022-Speed 3920.98 samples/sec  Loss 3.7224  LearningRate 0.0010  ProxyLR: 0.0496  Epoch: 23  Global Step: 132140   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:26:54,646-Speed 3903.44 samples/sec  Loss 3.6151  LearningRate 0.0010  ProxyLR: 0.0495  Epoch: 23  Global Step: 132150   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:26:57,271-Speed 3901.88 samples/sec  Loss 3.6347  LearningRate 0.0010  ProxyLR: 0.0494  Epoch: 23  Global Step: 132160   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:26:59,895-Speed 3903.82 samples/sec  Loss 3.6088  LearningRate 0.0010  ProxyLR: 0.0493  Epoch: 23  Global Step: 132170   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:02,521-Speed 3899.93 samples/sec  Loss 3.6989  LearningRate 0.0010  ProxyLR: 0.0492  Epoch: 23  Global Step: 132180   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:05,146-Speed 3901.77 samples/sec  Loss 3.6393  LearningRate 0.0010  ProxyLR: 0.0491  Epoch: 23  Global Step: 132190   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:07,774-Speed 3898.31 samples/sec  Loss 3.7050  LearningRate 0.0010  ProxyLR: 0.0490  Epoch: 23  Global Step: 132200   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:10,400-Speed 3899.36 samples/sec  Loss 3.7349  LearningRate 0.0010  ProxyLR: 0.0489  Epoch: 23  Global Step: 132210   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:13,028-Speed 3898.44 samples/sec  Loss 3.6537  LearningRate 0.0010  ProxyLR: 0.0488  Epoch: 23  Global Step: 132220   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:15,654-Speed 3900.30 samples/sec  Loss 3.6416  LearningRate 0.0010  ProxyLR: 0.0487  Epoch: 23  Global Step: 132230   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:27:18,279-Speed 3901.28 samples/sec  Loss 3.6923  LearningRate 0.0010  ProxyLR: 0.0486  Epoch: 23  Global Step: 132240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:20,907-Speed 3897.66 samples/sec  Loss 3.6718  LearningRate 0.0010  ProxyLR: 0.0485  Epoch: 23  Global Step: 132250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:23,535-Speed 3897.81 samples/sec  Loss 3.5907  LearningRate 0.0010  ProxyLR: 0.0484  Epoch: 23  Global Step: 132260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:26,161-Speed 3900.10 samples/sec  Loss 3.6676  LearningRate 0.0010  ProxyLR: 0.0483  Epoch: 23  Global Step: 132270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:28,788-Speed 3899.10 samples/sec  Loss 3.6359  LearningRate 0.0010  ProxyLR: 0.0482  Epoch: 23  Global Step: 132280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:31,417-Speed 3897.43 samples/sec  Loss 3.6659  LearningRate 0.0010  ProxyLR: 0.0481  Epoch: 23  Global Step: 132290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:34,043-Speed 3899.62 samples/sec  Loss 3.6344  LearningRate 0.0010  ProxyLR: 0.0480  Epoch: 23  Global Step: 132300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:36,669-Speed 3901.37 samples/sec  Loss 3.6616  LearningRate 0.0010  ProxyLR: 0.0479  Epoch: 23  Global Step: 132310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:39,295-Speed 3899.78 samples/sec  Loss 3.6508  LearningRate 0.0010  ProxyLR: 0.0478  Epoch: 23  Global Step: 132320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:41,921-Speed 3900.30 samples/sec  Loss 3.6167  LearningRate 0.0010  ProxyLR: 0.0477  Epoch: 23  Global Step: 132330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:44,534-Speed 3920.80 samples/sec  Loss 3.6607  LearningRate 0.0010  ProxyLR: 0.0476  Epoch: 23  Global Step: 132340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:47,159-Speed 3901.08 samples/sec  Loss 3.6750  LearningRate 0.0010  ProxyLR: 0.0475  Epoch: 23  Global Step: 132350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:49,785-Speed 3901.26 samples/sec  Loss 3.6616  LearningRate 0.0009  ProxyLR: 0.0474  Epoch: 23  Global Step: 132360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:52,411-Speed 3900.84 samples/sec  Loss 3.7098  LearningRate 0.0009  ProxyLR: 0.0473  Epoch: 23  Global Step: 132370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:55,037-Speed 3900.02 samples/sec  Loss 3.7139  LearningRate 0.0009  ProxyLR: 0.0472  Epoch: 23  Global Step: 132380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:27:57,664-Speed 3899.27 samples/sec  Loss 3.6453  LearningRate 0.0009  ProxyLR: 0.0471  Epoch: 23  Global Step: 132390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:00,293-Speed 3896.09 samples/sec  Loss 3.6651  LearningRate 0.0009  ProxyLR: 0.0470  Epoch: 23  Global Step: 132400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:02,923-Speed 3893.37 samples/sec  Loss 3.6246  LearningRate 0.0009  ProxyLR: 0.0469  Epoch: 23  Global Step: 132410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:05,555-Speed 3891.90 samples/sec  Loss 3.6584  LearningRate 0.0009  ProxyLR: 0.0469  Epoch: 23  Global Step: 132420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:08,188-Speed 3890.90 samples/sec  Loss 3.5666  LearningRate 0.0009  ProxyLR: 0.0468  Epoch: 23  Global Step: 132430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:10,805-Speed 3913.26 samples/sec  Loss 3.7055  LearningRate 0.0009  ProxyLR: 0.0467  Epoch: 23  Global Step: 132440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:13,438-Speed 3890.50 samples/sec  Loss 3.6773  LearningRate 0.0009  ProxyLR: 0.0466  Epoch: 23  Global Step: 132450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:16,069-Speed 3893.13 samples/sec  Loss 3.6657  LearningRate 0.0009  ProxyLR: 0.0465  Epoch: 23  Global Step: 132460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:18,700-Speed 3892.85 samples/sec  Loss 3.7222  LearningRate 0.0009  ProxyLR: 0.0464  Epoch: 23  Global Step: 132470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:21,332-Speed 3891.72 samples/sec  Loss 3.6715  LearningRate 0.0009  ProxyLR: 0.0463  Epoch: 23  Global Step: 132480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:23,966-Speed 3889.22 samples/sec  Loss 3.6427  LearningRate 0.0009  ProxyLR: 0.0462  Epoch: 23  Global Step: 132490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:26,586-Speed 3909.25 samples/sec  Loss 3.6596  LearningRate 0.0009  ProxyLR: 0.0461  Epoch: 23  Global Step: 132500   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:29,219-Speed 3889.86 samples/sec  Loss 3.6796  LearningRate 0.0009  ProxyLR: 0.0460  Epoch: 23  Global Step: 132510   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:31,852-Speed 3890.20 samples/sec  Loss 3.6683  LearningRate 0.0009  ProxyLR: 0.0459  Epoch: 23  Global Step: 132520   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:34,485-Speed 3890.14 samples/sec  Loss 3.6189  LearningRate 0.0009  ProxyLR: 0.0458  Epoch: 23  Global Step: 132530   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:37,117-Speed 3891.19 samples/sec  Loss 3.6008  LearningRate 0.0009  ProxyLR: 0.0457  Epoch: 23  Global Step: 132540   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:39,750-Speed 3890.63 samples/sec  Loss 3.7349  LearningRate 0.0009  ProxyLR: 0.0456  Epoch: 23  Global Step: 132550   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:42,383-Speed 3890.00 samples/sec  Loss 3.7526  LearningRate 0.0009  ProxyLR: 0.0455  Epoch: 23  Global Step: 132560   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:45,018-Speed 3888.04 samples/sec  Loss 3.7266  LearningRate 0.0009  ProxyLR: 0.0454  Epoch: 23  Global Step: 132570   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:47,652-Speed 3888.24 samples/sec  Loss 3.6041  LearningRate 0.0009  ProxyLR: 0.0453  Epoch: 23  Global Step: 132580   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:50,286-Speed 3888.19 samples/sec  Loss 3.6787  LearningRate 0.0009  ProxyLR: 0.0452  Epoch: 23  Global Step: 132590   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:28:52,922-Speed 3886.62 samples/sec  Loss 3.6913  LearningRate 0.0009  ProxyLR: 0.0451  Epoch: 23  Global Step: 132600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:55,556-Speed 3888.58 samples/sec  Loss 3.6812  LearningRate 0.0009  ProxyLR: 0.0450  Epoch: 23  Global Step: 132610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:28:58,191-Speed 3885.96 samples/sec  Loss 3.7583  LearningRate 0.0009  ProxyLR: 0.0449  Epoch: 23  Global Step: 132620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:00,828-Speed 3885.28 samples/sec  Loss 3.6689  LearningRate 0.0009  ProxyLR: 0.0449  Epoch: 23  Global Step: 132630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:03,464-Speed 3885.13 samples/sec  Loss 3.6664  LearningRate 0.0009  ProxyLR: 0.0448  Epoch: 23  Global Step: 132640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:06,100-Speed 3885.46 samples/sec  Loss 3.6298  LearningRate 0.0009  ProxyLR: 0.0447  Epoch: 23  Global Step: 132650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:08,735-Speed 3887.91 samples/sec  Loss 3.6753  LearningRate 0.0009  ProxyLR: 0.0446  Epoch: 23  Global Step: 132660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:11,368-Speed 3890.20 samples/sec  Loss 3.6871  LearningRate 0.0009  ProxyLR: 0.0445  Epoch: 23  Global Step: 132670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:14,002-Speed 3887.56 samples/sec  Loss 3.6249  LearningRate 0.0009  ProxyLR: 0.0444  Epoch: 23  Global Step: 132680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:16,637-Speed 3886.66 samples/sec  Loss 3.6611  LearningRate 0.0009  ProxyLR: 0.0443  Epoch: 23  Global Step: 132690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:19,258-Speed 3909.09 samples/sec  Loss 3.6600  LearningRate 0.0009  ProxyLR: 0.0442  Epoch: 23  Global Step: 132700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:21,891-Speed 3890.87 samples/sec  Loss 3.6489  LearningRate 0.0009  ProxyLR: 0.0441  Epoch: 23  Global Step: 132710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:24,523-Speed 3891.55 samples/sec  Loss 3.6145  LearningRate 0.0009  ProxyLR: 0.0440  Epoch: 23  Global Step: 132720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:27,158-Speed 3886.35 samples/sec  Loss 3.6558  LearningRate 0.0009  ProxyLR: 0.0439  Epoch: 23  Global Step: 132730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:29,792-Speed 3888.46 samples/sec  Loss 3.6371  LearningRate 0.0009  ProxyLR: 0.0438  Epoch: 23  Global Step: 132740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:32,425-Speed 3890.07 samples/sec  Loss 3.6959  LearningRate 0.0009  ProxyLR: 0.0437  Epoch: 23  Global Step: 132750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:35,058-Speed 3891.46 samples/sec  Loss 3.7098  LearningRate 0.0009  ProxyLR: 0.0436  Epoch: 23  Global Step: 132760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:37,687-Speed 3894.91 samples/sec  Loss 3.6936  LearningRate 0.0009  ProxyLR: 0.0435  Epoch: 23  Global Step: 132770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:40,318-Speed 3893.60 samples/sec  Loss 3.5896  LearningRate 0.0009  ProxyLR: 0.0434  Epoch: 23  Global Step: 132780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:42,950-Speed 3891.96 samples/sec  Loss 3.7022  LearningRate 0.0009  ProxyLR: 0.0434  Epoch: 23  Global Step: 132790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:45,566-Speed 3914.66 samples/sec  Loss 3.6426  LearningRate 0.0009  ProxyLR: 0.0433  Epoch: 23  Global Step: 132800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:48,197-Speed 3893.95 samples/sec  Loss 3.7101  LearningRate 0.0009  ProxyLR: 0.0432  Epoch: 23  Global Step: 132810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:50,829-Speed 3890.85 samples/sec  Loss 3.7559  LearningRate 0.0009  ProxyLR: 0.0431  Epoch: 23  Global Step: 132820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:53,462-Speed 3890.58 samples/sec  Loss 3.7416  LearningRate 0.0009  ProxyLR: 0.0430  Epoch: 23  Global Step: 132830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:56,091-Speed 3895.24 samples/sec  Loss 3.6355  LearningRate 0.0009  ProxyLR: 0.0429  Epoch: 23  Global Step: 132840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:29:58,724-Speed 3890.75 samples/sec  Loss 3.6704  LearningRate 0.0009  ProxyLR: 0.0428  Epoch: 23  Global Step: 132850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:01,356-Speed 3891.91 samples/sec  Loss 3.7139  LearningRate 0.0009  ProxyLR: 0.0427  Epoch: 23  Global Step: 132860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:03,988-Speed 3891.12 samples/sec  Loss 3.6738  LearningRate 0.0009  ProxyLR: 0.0426  Epoch: 23  Global Step: 132870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:06,620-Speed 3891.48 samples/sec  Loss 3.6204  LearningRate 0.0009  ProxyLR: 0.0425  Epoch: 23  Global Step: 132880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:09,251-Speed 3893.80 samples/sec  Loss 3.5935  LearningRate 0.0008  ProxyLR: 0.0424  Epoch: 23  Global Step: 132890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:11,883-Speed 3891.91 samples/sec  Loss 3.6731  LearningRate 0.0008  ProxyLR: 0.0423  Epoch: 23  Global Step: 132900   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:30:14,502-Speed 3910.89 samples/sec  Loss 3.6577  LearningRate 0.0008  ProxyLR: 0.0423  Epoch: 23  Global Step: 132910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:17,134-Speed 3890.63 samples/sec  Loss 3.6751  LearningRate 0.0008  ProxyLR: 0.0422  Epoch: 23  Global Step: 132920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:19,766-Speed 3891.79 samples/sec  Loss 3.6392  LearningRate 0.0008  ProxyLR: 0.0421  Epoch: 23  Global Step: 132930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:22,398-Speed 3891.18 samples/sec  Loss 3.6998  LearningRate 0.0008  ProxyLR: 0.0420  Epoch: 23  Global Step: 132940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:25,031-Speed 3891.45 samples/sec  Loss 3.6887  LearningRate 0.0008  ProxyLR: 0.0419  Epoch: 23  Global Step: 132950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:27,663-Speed 3891.28 samples/sec  Loss 3.7561  LearningRate 0.0008  ProxyLR: 0.0418  Epoch: 23  Global Step: 132960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:30,295-Speed 3890.42 samples/sec  Loss 3.6008  LearningRate 0.0008  ProxyLR: 0.0417  Epoch: 23  Global Step: 132970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:32,928-Speed 3891.42 samples/sec  Loss 3.6992  LearningRate 0.0008  ProxyLR: 0.0416  Epoch: 23  Global Step: 132980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:35,557-Speed 3894.68 samples/sec  Loss 3.6627  LearningRate 0.0008  ProxyLR: 0.0415  Epoch: 23  Global Step: 132990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:38,188-Speed 3893.41 samples/sec  Loss 3.6002  LearningRate 0.0008  ProxyLR: 0.0414  Epoch: 23  Global Step: 133000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:40,806-Speed 3913.09 samples/sec  Loss 3.6820  LearningRate 0.0008  ProxyLR: 0.0413  Epoch: 23  Global Step: 133010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:43,437-Speed 3893.17 samples/sec  Loss 3.6143  LearningRate 0.0008  ProxyLR: 0.0413  Epoch: 23  Global Step: 133020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:46,071-Speed 3888.18 samples/sec  Loss 3.6907  LearningRate 0.0008  ProxyLR: 0.0412  Epoch: 23  Global Step: 133030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:48,706-Speed 3887.23 samples/sec  Loss 3.6628  LearningRate 0.0008  ProxyLR: 0.0411  Epoch: 23  Global Step: 133040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:51,341-Speed 3887.97 samples/sec  Loss 3.7004  LearningRate 0.0008  ProxyLR: 0.0410  Epoch: 23  Global Step: 133050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:53,973-Speed 3891.69 samples/sec  Loss 3.6521  LearningRate 0.0008  ProxyLR: 0.0409  Epoch: 23  Global Step: 133060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:56,607-Speed 3888.79 samples/sec  Loss 3.6032  LearningRate 0.0008  ProxyLR: 0.0408  Epoch: 23  Global Step: 133070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:30:59,240-Speed 3889.79 samples/sec  Loss 3.6119  LearningRate 0.0008  ProxyLR: 0.0407  Epoch: 23  Global Step: 133080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:01,873-Speed 3890.17 samples/sec  Loss 3.5891  LearningRate 0.0008  ProxyLR: 0.0406  Epoch: 23  Global Step: 133090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:04,505-Speed 3890.76 samples/sec  Loss 3.6271  LearningRate 0.0008  ProxyLR: 0.0405  Epoch: 23  Global Step: 133100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:07,350-Speed 3600.11 samples/sec  Loss 3.6251  LearningRate 0.0008  ProxyLR: 0.0404  Epoch: 23  Global Step: 133110   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:09,984-Speed 3889.57 samples/sec  Loss 3.6917  LearningRate 0.0008  ProxyLR: 0.0404  Epoch: 23  Global Step: 133120   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:12,616-Speed 3891.88 samples/sec  Loss 3.6891  LearningRate 0.0008  ProxyLR: 0.0403  Epoch: 23  Global Step: 133130   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:15,251-Speed 3887.04 samples/sec  Loss 3.6297  LearningRate 0.0008  ProxyLR: 0.0402  Epoch: 23  Global Step: 133140   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:17,882-Speed 3892.17 samples/sec  Loss 3.7148  LearningRate 0.0008  ProxyLR: 0.0401  Epoch: 23  Global Step: 133150   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:20,515-Speed 3890.97 samples/sec  Loss 3.6316  LearningRate 0.0008  ProxyLR: 0.0400  Epoch: 23  Global Step: 133160   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:23,149-Speed 3888.69 samples/sec  Loss 3.6559  LearningRate 0.0008  ProxyLR: 0.0399  Epoch: 23  Global Step: 133170   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:25,780-Speed 3892.22 samples/sec  Loss 3.6696  LearningRate 0.0008  ProxyLR: 0.0398  Epoch: 23  Global Step: 133180   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:28,412-Speed 3892.15 samples/sec  Loss 3.6501  LearningRate 0.0008  ProxyLR: 0.0397  Epoch: 23  Global Step: 133190   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:31,044-Speed 3891.57 samples/sec  Loss 3.6586  LearningRate 0.0008  ProxyLR: 0.0396  Epoch: 23  Global Step: 133200   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:33,676-Speed 3890.99 samples/sec  Loss 3.6416  LearningRate 0.0008  ProxyLR: 0.0396  Epoch: 23  Global Step: 133210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:36,309-Speed 3890.50 samples/sec  Loss 3.6077  LearningRate 0.0008  ProxyLR: 0.0395  Epoch: 23  Global Step: 133220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:38,940-Speed 3892.68 samples/sec  Loss 3.5991  LearningRate 0.0008  ProxyLR: 0.0394  Epoch: 23  Global Step: 133230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:41,572-Speed 3892.14 samples/sec  Loss 3.6481  LearningRate 0.0008  ProxyLR: 0.0393  Epoch: 23  Global Step: 133240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:44,206-Speed 3889.34 samples/sec  Loss 3.6965  LearningRate 0.0008  ProxyLR: 0.0392  Epoch: 23  Global Step: 133250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:46,839-Speed 3889.99 samples/sec  Loss 3.6342  LearningRate 0.0008  ProxyLR: 0.0391  Epoch: 23  Global Step: 133260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:31:49,457-Speed 3912.69 samples/sec  Loss 3.6755  LearningRate 0.0008  ProxyLR: 0.0390  Epoch: 23  Global Step: 133270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:52,090-Speed 3889.26 samples/sec  Loss 3.6755  LearningRate 0.0008  ProxyLR: 0.0389  Epoch: 23  Global Step: 133280   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:54,723-Speed 3890.63 samples/sec  Loss 3.6906  LearningRate 0.0008  ProxyLR: 0.0388  Epoch: 23  Global Step: 133290   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:57,354-Speed 3893.36 samples/sec  Loss 3.7364  LearningRate 0.0008  ProxyLR: 0.0388  Epoch: 23  Global Step: 133300   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:31:59,986-Speed 3890.70 samples/sec  Loss 3.6248  LearningRate 0.0008  ProxyLR: 0.0387  Epoch: 23  Global Step: 133310   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:32:02,618-Speed 3892.41 samples/sec  Loss 3.6361  LearningRate 0.0008  ProxyLR: 0.0386  Epoch: 23  Global Step: 133320   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:32:05,249-Speed 3892.20 samples/sec  Loss 3.6450  LearningRate 0.0008  ProxyLR: 0.0385  Epoch: 23  Global Step: 133330   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:32:07,884-Speed 3887.64 samples/sec  Loss 3.6251  LearningRate 0.0008  ProxyLR: 0.0384  Epoch: 23  Global Step: 133340   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:32:10,514-Speed 3894.06 samples/sec  Loss 3.6731  LearningRate 0.0008  ProxyLR: 0.0383  Epoch: 23  Global Step: 133350   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:32:13,145-Speed 3893.02 samples/sec  Loss 3.6293  LearningRate 0.0008  ProxyLR: 0.0382  Epoch: 23  Global Step: 133360   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:32:15,778-Speed 3891.18 samples/sec  Loss 3.6836  LearningRate 0.0008  ProxyLR: 0.0382  Epoch: 23  Global Step: 133370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:18,411-Speed 3890.18 samples/sec  Loss 3.7500  LearningRate 0.0008  ProxyLR: 0.0381  Epoch: 23  Global Step: 133380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:21,043-Speed 3891.40 samples/sec  Loss 3.6279  LearningRate 0.0008  ProxyLR: 0.0380  Epoch: 23  Global Step: 133390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:23,674-Speed 3893.36 samples/sec  Loss 3.5934  LearningRate 0.0008  ProxyLR: 0.0379  Epoch: 23  Global Step: 133400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:26,307-Speed 3889.53 samples/sec  Loss 3.6649  LearningRate 0.0008  ProxyLR: 0.0378  Epoch: 23  Global Step: 133410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:28,940-Speed 3889.81 samples/sec  Loss 3.7060  LearningRate 0.0008  ProxyLR: 0.0377  Epoch: 23  Global Step: 133420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:31,573-Speed 3890.84 samples/sec  Loss 3.6630  LearningRate 0.0008  ProxyLR: 0.0376  Epoch: 23  Global Step: 133430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:34,205-Speed 3891.82 samples/sec  Loss 3.6766  LearningRate 0.0008  ProxyLR: 0.0375  Epoch: 23  Global Step: 133440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:36,836-Speed 3893.00 samples/sec  Loss 3.6490  LearningRate 0.0007  ProxyLR: 0.0375  Epoch: 23  Global Step: 133450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:39,468-Speed 3891.77 samples/sec  Loss 3.6940  LearningRate 0.0007  ProxyLR: 0.0374  Epoch: 23  Global Step: 133460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:42,100-Speed 3891.91 samples/sec  Loss 3.6311  LearningRate 0.0007  ProxyLR: 0.0373  Epoch: 23  Global Step: 133470   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:32:44,717-Speed 3913.09 samples/sec  Loss 3.6206  LearningRate 0.0007  ProxyLR: 0.0372  Epoch: 23  Global Step: 133480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:47,351-Speed 3889.07 samples/sec  Loss 3.6171  LearningRate 0.0007  ProxyLR: 0.0371  Epoch: 23  Global Step: 133490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:49,985-Speed 3888.73 samples/sec  Loss 3.6527  LearningRate 0.0007  ProxyLR: 0.0370  Epoch: 23  Global Step: 133500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:52,617-Speed 3891.99 samples/sec  Loss 3.5792  LearningRate 0.0007  ProxyLR: 0.0369  Epoch: 23  Global Step: 133510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:55,249-Speed 3891.06 samples/sec  Loss 3.6227  LearningRate 0.0007  ProxyLR: 0.0369  Epoch: 23  Global Step: 133520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:32:57,882-Speed 3890.27 samples/sec  Loss 3.6647  LearningRate 0.0007  ProxyLR: 0.0368  Epoch: 23  Global Step: 133530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:00,512-Speed 3894.59 samples/sec  Loss 3.6927  LearningRate 0.0007  ProxyLR: 0.0367  Epoch: 23  Global Step: 133540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:03,144-Speed 3892.10 samples/sec  Loss 3.6537  LearningRate 0.0007  ProxyLR: 0.0366  Epoch: 23  Global Step: 133550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:05,777-Speed 3888.92 samples/sec  Loss 3.6221  LearningRate 0.0007  ProxyLR: 0.0365  Epoch: 23  Global Step: 133560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:08,408-Speed 3893.98 samples/sec  Loss 3.6197  LearningRate 0.0007  ProxyLR: 0.0364  Epoch: 23  Global Step: 133570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:11,025-Speed 3913.34 samples/sec  Loss 3.5623  LearningRate 0.0007  ProxyLR: 0.0363  Epoch: 23  Global Step: 133580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:13,657-Speed 3892.40 samples/sec  Loss 3.6630  LearningRate 0.0007  ProxyLR: 0.0363  Epoch: 23  Global Step: 133590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:16,286-Speed 3894.85 samples/sec  Loss 3.6550  LearningRate 0.0007  ProxyLR: 0.0362  Epoch: 23  Global Step: 133600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:18,917-Speed 3892.92 samples/sec  Loss 3.6839  LearningRate 0.0007  ProxyLR: 0.0361  Epoch: 23  Global Step: 133610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:21,549-Speed 3892.49 samples/sec  Loss 3.7127  LearningRate 0.0007  ProxyLR: 0.0360  Epoch: 23  Global Step: 133620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:24,180-Speed 3892.19 samples/sec  Loss 3.6015  LearningRate 0.0007  ProxyLR: 0.0359  Epoch: 23  Global Step: 133630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:26,813-Speed 3891.04 samples/sec  Loss 3.6511  LearningRate 0.0007  ProxyLR: 0.0358  Epoch: 23  Global Step: 133640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:29,443-Speed 3894.63 samples/sec  Loss 3.6145  LearningRate 0.0007  ProxyLR: 0.0358  Epoch: 23  Global Step: 133650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:32,074-Speed 3893.72 samples/sec  Loss 3.6314  LearningRate 0.0007  ProxyLR: 0.0357  Epoch: 23  Global Step: 133660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:34,705-Speed 3892.05 samples/sec  Loss 3.7150  LearningRate 0.0007  ProxyLR: 0.0356  Epoch: 23  Global Step: 133670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:37,321-Speed 3915.65 samples/sec  Loss 3.6556  LearningRate 0.0007  ProxyLR: 0.0355  Epoch: 23  Global Step: 133680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:39,951-Speed 3894.31 samples/sec  Loss 3.6354  LearningRate 0.0007  ProxyLR: 0.0354  Epoch: 23  Global Step: 133690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:42,582-Speed 3892.97 samples/sec  Loss 3.6741  LearningRate 0.0007  ProxyLR: 0.0353  Epoch: 23  Global Step: 133700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:45,215-Speed 3891.17 samples/sec  Loss 3.6130  LearningRate 0.0007  ProxyLR: 0.0353  Epoch: 23  Global Step: 133710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:47,847-Speed 3891.50 samples/sec  Loss 3.6319  LearningRate 0.0007  ProxyLR: 0.0352  Epoch: 23  Global Step: 133720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:50,479-Speed 3891.43 samples/sec  Loss 3.6675  LearningRate 0.0007  ProxyLR: 0.0351  Epoch: 23  Global Step: 133730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:53,112-Speed 3890.44 samples/sec  Loss 3.7318  LearningRate 0.0007  ProxyLR: 0.0350  Epoch: 23  Global Step: 133740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:55,743-Speed 3892.99 samples/sec  Loss 3.6014  LearningRate 0.0007  ProxyLR: 0.0349  Epoch: 23  Global Step: 133750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:33:58,375-Speed 3891.86 samples/sec  Loss 3.6911  LearningRate 0.0007  ProxyLR: 0.0348  Epoch: 23  Global Step: 133760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:01,007-Speed 3890.36 samples/sec  Loss 3.6659  LearningRate 0.0007  ProxyLR: 0.0348  Epoch: 23  Global Step: 133770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:03,639-Speed 3892.97 samples/sec  Loss 3.6541  LearningRate 0.0007  ProxyLR: 0.0347  Epoch: 23  Global Step: 133780   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:34:06,256-Speed 3913.95 samples/sec  Loss 3.6336  LearningRate 0.0007  ProxyLR: 0.0346  Epoch: 23  Global Step: 133790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:08,888-Speed 3891.47 samples/sec  Loss 3.5964  LearningRate 0.0007  ProxyLR: 0.0345  Epoch: 23  Global Step: 133800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:11,505-Speed 3912.80 samples/sec  Loss 3.6232  LearningRate 0.0007  ProxyLR: 0.0344  Epoch: 23  Global Step: 133810   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:14,140-Speed 3887.67 samples/sec  Loss 3.5687  LearningRate 0.0007  ProxyLR: 0.0343  Epoch: 23  Global Step: 133820   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:16,773-Speed 3890.02 samples/sec  Loss 3.5947  LearningRate 0.0007  ProxyLR: 0.0343  Epoch: 23  Global Step: 133830   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:19,406-Speed 3890.97 samples/sec  Loss 3.6618  LearningRate 0.0007  ProxyLR: 0.0342  Epoch: 23  Global Step: 133840   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:22,036-Speed 3894.83 samples/sec  Loss 3.6276  LearningRate 0.0007  ProxyLR: 0.0341  Epoch: 23  Global Step: 133850   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:24,665-Speed 3894.70 samples/sec  Loss 3.7334  LearningRate 0.0007  ProxyLR: 0.0340  Epoch: 23  Global Step: 133860   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:27,296-Speed 3893.21 samples/sec  Loss 3.7100  LearningRate 0.0007  ProxyLR: 0.0339  Epoch: 23  Global Step: 133870   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:29,929-Speed 3890.69 samples/sec  Loss 3.6827  LearningRate 0.0007  ProxyLR: 0.0338  Epoch: 23  Global Step: 133880   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:32,559-Speed 3894.39 samples/sec  Loss 3.6622  LearningRate 0.0007  ProxyLR: 0.0338  Epoch: 23  Global Step: 133890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:35,191-Speed 3891.95 samples/sec  Loss 3.6653  LearningRate 0.0007  ProxyLR: 0.0337  Epoch: 23  Global Step: 133900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:34:37,822-Speed 3892.96 samples/sec  Loss 3.7080  LearningRate 0.0007  ProxyLR: 0.0336  Epoch: 23  Global Step: 133910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:40,453-Speed 3892.44 samples/sec  Loss 3.6298  LearningRate 0.0007  ProxyLR: 0.0335  Epoch: 23  Global Step: 133920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:43,084-Speed 3893.29 samples/sec  Loss 3.5714  LearningRate 0.0007  ProxyLR: 0.0334  Epoch: 23  Global Step: 133930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:45,717-Speed 3890.18 samples/sec  Loss 3.6012  LearningRate 0.0007  ProxyLR: 0.0334  Epoch: 23  Global Step: 133940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:48,346-Speed 3896.51 samples/sec  Loss 3.5913  LearningRate 0.0007  ProxyLR: 0.0333  Epoch: 23  Global Step: 133950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:50,977-Speed 3893.00 samples/sec  Loss 3.5797  LearningRate 0.0007  ProxyLR: 0.0332  Epoch: 23  Global Step: 133960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:53,608-Speed 3893.07 samples/sec  Loss 3.6378  LearningRate 0.0007  ProxyLR: 0.0331  Epoch: 23  Global Step: 133970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:56,240-Speed 3891.95 samples/sec  Loss 3.6485  LearningRate 0.0007  ProxyLR: 0.0330  Epoch: 23  Global Step: 133980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:34:58,870-Speed 3893.75 samples/sec  Loss 3.6224  LearningRate 0.0007  ProxyLR: 0.0330  Epoch: 23  Global Step: 133990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:35:01,502-Speed 3892.59 samples/sec  Loss 3.7526  LearningRate 0.0007  ProxyLR: 0.0329  Epoch: 23  Global Step: 134000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:35:50,891-[lfw][134000]XNorm: 22.055988
Training: 2023-05-05 01:35:50,891-[lfw][134000]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-05 01:35:50,891-[lfw][134000]Accuracy-Highest: 0.99800
Training: 2023-05-05 01:35:50,892-[lfw][134000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 01:35:50,892-[lfw][134000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 01:36:47,705-[cfp_fp][134000]XNorm: 21.842397
Training: 2023-05-05 01:36:47,706-[cfp_fp][134000]Accuracy-Flip: 0.98371+-0.00520
Training: 2023-05-05 01:36:47,706-[cfp_fp][134000]Accuracy-Highest: 0.98571
Training: 2023-05-05 01:36:47,706-[cfp_fp][134000]TPR@1stNon-Zero-FPR of 0.00029: 0.89514
Training: 2023-05-05 01:36:47,706-[cfp_fp][134000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 01:37:37,140-[agedb_30][134000]XNorm: 22.263688
Training: 2023-05-05 01:37:37,140-[agedb_30][134000]Accuracy-Flip: 0.97500+-0.00813
Training: 2023-05-05 01:37:37,140-[agedb_30][134000]Accuracy-Highest: 0.97617
Training: 2023-05-05 01:37:37,141-[agedb_30][134000]TPR@1stNon-Zero-FPR of 0.00033: 0.86233
Training: 2023-05-05 01:37:37,141-[agedb_30][134000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 01:38:27,948-[calfw][134000]XNorm: 22.209965
Training: 2023-05-05 01:38:27,948-[calfw][134000]Accuracy-Flip: 0.95817+-0.01279
Training: 2023-05-05 01:38:27,948-[calfw][134000]Accuracy-Highest: 0.95867
Training: 2023-05-05 01:38:27,948-[calfw][134000]TPR@1stNon-Zero-FPR of 0.00033: 0.84600
Training: 2023-05-05 01:38:27,949-[calfw][134000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 01:39:18,687-[cplfw][134000]XNorm: 21.214612
Training: 2023-05-05 01:39:18,687-[cplfw][134000]Accuracy-Flip: 0.93100+-0.01138
Training: 2023-05-05 01:39:18,687-[cplfw][134000]Accuracy-Highest: 0.93100
Training: 2023-05-05 01:39:18,687-[cplfw][134000]TPR@1stNon-Zero-FPR of 0.00033: 0.14333
Training: 2023-05-05 01:39:18,688-[cplfw][134000]Highest TPR@FPR: 0.14333
Training: 2023-05-05 01:39:21,978-Speed 39.31 samples/sec  Loss 3.6383  LearningRate 0.0007  ProxyLR: 0.0328  Epoch: 23  Global Step: 134010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:24,602-Speed 3904.99 samples/sec  Loss 3.5900  LearningRate 0.0007  ProxyLR: 0.0327  Epoch: 23  Global Step: 134020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:27,225-Speed 3904.17 samples/sec  Loss 3.6276  LearningRate 0.0007  ProxyLR: 0.0326  Epoch: 23  Global Step: 134030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:29,849-Speed 3903.62 samples/sec  Loss 3.6254  LearningRate 0.0007  ProxyLR: 0.0325  Epoch: 23  Global Step: 134040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:32,474-Speed 3901.70 samples/sec  Loss 3.6807  LearningRate 0.0006  ProxyLR: 0.0325  Epoch: 23  Global Step: 134050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:35,100-Speed 3900.77 samples/sec  Loss 3.6563  LearningRate 0.0006  ProxyLR: 0.0324  Epoch: 23  Global Step: 134060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:37,728-Speed 3898.34 samples/sec  Loss 3.6875  LearningRate 0.0006  ProxyLR: 0.0323  Epoch: 23  Global Step: 134070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:40,355-Speed 3899.53 samples/sec  Loss 3.6603  LearningRate 0.0006  ProxyLR: 0.0322  Epoch: 23  Global Step: 134080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:42,983-Speed 3897.41 samples/sec  Loss 3.7050  LearningRate 0.0006  ProxyLR: 0.0321  Epoch: 23  Global Step: 134090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:45,612-Speed 3895.93 samples/sec  Loss 3.6829  LearningRate 0.0006  ProxyLR: 0.0321  Epoch: 23  Global Step: 134100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:48,227-Speed 3917.24 samples/sec  Loss 3.6857  LearningRate 0.0006  ProxyLR: 0.0320  Epoch: 23  Global Step: 134110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:50,854-Speed 3898.09 samples/sec  Loss 3.6086  LearningRate 0.0006  ProxyLR: 0.0319  Epoch: 23  Global Step: 134120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:53,484-Speed 3894.95 samples/sec  Loss 3.5937  LearningRate 0.0006  ProxyLR: 0.0318  Epoch: 23  Global Step: 134130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:56,116-Speed 3890.80 samples/sec  Loss 3.6388  LearningRate 0.0006  ProxyLR: 0.0318  Epoch: 23  Global Step: 134140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:39:58,751-Speed 3887.91 samples/sec  Loss 3.6913  LearningRate 0.0006  ProxyLR: 0.0317  Epoch: 23  Global Step: 134150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:01,384-Speed 3889.95 samples/sec  Loss 3.7679  LearningRate 0.0006  ProxyLR: 0.0316  Epoch: 23  Global Step: 134160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:04,019-Speed 3887.10 samples/sec  Loss 3.5977  LearningRate 0.0006  ProxyLR: 0.0315  Epoch: 23  Global Step: 134170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:06,653-Speed 3888.86 samples/sec  Loss 3.7025  LearningRate 0.0006  ProxyLR: 0.0314  Epoch: 23  Global Step: 134180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:09,287-Speed 3888.94 samples/sec  Loss 3.5881  LearningRate 0.0006  ProxyLR: 0.0314  Epoch: 23  Global Step: 134190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:11,921-Speed 3887.87 samples/sec  Loss 3.6171  LearningRate 0.0006  ProxyLR: 0.0313  Epoch: 23  Global Step: 134200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:14,541-Speed 3910.64 samples/sec  Loss 3.6717  LearningRate 0.0006  ProxyLR: 0.0312  Epoch: 23  Global Step: 134210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:17,174-Speed 3889.96 samples/sec  Loss 3.6756  LearningRate 0.0006  ProxyLR: 0.0311  Epoch: 23  Global Step: 134220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:19,807-Speed 3889.51 samples/sec  Loss 3.6715  LearningRate 0.0006  ProxyLR: 0.0310  Epoch: 23  Global Step: 134230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:22,437-Speed 3894.33 samples/sec  Loss 3.6833  LearningRate 0.0006  ProxyLR: 0.0310  Epoch: 23  Global Step: 134240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:25,073-Speed 3885.87 samples/sec  Loss 3.5884  LearningRate 0.0006  ProxyLR: 0.0309  Epoch: 23  Global Step: 134250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:27,709-Speed 3886.74 samples/sec  Loss 3.7054  LearningRate 0.0006  ProxyLR: 0.0308  Epoch: 23  Global Step: 134260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:30,346-Speed 3883.66 samples/sec  Loss 3.6774  LearningRate 0.0006  ProxyLR: 0.0307  Epoch: 23  Global Step: 134270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:32,982-Speed 3885.61 samples/sec  Loss 3.5432  LearningRate 0.0006  ProxyLR: 0.0307  Epoch: 23  Global Step: 134280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:35,617-Speed 3886.80 samples/sec  Loss 3.6940  LearningRate 0.0006  ProxyLR: 0.0306  Epoch: 23  Global Step: 134290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:38,255-Speed 3884.01 samples/sec  Loss 3.5690  LearningRate 0.0006  ProxyLR: 0.0305  Epoch: 23  Global Step: 134300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:40,876-Speed 3906.94 samples/sec  Loss 3.6817  LearningRate 0.0006  ProxyLR: 0.0304  Epoch: 23  Global Step: 134310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:43,510-Speed 3889.59 samples/sec  Loss 3.7110  LearningRate 0.0006  ProxyLR: 0.0303  Epoch: 23  Global Step: 134320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:46,141-Speed 3893.22 samples/sec  Loss 3.6395  LearningRate 0.0006  ProxyLR: 0.0303  Epoch: 23  Global Step: 134330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:48,771-Speed 3894.04 samples/sec  Loss 3.6208  LearningRate 0.0006  ProxyLR: 0.0302  Epoch: 23  Global Step: 134340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:51,401-Speed 3894.27 samples/sec  Loss 3.6481  LearningRate 0.0006  ProxyLR: 0.0301  Epoch: 23  Global Step: 134350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:54,031-Speed 3895.41 samples/sec  Loss 3.7037  LearningRate 0.0006  ProxyLR: 0.0300  Epoch: 23  Global Step: 134360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:56,661-Speed 3893.98 samples/sec  Loss 3.6248  LearningRate 0.0006  ProxyLR: 0.0300  Epoch: 23  Global Step: 134370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:40:59,293-Speed 3892.07 samples/sec  Loss 3.6818  LearningRate 0.0006  ProxyLR: 0.0299  Epoch: 23  Global Step: 134380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:01,923-Speed 3894.25 samples/sec  Loss 3.6719  LearningRate 0.0006  ProxyLR: 0.0298  Epoch: 23  Global Step: 134390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:04,541-Speed 3912.79 samples/sec  Loss 3.6727  LearningRate 0.0006  ProxyLR: 0.0297  Epoch: 23  Global Step: 134400   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:07,172-Speed 3892.69 samples/sec  Loss 3.6481  LearningRate 0.0006  ProxyLR: 0.0296  Epoch: 23  Global Step: 134410   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:09,804-Speed 3892.07 samples/sec  Loss 3.6723  LearningRate 0.0006  ProxyLR: 0.0296  Epoch: 23  Global Step: 134420   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:12,437-Speed 3889.85 samples/sec  Loss 3.5950  LearningRate 0.0006  ProxyLR: 0.0295  Epoch: 23  Global Step: 134430   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:15,069-Speed 3891.18 samples/sec  Loss 3.6246  LearningRate 0.0006  ProxyLR: 0.0294  Epoch: 23  Global Step: 134440   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:17,701-Speed 3892.02 samples/sec  Loss 3.7692  LearningRate 0.0006  ProxyLR: 0.0293  Epoch: 23  Global Step: 134450   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:20,330-Speed 3895.68 samples/sec  Loss 3.6227  LearningRate 0.0006  ProxyLR: 0.0293  Epoch: 23  Global Step: 134460   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:22,960-Speed 3895.18 samples/sec  Loss 3.7202  LearningRate 0.0006  ProxyLR: 0.0292  Epoch: 23  Global Step: 134470   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:25,827-Speed 3571.95 samples/sec  Loss 3.6643  LearningRate 0.0006  ProxyLR: 0.0291  Epoch: 23  Global Step: 134480   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:28,457-Speed 3895.56 samples/sec  Loss 3.6283  LearningRate 0.0006  ProxyLR: 0.0290  Epoch: 23  Global Step: 134490   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:31,087-Speed 3894.47 samples/sec  Loss 3.5940  LearningRate 0.0006  ProxyLR: 0.0290  Epoch: 23  Global Step: 134500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:33,715-Speed 3896.37 samples/sec  Loss 3.6350  LearningRate 0.0006  ProxyLR: 0.0289  Epoch: 23  Global Step: 134510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:36,346-Speed 3893.17 samples/sec  Loss 3.6712  LearningRate 0.0006  ProxyLR: 0.0288  Epoch: 23  Global Step: 134520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:38,975-Speed 3896.15 samples/sec  Loss 3.6165  LearningRate 0.0006  ProxyLR: 0.0287  Epoch: 23  Global Step: 134530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:41,606-Speed 3893.23 samples/sec  Loss 3.6456  LearningRate 0.0006  ProxyLR: 0.0287  Epoch: 23  Global Step: 134540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:41:44,223-Speed 3914.94 samples/sec  Loss 3.6063  LearningRate 0.0006  ProxyLR: 0.0286  Epoch: 23  Global Step: 134550   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:46,852-Speed 3895.74 samples/sec  Loss 3.6507  LearningRate 0.0006  ProxyLR: 0.0285  Epoch: 23  Global Step: 134560   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:49,481-Speed 3895.71 samples/sec  Loss 3.6284  LearningRate 0.0006  ProxyLR: 0.0284  Epoch: 23  Global Step: 134570   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:52,112-Speed 3893.45 samples/sec  Loss 3.6370  LearningRate 0.0006  ProxyLR: 0.0284  Epoch: 23  Global Step: 134580   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:54,741-Speed 3895.98 samples/sec  Loss 3.6149  LearningRate 0.0006  ProxyLR: 0.0283  Epoch: 23  Global Step: 134590   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:41:57,371-Speed 3894.81 samples/sec  Loss 3.6074  LearningRate 0.0006  ProxyLR: 0.0282  Epoch: 23  Global Step: 134600   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:42:00,002-Speed 3893.34 samples/sec  Loss 3.6534  LearningRate 0.0006  ProxyLR: 0.0281  Epoch: 23  Global Step: 134610   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:42:02,634-Speed 3891.44 samples/sec  Loss 3.6198  LearningRate 0.0006  ProxyLR: 0.0281  Epoch: 23  Global Step: 134620   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:42:05,263-Speed 3895.41 samples/sec  Loss 3.6478  LearningRate 0.0006  ProxyLR: 0.0280  Epoch: 23  Global Step: 134630   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:42:07,894-Speed 3893.06 samples/sec  Loss 3.6257  LearningRate 0.0006  ProxyLR: 0.0279  Epoch: 23  Global Step: 134640   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:42:10,523-Speed 3895.38 samples/sec  Loss 3.6808  LearningRate 0.0006  ProxyLR: 0.0278  Epoch: 23  Global Step: 134650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:13,153-Speed 3895.40 samples/sec  Loss 3.6053  LearningRate 0.0006  ProxyLR: 0.0278  Epoch: 23  Global Step: 134660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:15,784-Speed 3892.34 samples/sec  Loss 3.5500  LearningRate 0.0006  ProxyLR: 0.0277  Epoch: 23  Global Step: 134670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:18,416-Speed 3892.68 samples/sec  Loss 3.6577  LearningRate 0.0006  ProxyLR: 0.0276  Epoch: 23  Global Step: 134680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:21,044-Speed 3897.45 samples/sec  Loss 3.6246  LearningRate 0.0006  ProxyLR: 0.0275  Epoch: 23  Global Step: 134690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:23,673-Speed 3895.96 samples/sec  Loss 3.6474  LearningRate 0.0005  ProxyLR: 0.0275  Epoch: 23  Global Step: 134700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:26,300-Speed 3898.71 samples/sec  Loss 3.6257  LearningRate 0.0005  ProxyLR: 0.0274  Epoch: 23  Global Step: 134710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:28,928-Speed 3897.64 samples/sec  Loss 3.6630  LearningRate 0.0005  ProxyLR: 0.0273  Epoch: 23  Global Step: 134720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:31,558-Speed 3894.03 samples/sec  Loss 3.7129  LearningRate 0.0005  ProxyLR: 0.0272  Epoch: 23  Global Step: 134730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:34,186-Speed 3897.79 samples/sec  Loss 3.6334  LearningRate 0.0005  ProxyLR: 0.0272  Epoch: 23  Global Step: 134740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:36,799-Speed 3919.52 samples/sec  Loss 3.6383  LearningRate 0.0005  ProxyLR: 0.0271  Epoch: 23  Global Step: 134750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:39,425-Speed 3900.70 samples/sec  Loss 3.6111  LearningRate 0.0005  ProxyLR: 0.0270  Epoch: 23  Global Step: 134760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:42,054-Speed 3897.19 samples/sec  Loss 3.6347  LearningRate 0.0005  ProxyLR: 0.0270  Epoch: 23  Global Step: 134770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:44,681-Speed 3897.82 samples/sec  Loss 3.5815  LearningRate 0.0005  ProxyLR: 0.0269  Epoch: 23  Global Step: 134780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:47,308-Speed 3899.28 samples/sec  Loss 3.5988  LearningRate 0.0005  ProxyLR: 0.0268  Epoch: 23  Global Step: 134790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:49,936-Speed 3898.09 samples/sec  Loss 3.5912  LearningRate 0.0005  ProxyLR: 0.0267  Epoch: 23  Global Step: 134800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:52,562-Speed 3899.56 samples/sec  Loss 3.6570  LearningRate 0.0005  ProxyLR: 0.0267  Epoch: 23  Global Step: 134810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:55,191-Speed 3896.32 samples/sec  Loss 3.6480  LearningRate 0.0005  ProxyLR: 0.0266  Epoch: 23  Global Step: 134820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:42:57,818-Speed 3898.46 samples/sec  Loss 3.6691  LearningRate 0.0005  ProxyLR: 0.0265  Epoch: 23  Global Step: 134830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:00,447-Speed 3896.91 samples/sec  Loss 3.5909  LearningRate 0.0005  ProxyLR: 0.0264  Epoch: 23  Global Step: 134840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:03,059-Speed 3920.81 samples/sec  Loss 3.6123  LearningRate 0.0005  ProxyLR: 0.0264  Epoch: 23  Global Step: 134850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:05,687-Speed 3898.34 samples/sec  Loss 3.6641  LearningRate 0.0005  ProxyLR: 0.0263  Epoch: 23  Global Step: 134860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:08,316-Speed 3896.40 samples/sec  Loss 3.6188  LearningRate 0.0005  ProxyLR: 0.0262  Epoch: 23  Global Step: 134870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:10,943-Speed 3898.66 samples/sec  Loss 3.6225  LearningRate 0.0005  ProxyLR: 0.0262  Epoch: 23  Global Step: 134880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:13,569-Speed 3901.16 samples/sec  Loss 3.7051  LearningRate 0.0005  ProxyLR: 0.0261  Epoch: 23  Global Step: 134890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:16,194-Speed 3901.56 samples/sec  Loss 3.6748  LearningRate 0.0005  ProxyLR: 0.0260  Epoch: 23  Global Step: 134900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:18,821-Speed 3899.25 samples/sec  Loss 3.6943  LearningRate 0.0005  ProxyLR: 0.0259  Epoch: 23  Global Step: 134910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:21,448-Speed 3898.56 samples/sec  Loss 3.6745  LearningRate 0.0005  ProxyLR: 0.0259  Epoch: 23  Global Step: 134920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:24,062-Speed 3918.81 samples/sec  Loss 3.6286  LearningRate 0.0005  ProxyLR: 0.0258  Epoch: 23  Global Step: 134930   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:26,688-Speed 3899.67 samples/sec  Loss 3.6726  LearningRate 0.0005  ProxyLR: 0.0257  Epoch: 23  Global Step: 134940   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:29,316-Speed 3898.74 samples/sec  Loss 3.6473  LearningRate 0.0005  ProxyLR: 0.0257  Epoch: 23  Global Step: 134950   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:31,941-Speed 3901.25 samples/sec  Loss 3.6030  LearningRate 0.0005  ProxyLR: 0.0256  Epoch: 23  Global Step: 134960   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:34,568-Speed 3899.51 samples/sec  Loss 3.6095  LearningRate 0.0005  ProxyLR: 0.0255  Epoch: 23  Global Step: 134970   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:37,195-Speed 3898.19 samples/sec  Loss 3.6873  LearningRate 0.0005  ProxyLR: 0.0254  Epoch: 23  Global Step: 134980   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:39,821-Speed 3900.61 samples/sec  Loss 3.6198  LearningRate 0.0005  ProxyLR: 0.0254  Epoch: 23  Global Step: 134990   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:42,447-Speed 3900.80 samples/sec  Loss 3.6485  LearningRate 0.0005  ProxyLR: 0.0253  Epoch: 23  Global Step: 135000   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:45,074-Speed 3898.22 samples/sec  Loss 3.6147  LearningRate 0.0005  ProxyLR: 0.0252  Epoch: 23  Global Step: 135010   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:47,699-Speed 3902.10 samples/sec  Loss 3.6883  LearningRate 0.0005  ProxyLR: 0.0252  Epoch: 23  Global Step: 135020   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:43:50,327-Speed 3897.47 samples/sec  Loss 3.6283  LearningRate 0.0005  ProxyLR: 0.0251  Epoch: 23  Global Step: 135030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:52,956-Speed 3897.00 samples/sec  Loss 3.6593  LearningRate 0.0005  ProxyLR: 0.0250  Epoch: 23  Global Step: 135040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:55,584-Speed 3897.31 samples/sec  Loss 3.6674  LearningRate 0.0005  ProxyLR: 0.0249  Epoch: 23  Global Step: 135050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:43:58,211-Speed 3899.40 samples/sec  Loss 3.6351  LearningRate 0.0005  ProxyLR: 0.0249  Epoch: 23  Global Step: 135060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:00,837-Speed 3899.70 samples/sec  Loss 3.5998  LearningRate 0.0005  ProxyLR: 0.0248  Epoch: 23  Global Step: 135070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:03,465-Speed 3898.27 samples/sec  Loss 3.5976  LearningRate 0.0005  ProxyLR: 0.0247  Epoch: 23  Global Step: 135080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:06,092-Speed 3899.37 samples/sec  Loss 3.6303  LearningRate 0.0005  ProxyLR: 0.0247  Epoch: 23  Global Step: 135090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:08,718-Speed 3899.60 samples/sec  Loss 3.6713  LearningRate 0.0005  ProxyLR: 0.0246  Epoch: 23  Global Step: 135100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:11,347-Speed 3896.98 samples/sec  Loss 3.6358  LearningRate 0.0005  ProxyLR: 0.0245  Epoch: 23  Global Step: 135110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:13,975-Speed 3897.45 samples/sec  Loss 3.6633  LearningRate 0.0005  ProxyLR: 0.0245  Epoch: 23  Global Step: 135120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:16,602-Speed 3898.50 samples/sec  Loss 3.7386  LearningRate 0.0005  ProxyLR: 0.0244  Epoch: 23  Global Step: 135130   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:44:19,217-Speed 3917.22 samples/sec  Loss 3.6317  LearningRate 0.0005  ProxyLR: 0.0243  Epoch: 23  Global Step: 135140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:21,845-Speed 3896.92 samples/sec  Loss 3.6717  LearningRate 0.0005  ProxyLR: 0.0242  Epoch: 23  Global Step: 135150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:24,471-Speed 3900.59 samples/sec  Loss 3.6068  LearningRate 0.0005  ProxyLR: 0.0242  Epoch: 23  Global Step: 135160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:27,083-Speed 3920.67 samples/sec  Loss 3.6590  LearningRate 0.0005  ProxyLR: 0.0241  Epoch: 23  Global Step: 135170   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:29,710-Speed 3898.90 samples/sec  Loss 3.6787  LearningRate 0.0005  ProxyLR: 0.0240  Epoch: 23  Global Step: 135180   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:32,338-Speed 3898.64 samples/sec  Loss 3.7061  LearningRate 0.0005  ProxyLR: 0.0240  Epoch: 23  Global Step: 135190   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:34,963-Speed 3901.03 samples/sec  Loss 3.6511  LearningRate 0.0005  ProxyLR: 0.0239  Epoch: 23  Global Step: 135200   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:37,589-Speed 3900.18 samples/sec  Loss 3.5915  LearningRate 0.0005  ProxyLR: 0.0238  Epoch: 23  Global Step: 135210   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:40,219-Speed 3895.05 samples/sec  Loss 3.7009  LearningRate 0.0005  ProxyLR: 0.0238  Epoch: 23  Global Step: 135220   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:42,852-Speed 3890.22 samples/sec  Loss 3.6056  LearningRate 0.0005  ProxyLR: 0.0237  Epoch: 23  Global Step: 135230   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:45,480-Speed 3897.97 samples/sec  Loss 3.6993  LearningRate 0.0005  ProxyLR: 0.0236  Epoch: 23  Global Step: 135240   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:48,107-Speed 3898.32 samples/sec  Loss 3.6136  LearningRate 0.0005  ProxyLR: 0.0236  Epoch: 23  Global Step: 135250   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:50,734-Speed 3899.91 samples/sec  Loss 3.6727  LearningRate 0.0005  ProxyLR: 0.0235  Epoch: 23  Global Step: 135260   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:44:53,359-Speed 3901.40 samples/sec  Loss 3.6434  LearningRate 0.0005  ProxyLR: 0.0234  Epoch: 23  Global Step: 135270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:55,987-Speed 3897.60 samples/sec  Loss 3.6260  LearningRate 0.0005  ProxyLR: 0.0234  Epoch: 23  Global Step: 135280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:44:58,614-Speed 3899.12 samples/sec  Loss 3.6479  LearningRate 0.0005  ProxyLR: 0.0233  Epoch: 23  Global Step: 135290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:01,239-Speed 3900.89 samples/sec  Loss 3.6923  LearningRate 0.0005  ProxyLR: 0.0232  Epoch: 23  Global Step: 135300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:03,866-Speed 3898.63 samples/sec  Loss 3.6617  LearningRate 0.0005  ProxyLR: 0.0232  Epoch: 23  Global Step: 135310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:06,493-Speed 3899.85 samples/sec  Loss 3.6999  LearningRate 0.0005  ProxyLR: 0.0231  Epoch: 23  Global Step: 135320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:09,120-Speed 3899.06 samples/sec  Loss 3.6072  LearningRate 0.0005  ProxyLR: 0.0230  Epoch: 23  Global Step: 135330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:11,745-Speed 3901.38 samples/sec  Loss 3.7188  LearningRate 0.0005  ProxyLR: 0.0230  Epoch: 23  Global Step: 135340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:14,373-Speed 3898.03 samples/sec  Loss 3.5993  LearningRate 0.0005  ProxyLR: 0.0229  Epoch: 23  Global Step: 135350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:17,000-Speed 3898.43 samples/sec  Loss 3.7263  LearningRate 0.0005  ProxyLR: 0.0228  Epoch: 23  Global Step: 135360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:19,614-Speed 3918.24 samples/sec  Loss 3.6643  LearningRate 0.0005  ProxyLR: 0.0227  Epoch: 23  Global Step: 135370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:22,242-Speed 3898.07 samples/sec  Loss 3.6465  LearningRate 0.0005  ProxyLR: 0.0227  Epoch: 23  Global Step: 135380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:24,869-Speed 3898.25 samples/sec  Loss 3.6612  LearningRate 0.0005  ProxyLR: 0.0226  Epoch: 23  Global Step: 135390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:27,497-Speed 3897.69 samples/sec  Loss 3.6770  LearningRate 0.0005  ProxyLR: 0.0225  Epoch: 23  Global Step: 135400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:30,123-Speed 3899.83 samples/sec  Loss 3.6522  LearningRate 0.0004  ProxyLR: 0.0225  Epoch: 23  Global Step: 135410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:32,750-Speed 3899.41 samples/sec  Loss 3.6398  LearningRate 0.0004  ProxyLR: 0.0224  Epoch: 23  Global Step: 135420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:35,379-Speed 3895.59 samples/sec  Loss 3.7039  LearningRate 0.0004  ProxyLR: 0.0223  Epoch: 23  Global Step: 135430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:38,010-Speed 3892.78 samples/sec  Loss 3.6112  LearningRate 0.0004  ProxyLR: 0.0223  Epoch: 23  Global Step: 135440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:40,642-Speed 3891.68 samples/sec  Loss 3.6759  LearningRate 0.0004  ProxyLR: 0.0222  Epoch: 23  Global Step: 135450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:43,272-Speed 3894.20 samples/sec  Loss 3.7211  LearningRate 0.0004  ProxyLR: 0.0221  Epoch: 23  Global Step: 135460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:45,892-Speed 3910.32 samples/sec  Loss 3.6259  LearningRate 0.0004  ProxyLR: 0.0221  Epoch: 23  Global Step: 135470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:48,523-Speed 3892.84 samples/sec  Loss 3.6255  LearningRate 0.0004  ProxyLR: 0.0220  Epoch: 23  Global Step: 135480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:51,154-Speed 3892.87 samples/sec  Loss 3.5996  LearningRate 0.0004  ProxyLR: 0.0220  Epoch: 23  Global Step: 135490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:53,785-Speed 3892.87 samples/sec  Loss 3.6234  LearningRate 0.0004  ProxyLR: 0.0219  Epoch: 23  Global Step: 135500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:56,416-Speed 3892.54 samples/sec  Loss 3.5802  LearningRate 0.0004  ProxyLR: 0.0218  Epoch: 23  Global Step: 135510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:45:59,047-Speed 3893.96 samples/sec  Loss 3.6682  LearningRate 0.0004  ProxyLR: 0.0218  Epoch: 23  Global Step: 135520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:01,677-Speed 3894.02 samples/sec  Loss 3.6440  LearningRate 0.0004  ProxyLR: 0.0217  Epoch: 23  Global Step: 135530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:04,306-Speed 3895.35 samples/sec  Loss 3.6681  LearningRate 0.0004  ProxyLR: 0.0216  Epoch: 23  Global Step: 135540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:06,935-Speed 3895.75 samples/sec  Loss 3.6373  LearningRate 0.0004  ProxyLR: 0.0216  Epoch: 23  Global Step: 135550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:09,566-Speed 3894.29 samples/sec  Loss 3.6171  LearningRate 0.0004  ProxyLR: 0.0215  Epoch: 23  Global Step: 135560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:12,182-Speed 3914.02 samples/sec  Loss 3.7310  LearningRate 0.0004  ProxyLR: 0.0214  Epoch: 23  Global Step: 135570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:14,813-Speed 3893.33 samples/sec  Loss 3.5964  LearningRate 0.0004  ProxyLR: 0.0214  Epoch: 23  Global Step: 135580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:17,444-Speed 3893.08 samples/sec  Loss 3.6443  LearningRate 0.0004  ProxyLR: 0.0213  Epoch: 23  Global Step: 135590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:20,074-Speed 3894.22 samples/sec  Loss 3.6065  LearningRate 0.0004  ProxyLR: 0.0212  Epoch: 23  Global Step: 135600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:22,705-Speed 3893.89 samples/sec  Loss 3.6638  LearningRate 0.0004  ProxyLR: 0.0212  Epoch: 23  Global Step: 135610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:25,335-Speed 3893.58 samples/sec  Loss 3.6313  LearningRate 0.0004  ProxyLR: 0.0211  Epoch: 23  Global Step: 135620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:27,967-Speed 3892.38 samples/sec  Loss 3.6626  LearningRate 0.0004  ProxyLR: 0.0210  Epoch: 23  Global Step: 135630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:30,597-Speed 3894.53 samples/sec  Loss 3.6820  LearningRate 0.0004  ProxyLR: 0.0210  Epoch: 23  Global Step: 135640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:33,225-Speed 3897.00 samples/sec  Loss 3.6418  LearningRate 0.0004  ProxyLR: 0.0209  Epoch: 23  Global Step: 135650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:35,855-Speed 3894.98 samples/sec  Loss 3.6841  LearningRate 0.0004  ProxyLR: 0.0208  Epoch: 23  Global Step: 135660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:38,484-Speed 3895.06 samples/sec  Loss 3.5701  LearningRate 0.0004  ProxyLR: 0.0208  Epoch: 23  Global Step: 135670   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 01:46:41,101-Speed 3914.44 samples/sec  Loss 3.6603  LearningRate 0.0004  ProxyLR: 0.0207  Epoch: 23  Global Step: 135680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:43,732-Speed 3893.49 samples/sec  Loss 3.6163  LearningRate 0.0004  ProxyLR: 0.0207  Epoch: 23  Global Step: 135690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:46,361-Speed 3894.71 samples/sec  Loss 3.6775  LearningRate 0.0004  ProxyLR: 0.0206  Epoch: 23  Global Step: 135700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:48,993-Speed 3892.33 samples/sec  Loss 3.6419  LearningRate 0.0004  ProxyLR: 0.0205  Epoch: 23  Global Step: 135710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:51,622-Speed 3895.90 samples/sec  Loss 3.6819  LearningRate 0.0004  ProxyLR: 0.0205  Epoch: 23  Global Step: 135720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:54,251-Speed 3896.29 samples/sec  Loss 3.6944  LearningRate 0.0004  ProxyLR: 0.0204  Epoch: 23  Global Step: 135730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:56,880-Speed 3894.72 samples/sec  Loss 3.6581  LearningRate 0.0004  ProxyLR: 0.0203  Epoch: 23  Global Step: 135740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:46:59,512-Speed 3892.46 samples/sec  Loss 3.6241  LearningRate 0.0004  ProxyLR: 0.0203  Epoch: 23  Global Step: 135750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:02,142-Speed 3894.24 samples/sec  Loss 3.5877  LearningRate 0.0004  ProxyLR: 0.0202  Epoch: 23  Global Step: 135760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:04,774-Speed 3891.27 samples/sec  Loss 3.6998  LearningRate 0.0004  ProxyLR: 0.0201  Epoch: 23  Global Step: 135770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:07,393-Speed 3911.29 samples/sec  Loss 3.6675  LearningRate 0.0004  ProxyLR: 0.0201  Epoch: 23  Global Step: 135780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:10,024-Speed 3892.50 samples/sec  Loss 3.6572  LearningRate 0.0004  ProxyLR: 0.0200  Epoch: 23  Global Step: 135790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:12,655-Speed 3893.59 samples/sec  Loss 3.5879  LearningRate 0.0004  ProxyLR: 0.0200  Epoch: 23  Global Step: 135800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:15,285-Speed 3893.51 samples/sec  Loss 3.6142  LearningRate 0.0004  ProxyLR: 0.0199  Epoch: 23  Global Step: 135810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:17,902-Speed 3914.34 samples/sec  Loss 3.5806  LearningRate 0.0004  ProxyLR: 0.0198  Epoch: 23  Global Step: 135820   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:20,534-Speed 3891.72 samples/sec  Loss 3.6136  LearningRate 0.0004  ProxyLR: 0.0198  Epoch: 23  Global Step: 135830   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:23,164-Speed 3893.73 samples/sec  Loss 3.6134  LearningRate 0.0004  ProxyLR: 0.0197  Epoch: 23  Global Step: 135840   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:25,796-Speed 3891.47 samples/sec  Loss 3.6436  LearningRate 0.0004  ProxyLR: 0.0196  Epoch: 23  Global Step: 135850   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:28,428-Speed 3891.72 samples/sec  Loss 3.6587  LearningRate 0.0004  ProxyLR: 0.0196  Epoch: 23  Global Step: 135860   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:31,060-Speed 3891.25 samples/sec  Loss 3.6295  LearningRate 0.0004  ProxyLR: 0.0195  Epoch: 23  Global Step: 135870   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:33,690-Speed 3894.91 samples/sec  Loss 3.6531  LearningRate 0.0004  ProxyLR: 0.0195  Epoch: 23  Global Step: 135880   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:36,323-Speed 3889.85 samples/sec  Loss 3.6498  LearningRate 0.0004  ProxyLR: 0.0194  Epoch: 23  Global Step: 135890   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:38,955-Speed 3891.16 samples/sec  Loss 3.6595  LearningRate 0.0004  ProxyLR: 0.0193  Epoch: 23  Global Step: 135900   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:41,585-Speed 3894.57 samples/sec  Loss 3.7098  LearningRate 0.0004  ProxyLR: 0.0193  Epoch: 23  Global Step: 135910   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 01:47:44,215-Speed 3894.39 samples/sec  Loss 3.7082  LearningRate 0.0004  ProxyLR: 0.0192  Epoch: 23  Global Step: 135920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:46,848-Speed 3890.04 samples/sec  Loss 3.6139  LearningRate 0.0004  ProxyLR: 0.0191  Epoch: 23  Global Step: 135930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:49,479-Speed 3893.77 samples/sec  Loss 3.6410  LearningRate 0.0004  ProxyLR: 0.0191  Epoch: 23  Global Step: 135940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:52,107-Speed 3896.32 samples/sec  Loss 3.6111  LearningRate 0.0004  ProxyLR: 0.0190  Epoch: 23  Global Step: 135950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:54,737-Speed 3894.83 samples/sec  Loss 3.6209  LearningRate 0.0004  ProxyLR: 0.0190  Epoch: 23  Global Step: 135960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:57,367-Speed 3894.39 samples/sec  Loss 3.7070  LearningRate 0.0004  ProxyLR: 0.0189  Epoch: 23  Global Step: 135970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:47:59,999-Speed 3891.64 samples/sec  Loss 3.7123  LearningRate 0.0004  ProxyLR: 0.0188  Epoch: 23  Global Step: 135980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:48:02,629-Speed 3894.35 samples/sec  Loss 3.6626  LearningRate 0.0004  ProxyLR: 0.0188  Epoch: 23  Global Step: 135990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:48:05,261-Speed 3892.28 samples/sec  Loss 3.6409  LearningRate 0.0004  ProxyLR: 0.0187  Epoch: 23  Global Step: 136000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:48:54,501-[lfw][136000]XNorm: 22.059588
Training: 2023-05-05 01:48:54,501-[lfw][136000]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-05 01:48:54,501-[lfw][136000]Accuracy-Highest: 0.99800
Training: 2023-05-05 01:48:54,501-[lfw][136000]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 01:48:54,501-[lfw][136000]Highest TPR@FPR: 0.99667
Training: 2023-05-05 01:49:51,201-[cfp_fp][136000]XNorm: 21.839014
Training: 2023-05-05 01:49:51,201-[cfp_fp][136000]Accuracy-Flip: 0.98486+-0.00462
Training: 2023-05-05 01:49:51,201-[cfp_fp][136000]Accuracy-Highest: 0.98571
Training: 2023-05-05 01:49:51,202-[cfp_fp][136000]TPR@1stNon-Zero-FPR of 0.00029: 0.88629
Training: 2023-05-05 01:49:51,202-[cfp_fp][136000]Highest TPR@FPR: 0.90657
Training: 2023-05-05 01:50:40,524-[agedb_30][136000]XNorm: 22.242029
Training: 2023-05-05 01:50:40,524-[agedb_30][136000]Accuracy-Flip: 0.97533+-0.00795
Training: 2023-05-05 01:50:40,525-[agedb_30][136000]Accuracy-Highest: 0.97617
Training: 2023-05-05 01:50:40,525-[agedb_30][136000]TPR@1stNon-Zero-FPR of 0.00033: 0.86400
Training: 2023-05-05 01:50:40,525-[agedb_30][136000]Highest TPR@FPR: 0.88033
Training: 2023-05-05 01:51:31,192-[calfw][136000]XNorm: 22.210135
Training: 2023-05-05 01:51:31,193-[calfw][136000]Accuracy-Flip: 0.95683+-0.01248
Training: 2023-05-05 01:51:31,193-[calfw][136000]Accuracy-Highest: 0.95867
Training: 2023-05-05 01:51:31,193-[calfw][136000]TPR@1stNon-Zero-FPR of 0.00033: 0.84133
Training: 2023-05-05 01:51:31,193-[calfw][136000]Highest TPR@FPR: 0.85533
Training: 2023-05-05 01:52:21,896-[cplfw][136000]XNorm: 21.222409
Training: 2023-05-05 01:52:21,896-[cplfw][136000]Accuracy-Flip: 0.92883+-0.01162
Training: 2023-05-05 01:52:21,896-[cplfw][136000]Accuracy-Highest: 0.93100
Training: 2023-05-05 01:52:21,897-[cplfw][136000]TPR@1stNon-Zero-FPR of 0.00033: 0.11800
Training: 2023-05-05 01:52:21,897-[cplfw][136000]Highest TPR@FPR: 0.14333
Training: 2023-05-05 01:52:25,173-Speed 39.40 samples/sec  Loss 3.6072  LearningRate 0.0004  ProxyLR: 0.0187  Epoch: 23  Global Step: 136010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:27,779-Speed 3930.96 samples/sec  Loss 3.6720  LearningRate 0.0004  ProxyLR: 0.0186  Epoch: 23  Global Step: 136020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:30,396-Speed 3913.20 samples/sec  Loss 3.6272  LearningRate 0.0004  ProxyLR: 0.0185  Epoch: 23  Global Step: 136030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:33,016-Speed 3909.97 samples/sec  Loss 3.6307  LearningRate 0.0004  ProxyLR: 0.0185  Epoch: 23  Global Step: 136040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:35,633-Speed 3912.67 samples/sec  Loss 3.6168  LearningRate 0.0004  ProxyLR: 0.0184  Epoch: 23  Global Step: 136050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:38,254-Speed 3907.61 samples/sec  Loss 3.6580  LearningRate 0.0004  ProxyLR: 0.0184  Epoch: 23  Global Step: 136060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:40,873-Speed 3912.52 samples/sec  Loss 3.5690  LearningRate 0.0004  ProxyLR: 0.0183  Epoch: 23  Global Step: 136070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:43,493-Speed 3909.41 samples/sec  Loss 3.6752  LearningRate 0.0004  ProxyLR: 0.0182  Epoch: 23  Global Step: 136080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:46,115-Speed 3906.34 samples/sec  Loss 3.6434  LearningRate 0.0004  ProxyLR: 0.0182  Epoch: 23  Global Step: 136090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:48,736-Speed 3908.27 samples/sec  Loss 3.6462  LearningRate 0.0004  ProxyLR: 0.0181  Epoch: 23  Global Step: 136100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:51,357-Speed 3908.06 samples/sec  Loss 3.6938  LearningRate 0.0004  ProxyLR: 0.0181  Epoch: 23  Global Step: 136110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:53,965-Speed 3926.70 samples/sec  Loss 3.5539  LearningRate 0.0004  ProxyLR: 0.0180  Epoch: 23  Global Step: 136120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:56,588-Speed 3904.11 samples/sec  Loss 3.6268  LearningRate 0.0004  ProxyLR: 0.0179  Epoch: 23  Global Step: 136130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:52:59,209-Speed 3908.58 samples/sec  Loss 3.6186  LearningRate 0.0004  ProxyLR: 0.0179  Epoch: 23  Global Step: 136140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:01,833-Speed 3903.14 samples/sec  Loss 3.5898  LearningRate 0.0004  ProxyLR: 0.0178  Epoch: 23  Global Step: 136150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:04,458-Speed 3902.37 samples/sec  Loss 3.5703  LearningRate 0.0004  ProxyLR: 0.0178  Epoch: 23  Global Step: 136160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:07,081-Speed 3904.28 samples/sec  Loss 3.6424  LearningRate 0.0004  ProxyLR: 0.0177  Epoch: 23  Global Step: 136170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:09,706-Speed 3902.50 samples/sec  Loss 3.6414  LearningRate 0.0004  ProxyLR: 0.0176  Epoch: 23  Global Step: 136180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:12,333-Speed 3898.89 samples/sec  Loss 3.6475  LearningRate 0.0004  ProxyLR: 0.0176  Epoch: 23  Global Step: 136190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:14,958-Speed 3901.68 samples/sec  Loss 3.6805  LearningRate 0.0004  ProxyLR: 0.0175  Epoch: 23  Global Step: 136200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:17,583-Speed 3901.75 samples/sec  Loss 3.6081  LearningRate 0.0003  ProxyLR: 0.0175  Epoch: 23  Global Step: 136210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:20,194-Speed 3923.63 samples/sec  Loss 3.6139  LearningRate 0.0003  ProxyLR: 0.0174  Epoch: 23  Global Step: 136220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:22,818-Speed 3903.04 samples/sec  Loss 3.6191  LearningRate 0.0003  ProxyLR: 0.0173  Epoch: 23  Global Step: 136230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:25,444-Speed 3900.47 samples/sec  Loss 3.6746  LearningRate 0.0003  ProxyLR: 0.0173  Epoch: 23  Global Step: 136240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:28,068-Speed 3902.63 samples/sec  Loss 3.5963  LearningRate 0.0003  ProxyLR: 0.0172  Epoch: 23  Global Step: 136250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:30,693-Speed 3901.70 samples/sec  Loss 3.6175  LearningRate 0.0003  ProxyLR: 0.0172  Epoch: 23  Global Step: 136260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:33,320-Speed 3899.66 samples/sec  Loss 3.6513  LearningRate 0.0003  ProxyLR: 0.0171  Epoch: 23  Global Step: 136270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:35,951-Speed 3893.25 samples/sec  Loss 3.6422  LearningRate 0.0003  ProxyLR: 0.0171  Epoch: 23  Global Step: 136280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:38,583-Speed 3891.06 samples/sec  Loss 3.6198  LearningRate 0.0003  ProxyLR: 0.0170  Epoch: 23  Global Step: 136290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:41,214-Speed 3892.66 samples/sec  Loss 3.5966  LearningRate 0.0003  ProxyLR: 0.0169  Epoch: 23  Global Step: 136300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:43,843-Speed 3895.88 samples/sec  Loss 3.6401  LearningRate 0.0003  ProxyLR: 0.0169  Epoch: 23  Global Step: 136310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 01:53:46,477-Speed 3888.32 samples/sec  Loss 3.6256  LearningRate 0.0003  ProxyLR: 0.0168  Epoch: 23  Global Step: 136320   Fp16 Grad Scale: 524288  Required: 0 hours
Training: 2023-05-05 01:53:49,099-Speed 3907.59 samples/sec  Loss 3.6100  LearningRate 0.0003  ProxyLR: 0.0168  Epoch: 23  Global Step: 136330   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:53:51,731-Speed 3891.49 samples/sec  Loss 3.7271  LearningRate 0.0003  ProxyLR: 0.0167  Epoch: 23  Global Step: 136340   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:53:54,363-Speed 3890.14 samples/sec  Loss 3.6592  LearningRate 0.0003  ProxyLR: 0.0166  Epoch: 23  Global Step: 136350   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:53:56,994-Speed 3893.82 samples/sec  Loss 3.6239  LearningRate 0.0003  ProxyLR: 0.0166  Epoch: 23  Global Step: 136360   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:53:59,625-Speed 3893.20 samples/sec  Loss 3.6497  LearningRate 0.0003  ProxyLR: 0.0165  Epoch: 23  Global Step: 136370   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:02,258-Speed 3890.10 samples/sec  Loss 3.6723  LearningRate 0.0003  ProxyLR: 0.0165  Epoch: 23  Global Step: 136380   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:04,890-Speed 3891.43 samples/sec  Loss 3.6734  LearningRate 0.0003  ProxyLR: 0.0164  Epoch: 23  Global Step: 136390   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:07,521-Speed 3891.70 samples/sec  Loss 3.6391  LearningRate 0.0003  ProxyLR: 0.0164  Epoch: 23  Global Step: 136400   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:10,157-Speed 3886.39 samples/sec  Loss 3.6212  LearningRate 0.0003  ProxyLR: 0.0163  Epoch: 23  Global Step: 136410   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:12,789-Speed 3890.64 samples/sec  Loss 3.6351  LearningRate 0.0003  ProxyLR: 0.0162  Epoch: 23  Global Step: 136420   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:15,410-Speed 3909.30 samples/sec  Loss 3.5600  LearningRate 0.0003  ProxyLR: 0.0162  Epoch: 23  Global Step: 136430   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:18,042-Speed 3891.47 samples/sec  Loss 3.7053  LearningRate 0.0003  ProxyLR: 0.0161  Epoch: 23  Global Step: 136440   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:20,670-Speed 3897.00 samples/sec  Loss 3.6409  LearningRate 0.0003  ProxyLR: 0.0161  Epoch: 23  Global Step: 136450   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:23,358-Speed 3810.42 samples/sec  Loss 3.7003  LearningRate 0.0003  ProxyLR: 0.0160  Epoch: 23  Global Step: 136460   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:32,278-Speed 1148.03 samples/sec  Loss 3.5380  LearningRate 0.0003  ProxyLR: 0.0160  Epoch: 24  Global Step: 136470   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:34,971-Speed 3803.29 samples/sec  Loss 3.6072  LearningRate 0.0003  ProxyLR: 0.0159  Epoch: 24  Global Step: 136480   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:37,605-Speed 3889.77 samples/sec  Loss 3.5327  LearningRate 0.0003  ProxyLR: 0.0159  Epoch: 24  Global Step: 136490   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:40,237-Speed 3890.11 samples/sec  Loss 3.5464  LearningRate 0.0003  ProxyLR: 0.0158  Epoch: 24  Global Step: 136500   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:42,872-Speed 3886.99 samples/sec  Loss 3.5285  LearningRate 0.0003  ProxyLR: 0.0157  Epoch: 24  Global Step: 136510   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:45,504-Speed 3891.76 samples/sec  Loss 3.5542  LearningRate 0.0003  ProxyLR: 0.0157  Epoch: 24  Global Step: 136520   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 01:54:48,158-Speed 3860.01 samples/sec  Loss 3.5564  LearningRate 0.0003  ProxyLR: 0.0156  Epoch: 24  Global Step: 136530   Fp16 Grad Scale: 262144  Required: 0 hours

Job Killed, continuing off Epoch-23-Weights...

Training: 2023-05-05 18:19:08,644-Speed 825.87 samples/sec  Loss 3.5657  LearningRate 0.0003  ProxyLR: 0.0159  Epoch: 0  Global Step: 20   Fp16 Grad Scale: 65536  Required: 3 hours
Training: 2023-05-05 18:19:19,129-Speed 976.73 samples/sec  Loss 3.5375  LearningRate 0.0003  ProxyLR: 0.0158  Epoch: 0  Global Step: 30   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:19:30,826-Speed 875.48 samples/sec  Loss 3.5759  LearningRate 0.0003  ProxyLR: 0.0158  Epoch: 0  Global Step: 40   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:19:40,752-Speed 1031.82 samples/sec  Loss 3.6041  LearningRate 0.0003  ProxyLR: 0.0157  Epoch: 0  Global Step: 50   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:19:51,197-Speed 980.40 samples/sec  Loss 3.5469  LearningRate 0.0003  ProxyLR: 0.0157  Epoch: 0  Global Step: 60   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:20:00,842-Speed 1061.83 samples/sec  Loss 3.5586  LearningRate 0.0003  ProxyLR: 0.0156  Epoch: 0  Global Step: 70   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:20:10,779-Speed 1030.61 samples/sec  Loss 3.5721  LearningRate 0.0003  ProxyLR: 0.0156  Epoch: 0  Global Step: 80   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:20:19,937-Speed 1118.22 samples/sec  Loss 3.5411  LearningRate 0.0003  ProxyLR: 0.0155  Epoch: 0  Global Step: 90   Fp16 Grad Scale: 65536  Required: 2 hours
Training: 2023-05-05 18:20:29,830-Speed 1035.14 samples/sec  Loss 3.5027  LearningRate 0.0003  ProxyLR: 0.0154  Epoch: 0  Global Step: 100   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:20:39,187-Speed 1094.54 samples/sec  Loss 3.4674  LearningRate 0.0003  ProxyLR: 0.0154  Epoch: 0  Global Step: 110   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:20:48,310-Speed 1122.58 samples/sec  Loss 3.5813  LearningRate 0.0003  ProxyLR: 0.0153  Epoch: 0  Global Step: 120   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:20:58,577-Speed 997.44 samples/sec  Loss 3.6350  LearningRate 0.0003  ProxyLR: 0.0153  Epoch: 0  Global Step: 130   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:08,332-Speed 1049.94 samples/sec  Loss 3.6151  LearningRate 0.0003  ProxyLR: 0.0152  Epoch: 0  Global Step: 140   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:17,143-Speed 1162.32 samples/sec  Loss 3.5586  LearningRate 0.0003  ProxyLR: 0.0152  Epoch: 0  Global Step: 150   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:25,982-Speed 1158.57 samples/sec  Loss 3.5630  LearningRate 0.0003  ProxyLR: 0.0151  Epoch: 0  Global Step: 160   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:33,854-Speed 1300.90 samples/sec  Loss 3.5385  LearningRate 0.0003  ProxyLR: 0.0151  Epoch: 0  Global Step: 170   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:42,443-Speed 1192.45 samples/sec  Loss 3.5879  LearningRate 0.0003  ProxyLR: 0.0150  Epoch: 0  Global Step: 180   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:50,217-Speed 1317.33 samples/sec  Loss 3.5809  LearningRate 0.0003  ProxyLR: 0.0149  Epoch: 0  Global Step: 190   Fp16 Grad Scale: 131072  Required: 2 hours
Training: 2023-05-05 18:21:57,785-Speed 1353.10 samples/sec  Loss 3.5623  LearningRate 0.0003  ProxyLR: 0.0149  Epoch: 0  Global Step: 200   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 18:22:05,668-Speed 1299.19 samples/sec  Loss 3.6497  LearningRate 0.0003  ProxyLR: 0.0148  Epoch: 0  Global Step: 210   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 18:22:12,536-Speed 1491.20 samples/sec  Loss 3.6049  LearningRate 0.0003  ProxyLR: 0.0148  Epoch: 0  Global Step: 220   Fp16 Grad Scale: 262144  Required: 2 hours
Training: 2023-05-05 18:22:20,041-Speed 1364.53 samples/sec  Loss 3.5957  LearningRate 0.0003  ProxyLR: 0.0147  Epoch: 0  Global Step: 230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:22:26,943-Speed 1483.89 samples/sec  Loss 3.5154  LearningRate 0.0003  ProxyLR: 0.0147  Epoch: 0  Global Step: 240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:22:33,193-Speed 1638.63 samples/sec  Loss 3.5506  LearningRate 0.0003  ProxyLR: 0.0146  Epoch: 0  Global Step: 250   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:22:39,932-Speed 1519.70 samples/sec  Loss 3.5780  LearningRate 0.0003  ProxyLR: 0.0146  Epoch: 0  Global Step: 260   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:22:45,930-Speed 1707.69 samples/sec  Loss 3.6663  LearningRate 0.0003  ProxyLR: 0.0145  Epoch: 0  Global Step: 270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:22:52,209-Speed 1631.09 samples/sec  Loss 3.5379  LearningRate 0.0003  ProxyLR: 0.0145  Epoch: 0  Global Step: 280   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:22:58,249-Speed 1695.63 samples/sec  Loss 3.5620  LearningRate 0.0003  ProxyLR: 0.0144  Epoch: 0  Global Step: 290   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:23:04,288-Speed 1695.84 samples/sec  Loss 3.6244  LearningRate 0.0003  ProxyLR: 0.0144  Epoch: 0  Global Step: 300   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:23:09,917-Speed 1819.35 samples/sec  Loss 3.5450  LearningRate 0.0003  ProxyLR: 0.0143  Epoch: 0  Global Step: 310   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:23:15,579-Speed 1809.79 samples/sec  Loss 3.5862  LearningRate 0.0003  ProxyLR: 0.0142  Epoch: 0  Global Step: 320   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:23:21,230-Speed 1812.31 samples/sec  Loss 3.6046  LearningRate 0.0003  ProxyLR: 0.0142  Epoch: 0  Global Step: 330   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:23:26,886-Speed 1810.81 samples/sec  Loss 3.5246  LearningRate 0.0003  ProxyLR: 0.0141  Epoch: 0  Global Step: 340   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:23:32,546-Speed 1809.20 samples/sec  Loss 3.5158  LearningRate 0.0003  ProxyLR: 0.0141  Epoch: 0  Global Step: 350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:23:38,201-Speed 1811.08 samples/sec  Loss 3.5240  LearningRate 0.0003  ProxyLR: 0.0140  Epoch: 0  Global Step: 360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:23:43,852-Speed 1812.36 samples/sec  Loss 3.5609  LearningRate 0.0003  ProxyLR: 0.0140  Epoch: 0  Global Step: 370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:23:49,499-Speed 1813.85 samples/sec  Loss 3.5780  LearningRate 0.0003  ProxyLR: 0.0139  Epoch: 0  Global Step: 380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:23:55,141-Speed 1815.35 samples/sec  Loss 3.5645  LearningRate 0.0003  ProxyLR: 0.0139  Epoch: 0  Global Step: 390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:00,782-Speed 1815.46 samples/sec  Loss 3.4883  LearningRate 0.0003  ProxyLR: 0.0138  Epoch: 0  Global Step: 400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:06,415-Speed 1818.21 samples/sec  Loss 3.5430  LearningRate 0.0003  ProxyLR: 0.0138  Epoch: 0  Global Step: 410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:12,049-Speed 1817.93 samples/sec  Loss 3.5449  LearningRate 0.0003  ProxyLR: 0.0137  Epoch: 0  Global Step: 420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:17,672-Speed 1821.33 samples/sec  Loss 3.5947  LearningRate 0.0003  ProxyLR: 0.0137  Epoch: 0  Global Step: 430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:23,310-Speed 1816.43 samples/sec  Loss 3.5644  LearningRate 0.0003  ProxyLR: 0.0136  Epoch: 0  Global Step: 440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:28,922-Speed 1825.08 samples/sec  Loss 3.5413  LearningRate 0.0003  ProxyLR: 0.0136  Epoch: 0  Global Step: 450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:34,559-Speed 1816.74 samples/sec  Loss 3.5601  LearningRate 0.0003  ProxyLR: 0.0135  Epoch: 0  Global Step: 460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:40,190-Speed 1818.73 samples/sec  Loss 3.5619  LearningRate 0.0003  ProxyLR: 0.0135  Epoch: 0  Global Step: 470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:45,823-Speed 1818.04 samples/sec  Loss 3.5159  LearningRate 0.0003  ProxyLR: 0.0134  Epoch: 0  Global Step: 480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:51,461-Speed 1818.77 samples/sec  Loss 3.5106  LearningRate 0.0003  ProxyLR: 0.0134  Epoch: 0  Global Step: 490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:24:57,094-Speed 1817.86 samples/sec  Loss 3.5460  LearningRate 0.0002  ProxyLR: 0.0133  Epoch: 0  Global Step: 500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:02,721-Speed 1820.37 samples/sec  Loss 3.6539  LearningRate 0.0002  ProxyLR: 0.0133  Epoch: 0  Global Step: 510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:08,342-Speed 1821.87 samples/sec  Loss 3.5874  LearningRate 0.0002  ProxyLR: 0.0132  Epoch: 0  Global Step: 520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:13,972-Speed 1819.03 samples/sec  Loss 3.6349  LearningRate 0.0002  ProxyLR: 0.0132  Epoch: 0  Global Step: 530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:19,613-Speed 1815.86 samples/sec  Loss 3.5194  LearningRate 0.0002  ProxyLR: 0.0131  Epoch: 0  Global Step: 540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:25,241-Speed 1819.76 samples/sec  Loss 3.5650  LearningRate 0.0002  ProxyLR: 0.0131  Epoch: 0  Global Step: 550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:30,881-Speed 1815.73 samples/sec  Loss 3.5713  LearningRate 0.0002  ProxyLR: 0.0130  Epoch: 0  Global Step: 560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:36,521-Speed 1815.95 samples/sec  Loss 3.4920  LearningRate 0.0002  ProxyLR: 0.0130  Epoch: 0  Global Step: 570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:42,169-Speed 1813.14 samples/sec  Loss 3.5569  LearningRate 0.0002  ProxyLR: 0.0129  Epoch: 0  Global Step: 580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:47,797-Speed 1819.88 samples/sec  Loss 3.5606  LearningRate 0.0002  ProxyLR: 0.0129  Epoch: 0  Global Step: 590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:53,438-Speed 1815.49 samples/sec  Loss 3.6110  LearningRate 0.0002  ProxyLR: 0.0128  Epoch: 0  Global Step: 600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:25:59,070-Speed 1818.46 samples/sec  Loss 3.6280  LearningRate 0.0002  ProxyLR: 0.0128  Epoch: 0  Global Step: 610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:04,705-Speed 1818.51 samples/sec  Loss 3.5318  LearningRate 0.0002  ProxyLR: 0.0127  Epoch: 0  Global Step: 620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:10,351-Speed 1813.83 samples/sec  Loss 3.5145  LearningRate 0.0002  ProxyLR: 0.0127  Epoch: 0  Global Step: 630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:15,987-Speed 1817.00 samples/sec  Loss 3.6073  LearningRate 0.0002  ProxyLR: 0.0126  Epoch: 0  Global Step: 640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:21,614-Speed 1820.15 samples/sec  Loss 3.6348  LearningRate 0.0002  ProxyLR: 0.0126  Epoch: 0  Global Step: 650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:27,225-Speed 1825.42 samples/sec  Loss 3.5520  LearningRate 0.0002  ProxyLR: 0.0125  Epoch: 0  Global Step: 660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:32,864-Speed 1816.24 samples/sec  Loss 3.6245  LearningRate 0.0002  ProxyLR: 0.0125  Epoch: 0  Global Step: 670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:38,502-Speed 1816.28 samples/sec  Loss 3.5440  LearningRate 0.0002  ProxyLR: 0.0124  Epoch: 0  Global Step: 680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:44,146-Speed 1814.77 samples/sec  Loss 3.5941  LearningRate 0.0002  ProxyLR: 0.0124  Epoch: 0  Global Step: 690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:49,802-Speed 1811.04 samples/sec  Loss 3.6041  LearningRate 0.0002  ProxyLR: 0.0123  Epoch: 0  Global Step: 700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:26:55,434-Speed 1818.33 samples/sec  Loss 3.5020  LearningRate 0.0002  ProxyLR: 0.0123  Epoch: 0  Global Step: 710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:01,078-Speed 1814.92 samples/sec  Loss 3.5692  LearningRate 0.0002  ProxyLR: 0.0122  Epoch: 0  Global Step: 720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:06,716-Speed 1816.38 samples/sec  Loss 3.5675  LearningRate 0.0002  ProxyLR: 0.0122  Epoch: 0  Global Step: 730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:12,353-Speed 1816.92 samples/sec  Loss 3.5784  LearningRate 0.0002  ProxyLR: 0.0121  Epoch: 0  Global Step: 740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:17,983-Speed 1818.96 samples/sec  Loss 3.5877  LearningRate 0.0002  ProxyLR: 0.0121  Epoch: 0  Global Step: 750   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 18:27:23,612-Speed 1819.46 samples/sec  Loss 3.5291  LearningRate 0.0002  ProxyLR: 0.0120  Epoch: 0  Global Step: 760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:29,257-Speed 1814.39 samples/sec  Loss 3.5376  LearningRate 0.0002  ProxyLR: 0.0120  Epoch: 0  Global Step: 770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:34,887-Speed 1819.37 samples/sec  Loss 3.5870  LearningRate 0.0002  ProxyLR: 0.0119  Epoch: 0  Global Step: 780   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:40,533-Speed 1813.85 samples/sec  Loss 3.6208  LearningRate 0.0002  ProxyLR: 0.0119  Epoch: 0  Global Step: 790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:46,170-Speed 1816.78 samples/sec  Loss 3.5283  LearningRate 0.0002  ProxyLR: 0.0118  Epoch: 0  Global Step: 800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:51,827-Speed 1810.19 samples/sec  Loss 3.5303  LearningRate 0.0002  ProxyLR: 0.0118  Epoch: 0  Global Step: 810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:27:57,457-Speed 1819.23 samples/sec  Loss 3.5730  LearningRate 0.0002  ProxyLR: 0.0117  Epoch: 0  Global Step: 820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:03,089-Speed 1818.39 samples/sec  Loss 3.5808  LearningRate 0.0002  ProxyLR: 0.0117  Epoch: 0  Global Step: 830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:08,722-Speed 1818.08 samples/sec  Loss 3.5516  LearningRate 0.0002  ProxyLR: 0.0116  Epoch: 0  Global Step: 840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:14,374-Speed 1812.21 samples/sec  Loss 3.5592  LearningRate 0.0002  ProxyLR: 0.0116  Epoch: 0  Global Step: 850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:20,014-Speed 1815.70 samples/sec  Loss 3.5302  LearningRate 0.0002  ProxyLR: 0.0115  Epoch: 0  Global Step: 860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:25,655-Speed 1815.57 samples/sec  Loss 3.6289  LearningRate 0.0002  ProxyLR: 0.0115  Epoch: 0  Global Step: 870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:31,303-Speed 1813.44 samples/sec  Loss 3.5691  LearningRate 0.0002  ProxyLR: 0.0114  Epoch: 0  Global Step: 880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:36,951-Speed 1813.33 samples/sec  Loss 3.6302  LearningRate 0.0002  ProxyLR: 0.0114  Epoch: 0  Global Step: 890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:42,573-Speed 1821.59 samples/sec  Loss 3.5766  LearningRate 0.0002  ProxyLR: 0.0113  Epoch: 0  Global Step: 900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:48,217-Speed 1814.53 samples/sec  Loss 3.5683  LearningRate 0.0002  ProxyLR: 0.0113  Epoch: 0  Global Step: 910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:53,850-Speed 1818.33 samples/sec  Loss 3.6009  LearningRate 0.0002  ProxyLR: 0.0112  Epoch: 0  Global Step: 920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:28:59,490-Speed 1815.61 samples/sec  Loss 3.6593  LearningRate 0.0002  ProxyLR: 0.0112  Epoch: 0  Global Step: 930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:05,152-Speed 1808.94 samples/sec  Loss 3.5614  LearningRate 0.0002  ProxyLR: 0.0111  Epoch: 0  Global Step: 940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:10,769-Speed 1823.99 samples/sec  Loss 3.5312  LearningRate 0.0002  ProxyLR: 0.0111  Epoch: 0  Global Step: 950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:16,376-Speed 1826.46 samples/sec  Loss 3.5970  LearningRate 0.0002  ProxyLR: 0.0111  Epoch: 0  Global Step: 960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:22,024-Speed 1813.42 samples/sec  Loss 3.5659  LearningRate 0.0002  ProxyLR: 0.0110  Epoch: 0  Global Step: 970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:27,658-Speed 1817.80 samples/sec  Loss 3.5565  LearningRate 0.0002  ProxyLR: 0.0110  Epoch: 0  Global Step: 980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:33,301-Speed 1814.85 samples/sec  Loss 3.6377  LearningRate 0.0002  ProxyLR: 0.0109  Epoch: 0  Global Step: 990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:38,937-Speed 1817.29 samples/sec  Loss 3.5022  LearningRate 0.0002  ProxyLR: 0.0109  Epoch: 0  Global Step: 1000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:44,570-Speed 1818.23 samples/sec  Loss 3.5544  LearningRate 0.0002  ProxyLR: 0.0108  Epoch: 0  Global Step: 1010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:50,212-Speed 1815.01 samples/sec  Loss 3.5402  LearningRate 0.0002  ProxyLR: 0.0108  Epoch: 0  Global Step: 1020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:29:55,847-Speed 1817.47 samples/sec  Loss 3.6184  LearningRate 0.0002  ProxyLR: 0.0107  Epoch: 0  Global Step: 1030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:01,481-Speed 1817.88 samples/sec  Loss 3.6126  LearningRate 0.0002  ProxyLR: 0.0107  Epoch: 0  Global Step: 1040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:07,126-Speed 1814.36 samples/sec  Loss 3.5550  LearningRate 0.0002  ProxyLR: 0.0106  Epoch: 0  Global Step: 1050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:12,758-Speed 1818.35 samples/sec  Loss 3.5113  LearningRate 0.0002  ProxyLR: 0.0106  Epoch: 0  Global Step: 1060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:18,392-Speed 1817.76 samples/sec  Loss 3.5380  LearningRate 0.0002  ProxyLR: 0.0105  Epoch: 0  Global Step: 1070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:24,027-Speed 1817.65 samples/sec  Loss 3.5755  LearningRate 0.0002  ProxyLR: 0.0105  Epoch: 0  Global Step: 1080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:29,668-Speed 1815.62 samples/sec  Loss 3.5328  LearningRate 0.0002  ProxyLR: 0.0105  Epoch: 0  Global Step: 1090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:35,295-Speed 1819.89 samples/sec  Loss 3.6414  LearningRate 0.0002  ProxyLR: 0.0104  Epoch: 0  Global Step: 1100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:40,931-Speed 1817.31 samples/sec  Loss 3.5120  LearningRate 0.0002  ProxyLR: 0.0104  Epoch: 0  Global Step: 1110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:46,567-Speed 1817.03 samples/sec  Loss 3.5884  LearningRate 0.0002  ProxyLR: 0.0103  Epoch: 0  Global Step: 1120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:52,202-Speed 1817.62 samples/sec  Loss 3.5855  LearningRate 0.0002  ProxyLR: 0.0103  Epoch: 0  Global Step: 1130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:30:57,829-Speed 1819.95 samples/sec  Loss 3.5990  LearningRate 0.0002  ProxyLR: 0.0102  Epoch: 0  Global Step: 1140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:31:03,464-Speed 1817.64 samples/sec  Loss 3.5326  LearningRate 0.0002  ProxyLR: 0.0102  Epoch: 0  Global Step: 1150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:31:09,073-Speed 1825.76 samples/sec  Loss 3.6078  LearningRate 0.0002  ProxyLR: 0.0101  Epoch: 0  Global Step: 1160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:31:14,698-Speed 1820.72 samples/sec  Loss 3.6182  LearningRate 0.0002  ProxyLR: 0.0101  Epoch: 0  Global Step: 1170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:31:20,323-Speed 1820.97 samples/sec  Loss 3.5898  LearningRate 0.0002  ProxyLR: 0.0100  Epoch: 0  Global Step: 1180   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:25,960-Speed 1816.77 samples/sec  Loss 3.5316  LearningRate 0.0002  ProxyLR: 0.0100  Epoch: 0  Global Step: 1190   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:31,597-Speed 1816.81 samples/sec  Loss 3.5918  LearningRate 0.0002  ProxyLR: 0.0100  Epoch: 0  Global Step: 1200   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:37,241-Speed 1814.56 samples/sec  Loss 3.5703  LearningRate 0.0002  ProxyLR: 0.0099  Epoch: 0  Global Step: 1210   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:42,871-Speed 1819.42 samples/sec  Loss 3.6297  LearningRate 0.0002  ProxyLR: 0.0099  Epoch: 0  Global Step: 1220   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:48,509-Speed 1816.35 samples/sec  Loss 3.6434  LearningRate 0.0002  ProxyLR: 0.0098  Epoch: 0  Global Step: 1230   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:54,152-Speed 1815.21 samples/sec  Loss 3.6040  LearningRate 0.0002  ProxyLR: 0.0098  Epoch: 0  Global Step: 1240   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:31:59,785-Speed 1818.20 samples/sec  Loss 3.6355  LearningRate 0.0002  ProxyLR: 0.0097  Epoch: 0  Global Step: 1250   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:32:05,403-Speed 1822.75 samples/sec  Loss 3.5049  LearningRate 0.0002  ProxyLR: 0.0097  Epoch: 0  Global Step: 1260   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:32:11,038-Speed 1817.73 samples/sec  Loss 3.6146  LearningRate 0.0002  ProxyLR: 0.0097  Epoch: 0  Global Step: 1270   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:32:16,672-Speed 1817.98 samples/sec  Loss 3.5516  LearningRate 0.0002  ProxyLR: 0.0096  Epoch: 0  Global Step: 1280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:22,313-Speed 1815.53 samples/sec  Loss 3.6063  LearningRate 0.0002  ProxyLR: 0.0096  Epoch: 0  Global Step: 1290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:27,950-Speed 1817.06 samples/sec  Loss 3.5592  LearningRate 0.0002  ProxyLR: 0.0095  Epoch: 0  Global Step: 1300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:33,597-Speed 1813.50 samples/sec  Loss 3.5204  LearningRate 0.0002  ProxyLR: 0.0095  Epoch: 0  Global Step: 1310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:39,236-Speed 1816.21 samples/sec  Loss 3.5696  LearningRate 0.0002  ProxyLR: 0.0094  Epoch: 0  Global Step: 1320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:44,875-Speed 1816.23 samples/sec  Loss 3.5332  LearningRate 0.0002  ProxyLR: 0.0094  Epoch: 0  Global Step: 1330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:50,516-Speed 1815.58 samples/sec  Loss 3.5972  LearningRate 0.0002  ProxyLR: 0.0093  Epoch: 0  Global Step: 1340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:32:56,154-Speed 1816.55 samples/sec  Loss 3.5975  LearningRate 0.0002  ProxyLR: 0.0093  Epoch: 0  Global Step: 1350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:01,784-Speed 1819.29 samples/sec  Loss 3.5938  LearningRate 0.0002  ProxyLR: 0.0093  Epoch: 0  Global Step: 1360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:07,427-Speed 1814.81 samples/sec  Loss 3.5448  LearningRate 0.0002  ProxyLR: 0.0092  Epoch: 0  Global Step: 1370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:13,050-Speed 1821.43 samples/sec  Loss 3.5612  LearningRate 0.0002  ProxyLR: 0.0092  Epoch: 0  Global Step: 1380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:18,690-Speed 1815.82 samples/sec  Loss 3.6041  LearningRate 0.0002  ProxyLR: 0.0091  Epoch: 0  Global Step: 1390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:24,327-Speed 1816.75 samples/sec  Loss 3.5650  LearningRate 0.0002  ProxyLR: 0.0091  Epoch: 0  Global Step: 1400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:29,977-Speed 1813.26 samples/sec  Loss 3.5274  LearningRate 0.0002  ProxyLR: 0.0090  Epoch: 0  Global Step: 1410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:35,617-Speed 1815.86 samples/sec  Loss 3.5279  LearningRate 0.0002  ProxyLR: 0.0090  Epoch: 0  Global Step: 1420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:41,248-Speed 1818.94 samples/sec  Loss 3.5144  LearningRate 0.0002  ProxyLR: 0.0090  Epoch: 0  Global Step: 1430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:46,895-Speed 1813.84 samples/sec  Loss 3.5613  LearningRate 0.0002  ProxyLR: 0.0089  Epoch: 0  Global Step: 1440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:52,539-Speed 1814.47 samples/sec  Loss 3.5285  LearningRate 0.0002  ProxyLR: 0.0089  Epoch: 0  Global Step: 1450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:33:58,175-Speed 1817.20 samples/sec  Loss 3.5892  LearningRate 0.0002  ProxyLR: 0.0088  Epoch: 0  Global Step: 1460   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:03,812-Speed 1816.98 samples/sec  Loss 3.5107  LearningRate 0.0002  ProxyLR: 0.0088  Epoch: 0  Global Step: 1470   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:09,432-Speed 1822.31 samples/sec  Loss 3.5787  LearningRate 0.0002  ProxyLR: 0.0088  Epoch: 0  Global Step: 1480   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:15,072-Speed 1815.84 samples/sec  Loss 3.6137  LearningRate 0.0002  ProxyLR: 0.0087  Epoch: 0  Global Step: 1490   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:20,730-Speed 1810.21 samples/sec  Loss 3.5622  LearningRate 0.0002  ProxyLR: 0.0087  Epoch: 0  Global Step: 1500   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:26,356-Speed 1820.29 samples/sec  Loss 3.5202  LearningRate 0.0002  ProxyLR: 0.0086  Epoch: 0  Global Step: 1510   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:31,991-Speed 1817.68 samples/sec  Loss 3.5664  LearningRate 0.0002  ProxyLR: 0.0086  Epoch: 0  Global Step: 1520   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:34:37,619-Speed 1819.58 samples/sec  Loss 3.5574  LearningRate 0.0002  ProxyLR: 0.0085  Epoch: 0  Global Step: 1530   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:35:33,655-[lfw][1530]XNorm: 22.044802
Training: 2023-05-05 18:35:33,655-[lfw][1530]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-05 18:35:33,655-[lfw][1530]Accuracy-Highest: 0.99750
Training: 2023-05-05 18:35:33,656-[lfw][1530]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 18:35:33,656-[lfw][1530]Highest TPR@FPR: 0.99600
Training: 2023-05-05 18:36:38,332-[cfp_fp][1530]XNorm: 21.824731
Training: 2023-05-05 18:36:38,333-[cfp_fp][1530]Accuracy-Flip: 0.98571+-0.00461
Training: 2023-05-05 18:36:38,333-[cfp_fp][1530]Accuracy-Highest: 0.98571
Training: 2023-05-05 18:36:38,333-[cfp_fp][1530]TPR@1stNon-Zero-FPR of 0.00029: 0.88743
Training: 2023-05-05 18:36:38,333-[cfp_fp][1530]Highest TPR@FPR: 0.88743
Training: 2023-05-05 18:37:34,341-[agedb_30][1530]XNorm: 22.242385
Training: 2023-05-05 18:37:34,342-[agedb_30][1530]Accuracy-Flip: 0.97550+-0.00827
Training: 2023-05-05 18:37:34,342-[agedb_30][1530]Accuracy-Highest: 0.97550
Training: 2023-05-05 18:37:34,342-[agedb_30][1530]TPR@1stNon-Zero-FPR of 0.00033: 0.86333
Training: 2023-05-05 18:37:34,342-[agedb_30][1530]Highest TPR@FPR: 0.86333
Training: 2023-05-05 18:38:31,804-[calfw][1530]XNorm: 22.194504
Training: 2023-05-05 18:38:31,805-[calfw][1530]Accuracy-Flip: 0.95800+-0.01245
Training: 2023-05-05 18:38:31,805-[calfw][1530]Accuracy-Highest: 0.95800
Training: 2023-05-05 18:38:31,805-[calfw][1530]TPR@1stNon-Zero-FPR of 0.00033: 0.84533
Training: 2023-05-05 18:38:31,805-[calfw][1530]Highest TPR@FPR: 0.84533
Training: 2023-05-05 18:39:29,219-[cplfw][1530]XNorm: 21.225292
Training: 2023-05-05 18:39:29,219-[cplfw][1530]Accuracy-Flip: 0.92817+-0.01212
Training: 2023-05-05 18:39:29,220-[cplfw][1530]Accuracy-Highest: 0.92817
Training: 2023-05-05 18:39:29,220-[cplfw][1530]TPR@1stNon-Zero-FPR of 0.00033: 0.12667
Training: 2023-05-05 18:39:29,220-[cplfw][1530]Highest TPR@FPR: 0.12667
Training: 2023-05-05 18:39:35,566-Speed 34.37 samples/sec  Loss 3.5181  LearningRate 0.0002  ProxyLR: 0.0085  Epoch: 0  Global Step: 1540   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:39:41,187-Speed 1822.66 samples/sec  Loss 3.5339  LearningRate 0.0002  ProxyLR: 0.0085  Epoch: 0  Global Step: 1550   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:39:46,810-Speed 1821.29 samples/sec  Loss 3.4927  LearningRate 0.0002  ProxyLR: 0.0084  Epoch: 0  Global Step: 1560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:39:52,433-Speed 1821.52 samples/sec  Loss 3.5246  LearningRate 0.0002  ProxyLR: 0.0084  Epoch: 0  Global Step: 1570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:39:58,035-Speed 1828.03 samples/sec  Loss 3.6337  LearningRate 0.0002  ProxyLR: 0.0083  Epoch: 0  Global Step: 1580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:03,654-Speed 1822.75 samples/sec  Loss 3.5116  LearningRate 0.0002  ProxyLR: 0.0083  Epoch: 0  Global Step: 1590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:09,281-Speed 1820.17 samples/sec  Loss 3.6273  LearningRate 0.0002  ProxyLR: 0.0083  Epoch: 0  Global Step: 1600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:14,919-Speed 1816.51 samples/sec  Loss 3.6080  LearningRate 0.0002  ProxyLR: 0.0082  Epoch: 0  Global Step: 1610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:20,552-Speed 1818.41 samples/sec  Loss 3.5653  LearningRate 0.0002  ProxyLR: 0.0082  Epoch: 0  Global Step: 1620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:26,186-Speed 1817.52 samples/sec  Loss 3.6091  LearningRate 0.0002  ProxyLR: 0.0081  Epoch: 0  Global Step: 1630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:31,823-Speed 1817.01 samples/sec  Loss 3.5160  LearningRate 0.0002  ProxyLR: 0.0081  Epoch: 0  Global Step: 1640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:37,454-Speed 1818.74 samples/sec  Loss 3.5713  LearningRate 0.0002  ProxyLR: 0.0081  Epoch: 0  Global Step: 1650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:43,087-Speed 1818.26 samples/sec  Loss 3.5561  LearningRate 0.0002  ProxyLR: 0.0080  Epoch: 0  Global Step: 1660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:48,716-Speed 1819.30 samples/sec  Loss 3.5885  LearningRate 0.0001  ProxyLR: 0.0080  Epoch: 0  Global Step: 1670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:54,338-Speed 1821.99 samples/sec  Loss 3.5518  LearningRate 0.0001  ProxyLR: 0.0079  Epoch: 0  Global Step: 1680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:40:59,968-Speed 1818.89 samples/sec  Loss 3.5738  LearningRate 0.0001  ProxyLR: 0.0079  Epoch: 0  Global Step: 1690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:05,599-Speed 1819.18 samples/sec  Loss 3.5467  LearningRate 0.0001  ProxyLR: 0.0079  Epoch: 0  Global Step: 1700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:11,236-Speed 1816.66 samples/sec  Loss 3.5441  LearningRate 0.0001  ProxyLR: 0.0078  Epoch: 0  Global Step: 1710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:16,878-Speed 1815.38 samples/sec  Loss 3.5298  LearningRate 0.0001  ProxyLR: 0.0078  Epoch: 0  Global Step: 1720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:22,524-Speed 1814.19 samples/sec  Loss 3.5562  LearningRate 0.0001  ProxyLR: 0.0077  Epoch: 0  Global Step: 1730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:28,154-Speed 1818.89 samples/sec  Loss 3.5668  LearningRate 0.0001  ProxyLR: 0.0077  Epoch: 0  Global Step: 1740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:33,786-Speed 1818.63 samples/sec  Loss 3.5737  LearningRate 0.0001  ProxyLR: 0.0077  Epoch: 0  Global Step: 1750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:39,413-Speed 1820.25 samples/sec  Loss 3.5493  LearningRate 0.0001  ProxyLR: 0.0076  Epoch: 0  Global Step: 1760   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:45,065-Speed 1811.90 samples/sec  Loss 3.5646  LearningRate 0.0001  ProxyLR: 0.0076  Epoch: 0  Global Step: 1770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:41:50,702-Speed 1816.71 samples/sec  Loss 3.6017  LearningRate 0.0001  ProxyLR: 0.0076  Epoch: 0  Global Step: 1780   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 18:41:56,325-Speed 1821.65 samples/sec  Loss 3.5245  LearningRate 0.0001  ProxyLR: 0.0075  Epoch: 0  Global Step: 1790   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:01,958-Speed 1818.06 samples/sec  Loss 3.6268  LearningRate 0.0001  ProxyLR: 0.0075  Epoch: 0  Global Step: 1800   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:07,594-Speed 1817.25 samples/sec  Loss 3.5399  LearningRate 0.0001  ProxyLR: 0.0074  Epoch: 0  Global Step: 1810   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:13,209-Speed 1823.98 samples/sec  Loss 3.4913  LearningRate 0.0001  ProxyLR: 0.0074  Epoch: 0  Global Step: 1820   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:18,843-Speed 1817.76 samples/sec  Loss 3.5702  LearningRate 0.0001  ProxyLR: 0.0074  Epoch: 0  Global Step: 1830   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:24,485-Speed 1815.36 samples/sec  Loss 3.5542  LearningRate 0.0001  ProxyLR: 0.0073  Epoch: 0  Global Step: 1840   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:30,128-Speed 1814.99 samples/sec  Loss 3.6204  LearningRate 0.0001  ProxyLR: 0.0073  Epoch: 0  Global Step: 1850   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:35,771-Speed 1814.94 samples/sec  Loss 3.5726  LearningRate 0.0001  ProxyLR: 0.0072  Epoch: 0  Global Step: 1860   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:41,397-Speed 1820.36 samples/sec  Loss 3.6391  LearningRate 0.0001  ProxyLR: 0.0072  Epoch: 0  Global Step: 1870   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:47,031-Speed 1817.87 samples/sec  Loss 3.5297  LearningRate 0.0001  ProxyLR: 0.0072  Epoch: 0  Global Step: 1880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:52,654-Speed 1821.57 samples/sec  Loss 3.5257  LearningRate 0.0001  ProxyLR: 0.0071  Epoch: 0  Global Step: 1890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:42:58,273-Speed 1822.52 samples/sec  Loss 3.5808  LearningRate 0.0001  ProxyLR: 0.0071  Epoch: 0  Global Step: 1900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:03,914-Speed 1816.00 samples/sec  Loss 3.5196  LearningRate 0.0001  ProxyLR: 0.0071  Epoch: 0  Global Step: 1910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:09,570-Speed 1810.60 samples/sec  Loss 3.5947  LearningRate 0.0001  ProxyLR: 0.0070  Epoch: 0  Global Step: 1920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:15,213-Speed 1815.01 samples/sec  Loss 3.6017  LearningRate 0.0001  ProxyLR: 0.0070  Epoch: 0  Global Step: 1930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:20,849-Speed 1817.13 samples/sec  Loss 3.5376  LearningRate 0.0001  ProxyLR: 0.0069  Epoch: 0  Global Step: 1940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:26,486-Speed 1816.81 samples/sec  Loss 3.5563  LearningRate 0.0001  ProxyLR: 0.0069  Epoch: 0  Global Step: 1950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:32,125-Speed 1816.45 samples/sec  Loss 3.6151  LearningRate 0.0001  ProxyLR: 0.0069  Epoch: 0  Global Step: 1960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:37,763-Speed 1816.49 samples/sec  Loss 3.5432  LearningRate 0.0001  ProxyLR: 0.0068  Epoch: 0  Global Step: 1970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:43,399-Speed 1817.00 samples/sec  Loss 3.5702  LearningRate 0.0001  ProxyLR: 0.0068  Epoch: 0  Global Step: 1980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:49,027-Speed 1819.94 samples/sec  Loss 3.5600  LearningRate 0.0001  ProxyLR: 0.0068  Epoch: 0  Global Step: 1990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:43:54,664-Speed 1816.70 samples/sec  Loss 3.6366  LearningRate 0.0001  ProxyLR: 0.0067  Epoch: 0  Global Step: 2000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:00,285-Speed 1821.86 samples/sec  Loss 3.5574  LearningRate 0.0001  ProxyLR: 0.0067  Epoch: 0  Global Step: 2010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:05,930-Speed 1814.65 samples/sec  Loss 3.5396  LearningRate 0.0001  ProxyLR: 0.0067  Epoch: 0  Global Step: 2020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:11,545-Speed 1823.96 samples/sec  Loss 3.5396  LearningRate 0.0001  ProxyLR: 0.0066  Epoch: 0  Global Step: 2030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:17,188-Speed 1814.97 samples/sec  Loss 3.6030  LearningRate 0.0001  ProxyLR: 0.0066  Epoch: 0  Global Step: 2040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:22,839-Speed 1812.36 samples/sec  Loss 3.5235  LearningRate 0.0001  ProxyLR: 0.0065  Epoch: 0  Global Step: 2050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:28,477-Speed 1816.72 samples/sec  Loss 3.6497  LearningRate 0.0001  ProxyLR: 0.0065  Epoch: 0  Global Step: 2060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:34,119-Speed 1814.99 samples/sec  Loss 3.6621  LearningRate 0.0001  ProxyLR: 0.0065  Epoch: 0  Global Step: 2070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:39,770-Speed 1812.47 samples/sec  Loss 3.5686  LearningRate 0.0001  ProxyLR: 0.0064  Epoch: 0  Global Step: 2080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:45,380-Speed 1825.46 samples/sec  Loss 3.6011  LearningRate 0.0001  ProxyLR: 0.0064  Epoch: 0  Global Step: 2090   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:51,012-Speed 1818.49 samples/sec  Loss 3.6167  LearningRate 0.0001  ProxyLR: 0.0064  Epoch: 0  Global Step: 2100   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:44:56,641-Speed 1819.33 samples/sec  Loss 3.5850  LearningRate 0.0001  ProxyLR: 0.0063  Epoch: 0  Global Step: 2110   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:02,270-Speed 1819.80 samples/sec  Loss 3.5479  LearningRate 0.0001  ProxyLR: 0.0063  Epoch: 0  Global Step: 2120   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:07,913-Speed 1814.78 samples/sec  Loss 3.5163  LearningRate 0.0001  ProxyLR: 0.0063  Epoch: 0  Global Step: 2130   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:13,553-Speed 1815.74 samples/sec  Loss 3.5183  LearningRate 0.0001  ProxyLR: 0.0062  Epoch: 0  Global Step: 2140   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:19,198-Speed 1814.27 samples/sec  Loss 3.4783  LearningRate 0.0001  ProxyLR: 0.0062  Epoch: 0  Global Step: 2150   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:24,849-Speed 1812.64 samples/sec  Loss 3.5972  LearningRate 0.0001  ProxyLR: 0.0062  Epoch: 0  Global Step: 2160   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:30,489-Speed 1815.70 samples/sec  Loss 3.6012  LearningRate 0.0001  ProxyLR: 0.0061  Epoch: 0  Global Step: 2170   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:36,131-Speed 1815.41 samples/sec  Loss 3.5426  LearningRate 0.0001  ProxyLR: 0.0061  Epoch: 0  Global Step: 2180   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:41,751-Speed 1822.42 samples/sec  Loss 3.5877  LearningRate 0.0001  ProxyLR: 0.0060  Epoch: 0  Global Step: 2190   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:47,390-Speed 1816.03 samples/sec  Loss 3.5364  LearningRate 0.0001  ProxyLR: 0.0060  Epoch: 0  Global Step: 2200   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:53,042-Speed 1812.24 samples/sec  Loss 3.5810  LearningRate 0.0001  ProxyLR: 0.0060  Epoch: 0  Global Step: 2210   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:45:58,678-Speed 1817.08 samples/sec  Loss 3.5520  LearningRate 0.0001  ProxyLR: 0.0059  Epoch: 0  Global Step: 2220   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:04,314-Speed 1817.33 samples/sec  Loss 3.4685  LearningRate 0.0001  ProxyLR: 0.0059  Epoch: 0  Global Step: 2230   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:09,969-Speed 1810.93 samples/sec  Loss 3.5505  LearningRate 0.0001  ProxyLR: 0.0059  Epoch: 0  Global Step: 2240   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:15,590-Speed 1822.19 samples/sec  Loss 3.5716  LearningRate 0.0001  ProxyLR: 0.0058  Epoch: 0  Global Step: 2250   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:21,228-Speed 1816.55 samples/sec  Loss 3.5850  LearningRate 0.0001  ProxyLR: 0.0058  Epoch: 0  Global Step: 2260   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:26,863-Speed 1817.41 samples/sec  Loss 3.5645  LearningRate 0.0001  ProxyLR: 0.0058  Epoch: 0  Global Step: 2270   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:32,516-Speed 1811.92 samples/sec  Loss 3.5456  LearningRate 0.0001  ProxyLR: 0.0057  Epoch: 0  Global Step: 2280   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:38,156-Speed 1815.93 samples/sec  Loss 3.5510  LearningRate 0.0001  ProxyLR: 0.0057  Epoch: 0  Global Step: 2290   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:43,780-Speed 1821.04 samples/sec  Loss 3.4832  LearningRate 0.0001  ProxyLR: 0.0057  Epoch: 0  Global Step: 2300   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:49,417-Speed 1816.51 samples/sec  Loss 3.5844  LearningRate 0.0001  ProxyLR: 0.0056  Epoch: 0  Global Step: 2310   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:46:55,060-Speed 1815.11 samples/sec  Loss 3.5286  LearningRate 0.0001  ProxyLR: 0.0056  Epoch: 0  Global Step: 2320   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:00,696-Speed 1817.30 samples/sec  Loss 3.5658  LearningRate 0.0001  ProxyLR: 0.0056  Epoch: 0  Global Step: 2330   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:06,348-Speed 1812.10 samples/sec  Loss 3.4850  LearningRate 0.0001  ProxyLR: 0.0055  Epoch: 0  Global Step: 2340   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:11,981-Speed 1818.20 samples/sec  Loss 3.5304  LearningRate 0.0001  ProxyLR: 0.0055  Epoch: 0  Global Step: 2350   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:17,617-Speed 1816.94 samples/sec  Loss 3.5653  LearningRate 0.0001  ProxyLR: 0.0055  Epoch: 0  Global Step: 2360   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:23,249-Speed 1818.75 samples/sec  Loss 3.5940  LearningRate 0.0001  ProxyLR: 0.0054  Epoch: 0  Global Step: 2370   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:28,884-Speed 1817.48 samples/sec  Loss 3.6422  LearningRate 0.0001  ProxyLR: 0.0054  Epoch: 0  Global Step: 2380   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:34,516-Speed 1818.54 samples/sec  Loss 3.5831  LearningRate 0.0001  ProxyLR: 0.0054  Epoch: 0  Global Step: 2390   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:40,159-Speed 1814.78 samples/sec  Loss 3.5255  LearningRate 0.0001  ProxyLR: 0.0053  Epoch: 0  Global Step: 2400   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:45,791-Speed 1818.39 samples/sec  Loss 3.5473  LearningRate 0.0001  ProxyLR: 0.0053  Epoch: 0  Global Step: 2410   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:51,433-Speed 1815.82 samples/sec  Loss 3.5411  LearningRate 0.0001  ProxyLR: 0.0053  Epoch: 0  Global Step: 2420   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:47:57,076-Speed 1814.92 samples/sec  Loss 3.5803  LearningRate 0.0001  ProxyLR: 0.0052  Epoch: 0  Global Step: 2430   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:48:02,722-Speed 1814.23 samples/sec  Loss 3.5773  LearningRate 0.0001  ProxyLR: 0.0052  Epoch: 0  Global Step: 2440   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:48:08,352-Speed 1819.05 samples/sec  Loss 3.6088  LearningRate 0.0001  ProxyLR: 0.0052  Epoch: 0  Global Step: 2450   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:48:13,978-Speed 1820.21 samples/sec  Loss 3.5879  LearningRate 0.0001  ProxyLR: 0.0052  Epoch: 0  Global Step: 2460   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:19,621-Speed 1815.17 samples/sec  Loss 3.5171  LearningRate 0.0001  ProxyLR: 0.0051  Epoch: 0  Global Step: 2470   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:25,251-Speed 1818.81 samples/sec  Loss 3.5443  LearningRate 0.0001  ProxyLR: 0.0051  Epoch: 0  Global Step: 2480   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:30,877-Speed 1820.58 samples/sec  Loss 3.4560  LearningRate 0.0001  ProxyLR: 0.0051  Epoch: 0  Global Step: 2490   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:36,509-Speed 1818.50 samples/sec  Loss 3.5187  LearningRate 0.0001  ProxyLR: 0.0050  Epoch: 0  Global Step: 2500   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:42,148-Speed 1816.09 samples/sec  Loss 3.5455  LearningRate 0.0001  ProxyLR: 0.0050  Epoch: 0  Global Step: 2510   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:47,768-Speed 1822.49 samples/sec  Loss 3.5422  LearningRate 0.0001  ProxyLR: 0.0050  Epoch: 0  Global Step: 2520   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:53,411-Speed 1814.92 samples/sec  Loss 3.5335  LearningRate 0.0001  ProxyLR: 0.0049  Epoch: 0  Global Step: 2530   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:48:59,052-Speed 1815.62 samples/sec  Loss 3.5701  LearningRate 0.0001  ProxyLR: 0.0049  Epoch: 0  Global Step: 2540   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:49:04,685-Speed 1818.14 samples/sec  Loss 3.5646  LearningRate 0.0001  ProxyLR: 0.0049  Epoch: 0  Global Step: 2550   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:49:10,329-Speed 1814.66 samples/sec  Loss 3.5832  LearningRate 0.0001  ProxyLR: 0.0048  Epoch: 0  Global Step: 2560   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:15,960-Speed 1818.78 samples/sec  Loss 3.6078  LearningRate 0.0001  ProxyLR: 0.0048  Epoch: 0  Global Step: 2570   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:21,610-Speed 1812.78 samples/sec  Loss 3.5553  LearningRate 0.0001  ProxyLR: 0.0048  Epoch: 0  Global Step: 2580   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:27,260-Speed 1813.12 samples/sec  Loss 3.4515  LearningRate 0.0001  ProxyLR: 0.0047  Epoch: 0  Global Step: 2590   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:32,905-Speed 1814.26 samples/sec  Loss 3.5636  LearningRate 0.0001  ProxyLR: 0.0047  Epoch: 0  Global Step: 2600   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:38,535-Speed 1819.16 samples/sec  Loss 3.6160  LearningRate 0.0001  ProxyLR: 0.0047  Epoch: 0  Global Step: 2610   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:44,186-Speed 1812.36 samples/sec  Loss 3.5471  LearningRate 0.0001  ProxyLR: 0.0047  Epoch: 0  Global Step: 2620   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:49,840-Speed 1811.52 samples/sec  Loss 3.5274  LearningRate 0.0001  ProxyLR: 0.0046  Epoch: 0  Global Step: 2630   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:49:55,485-Speed 1813.99 samples/sec  Loss 3.5107  LearningRate 0.0001  ProxyLR: 0.0046  Epoch: 0  Global Step: 2640   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:01,137-Speed 1812.32 samples/sec  Loss 3.5542  LearningRate 0.0001  ProxyLR: 0.0046  Epoch: 0  Global Step: 2650   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:06,753-Speed 1823.56 samples/sec  Loss 3.5674  LearningRate 0.0001  ProxyLR: 0.0045  Epoch: 0  Global Step: 2660   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:12,394-Speed 1815.50 samples/sec  Loss 3.5719  LearningRate 0.0001  ProxyLR: 0.0045  Epoch: 0  Global Step: 2670   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:18,054-Speed 1809.63 samples/sec  Loss 3.6038  LearningRate 0.0001  ProxyLR: 0.0045  Epoch: 0  Global Step: 2680   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:23,690-Speed 1817.05 samples/sec  Loss 3.5709  LearningRate 0.0001  ProxyLR: 0.0044  Epoch: 0  Global Step: 2690   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:29,327-Speed 1817.52 samples/sec  Loss 3.5591  LearningRate 0.0001  ProxyLR: 0.0044  Epoch: 0  Global Step: 2700   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:34,966-Speed 1816.24 samples/sec  Loss 3.5510  LearningRate 0.0001  ProxyLR: 0.0044  Epoch: 0  Global Step: 2710   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:40,597-Speed 1818.68 samples/sec  Loss 3.5209  LearningRate 0.0001  ProxyLR: 0.0044  Epoch: 0  Global Step: 2720   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:46,231-Speed 1817.83 samples/sec  Loss 3.5844  LearningRate 0.0001  ProxyLR: 0.0043  Epoch: 0  Global Step: 2730   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:51,868-Speed 1816.83 samples/sec  Loss 3.5870  LearningRate 0.0001  ProxyLR: 0.0043  Epoch: 0  Global Step: 2740   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:50:57,516-Speed 1813.57 samples/sec  Loss 3.5261  LearningRate 0.0001  ProxyLR: 0.0043  Epoch: 0  Global Step: 2750   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:51:03,163-Speed 1813.44 samples/sec  Loss 3.5450  LearningRate 0.0001  ProxyLR: 0.0042  Epoch: 0  Global Step: 2760   Fp16 Grad Scale: 524288  Required: 1 hours
Training: 2023-05-05 18:51:08,797-Speed 1817.68 samples/sec  Loss 3.5396  LearningRate 0.0001  ProxyLR: 0.0042  Epoch: 0  Global Step: 2770   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:51:14,435-Speed 1816.55 samples/sec  Loss 3.6004  LearningRate 0.0001  ProxyLR: 0.0042  Epoch: 0  Global Step: 2780   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:20,068-Speed 1818.37 samples/sec  Loss 3.5481  LearningRate 0.0001  ProxyLR: 0.0042  Epoch: 0  Global Step: 2790   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:25,706-Speed 1816.37 samples/sec  Loss 3.6012  LearningRate 0.0001  ProxyLR: 0.0041  Epoch: 0  Global Step: 2800   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:31,347-Speed 1815.65 samples/sec  Loss 3.5645  LearningRate 0.0001  ProxyLR: 0.0041  Epoch: 0  Global Step: 2810   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:36,977-Speed 1819.24 samples/sec  Loss 3.5771  LearningRate 0.0001  ProxyLR: 0.0041  Epoch: 0  Global Step: 2820   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:42,623-Speed 1813.80 samples/sec  Loss 3.5826  LearningRate 0.0001  ProxyLR: 0.0040  Epoch: 0  Global Step: 2830   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:48,252-Speed 1819.39 samples/sec  Loss 3.5859  LearningRate 0.0001  ProxyLR: 0.0040  Epoch: 0  Global Step: 2840   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:53,885-Speed 1818.13 samples/sec  Loss 3.5674  LearningRate 0.0001  ProxyLR: 0.0040  Epoch: 0  Global Step: 2850   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:51:59,543-Speed 1810.38 samples/sec  Loss 3.5860  LearningRate 0.0001  ProxyLR: 0.0040  Epoch: 0  Global Step: 2860   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:52:05,176-Speed 1818.04 samples/sec  Loss 3.5414  LearningRate 0.0001  ProxyLR: 0.0039  Epoch: 0  Global Step: 2870   Fp16 Grad Scale: 131072  Required: 1 hours
Training: 2023-05-05 18:52:10,816-Speed 1816.11 samples/sec  Loss 3.6122  LearningRate 0.0001  ProxyLR: 0.0039  Epoch: 0  Global Step: 2880   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:16,450-Speed 1817.70 samples/sec  Loss 3.5010  LearningRate 0.0001  ProxyLR: 0.0039  Epoch: 0  Global Step: 2890   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:22,103-Speed 1811.80 samples/sec  Loss 3.5554  LearningRate 0.0001  ProxyLR: 0.0038  Epoch: 0  Global Step: 2900   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:27,758-Speed 1810.95 samples/sec  Loss 3.6715  LearningRate 0.0001  ProxyLR: 0.0038  Epoch: 0  Global Step: 2910   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:33,395-Speed 1817.04 samples/sec  Loss 3.5797  LearningRate 0.0001  ProxyLR: 0.0038  Epoch: 0  Global Step: 2920   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:39,030-Speed 1817.97 samples/sec  Loss 3.5374  LearningRate 0.0001  ProxyLR: 0.0038  Epoch: 0  Global Step: 2930   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:44,667-Speed 1816.68 samples/sec  Loss 3.6032  LearningRate 0.0001  ProxyLR: 0.0037  Epoch: 0  Global Step: 2940   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:50,310-Speed 1814.86 samples/sec  Loss 3.6230  LearningRate 0.0001  ProxyLR: 0.0037  Epoch: 0  Global Step: 2950   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:52:55,952-Speed 1815.35 samples/sec  Loss 3.5067  LearningRate 0.0001  ProxyLR: 0.0037  Epoch: 0  Global Step: 2960   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:01,584-Speed 1818.63 samples/sec  Loss 3.5609  LearningRate 0.0001  ProxyLR: 0.0037  Epoch: 0  Global Step: 2970   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:07,202-Speed 1822.94 samples/sec  Loss 3.6321  LearningRate 0.0001  ProxyLR: 0.0036  Epoch: 0  Global Step: 2980   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:12,845-Speed 1815.07 samples/sec  Loss 3.5434  LearningRate 0.0001  ProxyLR: 0.0036  Epoch: 0  Global Step: 2990   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:18,492-Speed 1813.34 samples/sec  Loss 3.5631  LearningRate 0.0001  ProxyLR: 0.0036  Epoch: 0  Global Step: 3000   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:24,124-Speed 1818.42 samples/sec  Loss 3.5065  LearningRate 0.0001  ProxyLR: 0.0035  Epoch: 0  Global Step: 3010   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:29,771-Speed 1813.97 samples/sec  Loss 3.5788  LearningRate 0.0001  ProxyLR: 0.0035  Epoch: 0  Global Step: 3020   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:35,406-Speed 1817.35 samples/sec  Loss 3.5553  LearningRate 0.0001  ProxyLR: 0.0035  Epoch: 0  Global Step: 3030   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:41,045-Speed 1816.64 samples/sec  Loss 3.6476  LearningRate 0.0001  ProxyLR: 0.0035  Epoch: 0  Global Step: 3040   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:46,686-Speed 1815.35 samples/sec  Loss 3.5740  LearningRate 0.0001  ProxyLR: 0.0034  Epoch: 0  Global Step: 3050   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:52,347-Speed 1809.12 samples/sec  Loss 3.5822  LearningRate 0.0001  ProxyLR: 0.0034  Epoch: 0  Global Step: 3060   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:53:57,981-Speed 1817.87 samples/sec  Loss 3.5531  LearningRate 0.0001  ProxyLR: 0.0034  Epoch: 0  Global Step: 3070   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:54:03,603-Speed 1821.82 samples/sec  Loss 3.5139  LearningRate 0.0001  ProxyLR: 0.0034  Epoch: 0  Global Step: 3080   Fp16 Grad Scale: 262144  Required: 1 hours
Training: 2023-05-05 18:54:09,231-Speed 1819.73 samples/sec  Loss 3.5313  LearningRate 0.0001  ProxyLR: 0.0033  Epoch: 0  Global Step: 3090   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:14,864-Speed 1818.25 samples/sec  Loss 3.5664  LearningRate 0.0001  ProxyLR: 0.0033  Epoch: 0  Global Step: 3100   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:20,496-Speed 1818.61 samples/sec  Loss 3.5438  LearningRate 0.0001  ProxyLR: 0.0033  Epoch: 0  Global Step: 3110   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:26,135-Speed 1816.21 samples/sec  Loss 3.5975  LearningRate 0.0001  ProxyLR: 0.0033  Epoch: 0  Global Step: 3120   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:31,799-Speed 1808.22 samples/sec  Loss 3.5589  LearningRate 0.0001  ProxyLR: 0.0032  Epoch: 0  Global Step: 3130   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:37,431-Speed 1818.51 samples/sec  Loss 3.5643  LearningRate 0.0001  ProxyLR: 0.0032  Epoch: 0  Global Step: 3140   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:43,078-Speed 1813.76 samples/sec  Loss 3.5289  LearningRate 0.0001  ProxyLR: 0.0032  Epoch: 0  Global Step: 3150   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:48,719-Speed 1815.45 samples/sec  Loss 3.5209  LearningRate 0.0001  ProxyLR: 0.0032  Epoch: 0  Global Step: 3160   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:54:54,364-Speed 1814.31 samples/sec  Loss 3.4884  LearningRate 0.0001  ProxyLR: 0.0031  Epoch: 0  Global Step: 3170   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:00,006-Speed 1815.40 samples/sec  Loss 3.5511  LearningRate 0.0001  ProxyLR: 0.0031  Epoch: 0  Global Step: 3180   Fp16 Grad Scale: 524288  Required: 0 hours
Training: 2023-05-05 18:55:05,644-Speed 1816.46 samples/sec  Loss 3.5912  LearningRate 0.0001  ProxyLR: 0.0031  Epoch: 0  Global Step: 3190   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:11,278-Speed 1817.70 samples/sec  Loss 3.5607  LearningRate 0.0001  ProxyLR: 0.0031  Epoch: 0  Global Step: 3200   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:16,911-Speed 1818.02 samples/sec  Loss 3.5708  LearningRate 0.0001  ProxyLR: 0.0030  Epoch: 0  Global Step: 3210   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:22,557-Speed 1814.27 samples/sec  Loss 3.5078  LearningRate 0.0001  ProxyLR: 0.0030  Epoch: 0  Global Step: 3220   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:28,207-Speed 1812.75 samples/sec  Loss 3.5161  LearningRate 0.0001  ProxyLR: 0.0030  Epoch: 0  Global Step: 3230   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:33,847-Speed 1815.70 samples/sec  Loss 3.5234  LearningRate 0.0001  ProxyLR: 0.0030  Epoch: 0  Global Step: 3240   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:39,482-Speed 1817.58 samples/sec  Loss 3.5676  LearningRate 0.0001  ProxyLR: 0.0029  Epoch: 0  Global Step: 3250   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:45,116-Speed 1818.14 samples/sec  Loss 3.5430  LearningRate 0.0001  ProxyLR: 0.0029  Epoch: 0  Global Step: 3260   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:50,752-Speed 1817.34 samples/sec  Loss 3.5033  LearningRate 0.0001  ProxyLR: 0.0029  Epoch: 0  Global Step: 3270   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:55:56,393-Speed 1815.55 samples/sec  Loss 3.5494  LearningRate 0.0001  ProxyLR: 0.0029  Epoch: 0  Global Step: 3280   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:02,020-Speed 1820.31 samples/sec  Loss 3.6102  LearningRate 0.0001  ProxyLR: 0.0028  Epoch: 0  Global Step: 3290   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:07,660-Speed 1815.60 samples/sec  Loss 3.6212  LearningRate 0.0001  ProxyLR: 0.0028  Epoch: 0  Global Step: 3300   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:13,301-Speed 1815.80 samples/sec  Loss 3.5251  LearningRate 0.0001  ProxyLR: 0.0028  Epoch: 0  Global Step: 3310   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:18,936-Speed 1817.33 samples/sec  Loss 3.5865  LearningRate 0.0001  ProxyLR: 0.0028  Epoch: 0  Global Step: 3320   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:24,586-Speed 1816.18 samples/sec  Loss 3.5474  LearningRate 0.0001  ProxyLR: 0.0027  Epoch: 0  Global Step: 3330   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:30,221-Speed 1817.22 samples/sec  Loss 3.6271  LearningRate 0.0001  ProxyLR: 0.0027  Epoch: 0  Global Step: 3340   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:35,847-Speed 1820.44 samples/sec  Loss 3.5741  LearningRate 0.0001  ProxyLR: 0.0027  Epoch: 0  Global Step: 3350   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:41,483-Speed 1817.23 samples/sec  Loss 3.5100  LearningRate 0.0001  ProxyLR: 0.0027  Epoch: 0  Global Step: 3360   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:56:47,096-Speed 1824.66 samples/sec  Loss 3.6389  LearningRate 0.0000  ProxyLR: 0.0027  Epoch: 0  Global Step: 3370   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:56:52,757-Speed 1809.29 samples/sec  Loss 3.5769  LearningRate 0.0000  ProxyLR: 0.0026  Epoch: 0  Global Step: 3380   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:56:58,404-Speed 1813.56 samples/sec  Loss 3.6062  LearningRate 0.0000  ProxyLR: 0.0026  Epoch: 0  Global Step: 3390   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:04,038-Speed 1817.87 samples/sec  Loss 3.5473  LearningRate 0.0000  ProxyLR: 0.0026  Epoch: 0  Global Step: 3400   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:09,681-Speed 1815.04 samples/sec  Loss 3.5564  LearningRate 0.0000  ProxyLR: 0.0026  Epoch: 0  Global Step: 3410   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:15,321-Speed 1816.04 samples/sec  Loss 3.5499  LearningRate 0.0000  ProxyLR: 0.0025  Epoch: 0  Global Step: 3420   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:20,965-Speed 1814.56 samples/sec  Loss 3.4938  LearningRate 0.0000  ProxyLR: 0.0025  Epoch: 0  Global Step: 3430   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:26,607-Speed 1815.26 samples/sec  Loss 3.5750  LearningRate 0.0000  ProxyLR: 0.0025  Epoch: 0  Global Step: 3440   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:32,242-Speed 1817.51 samples/sec  Loss 3.5850  LearningRate 0.0000  ProxyLR: 0.0025  Epoch: 0  Global Step: 3450   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:37,886-Speed 1814.60 samples/sec  Loss 3.5014  LearningRate 0.0000  ProxyLR: 0.0025  Epoch: 0  Global Step: 3460   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 18:57:43,529-Speed 1814.90 samples/sec  Loss 3.5681  LearningRate 0.0000  ProxyLR: 0.0024  Epoch: 0  Global Step: 3470   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:57:49,161-Speed 1818.41 samples/sec  Loss 3.5611  LearningRate 0.0000  ProxyLR: 0.0024  Epoch: 0  Global Step: 3480   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:57:54,795-Speed 1817.88 samples/sec  Loss 3.5622  LearningRate 0.0000  ProxyLR: 0.0024  Epoch: 0  Global Step: 3490   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:58:00,421-Speed 1820.55 samples/sec  Loss 3.6469  LearningRate 0.0000  ProxyLR: 0.0024  Epoch: 0  Global Step: 3500   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:58:06,064-Speed 1814.91 samples/sec  Loss 3.6519  LearningRate 0.0000  ProxyLR: 0.0023  Epoch: 0  Global Step: 3510   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:58:11,705-Speed 1815.70 samples/sec  Loss 3.5482  LearningRate 0.0000  ProxyLR: 0.0023  Epoch: 0  Global Step: 3520   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:58:17,351-Speed 1814.02 samples/sec  Loss 3.6184  LearningRate 0.0000  ProxyLR: 0.0023  Epoch: 0  Global Step: 3530   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 18:59:13,458-[lfw][3530]XNorm: 22.034983
Training: 2023-05-05 18:59:13,458-[lfw][3530]Accuracy-Flip: 0.99750+-0.00239
Training: 2023-05-05 18:59:13,459-[lfw][3530]Accuracy-Highest: 0.99750
Training: 2023-05-05 18:59:13,459-[lfw][3530]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 18:59:13,459-[lfw][3530]Highest TPR@FPR: 0.99600
Training: 2023-05-05 19:00:18,272-[cfp_fp][3530]XNorm: 21.824411
Training: 2023-05-05 19:00:18,273-[cfp_fp][3530]Accuracy-Flip: 0.98457+-0.00490
Training: 2023-05-05 19:00:18,273-[cfp_fp][3530]Accuracy-Highest: 0.98571
Training: 2023-05-05 19:00:18,273-[cfp_fp][3530]TPR@1stNon-Zero-FPR of 0.00029: 0.88314
Training: 2023-05-05 19:00:18,273-[cfp_fp][3530]Highest TPR@FPR: 0.88743
Training: 2023-05-05 19:01:14,420-[agedb_30][3530]XNorm: 22.242156
Training: 2023-05-05 19:01:14,421-[agedb_30][3530]Accuracy-Flip: 0.97533+-0.00816
Training: 2023-05-05 19:01:14,421-[agedb_30][3530]Accuracy-Highest: 0.97550
Training: 2023-05-05 19:01:14,421-[agedb_30][3530]TPR@1stNon-Zero-FPR of 0.00033: 0.86300
Training: 2023-05-05 19:01:14,421-[agedb_30][3530]Highest TPR@FPR: 0.86333
Training: 2023-05-05 19:02:12,089-[calfw][3530]XNorm: 22.185794
Training: 2023-05-05 19:02:12,089-[calfw][3530]Accuracy-Flip: 0.95733+-0.01232
Training: 2023-05-05 19:02:12,090-[calfw][3530]Accuracy-Highest: 0.95800
Training: 2023-05-05 19:02:12,090-[calfw][3530]TPR@1stNon-Zero-FPR of 0.00033: 0.84333
Training: 2023-05-05 19:02:12,090-[calfw][3530]Highest TPR@FPR: 0.84533
Training: 2023-05-05 19:03:09,728-[cplfw][3530]XNorm: 21.219949
Training: 2023-05-05 19:03:09,729-[cplfw][3530]Accuracy-Flip: 0.92817+-0.01071
Training: 2023-05-05 19:03:09,729-[cplfw][3530]Accuracy-Highest: 0.92817
Training: 2023-05-05 19:03:09,729-[cplfw][3530]TPR@1stNon-Zero-FPR of 0.00033: 0.12333
Training: 2023-05-05 19:03:09,730-[cplfw][3530]Highest TPR@FPR: 0.12667
Training: 2023-05-05 19:03:16,473-Speed 34.23 samples/sec  Loss 3.6160  LearningRate 0.0000  ProxyLR: 0.0023  Epoch: 0  Global Step: 3540   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:22,092-Speed 1822.97 samples/sec  Loss 3.4978  LearningRate 0.0000  ProxyLR: 0.0023  Epoch: 0  Global Step: 3550   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:27,708-Speed 1823.39 samples/sec  Loss 3.5686  LearningRate 0.0000  ProxyLR: 0.0022  Epoch: 0  Global Step: 3560   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:33,333-Speed 1821.09 samples/sec  Loss 3.6186  LearningRate 0.0000  ProxyLR: 0.0022  Epoch: 0  Global Step: 3570   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:38,958-Speed 1820.68 samples/sec  Loss 3.6279  LearningRate 0.0000  ProxyLR: 0.0022  Epoch: 0  Global Step: 3580   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:44,585-Speed 1819.90 samples/sec  Loss 3.5241  LearningRate 0.0000  ProxyLR: 0.0022  Epoch: 0  Global Step: 3590   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:50,218-Speed 1818.00 samples/sec  Loss 3.4775  LearningRate 0.0000  ProxyLR: 0.0022  Epoch: 0  Global Step: 3600   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:03:55,840-Speed 1822.08 samples/sec  Loss 3.6522  LearningRate 0.0000  ProxyLR: 0.0021  Epoch: 0  Global Step: 3610   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:04:01,466-Speed 1820.14 samples/sec  Loss 3.6108  LearningRate 0.0000  ProxyLR: 0.0021  Epoch: 0  Global Step: 3620   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:04:07,096-Speed 1819.52 samples/sec  Loss 3.5747  LearningRate 0.0000  ProxyLR: 0.0021  Epoch: 0  Global Step: 3630   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:04:12,698-Speed 1828.08 samples/sec  Loss 3.5319  LearningRate 0.0000  ProxyLR: 0.0021  Epoch: 0  Global Step: 3640   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:18,341-Speed 1814.89 samples/sec  Loss 3.5841  LearningRate 0.0000  ProxyLR: 0.0021  Epoch: 0  Global Step: 3650   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:23,976-Speed 1817.71 samples/sec  Loss 3.5820  LearningRate 0.0000  ProxyLR: 0.0020  Epoch: 0  Global Step: 3660   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:29,601-Speed 1820.44 samples/sec  Loss 3.5469  LearningRate 0.0000  ProxyLR: 0.0020  Epoch: 0  Global Step: 3670   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:35,243-Speed 1815.38 samples/sec  Loss 3.5884  LearningRate 0.0000  ProxyLR: 0.0020  Epoch: 0  Global Step: 3680   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:40,867-Speed 1821.12 samples/sec  Loss 3.5778  LearningRate 0.0000  ProxyLR: 0.0020  Epoch: 0  Global Step: 3690   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:46,496-Speed 1819.39 samples/sec  Loss 3.6064  LearningRate 0.0000  ProxyLR: 0.0020  Epoch: 0  Global Step: 3700   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:52,138-Speed 1815.50 samples/sec  Loss 3.5786  LearningRate 0.0000  ProxyLR: 0.0019  Epoch: 0  Global Step: 3710   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:04:57,770-Speed 1818.32 samples/sec  Loss 3.5539  LearningRate 0.0000  ProxyLR: 0.0019  Epoch: 0  Global Step: 3720   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:05:03,400-Speed 1819.22 samples/sec  Loss 3.4583  LearningRate 0.0000  ProxyLR: 0.0019  Epoch: 0  Global Step: 3730   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:05:09,039-Speed 1816.04 samples/sec  Loss 3.6077  LearningRate 0.0000  ProxyLR: 0.0019  Epoch: 0  Global Step: 3740   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:14,667-Speed 1819.84 samples/sec  Loss 3.4944  LearningRate 0.0000  ProxyLR: 0.0019  Epoch: 0  Global Step: 3750   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:20,298-Speed 1818.70 samples/sec  Loss 3.6154  LearningRate 0.0000  ProxyLR: 0.0018  Epoch: 0  Global Step: 3760   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:25,914-Speed 1823.88 samples/sec  Loss 3.6458  LearningRate 0.0000  ProxyLR: 0.0018  Epoch: 0  Global Step: 3770   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:31,545-Speed 1818.61 samples/sec  Loss 3.5900  LearningRate 0.0000  ProxyLR: 0.0018  Epoch: 0  Global Step: 3780   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:37,170-Speed 1821.50 samples/sec  Loss 3.5185  LearningRate 0.0000  ProxyLR: 0.0018  Epoch: 0  Global Step: 3790   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:42,810-Speed 1815.70 samples/sec  Loss 3.5935  LearningRate 0.0000  ProxyLR: 0.0018  Epoch: 0  Global Step: 3800   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:48,439-Speed 1819.43 samples/sec  Loss 3.5202  LearningRate 0.0000  ProxyLR: 0.0017  Epoch: 0  Global Step: 3810   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:54,079-Speed 1815.87 samples/sec  Loss 3.5570  LearningRate 0.0000  ProxyLR: 0.0017  Epoch: 0  Global Step: 3820   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:05:59,694-Speed 1823.87 samples/sec  Loss 3.5056  LearningRate 0.0000  ProxyLR: 0.0017  Epoch: 0  Global Step: 3830   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:05,316-Speed 1821.63 samples/sec  Loss 3.5957  LearningRate 0.0000  ProxyLR: 0.0017  Epoch: 0  Global Step: 3840   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:10,950-Speed 1817.91 samples/sec  Loss 3.5877  LearningRate 0.0000  ProxyLR: 0.0017  Epoch: 0  Global Step: 3850   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:16,584-Speed 1817.87 samples/sec  Loss 3.5694  LearningRate 0.0000  ProxyLR: 0.0017  Epoch: 0  Global Step: 3860   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:22,224-Speed 1815.80 samples/sec  Loss 3.5878  LearningRate 0.0000  ProxyLR: 0.0016  Epoch: 0  Global Step: 3870   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:27,856-Speed 1818.83 samples/sec  Loss 3.6361  LearningRate 0.0000  ProxyLR: 0.0016  Epoch: 0  Global Step: 3880   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:33,491-Speed 1817.28 samples/sec  Loss 3.5378  LearningRate 0.0000  ProxyLR: 0.0016  Epoch: 0  Global Step: 3890   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:39,122-Speed 1818.75 samples/sec  Loss 3.5366  LearningRate 0.0000  ProxyLR: 0.0016  Epoch: 0  Global Step: 3900   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:44,766-Speed 1814.59 samples/sec  Loss 3.6355  LearningRate 0.0000  ProxyLR: 0.0016  Epoch: 0  Global Step: 3910   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:50,407-Speed 1815.70 samples/sec  Loss 3.5700  LearningRate 0.0000  ProxyLR: 0.0015  Epoch: 0  Global Step: 3920   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:06:56,043-Speed 1817.23 samples/sec  Loss 3.5149  LearningRate 0.0000  ProxyLR: 0.0015  Epoch: 0  Global Step: 3930   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:01,678-Speed 1817.66 samples/sec  Loss 3.5621  LearningRate 0.0000  ProxyLR: 0.0015  Epoch: 0  Global Step: 3940   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:07,311-Speed 1817.93 samples/sec  Loss 3.6401  LearningRate 0.0000  ProxyLR: 0.0015  Epoch: 0  Global Step: 3950   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:12,950-Speed 1816.39 samples/sec  Loss 3.6579  LearningRate 0.0000  ProxyLR: 0.0015  Epoch: 0  Global Step: 3960   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:18,580-Speed 1819.10 samples/sec  Loss 3.5322  LearningRate 0.0000  ProxyLR: 0.0015  Epoch: 0  Global Step: 3970   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:24,221-Speed 1815.60 samples/sec  Loss 3.5792  LearningRate 0.0000  ProxyLR: 0.0014  Epoch: 0  Global Step: 3980   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:29,855-Speed 1818.11 samples/sec  Loss 3.5962  LearningRate 0.0000  ProxyLR: 0.0014  Epoch: 0  Global Step: 3990   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:35,503-Speed 1813.23 samples/sec  Loss 3.5879  LearningRate 0.0000  ProxyLR: 0.0014  Epoch: 0  Global Step: 4000   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:41,144-Speed 1815.30 samples/sec  Loss 3.5860  LearningRate 0.0000  ProxyLR: 0.0014  Epoch: 0  Global Step: 4010   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:46,781-Speed 1817.00 samples/sec  Loss 3.5517  LearningRate 0.0000  ProxyLR: 0.0014  Epoch: 0  Global Step: 4020   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:52,419-Speed 1816.65 samples/sec  Loss 3.5511  LearningRate 0.0000  ProxyLR: 0.0014  Epoch: 0  Global Step: 4030   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:07:58,046-Speed 1819.94 samples/sec  Loss 3.5059  LearningRate 0.0000  ProxyLR: 0.0013  Epoch: 0  Global Step: 4040   Fp16 Grad Scale: 524288  Required: 0 hours
Training: 2023-05-05 19:08:03,670-Speed 1821.17 samples/sec  Loss 3.6085  LearningRate 0.0000  ProxyLR: 0.0013  Epoch: 0  Global Step: 4050   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:09,324-Speed 1811.53 samples/sec  Loss 3.5112  LearningRate 0.0000  ProxyLR: 0.0013  Epoch: 0  Global Step: 4060   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:14,957-Speed 1818.06 samples/sec  Loss 3.5620  LearningRate 0.0000  ProxyLR: 0.0013  Epoch: 0  Global Step: 4070   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:20,606-Speed 1812.91 samples/sec  Loss 3.5144  LearningRate 0.0000  ProxyLR: 0.0013  Epoch: 0  Global Step: 4080   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:26,244-Speed 1816.56 samples/sec  Loss 3.6115  LearningRate 0.0000  ProxyLR: 0.0013  Epoch: 0  Global Step: 4090   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:31,872-Speed 1819.79 samples/sec  Loss 3.5253  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4100   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:37,518-Speed 1814.15 samples/sec  Loss 3.6158  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4110   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:43,162-Speed 1814.35 samples/sec  Loss 3.5727  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4120   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:48,800-Speed 1816.52 samples/sec  Loss 3.5545  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4130   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:08:54,441-Speed 1815.86 samples/sec  Loss 3.6240  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4140   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:00,068-Speed 1820.09 samples/sec  Loss 3.5026  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4150   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:05,697-Speed 1819.39 samples/sec  Loss 3.5730  LearningRate 0.0000  ProxyLR: 0.0012  Epoch: 0  Global Step: 4160   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:11,343-Speed 1813.90 samples/sec  Loss 3.5238  LearningRate 0.0000  ProxyLR: 0.0011  Epoch: 0  Global Step: 4170   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:16,980-Speed 1816.97 samples/sec  Loss 3.5231  LearningRate 0.0000  ProxyLR: 0.0011  Epoch: 0  Global Step: 4180   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:22,622-Speed 1815.40 samples/sec  Loss 3.6129  LearningRate 0.0000  ProxyLR: 0.0011  Epoch: 0  Global Step: 4190   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:28,256-Speed 1817.86 samples/sec  Loss 3.5542  LearningRate 0.0000  ProxyLR: 0.0011  Epoch: 0  Global Step: 4200   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:33,890-Speed 1817.95 samples/sec  Loss 3.6171  LearningRate 0.0000  ProxyLR: 0.0011  Epoch: 0  Global Step: 4210   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:39,541-Speed 1812.06 samples/sec  Loss 3.4936  LearningRate 0.0000  ProxyLR: 0.0011  Epoch: 0  Global Step: 4220   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:45,166-Speed 1820.70 samples/sec  Loss 3.5844  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4230   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:50,797-Speed 1818.82 samples/sec  Loss 3.5844  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4240   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:09:56,422-Speed 1820.90 samples/sec  Loss 3.5673  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4250   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:02,068-Speed 1813.81 samples/sec  Loss 3.5817  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4260   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:07,713-Speed 1814.35 samples/sec  Loss 3.4830  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4270   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:13,356-Speed 1815.08 samples/sec  Loss 3.5879  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4280   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:18,986-Speed 1818.96 samples/sec  Loss 3.5750  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4290   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:24,625-Speed 1816.43 samples/sec  Loss 3.5909  LearningRate 0.0000  ProxyLR: 0.0010  Epoch: 0  Global Step: 4300   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:30,253-Speed 1819.64 samples/sec  Loss 3.5391  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4310   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:35,884-Speed 1818.84 samples/sec  Loss 3.5855  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4320   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:10:41,504-Speed 1822.20 samples/sec  Loss 3.5077  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4330   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:10:47,166-Speed 1809.11 samples/sec  Loss 3.5464  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4340   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:10:52,796-Speed 1818.87 samples/sec  Loss 3.5995  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4350   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:10:58,431-Speed 1817.84 samples/sec  Loss 3.5071  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4360   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:04,080-Speed 1812.81 samples/sec  Loss 3.5513  LearningRate 0.0000  ProxyLR: 0.0009  Epoch: 0  Global Step: 4370   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:09,726-Speed 1813.78 samples/sec  Loss 3.5587  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4380   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:15,374-Speed 1813.48 samples/sec  Loss 3.6099  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4390   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:21,015-Speed 1815.55 samples/sec  Loss 3.5354  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4400   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:26,658-Speed 1815.06 samples/sec  Loss 3.5364  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4410   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:32,299-Speed 1815.58 samples/sec  Loss 3.5467  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4420   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:11:37,937-Speed 1816.64 samples/sec  Loss 3.5855  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4430   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:11:43,574-Speed 1816.80 samples/sec  Loss 3.6087  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4440   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:11:49,206-Speed 1818.38 samples/sec  Loss 3.5699  LearningRate 0.0000  ProxyLR: 0.0008  Epoch: 0  Global Step: 4450   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:11:54,832-Speed 1820.25 samples/sec  Loss 3.5066  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4460   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:00,467-Speed 1817.77 samples/sec  Loss 3.4644  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4470   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:06,109-Speed 1815.10 samples/sec  Loss 3.5415  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4480   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:11,755-Speed 1814.15 samples/sec  Loss 3.5509  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4490   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:17,396-Speed 1815.49 samples/sec  Loss 3.5452  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4500   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:23,039-Speed 1814.96 samples/sec  Loss 3.5526  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4510   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:28,678-Speed 1816.40 samples/sec  Loss 3.5739  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4520   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:34,307-Speed 1819.38 samples/sec  Loss 3.5171  LearningRate 0.0000  ProxyLR: 0.0007  Epoch: 0  Global Step: 4530   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:39,949-Speed 1815.32 samples/sec  Loss 3.4951  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4540   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:45,587-Speed 1816.34 samples/sec  Loss 3.5284  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4550   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:51,234-Speed 1813.74 samples/sec  Loss 3.5457  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4560   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:12:56,862-Speed 1819.52 samples/sec  Loss 3.5258  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4570   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:02,503-Speed 1816.45 samples/sec  Loss 3.5614  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4580   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:08,145-Speed 1815.12 samples/sec  Loss 3.5193  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4590   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:13,779-Speed 1817.74 samples/sec  Loss 3.5186  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4600   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:19,413-Speed 1818.02 samples/sec  Loss 3.6199  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4610   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:25,061-Speed 1813.09 samples/sec  Loss 3.5438  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4620   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:30,679-Speed 1823.38 samples/sec  Loss 3.5064  LearningRate 0.0000  ProxyLR: 0.0006  Epoch: 0  Global Step: 4630   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:36,328-Speed 1812.79 samples/sec  Loss 3.5145  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4640   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:41,961-Speed 1818.24 samples/sec  Loss 3.5766  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4650   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:47,608-Speed 1813.74 samples/sec  Loss 3.5892  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4660   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:53,252-Speed 1814.54 samples/sec  Loss 3.5557  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4670   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:13:58,893-Speed 1815.59 samples/sec  Loss 3.5291  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4680   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:04,532-Speed 1816.16 samples/sec  Loss 3.5733  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4690   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:10,174-Speed 1815.19 samples/sec  Loss 3.5354  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4700   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:15,818-Speed 1814.63 samples/sec  Loss 3.5922  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4710   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:21,459-Speed 1815.74 samples/sec  Loss 3.5568  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4720   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:27,094-Speed 1817.61 samples/sec  Loss 3.6196  LearningRate 0.0000  ProxyLR: 0.0005  Epoch: 0  Global Step: 4730   Fp16 Grad Scale: 524288  Required: 0 hours
Training: 2023-05-05 19:14:32,723-Speed 1819.39 samples/sec  Loss 3.5522  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4740   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:38,362-Speed 1816.17 samples/sec  Loss 3.5316  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4750   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:14:43,989-Speed 1820.18 samples/sec  Loss 3.5408  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4760   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:14:49,635-Speed 1813.73 samples/sec  Loss 3.4908  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4770   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:14:55,272-Speed 1817.10 samples/sec  Loss 3.6017  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4780   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:00,906-Speed 1817.93 samples/sec  Loss 3.5694  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4790   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:06,540-Speed 1817.56 samples/sec  Loss 3.5502  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4800   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:12,178-Speed 1816.73 samples/sec  Loss 3.4538  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4810   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:17,805-Speed 1819.99 samples/sec  Loss 3.6433  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4820   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:23,455-Speed 1812.95 samples/sec  Loss 3.5644  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4830   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:29,102-Speed 1813.46 samples/sec  Loss 3.5167  LearningRate 0.0000  ProxyLR: 0.0004  Epoch: 0  Global Step: 4840   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:34,742-Speed 1815.86 samples/sec  Loss 3.5568  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4850   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:15:40,389-Speed 1813.58 samples/sec  Loss 3.5795  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4860   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:15:46,023-Speed 1817.98 samples/sec  Loss 3.5741  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4870   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:15:51,665-Speed 1815.03 samples/sec  Loss 3.5169  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4880   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:15:57,307-Speed 1815.50 samples/sec  Loss 3.5486  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4890   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:02,959-Speed 1811.99 samples/sec  Loss 3.5947  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4900   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:08,601-Speed 1815.27 samples/sec  Loss 3.6539  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4910   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:14,234-Speed 1818.26 samples/sec  Loss 3.5884  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4920   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:19,867-Speed 1818.16 samples/sec  Loss 3.5535  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4930   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:25,503-Speed 1817.13 samples/sec  Loss 3.5213  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4940   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:31,144-Speed 1815.69 samples/sec  Loss 3.5874  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4950   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:36,756-Speed 1824.92 samples/sec  Loss 3.5866  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4960   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:42,403-Speed 1813.68 samples/sec  Loss 3.6300  LearningRate 0.0000  ProxyLR: 0.0003  Epoch: 0  Global Step: 4970   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:48,040-Speed 1816.97 samples/sec  Loss 3.6469  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 4980   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:53,676-Speed 1816.94 samples/sec  Loss 3.5788  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 4990   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:16:59,329-Speed 1812.06 samples/sec  Loss 3.5336  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5000   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:04,972-Speed 1814.81 samples/sec  Loss 3.5635  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5010   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:10,612-Speed 1815.80 samples/sec  Loss 3.5473  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5020   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:16,245-Speed 1818.15 samples/sec  Loss 3.5374  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5030   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:21,887-Speed 1815.30 samples/sec  Loss 3.5523  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5040   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:27,530-Speed 1814.91 samples/sec  Loss 3.5839  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5050   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:33,159-Speed 1819.45 samples/sec  Loss 3.5398  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5060   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:38,799-Speed 1815.90 samples/sec  Loss 3.5663  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5070   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:44,442-Speed 1815.07 samples/sec  Loss 3.5036  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5080   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:50,090-Speed 1813.23 samples/sec  Loss 3.5460  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5090   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:17:55,721-Speed 1818.55 samples/sec  Loss 3.5263  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5100   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:01,376-Speed 1811.42 samples/sec  Loss 3.5663  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5110   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:06,994-Speed 1823.05 samples/sec  Loss 3.6234  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5120   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:12,629-Speed 1817.53 samples/sec  Loss 3.5249  LearningRate 0.0000  ProxyLR: 0.0002  Epoch: 0  Global Step: 5130   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:18,257-Speed 1819.67 samples/sec  Loss 3.5997  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5140   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:23,898-Speed 1815.53 samples/sec  Loss 3.5557  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5150   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:29,526-Speed 1820.38 samples/sec  Loss 3.6128  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5160   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:35,171-Speed 1814.16 samples/sec  Loss 3.5901  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5170   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:40,796-Speed 1820.93 samples/sec  Loss 3.5514  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5180   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:46,432-Speed 1817.22 samples/sec  Loss 3.5931  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5190   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:52,092-Speed 1809.20 samples/sec  Loss 3.5125  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5200   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:18:57,719-Speed 1820.87 samples/sec  Loss 3.6304  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5210   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:03,360-Speed 1815.46 samples/sec  Loss 3.5844  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5220   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:09,020-Speed 1809.59 samples/sec  Loss 3.5398  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5230   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:14,658-Speed 1816.41 samples/sec  Loss 3.6094  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5240   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:20,295-Speed 1816.82 samples/sec  Loss 3.5060  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5250   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:25,924-Speed 1819.37 samples/sec  Loss 3.5426  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5260   Fp16 Grad Scale: 524288  Required: 0 hours
Training: 2023-05-05 19:19:31,558-Speed 1818.16 samples/sec  Loss 3.6287  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5270   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:37,208-Speed 1812.53 samples/sec  Loss 3.5326  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5280   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:42,846-Speed 1816.76 samples/sec  Loss 3.6135  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5290   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:48,487-Speed 1815.56 samples/sec  Loss 3.5576  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5300   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:54,128-Speed 1815.32 samples/sec  Loss 3.5495  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5310   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:19:59,771-Speed 1815.12 samples/sec  Loss 3.6045  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5320   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:05,416-Speed 1813.99 samples/sec  Loss 3.6185  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5330   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:11,057-Speed 1815.81 samples/sec  Loss 3.6779  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5340   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:16,685-Speed 1819.73 samples/sec  Loss 3.5926  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5350   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:22,324-Speed 1816.37 samples/sec  Loss 3.5654  LearningRate 0.0000  ProxyLR: 0.0001  Epoch: 0  Global Step: 5360   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:27,947-Speed 1821.29 samples/sec  Loss 3.5541  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5370   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:33,572-Speed 1820.92 samples/sec  Loss 3.5903  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5380   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:20:39,191-Speed 1822.64 samples/sec  Loss 3.5671  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5390   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:20:44,825-Speed 1817.75 samples/sec  Loss 3.5624  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5400   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:20:50,466-Speed 1815.57 samples/sec  Loss 3.5403  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5410   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:20:56,101-Speed 1817.65 samples/sec  Loss 3.5547  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5420   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:01,731-Speed 1819.12 samples/sec  Loss 3.5514  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5430   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:07,369-Speed 1816.30 samples/sec  Loss 3.6064  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5440   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:13,009-Speed 1816.16 samples/sec  Loss 3.5288  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5450   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:18,646-Speed 1816.59 samples/sec  Loss 3.5761  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5460   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:24,276-Speed 1819.31 samples/sec  Loss 3.5461  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5470   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:29,912-Speed 1817.15 samples/sec  Loss 3.5655  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5480   Fp16 Grad Scale: 131072  Required: 0 hours
Training: 2023-05-05 19:21:35,548-Speed 1817.11 samples/sec  Loss 3.5362  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5490   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:21:41,193-Speed 1814.62 samples/sec  Loss 3.5465  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5500   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:21:46,838-Speed 1814.28 samples/sec  Loss 3.5852  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5510   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:21:52,467-Speed 1819.93 samples/sec  Loss 3.4925  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5520   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:21:58,113-Speed 1814.01 samples/sec  Loss 3.6480  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5530   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:22:54,100-[lfw][5530]XNorm: 22.083500
Training: 2023-05-05 19:22:54,100-[lfw][5530]Accuracy-Flip: 0.99733+-0.00238
Training: 2023-05-05 19:22:54,101-[lfw][5530]Accuracy-Highest: 0.99750
Training: 2023-05-05 19:22:54,101-[lfw][5530]TPR@1stNon-Zero-FPR of 0.00033: 0.99600
Training: 2023-05-05 19:22:54,101-[lfw][5530]Highest TPR@FPR: 0.99600
Training: 2023-05-05 19:23:58,820-[cfp_fp][5530]XNorm: 21.865139
Training: 2023-05-05 19:23:58,820-[cfp_fp][5530]Accuracy-Flip: 0.98500+-0.00504
Training: 2023-05-05 19:23:58,820-[cfp_fp][5530]Accuracy-Highest: 0.98571
Training: 2023-05-05 19:23:58,821-[cfp_fp][5530]TPR@1stNon-Zero-FPR of 0.00029: 0.88571
Training: 2023-05-05 19:23:58,821-[cfp_fp][5530]Highest TPR@FPR: 0.88743
Training: 2023-05-05 19:24:54,883-[agedb_30][5530]XNorm: 22.291445
Training: 2023-05-05 19:24:54,884-[agedb_30][5530]Accuracy-Flip: 0.97517+-0.00841
Training: 2023-05-05 19:24:54,884-[agedb_30][5530]Accuracy-Highest: 0.97550
Training: 2023-05-05 19:24:54,884-[agedb_30][5530]TPR@1stNon-Zero-FPR of 0.00033: 0.86467
Training: 2023-05-05 19:24:54,885-[agedb_30][5530]Highest TPR@FPR: 0.86467
Training: 2023-05-05 19:25:52,473-[calfw][5530]XNorm: 22.230975
Training: 2023-05-05 19:25:52,474-[calfw][5530]Accuracy-Flip: 0.95700+-0.01220
Training: 2023-05-05 19:25:52,474-[calfw][5530]Accuracy-Highest: 0.95800
Training: 2023-05-05 19:25:52,475-[calfw][5530]TPR@1stNon-Zero-FPR of 0.00033: 0.84333
Training: 2023-05-05 19:25:52,475-[calfw][5530]Highest TPR@FPR: 0.84533
Training: 2023-05-05 19:26:49,983-[cplfw][5530]XNorm: 21.258733
Training: 2023-05-05 19:26:49,984-[cplfw][5530]Accuracy-Flip: 0.92800+-0.01046
Training: 2023-05-05 19:26:49,984-[cplfw][5530]Accuracy-Highest: 0.92817
Training: 2023-05-05 19:26:49,984-[cplfw][5530]TPR@1stNon-Zero-FPR of 0.00033: 0.12500
Training: 2023-05-05 19:26:49,984-[cplfw][5530]Highest TPR@FPR: 0.12667
Training: 2023-05-05 19:26:56,296-Speed 34.34 samples/sec  Loss 3.5862  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5540   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:01,916-Speed 1822.33 samples/sec  Loss 3.5504  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5550   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:07,530-Speed 1824.26 samples/sec  Loss 3.5597  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5560   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:13,143-Speed 1824.80 samples/sec  Loss 3.5482  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5570   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:18,766-Speed 1821.18 samples/sec  Loss 3.4942  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5580   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:24,380-Speed 1824.45 samples/sec  Loss 3.5557  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5590   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:29,991-Speed 1825.26 samples/sec  Loss 3.5241  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5600   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:35,629-Speed 1816.53 samples/sec  Loss 3.5886  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5610   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:41,250-Speed 1822.04 samples/sec  Loss 3.5643  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5620   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:46,867-Speed 1823.44 samples/sec  Loss 3.4591  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5630   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:52,494-Speed 1820.01 samples/sec  Loss 3.5491  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5640   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:27:58,130-Speed 1817.22 samples/sec  Loss 3.6295  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5650   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:28:03,763-Speed 1818.09 samples/sec  Loss 3.5526  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5660   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:28:09,401-Speed 1816.66 samples/sec  Loss 3.6239  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5670   Fp16 Grad Scale: 262144  Required: 0 hours
Training: 2023-05-05 19:28:15,070-Speed 1806.56 samples/sec  Loss 3.6351  LearningRate 0.0000  ProxyLR: 0.0000  Epoch: 0  Global Step: 5680   Fp16 Grad Scale: 262144  Required: 0 hours
